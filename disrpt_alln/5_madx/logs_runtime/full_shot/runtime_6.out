-------------------------------------------------------------------
Lang1:  deu.rst.pcc    Lang2:  fra.sdrt.annodis
Saving run to:  runs/full_shot/FullShot=v4_deu.rst.pcc_fra.sdrt.annodis_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2164 examples
read 241 examples
read 260 examples
read 2185 examples
read 528 examples
read 625 examples
Total prediction labels:  40
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (fr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=40, bias=True)
    )
  )
)
{'train@deu.rst.pcc_loss': 3.26244854927063, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.09796672828096119, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.00949278803445414, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.05674290393982118, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.039120539835102726, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.262448310852051, 'train@deu.rst.pcc_runtime': 26.861, 'train@deu.rst.pcc_samples_per_second': 80.563, 'train@deu.rst.pcc_steps_per_second': 2.532, 'epoch': 1.0}
{'loss': 3.4984, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.2820847034454346, 'eval_accuracy@deu.rst.pcc': 0.12033195020746888, 'eval_f1@deu.rst.pcc': 0.013489747043855712, 'eval_precision@deu.rst.pcc': 0.018696581196581196, 'eval_recall@deu.rst.pcc': 0.04315476190476191, 'eval_loss@deu.rst.pcc': 3.2820844650268555, 'eval_runtime': 3.3704, 'eval_samples_per_second': 71.504, 'eval_steps_per_second': 2.374, 'epoch': 1.0}
{'train@deu.rst.pcc_loss': 2.993964433670044, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.10813308687615526, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.01996021209692189, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.04935892539207794, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.04373457037727288, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.993964672088623, 'train@deu.rst.pcc_runtime': 26.439, 'train@deu.rst.pcc_samples_per_second': 81.849, 'train@deu.rst.pcc_steps_per_second': 2.572, 'epoch': 2.0}
{'loss': 3.1151, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.0321621894836426, 'eval_accuracy@deu.rst.pcc': 0.12863070539419086, 'eval_f1@deu.rst.pcc': 0.02607667369180527, 'eval_precision@deu.rst.pcc': 0.04777080162354135, 'eval_recall@deu.rst.pcc': 0.05032814407814409, 'eval_loss@deu.rst.pcc': 3.0321621894836426, 'eval_runtime': 3.6675, 'eval_samples_per_second': 65.712, 'eval_steps_per_second': 2.181, 'epoch': 2.0}
{'train@deu.rst.pcc_loss': 2.920736789703369, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.128003696857671, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.0294161091873208, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.031094019285250817, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.05316157181326798, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.9207370281219482, 'train@deu.rst.pcc_runtime': 26.4277, 'train@deu.rst.pcc_samples_per_second': 81.884, 'train@deu.rst.pcc_steps_per_second': 2.573, 'epoch': 3.0}
{'loss': 2.9848, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.973594903945923, 'eval_accuracy@deu.rst.pcc': 0.14937759336099585, 'eval_f1@deu.rst.pcc': 0.0324254187115031, 'eval_precision@deu.rst.pcc': 0.03636162751982993, 'eval_recall@deu.rst.pcc': 0.0653998778998779, 'eval_loss@deu.rst.pcc': 2.973595380783081, 'eval_runtime': 3.3661, 'eval_samples_per_second': 71.596, 'eval_steps_per_second': 2.377, 'epoch': 3.0}
{'train@deu.rst.pcc_loss': 2.874619483947754, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.16173752310536044, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.05933916732470552, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.06420410354690934, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.07979320193331468, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.874619483947754, 'train@deu.rst.pcc_runtime': 26.5005, 'train@deu.rst.pcc_samples_per_second': 81.659, 'train@deu.rst.pcc_steps_per_second': 2.566, 'epoch': 4.0}
{'loss': 2.9341, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.9390065670013428, 'eval_accuracy@deu.rst.pcc': 0.14937759336099585, 'eval_f1@deu.rst.pcc': 0.04218711458294792, 'eval_precision@deu.rst.pcc': 0.04610606784519828, 'eval_recall@deu.rst.pcc': 0.06857956857956858, 'eval_loss@deu.rst.pcc': 2.939006805419922, 'eval_runtime': 3.3877, 'eval_samples_per_second': 71.14, 'eval_steps_per_second': 2.362, 'epoch': 4.0}
{'train@deu.rst.pcc_loss': 2.832502841949463, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.18715341959334567, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.06966396130642107, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.08489088757548485, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.10473355769549328, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.832503080368042, 'train@deu.rst.pcc_runtime': 26.489, 'train@deu.rst.pcc_samples_per_second': 81.694, 'train@deu.rst.pcc_steps_per_second': 2.567, 'epoch': 5.0}
{'loss': 2.8801, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.904404401779175, 'eval_accuracy@deu.rst.pcc': 0.16597510373443983, 'eval_f1@deu.rst.pcc': 0.05237583764801507, 'eval_precision@deu.rst.pcc': 0.04830639132109721, 'eval_recall@deu.rst.pcc': 0.09230642043142044, 'eval_loss@deu.rst.pcc': 2.904404401779175, 'eval_runtime': 3.3911, 'eval_samples_per_second': 71.069, 'eval_steps_per_second': 2.359, 'epoch': 5.0}
{'train@deu.rst.pcc_loss': 2.7968976497650146, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.1987060998151571, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.07150176975133914, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.09055009200045419, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.1154790789430445, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7968976497650146, 'train@deu.rst.pcc_runtime': 26.446, 'train@deu.rst.pcc_samples_per_second': 81.827, 'train@deu.rst.pcc_steps_per_second': 2.571, 'epoch': 6.0}
{'loss': 2.8506, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.8750545978546143, 'eval_accuracy@deu.rst.pcc': 0.1825726141078838, 'eval_f1@deu.rst.pcc': 0.0613896564300325, 'eval_precision@deu.rst.pcc': 0.08620265791051529, 'eval_recall@deu.rst.pcc': 0.11574710012210014, 'eval_loss@deu.rst.pcc': 2.8750550746917725, 'eval_runtime': 3.3822, 'eval_samples_per_second': 71.255, 'eval_steps_per_second': 2.365, 'epoch': 6.0}
{'train@deu.rst.pcc_loss': 2.7690036296844482, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.20194085027726433, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.0742751858668152, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.0892617400427396, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.11861734742422492, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7690036296844482, 'train@deu.rst.pcc_runtime': 26.4798, 'train@deu.rst.pcc_samples_per_second': 81.723, 'train@deu.rst.pcc_steps_per_second': 2.568, 'epoch': 7.0}
{'loss': 2.8154, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.8531460762023926, 'eval_accuracy@deu.rst.pcc': 0.18672199170124482, 'eval_f1@deu.rst.pcc': 0.061513416872272374, 'eval_precision@deu.rst.pcc': 0.08159313958107049, 'eval_recall@deu.rst.pcc': 0.12467567155067155, 'eval_loss@deu.rst.pcc': 2.8531463146209717, 'eval_runtime': 3.3657, 'eval_samples_per_second': 71.604, 'eval_steps_per_second': 2.377, 'epoch': 7.0}
{'train@deu.rst.pcc_loss': 2.746990919113159, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.20378927911275416, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.0750234889923507, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.09404693045028062, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.11996793850863553, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7469911575317383, 'train@deu.rst.pcc_runtime': 26.523, 'train@deu.rst.pcc_samples_per_second': 81.59, 'train@deu.rst.pcc_steps_per_second': 2.564, 'epoch': 8.0}
{'loss': 2.793, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.836768627166748, 'eval_accuracy@deu.rst.pcc': 0.1825726141078838, 'eval_f1@deu.rst.pcc': 0.05803860102922253, 'eval_precision@deu.rst.pcc': 0.07733458591217211, 'eval_recall@deu.rst.pcc': 0.12236085673585673, 'eval_loss@deu.rst.pcc': 2.836768388748169, 'eval_runtime': 3.3648, 'eval_samples_per_second': 71.623, 'eval_steps_per_second': 2.378, 'epoch': 8.0}
{'train@deu.rst.pcc_loss': 2.729497194290161, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2097966728280961, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.07960849484336546, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.09385789131468014, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.12377212172306976, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7294974327087402, 'train@deu.rst.pcc_runtime': 26.4533, 'train@deu.rst.pcc_samples_per_second': 81.804, 'train@deu.rst.pcc_steps_per_second': 2.571, 'epoch': 9.0}
{'loss': 2.7746, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.825512409210205, 'eval_accuracy@deu.rst.pcc': 0.1825726141078838, 'eval_f1@deu.rst.pcc': 0.05798300810476373, 'eval_precision@deu.rst.pcc': 0.07731493043993044, 'eval_recall@deu.rst.pcc': 0.12236085673585673, 'eval_loss@deu.rst.pcc': 2.825512647628784, 'eval_runtime': 3.4452, 'eval_samples_per_second': 69.952, 'eval_steps_per_second': 2.322, 'epoch': 9.0}
{'train@deu.rst.pcc_loss': 2.717681884765625, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.21256931608133087, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08293764530260998, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.0952534555884468, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.12652754554579101, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.717681884765625, 'train@deu.rst.pcc_runtime': 26.6063, 'train@deu.rst.pcc_samples_per_second': 81.334, 'train@deu.rst.pcc_steps_per_second': 2.556, 'epoch': 10.0}
{'loss': 2.7633, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.8140110969543457, 'eval_accuracy@deu.rst.pcc': 0.1908713692946058, 'eval_f1@deu.rst.pcc': 0.06800982476138193, 'eval_precision@deu.rst.pcc': 0.0775138380711136, 'eval_recall@deu.rst.pcc': 0.12877111314611314, 'eval_loss@deu.rst.pcc': 2.8140106201171875, 'eval_runtime': 3.3995, 'eval_samples_per_second': 70.893, 'eval_steps_per_second': 2.353, 'epoch': 10.0}
{'train@deu.rst.pcc_loss': 2.710348606109619, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2144177449168207, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.0854493122654416, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.09983067174352327, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.12814183761910886, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.710348606109619, 'train@deu.rst.pcc_runtime': 26.5043, 'train@deu.rst.pcc_samples_per_second': 81.647, 'train@deu.rst.pcc_steps_per_second': 2.566, 'epoch': 11.0}
{'loss': 2.7426, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.80825138092041, 'eval_accuracy@deu.rst.pcc': 0.1908713692946058, 'eval_f1@deu.rst.pcc': 0.06842379060366865, 'eval_precision@deu.rst.pcc': 0.07790266798418972, 'eval_recall@deu.rst.pcc': 0.12877111314611314, 'eval_loss@deu.rst.pcc': 2.80825138092041, 'eval_runtime': 3.3992, 'eval_samples_per_second': 70.898, 'eval_steps_per_second': 2.353, 'epoch': 11.0}
{'train@deu.rst.pcc_loss': 2.707979440689087, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.21487985212569316, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08610982594042646, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.10205330412913716, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.12879245844295023, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.707979440689087, 'train@deu.rst.pcc_runtime': 26.4853, 'train@deu.rst.pcc_samples_per_second': 81.706, 'train@deu.rst.pcc_steps_per_second': 2.567, 'epoch': 12.0}
{'loss': 2.7425, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.8058645725250244, 'eval_accuracy@deu.rst.pcc': 0.1908713692946058, 'eval_f1@deu.rst.pcc': 0.06800982476138193, 'eval_precision@deu.rst.pcc': 0.0775138380711136, 'eval_recall@deu.rst.pcc': 0.12877111314611314, 'eval_loss@deu.rst.pcc': 2.8058650493621826, 'eval_runtime': 3.3269, 'eval_samples_per_second': 72.439, 'eval_steps_per_second': 2.405, 'epoch': 12.0}
{'train_runtime': 1035.1693, 'train_samples_per_second': 25.086, 'train_steps_per_second': 0.788, 'train_loss': 2.9078710032444373, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.9079
  train_runtime            = 0:17:15.16
  train_samples_per_second =     25.086
  train_steps_per_second   =      0.788
{'train@fra.sdrt.annodis_loss': 2.5372867584228516, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.21967963386727687, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.04380694512007325, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.04428681985398472, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.06348626361289343, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.5372867584228516, 'train@fra.sdrt.annodis_runtime': 26.6498, 'train@fra.sdrt.annodis_samples_per_second': 81.989, 'train@fra.sdrt.annodis_steps_per_second': 2.589, 'epoch': 1.0}
{'loss': 2.8945, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.5348637104034424, 'eval_accuracy@fra.sdrt.annodis': 0.23106060606060605, 'eval_f1@fra.sdrt.annodis': 0.04177214896036333, 'eval_precision@fra.sdrt.annodis': 0.03797310083806434, 'eval_recall@fra.sdrt.annodis': 0.06288600819580398, 'eval_loss@fra.sdrt.annodis': 2.5348641872406006, 'eval_runtime': 6.8232, 'eval_samples_per_second': 77.383, 'eval_steps_per_second': 2.492, 'epoch': 1.0}
{'train@fra.sdrt.annodis_loss': 2.3270022869110107, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.282837528604119, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.06251504959949382, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.08936334526702477, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.08616297769184886, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.3270022869110107, 'train@fra.sdrt.annodis_runtime': 26.6731, 'train@fra.sdrt.annodis_samples_per_second': 81.918, 'train@fra.sdrt.annodis_steps_per_second': 2.587, 'epoch': 2.0}
{'loss': 2.4432, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.334247589111328, 'eval_accuracy@fra.sdrt.annodis': 0.2840909090909091, 'eval_f1@fra.sdrt.annodis': 0.059413172085354224, 'eval_precision@fra.sdrt.annodis': 0.04665019331564331, 'eval_recall@fra.sdrt.annodis': 0.08238629768570925, 'eval_loss@fra.sdrt.annodis': 2.3342478275299072, 'eval_runtime': 6.8365, 'eval_samples_per_second': 77.233, 'eval_steps_per_second': 2.487, 'epoch': 2.0}
{'train@fra.sdrt.annodis_loss': 2.233989715576172, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.30755148741418764, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.06909837565752627, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.08400214180356023, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.0930677897606976, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.233989715576172, 'train@fra.sdrt.annodis_runtime': 27.0554, 'train@fra.sdrt.annodis_samples_per_second': 80.76, 'train@fra.sdrt.annodis_steps_per_second': 2.55, 'epoch': 3.0}
{'loss': 2.311, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.25146484375, 'eval_accuracy@fra.sdrt.annodis': 0.29545454545454547, 'eval_f1@fra.sdrt.annodis': 0.06247103566110136, 'eval_precision@fra.sdrt.annodis': 0.0495061405996359, 'eval_recall@fra.sdrt.annodis': 0.08565124487589008, 'eval_loss@fra.sdrt.annodis': 2.251465320587158, 'eval_runtime': 6.8143, 'eval_samples_per_second': 77.484, 'eval_steps_per_second': 2.495, 'epoch': 3.0}
{'train@fra.sdrt.annodis_loss': 2.15539813041687, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.36384439359267734, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.11020010039060534, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12411994494578232, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1364471744039884, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.155398368835449, 'train@fra.sdrt.annodis_runtime': 26.6598, 'train@fra.sdrt.annodis_samples_per_second': 81.959, 'train@fra.sdrt.annodis_steps_per_second': 2.588, 'epoch': 4.0}
{'loss': 2.2335, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.179471731185913, 'eval_accuracy@fra.sdrt.annodis': 0.3371212121212121, 'eval_f1@fra.sdrt.annodis': 0.09538206457464689, 'eval_precision@fra.sdrt.annodis': 0.08547869120008424, 'eval_recall@fra.sdrt.annodis': 0.11522235794322616, 'eval_loss@fra.sdrt.annodis': 2.179471969604492, 'eval_runtime': 6.8226, 'eval_samples_per_second': 77.39, 'eval_steps_per_second': 2.492, 'epoch': 4.0}
{'train@fra.sdrt.annodis_loss': 2.084104061126709, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.3890160183066362, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.11518242413075279, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12345723880030755, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.15126889913534092, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.08410382270813, 'train@fra.sdrt.annodis_runtime': 26.7015, 'train@fra.sdrt.annodis_samples_per_second': 81.83, 'train@fra.sdrt.annodis_steps_per_second': 2.584, 'epoch': 5.0}
{'loss': 2.1602, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.121330738067627, 'eval_accuracy@fra.sdrt.annodis': 0.3712121212121212, 'eval_f1@fra.sdrt.annodis': 0.1021775483644678, 'eval_precision@fra.sdrt.annodis': 0.08641769460119067, 'eval_recall@fra.sdrt.annodis': 0.1292788981435572, 'eval_loss@fra.sdrt.annodis': 2.121330738067627, 'eval_runtime': 6.7821, 'eval_samples_per_second': 77.852, 'eval_steps_per_second': 2.507, 'epoch': 5.0}
{'train@fra.sdrt.annodis_loss': 2.0222678184509277, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.39633867276887874, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.11918443707417427, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.1262639346528865, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.15509292373323197, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.022268056869507, 'train@fra.sdrt.annodis_runtime': 26.6982, 'train@fra.sdrt.annodis_samples_per_second': 81.841, 'train@fra.sdrt.annodis_steps_per_second': 2.584, 'epoch': 6.0}
{'loss': 2.0975, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.0683019161224365, 'eval_accuracy@fra.sdrt.annodis': 0.3806818181818182, 'eval_f1@fra.sdrt.annodis': 0.1071709378660131, 'eval_precision@fra.sdrt.annodis': 0.1221244615601775, 'eval_recall@fra.sdrt.annodis': 0.13490416697616775, 'eval_loss@fra.sdrt.annodis': 2.0683019161224365, 'eval_runtime': 6.7735, 'eval_samples_per_second': 77.951, 'eval_steps_per_second': 2.51, 'epoch': 6.0}
{'train@fra.sdrt.annodis_loss': 1.9732671976089478, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4073226544622426, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.12436521188037537, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12340795198170768, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1595747000387052, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.9732674360275269, 'train@fra.sdrt.annodis_runtime': 26.7124, 'train@fra.sdrt.annodis_samples_per_second': 81.797, 'train@fra.sdrt.annodis_steps_per_second': 2.583, 'epoch': 7.0}
{'loss': 2.0403, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.0276756286621094, 'eval_accuracy@fra.sdrt.annodis': 0.3787878787878788, 'eval_f1@fra.sdrt.annodis': 0.10758051838596933, 'eval_precision@fra.sdrt.annodis': 0.10131041616900466, 'eval_recall@fra.sdrt.annodis': 0.13548102200601891, 'eval_loss@fra.sdrt.annodis': 2.0276756286621094, 'eval_runtime': 6.8046, 'eval_samples_per_second': 77.595, 'eval_steps_per_second': 2.498, 'epoch': 7.0}
{'train@fra.sdrt.annodis_loss': 1.9381489753723145, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4096109839816934, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.12678841174339184, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12098400274294724, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1615588140138695, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.9381489753723145, 'train@fra.sdrt.annodis_runtime': 26.7372, 'train@fra.sdrt.annodis_samples_per_second': 81.721, 'train@fra.sdrt.annodis_steps_per_second': 2.581, 'epoch': 8.0}
{'loss': 2.0097, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.9985915422439575, 'eval_accuracy@fra.sdrt.annodis': 0.38257575757575757, 'eval_f1@fra.sdrt.annodis': 0.1101228201971971, 'eval_precision@fra.sdrt.annodis': 0.10313417259453075, 'eval_recall@fra.sdrt.annodis': 0.13722667257328672, 'eval_loss@fra.sdrt.annodis': 1.9985917806625366, 'eval_runtime': 6.7665, 'eval_samples_per_second': 78.031, 'eval_steps_per_second': 2.512, 'epoch': 8.0}
{'train@fra.sdrt.annodis_loss': 1.912844181060791, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.41235697940503435, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.12794621153535063, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12230466879849791, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1627056658716134, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.912844181060791, 'train@fra.sdrt.annodis_runtime': 26.665, 'train@fra.sdrt.annodis_samples_per_second': 81.943, 'train@fra.sdrt.annodis_steps_per_second': 2.588, 'epoch': 9.0}
{'loss': 1.9718, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.977796196937561, 'eval_accuracy@fra.sdrt.annodis': 0.38446969696969696, 'eval_f1@fra.sdrt.annodis': 0.11053477773044851, 'eval_precision@fra.sdrt.annodis': 0.10347074332943702, 'eval_recall@fra.sdrt.annodis': 0.13757922311621457, 'eval_loss@fra.sdrt.annodis': 1.977796196937561, 'eval_runtime': 6.8058, 'eval_samples_per_second': 77.58, 'eval_steps_per_second': 2.498, 'epoch': 9.0}
{'train@fra.sdrt.annodis_loss': 1.8953218460083008, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.41693363844393594, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.12964607395994746, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12238731175151825, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.16456021767453263, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.8953218460083008, 'train@fra.sdrt.annodis_runtime': 26.6683, 'train@fra.sdrt.annodis_samples_per_second': 81.932, 'train@fra.sdrt.annodis_steps_per_second': 2.587, 'epoch': 10.0}
{'loss': 1.9508, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.964016079902649, 'eval_accuracy@fra.sdrt.annodis': 0.3806818181818182, 'eval_f1@fra.sdrt.annodis': 0.10972436550936432, 'eval_precision@fra.sdrt.annodis': 0.10131865770163642, 'eval_recall@fra.sdrt.annodis': 0.13639285155947262, 'eval_loss@fra.sdrt.annodis': 1.9640158414840698, 'eval_runtime': 6.8296, 'eval_samples_per_second': 77.311, 'eval_steps_per_second': 2.489, 'epoch': 10.0}
{'train@fra.sdrt.annodis_loss': 1.8852710723876953, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4242562929061785, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.13215474213253292, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12419873159178639, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.16711703934290278, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.8852710723876953, 'train@fra.sdrt.annodis_runtime': 26.6843, 'train@fra.sdrt.annodis_samples_per_second': 81.883, 'train@fra.sdrt.annodis_steps_per_second': 2.586, 'epoch': 11.0}
{'loss': 1.9338, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.9548002481460571, 'eval_accuracy@fra.sdrt.annodis': 0.38825757575757575, 'eval_f1@fra.sdrt.annodis': 0.11565659398381659, 'eval_precision@fra.sdrt.annodis': 0.10925615164258369, 'eval_recall@fra.sdrt.annodis': 0.14051654046577022, 'eval_loss@fra.sdrt.annodis': 1.954800009727478, 'eval_runtime': 6.8322, 'eval_samples_per_second': 77.281, 'eval_steps_per_second': 2.488, 'epoch': 11.0}
{'train@fra.sdrt.annodis_loss': 1.8816131353378296, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4251716247139588, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.13288979136729184, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12312939173338927, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1677490123834124, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.8816131353378296, 'train@fra.sdrt.annodis_runtime': 26.6979, 'train@fra.sdrt.annodis_samples_per_second': 81.842, 'train@fra.sdrt.annodis_steps_per_second': 2.584, 'epoch': 12.0}
{'loss': 1.9342, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.9523630142211914, 'eval_accuracy@fra.sdrt.annodis': 0.38636363636363635, 'eval_f1@fra.sdrt.annodis': 0.11657949007169671, 'eval_precision@fra.sdrt.annodis': 0.11068394889873467, 'eval_recall@fra.sdrt.annodis': 0.14066713803611586, 'eval_loss@fra.sdrt.annodis': 1.9523627758026123, 'eval_runtime': 6.8009, 'eval_samples_per_second': 77.637, 'eval_steps_per_second': 2.5, 'epoch': 12.0}
{'train_runtime': 1078.6625, 'train_samples_per_second': 24.308, 'train_steps_per_second': 0.768, 'train_loss': 2.165047226320718, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.9079
  train_runtime            = 0:17:15.16
  train_samples_per_second =     25.086
  train_steps_per_second   =      0.788
-------------------------------------------------------------------
Lang1:  eng.pdtb.pdtb    Lang2:  fra.sdrt.annodis
Saving run to:  runs/full_shot/FullShot=v4_eng.pdtb.pdtb_fra.sdrt.annodis_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 43920 examples
read 1674 examples
read 2257 examples
read 2185 examples
read 528 examples
read 625 examples
Total prediction labels:  41
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (fr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=41, bias=True)
    )
  )
)
{'train@eng.pdtb.pdtb_loss': 1.2517441511154175, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6014799635701275, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.27552695331377874, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.3556633765236917, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.26609647921486923, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.251744031906128, 'train@eng.pdtb.pdtb_runtime': 519.2772, 'train@eng.pdtb.pdtb_samples_per_second': 84.579, 'train@eng.pdtb.pdtb_steps_per_second': 2.644, 'epoch': 1.0}
{'loss': 1.8374, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.1776493787765503, 'eval_accuracy@eng.pdtb.pdtb': 0.6308243727598566, 'eval_f1@eng.pdtb.pdtb': 0.32860032360897656, 'eval_precision@eng.pdtb.pdtb': 0.4081344585214892, 'eval_recall@eng.pdtb.pdtb': 0.3195463280592884, 'eval_loss@eng.pdtb.pdtb': 1.1776493787765503, 'eval_runtime': 20.3737, 'eval_samples_per_second': 82.165, 'eval_steps_per_second': 2.601, 'epoch': 1.0}
{'train@eng.pdtb.pdtb_loss': 1.093786358833313, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6400956284153005, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.3718032937959593, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.4310085578393162, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.35945198289247415, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.0937862396240234, 'train@eng.pdtb.pdtb_runtime': 519.2364, 'train@eng.pdtb.pdtb_samples_per_second': 84.586, 'train@eng.pdtb.pdtb_steps_per_second': 2.644, 'epoch': 2.0}
{'loss': 1.2109, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.0270347595214844, 'eval_accuracy@eng.pdtb.pdtb': 0.6666666666666666, 'eval_f1@eng.pdtb.pdtb': 0.42621829688373924, 'eval_precision@eng.pdtb.pdtb': 0.4943029241379905, 'eval_recall@eng.pdtb.pdtb': 0.4101499748286795, 'eval_loss@eng.pdtb.pdtb': 1.0270347595214844, 'eval_runtime': 20.2844, 'eval_samples_per_second': 82.527, 'eval_steps_per_second': 2.613, 'epoch': 2.0}
{'train@eng.pdtb.pdtb_loss': 1.0456533432006836, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.654143897996357, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.42915230575884306, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.4721045712199058, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.41041666323145515, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.0456534624099731, 'train@eng.pdtb.pdtb_runtime': 518.968, 'train@eng.pdtb.pdtb_samples_per_second': 84.629, 'train@eng.pdtb.pdtb_steps_per_second': 2.646, 'epoch': 3.0}
{'loss': 1.1184, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 0.9918559789657593, 'eval_accuracy@eng.pdtb.pdtb': 0.6827956989247311, 'eval_f1@eng.pdtb.pdtb': 0.46806340774207333, 'eval_precision@eng.pdtb.pdtb': 0.5267493561194658, 'eval_recall@eng.pdtb.pdtb': 0.447181117214433, 'eval_loss@eng.pdtb.pdtb': 0.9918559193611145, 'eval_runtime': 20.2508, 'eval_samples_per_second': 82.663, 'eval_steps_per_second': 2.617, 'epoch': 3.0}
{'train@eng.pdtb.pdtb_loss': 0.9995550513267517, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6669854280510018, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.45010834534557675, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.4760277401359728, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4430309166723788, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9995549321174622, 'train@eng.pdtb.pdtb_runtime': 519.0548, 'train@eng.pdtb.pdtb_samples_per_second': 84.615, 'train@eng.pdtb.pdtb_steps_per_second': 2.645, 'epoch': 4.0}
{'loss': 1.0755, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 0.9538118243217468, 'eval_accuracy@eng.pdtb.pdtb': 0.6833930704898447, 'eval_f1@eng.pdtb.pdtb': 0.5073021423692883, 'eval_precision@eng.pdtb.pdtb': 0.5507882617261677, 'eval_recall@eng.pdtb.pdtb': 0.4927226510406639, 'eval_loss@eng.pdtb.pdtb': 0.9538118243217468, 'eval_runtime': 20.2633, 'eval_samples_per_second': 82.612, 'eval_steps_per_second': 2.616, 'epoch': 4.0}
{'train@eng.pdtb.pdtb_loss': 0.9777953624725342, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6724271402550092, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.45770185870349495, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.47986334510147266, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4529397324789737, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9777953624725342, 'train@eng.pdtb.pdtb_runtime': 518.75, 'train@eng.pdtb.pdtb_samples_per_second': 84.665, 'train@eng.pdtb.pdtb_steps_per_second': 2.647, 'epoch': 5.0}
{'loss': 1.043, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 0.9374520182609558, 'eval_accuracy@eng.pdtb.pdtb': 0.6875746714456392, 'eval_f1@eng.pdtb.pdtb': 0.5177766685037872, 'eval_precision@eng.pdtb.pdtb': 0.5487255008934498, 'eval_recall@eng.pdtb.pdtb': 0.5125093021587821, 'eval_loss@eng.pdtb.pdtb': 0.9374521374702454, 'eval_runtime': 20.2994, 'eval_samples_per_second': 82.466, 'eval_steps_per_second': 2.611, 'epoch': 5.0}
{'train@eng.pdtb.pdtb_loss': 0.958629310131073, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.679667577413479, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4651724696915982, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5286032823595169, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.46053577125388434, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.958629310131073, 'train@eng.pdtb.pdtb_runtime': 519.3519, 'train@eng.pdtb.pdtb_samples_per_second': 84.567, 'train@eng.pdtb.pdtb_steps_per_second': 2.644, 'epoch': 6.0}
{'loss': 1.0218, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 0.9309206008911133, 'eval_accuracy@eng.pdtb.pdtb': 0.6857825567502986, 'eval_f1@eng.pdtb.pdtb': 0.5237675423385515, 'eval_precision@eng.pdtb.pdtb': 0.5526238954842362, 'eval_recall@eng.pdtb.pdtb': 0.5166815725242996, 'eval_loss@eng.pdtb.pdtb': 0.9309206008911133, 'eval_runtime': 20.3482, 'eval_samples_per_second': 82.268, 'eval_steps_per_second': 2.605, 'epoch': 6.0}
{'train@eng.pdtb.pdtb_loss': 0.947725772857666, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6805100182149363, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.46690395337337576, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5353072704402774, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4583967883937302, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9477258920669556, 'train@eng.pdtb.pdtb_runtime': 518.9229, 'train@eng.pdtb.pdtb_samples_per_second': 84.637, 'train@eng.pdtb.pdtb_steps_per_second': 2.646, 'epoch': 7.0}
{'loss': 1.0059, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 0.917182207107544, 'eval_accuracy@eng.pdtb.pdtb': 0.6899641577060932, 'eval_f1@eng.pdtb.pdtb': 0.5304823251404625, 'eval_precision@eng.pdtb.pdtb': 0.5781421651860594, 'eval_recall@eng.pdtb.pdtb': 0.513422484686552, 'eval_loss@eng.pdtb.pdtb': 0.9171821475028992, 'eval_runtime': 20.2857, 'eval_samples_per_second': 82.521, 'eval_steps_per_second': 2.613, 'epoch': 7.0}
{'train@eng.pdtb.pdtb_loss': 0.9351935386657715, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6844034608378871, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.47324673191505695, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5087224400487623, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4706743893338301, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9351934194564819, 'train@eng.pdtb.pdtb_runtime': 518.7298, 'train@eng.pdtb.pdtb_samples_per_second': 84.668, 'train@eng.pdtb.pdtb_steps_per_second': 2.647, 'epoch': 8.0}
{'loss': 0.9945, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 0.9133424758911133, 'eval_accuracy@eng.pdtb.pdtb': 0.6899641577060932, 'eval_f1@eng.pdtb.pdtb': 0.5355410701491949, 'eval_precision@eng.pdtb.pdtb': 0.56144009678505, 'eval_recall@eng.pdtb.pdtb': 0.5304425217863235, 'eval_loss@eng.pdtb.pdtb': 0.9133424758911133, 'eval_runtime': 20.2002, 'eval_samples_per_second': 82.87, 'eval_steps_per_second': 2.624, 'epoch': 8.0}
{'train@eng.pdtb.pdtb_loss': 0.9272084832191467, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6867030965391621, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.47685223929448206, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5781633805151811, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.47147120753001515, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9272085428237915, 'train@eng.pdtb.pdtb_runtime': 518.7227, 'train@eng.pdtb.pdtb_samples_per_second': 84.67, 'train@eng.pdtb.pdtb_steps_per_second': 2.647, 'epoch': 9.0}
{'loss': 0.9829, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 0.9112378358840942, 'eval_accuracy@eng.pdtb.pdtb': 0.6905615292712067, 'eval_f1@eng.pdtb.pdtb': 0.5448050729131847, 'eval_precision@eng.pdtb.pdtb': 0.5742980559956888, 'eval_recall@eng.pdtb.pdtb': 0.5341206964904262, 'eval_loss@eng.pdtb.pdtb': 0.9112378358840942, 'eval_runtime': 20.2885, 'eval_samples_per_second': 82.51, 'eval_steps_per_second': 2.612, 'epoch': 9.0}
{'train@eng.pdtb.pdtb_loss': 0.9243015646934509, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6874544626593807, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4768431047397375, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.579763092056097, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4714978649076998, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9243015646934509, 'train@eng.pdtb.pdtb_runtime': 512.7008, 'train@eng.pdtb.pdtb_samples_per_second': 85.664, 'train@eng.pdtb.pdtb_steps_per_second': 2.678, 'epoch': 10.0}
{'loss': 0.9767, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 0.9063858985900879, 'eval_accuracy@eng.pdtb.pdtb': 0.6875746714456392, 'eval_f1@eng.pdtb.pdtb': 0.5372794576240714, 'eval_precision@eng.pdtb.pdtb': 0.573727499537688, 'eval_recall@eng.pdtb.pdtb': 0.5236201664264103, 'eval_loss@eng.pdtb.pdtb': 0.9063858985900879, 'eval_runtime': 19.8927, 'eval_samples_per_second': 84.151, 'eval_steps_per_second': 2.664, 'epoch': 10.0}
{'train@eng.pdtb.pdtb_loss': 0.9191852807998657, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6894353369763205, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.47942939733967505, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5653347781452307, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4763121962055113, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9191853404045105, 'train@eng.pdtb.pdtb_runtime': 512.623, 'train@eng.pdtb.pdtb_samples_per_second': 85.677, 'train@eng.pdtb.pdtb_steps_per_second': 2.678, 'epoch': 11.0}
{'loss': 0.9714, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.9060733318328857, 'eval_accuracy@eng.pdtb.pdtb': 0.6845878136200717, 'eval_f1@eng.pdtb.pdtb': 0.5417695921074326, 'eval_precision@eng.pdtb.pdtb': 0.5606077687620548, 'eval_recall@eng.pdtb.pdtb': 0.5390029752889818, 'eval_loss@eng.pdtb.pdtb': 0.9060733318328857, 'eval_runtime': 19.905, 'eval_samples_per_second': 84.099, 'eval_steps_per_second': 2.663, 'epoch': 11.0}
{'train@eng.pdtb.pdtb_loss': 0.918082058429718, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6895491803278688, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.47882128796913725, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5677918850381181, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4739715639231275, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9180819988250732, 'train@eng.pdtb.pdtb_runtime': 512.8294, 'train@eng.pdtb.pdtb_samples_per_second': 85.643, 'train@eng.pdtb.pdtb_steps_per_second': 2.677, 'epoch': 12.0}
{'loss': 0.9685, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.9043073654174805, 'eval_accuracy@eng.pdtb.pdtb': 0.6875746714456392, 'eval_f1@eng.pdtb.pdtb': 0.5373316670147994, 'eval_precision@eng.pdtb.pdtb': 0.5632004783146989, 'eval_recall@eng.pdtb.pdtb': 0.5295599042962825, 'eval_loss@eng.pdtb.pdtb': 0.9043073058128357, 'eval_runtime': 19.9481, 'eval_samples_per_second': 83.918, 'eval_steps_per_second': 2.657, 'epoch': 12.0}
{'train_runtime': 19688.8714, 'train_samples_per_second': 26.768, 'train_steps_per_second': 0.837, 'train_loss': 1.1005783062532244, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.1006
  train_runtime            = 5:28:08.87
  train_samples_per_second =     26.768
  train_steps_per_second   =      0.837
{'train@fra.sdrt.annodis_loss': 2.3437986373901367, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.2764302059496567, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.08685566293147168, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.10858006294816976, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.09734615996592194, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.3437986373901367, 'train@fra.sdrt.annodis_runtime': 26.315, 'train@fra.sdrt.annodis_samples_per_second': 83.032, 'train@fra.sdrt.annodis_steps_per_second': 2.622, 'epoch': 1.0}
{'loss': 2.9821, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.4179701805114746, 'eval_accuracy@fra.sdrt.annodis': 0.24431818181818182, 'eval_f1@fra.sdrt.annodis': 0.0553017300141157, 'eval_precision@fra.sdrt.annodis': 0.05533486167636907, 'eval_recall@fra.sdrt.annodis': 0.07298403306462362, 'eval_loss@fra.sdrt.annodis': 2.4179701805114746, 'eval_runtime': 6.6502, 'eval_samples_per_second': 79.396, 'eval_steps_per_second': 2.556, 'epoch': 1.0}
{'train@fra.sdrt.annodis_loss': 2.0446765422821045, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.3876430205949657, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.15237555567879474, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.18546134609452627, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.15322019118628571, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.0446765422821045, 'train@fra.sdrt.annodis_runtime': 26.3154, 'train@fra.sdrt.annodis_samples_per_second': 83.031, 'train@fra.sdrt.annodis_steps_per_second': 2.622, 'epoch': 2.0}
{'loss': 2.2294, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.101539134979248, 'eval_accuracy@fra.sdrt.annodis': 0.3541666666666667, 'eval_f1@fra.sdrt.annodis': 0.12047700724373461, 'eval_precision@fra.sdrt.annodis': 0.16673127830698253, 'eval_recall@fra.sdrt.annodis': 0.13044909719105605, 'eval_loss@fra.sdrt.annodis': 2.101539134979248, 'eval_runtime': 6.6272, 'eval_samples_per_second': 79.672, 'eval_steps_per_second': 2.565, 'epoch': 2.0}
{'train@fra.sdrt.annodis_loss': 1.8912267684936523, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4402745995423341, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.2061908911995333, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.22662069341184962, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.21120841935988996, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.8912272453308105, 'train@fra.sdrt.annodis_runtime': 26.2813, 'train@fra.sdrt.annodis_samples_per_second': 83.139, 'train@fra.sdrt.annodis_steps_per_second': 2.625, 'epoch': 3.0}
{'loss': 2.0472, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.95201575756073, 'eval_accuracy@fra.sdrt.annodis': 0.4053030303030303, 'eval_f1@fra.sdrt.annodis': 0.16128506013880206, 'eval_precision@fra.sdrt.annodis': 0.18574045472712528, 'eval_recall@fra.sdrt.annodis': 0.17085467828330644, 'eval_loss@fra.sdrt.annodis': 1.9520155191421509, 'eval_runtime': 6.644, 'eval_samples_per_second': 79.47, 'eval_steps_per_second': 2.559, 'epoch': 3.0}
{'train@fra.sdrt.annodis_loss': 1.78594172000885, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4768878718535469, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.23234540386659214, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.2899518939923736, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.24247196365547577, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.78594172000885, 'train@fra.sdrt.annodis_runtime': 26.2802, 'train@fra.sdrt.annodis_samples_per_second': 83.143, 'train@fra.sdrt.annodis_steps_per_second': 2.626, 'epoch': 4.0}
{'loss': 1.9168, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.8576922416687012, 'eval_accuracy@fra.sdrt.annodis': 0.4412878787878788, 'eval_f1@fra.sdrt.annodis': 0.1902051225248289, 'eval_precision@fra.sdrt.annodis': 0.21134609853136863, 'eval_recall@fra.sdrt.annodis': 0.1977243648381856, 'eval_loss@fra.sdrt.annodis': 1.857692003250122, 'eval_runtime': 6.6564, 'eval_samples_per_second': 79.322, 'eval_steps_per_second': 2.554, 'epoch': 4.0}
{'train@fra.sdrt.annodis_loss': 1.710440754890442, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4988558352402746, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.2629644202501359, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.31527427684473547, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.272444800243181, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.7104408740997314, 'train@fra.sdrt.annodis_runtime': 26.2969, 'train@fra.sdrt.annodis_samples_per_second': 83.09, 'train@fra.sdrt.annodis_steps_per_second': 2.624, 'epoch': 5.0}
{'loss': 1.8225, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.7904682159423828, 'eval_accuracy@fra.sdrt.annodis': 0.4734848484848485, 'eval_f1@fra.sdrt.annodis': 0.22294934332146754, 'eval_precision@fra.sdrt.annodis': 0.23211040141961167, 'eval_recall@fra.sdrt.annodis': 0.22785661351221487, 'eval_loss@fra.sdrt.annodis': 1.7904683351516724, 'eval_runtime': 6.6602, 'eval_samples_per_second': 79.277, 'eval_steps_per_second': 2.552, 'epoch': 5.0}
{'train@fra.sdrt.annodis_loss': 1.653977632522583, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.51441647597254, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.27883371211215974, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.33578825033574206, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.28814301990460633, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.653977632522583, 'train@fra.sdrt.annodis_runtime': 26.3529, 'train@fra.sdrt.annodis_samples_per_second': 82.913, 'train@fra.sdrt.annodis_steps_per_second': 2.618, 'epoch': 6.0}
{'loss': 1.7529, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.7510390281677246, 'eval_accuracy@fra.sdrt.annodis': 0.48295454545454547, 'eval_f1@fra.sdrt.annodis': 0.22985339281032424, 'eval_precision@fra.sdrt.annodis': 0.23622445159389205, 'eval_recall@fra.sdrt.annodis': 0.23833227168608273, 'eval_loss@fra.sdrt.annodis': 1.7510390281677246, 'eval_runtime': 6.6824, 'eval_samples_per_second': 79.014, 'eval_steps_per_second': 2.544, 'epoch': 6.0}
{'train@fra.sdrt.annodis_loss': 1.611219048500061, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.525858123569794, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.29362069441864347, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.3425818382572739, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.3021256172456222, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.611219048500061, 'train@fra.sdrt.annodis_runtime': 26.3084, 'train@fra.sdrt.annodis_samples_per_second': 83.053, 'train@fra.sdrt.annodis_steps_per_second': 2.623, 'epoch': 7.0}
{'loss': 1.6964, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.7178198099136353, 'eval_accuracy@fra.sdrt.annodis': 0.4943181818181818, 'eval_f1@fra.sdrt.annodis': 0.2397891886457218, 'eval_precision@fra.sdrt.annodis': 0.2458966691880361, 'eval_recall@fra.sdrt.annodis': 0.24750510587266078, 'eval_loss@fra.sdrt.annodis': 1.7178199291229248, 'eval_runtime': 6.633, 'eval_samples_per_second': 79.602, 'eval_steps_per_second': 2.563, 'epoch': 7.0}
{'train@fra.sdrt.annodis_loss': 1.5820125341415405, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.531350114416476, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.2998106818306705, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.3398273875169195, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.3109333099665624, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.5820125341415405, 'train@fra.sdrt.annodis_runtime': 26.2957, 'train@fra.sdrt.annodis_samples_per_second': 83.093, 'train@fra.sdrt.annodis_steps_per_second': 2.624, 'epoch': 8.0}
{'loss': 1.6668, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.698664903640747, 'eval_accuracy@fra.sdrt.annodis': 0.48863636363636365, 'eval_f1@fra.sdrt.annodis': 0.23787335296342377, 'eval_precision@fra.sdrt.annodis': 0.24094231161415142, 'eval_recall@fra.sdrt.annodis': 0.24732246135450422, 'eval_loss@fra.sdrt.annodis': 1.6986651420593262, 'eval_runtime': 6.6563, 'eval_samples_per_second': 79.323, 'eval_steps_per_second': 2.554, 'epoch': 8.0}
{'train@fra.sdrt.annodis_loss': 1.561453104019165, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.5345537757437071, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.2994347922187078, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.3395458484064089, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.3118842622822988, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.5614529848098755, 'train@fra.sdrt.annodis_runtime': 26.2499, 'train@fra.sdrt.annodis_samples_per_second': 83.238, 'train@fra.sdrt.annodis_steps_per_second': 2.629, 'epoch': 9.0}
{'loss': 1.6455, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.686899185180664, 'eval_accuracy@fra.sdrt.annodis': 0.5, 'eval_f1@fra.sdrt.annodis': 0.25806510789704645, 'eval_precision@fra.sdrt.annodis': 0.27429337187618763, 'eval_recall@fra.sdrt.annodis': 0.26271007751971576, 'eval_loss@fra.sdrt.annodis': 1.686899185180664, 'eval_runtime': 6.6437, 'eval_samples_per_second': 79.474, 'eval_steps_per_second': 2.559, 'epoch': 9.0}
{'train@fra.sdrt.annodis_loss': 1.546515703201294, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.5386727688787185, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.3020065599873551, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.3957816119980685, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.31315217984238497, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.5465158224105835, 'train@fra.sdrt.annodis_runtime': 26.2516, 'train@fra.sdrt.annodis_samples_per_second': 83.233, 'train@fra.sdrt.annodis_steps_per_second': 2.628, 'epoch': 10.0}
{'loss': 1.6202, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.6789648532867432, 'eval_accuracy@fra.sdrt.annodis': 0.5037878787878788, 'eval_f1@fra.sdrt.annodis': 0.25981297679086435, 'eval_precision@fra.sdrt.annodis': 0.2752738145696308, 'eval_recall@fra.sdrt.annodis': 0.2651068261578297, 'eval_loss@fra.sdrt.annodis': 1.6789652109146118, 'eval_runtime': 6.6349, 'eval_samples_per_second': 79.579, 'eval_steps_per_second': 2.562, 'epoch': 10.0}
{'train@fra.sdrt.annodis_loss': 1.538650393486023, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.5423340961098398, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.3085800702312416, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.4028386363412619, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.32044330218937317, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.5386505126953125, 'train@fra.sdrt.annodis_runtime': 26.3146, 'train@fra.sdrt.annodis_samples_per_second': 83.034, 'train@fra.sdrt.annodis_steps_per_second': 2.622, 'epoch': 11.0}
{'loss': 1.6148, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.672501564025879, 'eval_accuracy@fra.sdrt.annodis': 0.5037878787878788, 'eval_f1@fra.sdrt.annodis': 0.25794605999279807, 'eval_precision@fra.sdrt.annodis': 0.2661942890786132, 'eval_recall@fra.sdrt.annodis': 0.265672501085487, 'eval_loss@fra.sdrt.annodis': 1.672501564025879, 'eval_runtime': 6.6432, 'eval_samples_per_second': 79.48, 'eval_steps_per_second': 2.559, 'epoch': 11.0}
{'train@fra.sdrt.annodis_loss': 1.5356476306915283, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.5414187643020595, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.30876786339853696, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.4024466805032849, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.32010178084937624, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.5356478691101074, 'train@fra.sdrt.annodis_runtime': 26.2531, 'train@fra.sdrt.annodis_samples_per_second': 83.228, 'train@fra.sdrt.annodis_steps_per_second': 2.628, 'epoch': 12.0}
{'loss': 1.6072, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.670338749885559, 'eval_accuracy@fra.sdrt.annodis': 0.5037878787878788, 'eval_f1@fra.sdrt.annodis': 0.2584988130124851, 'eval_precision@fra.sdrt.annodis': 0.26789694921377033, 'eval_recall@fra.sdrt.annodis': 0.265672501085487, 'eval_loss@fra.sdrt.annodis': 1.6703386306762695, 'eval_runtime': 6.6284, 'eval_samples_per_second': 79.657, 'eval_steps_per_second': 2.565, 'epoch': 12.0}
{'train_runtime': 1059.5984, 'train_samples_per_second': 24.745, 'train_steps_per_second': 0.781, 'train_loss': 1.8834808275895418, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.1006
  train_runtime            = 5:28:08.87
  train_samples_per_second =     26.768
  train_steps_per_second   =      0.837
-------------------------------------------------------------------
Lang1:  eng.rst.gum    Lang2:  fra.sdrt.annodis
Saving run to:  runs/full_shot/FullShot=v4_eng.rst.gum_fra.sdrt.annodis_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 13897 examples
read 2149 examples
read 2091 examples
read 2185 examples
read 528 examples
read 625 examples
Total prediction labels:  36
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (fr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=36, bias=True)
    )
  )
)
{'train@eng.rst.gum_loss': 2.5473201274871826, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.23890048211844284, 'train@eng.rst.gum_f1@eng.rst.gum': 0.030965216904878322, 'train@eng.rst.gum_precision@eng.rst.gum': 0.05116345318898245, 'train@eng.rst.gum_recall@eng.rst.gum': 0.052023289148871756, 'train@eng.rst.gum_loss@eng.rst.gum': 2.5473201274871826, 'train@eng.rst.gum_runtime': 162.234, 'train@eng.rst.gum_samples_per_second': 85.66, 'train@eng.rst.gum_steps_per_second': 2.681, 'epoch': 1.0}
{'loss': 2.8211, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.626255750656128, 'eval_accuracy@eng.rst.gum': 0.23778501628664495, 'eval_f1@eng.rst.gum': 0.03599021639393591, 'eval_precision@eng.rst.gum': 0.09215342347942014, 'eval_recall@eng.rst.gum': 0.05618101186078051, 'eval_loss@eng.rst.gum': 2.626255750656128, 'eval_runtime': 25.4122, 'eval_samples_per_second': 84.566, 'eval_steps_per_second': 2.676, 'epoch': 1.0}
{'train@eng.rst.gum_loss': 2.084015130996704, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.3928905519176801, 'train@eng.rst.gum_f1@eng.rst.gum': 0.1482672553649133, 'train@eng.rst.gum_precision@eng.rst.gum': 0.20634964633578057, 'train@eng.rst.gum_recall@eng.rst.gum': 0.16161505349256838, 'train@eng.rst.gum_loss@eng.rst.gum': 2.084015369415283, 'train@eng.rst.gum_runtime': 162.2188, 'train@eng.rst.gum_samples_per_second': 85.668, 'train@eng.rst.gum_steps_per_second': 2.682, 'epoch': 2.0}
{'loss': 2.3804, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.1863620281219482, 'eval_accuracy@eng.rst.gum': 0.36389018147975805, 'eval_f1@eng.rst.gum': 0.1423557588078376, 'eval_precision@eng.rst.gum': 0.2102019768395674, 'eval_recall@eng.rst.gum': 0.16088049065575652, 'eval_loss@eng.rst.gum': 2.186361789703369, 'eval_runtime': 25.424, 'eval_samples_per_second': 84.527, 'eval_steps_per_second': 2.675, 'epoch': 2.0}
{'train@eng.rst.gum_loss': 1.7994762659072876, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.482981938547888, 'train@eng.rst.gum_f1@eng.rst.gum': 0.2666168725435219, 'train@eng.rst.gum_precision@eng.rst.gum': 0.33982871215319693, 'train@eng.rst.gum_recall@eng.rst.gum': 0.2804007194139858, 'train@eng.rst.gum_loss@eng.rst.gum': 1.799476146697998, 'train@eng.rst.gum_runtime': 162.2886, 'train@eng.rst.gum_samples_per_second': 85.631, 'train@eng.rst.gum_steps_per_second': 2.68, 'epoch': 3.0}
{'loss': 2.006, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.9306188821792603, 'eval_accuracy@eng.rst.gum': 0.4448580735225686, 'eval_f1@eng.rst.gum': 0.24631215435679701, 'eval_precision@eng.rst.gum': 0.27026189518947824, 'eval_recall@eng.rst.gum': 0.26398948233835656, 'eval_loss@eng.rst.gum': 1.9306188821792603, 'eval_runtime': 25.4434, 'eval_samples_per_second': 84.462, 'eval_steps_per_second': 2.673, 'epoch': 3.0}
{'train@eng.rst.gum_loss': 1.6704949140548706, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5091746420090667, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3016540747076897, 'train@eng.rst.gum_precision@eng.rst.gum': 0.3953221494084816, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3135231339052081, 'train@eng.rst.gum_loss@eng.rst.gum': 1.6704946756362915, 'train@eng.rst.gum_runtime': 162.3332, 'train@eng.rst.gum_samples_per_second': 85.608, 'train@eng.rst.gum_steps_per_second': 2.68, 'epoch': 4.0}
{'loss': 1.8107, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.8328583240509033, 'eval_accuracy@eng.rst.gum': 0.4644020474639367, 'eval_f1@eng.rst.gum': 0.2797408328076101, 'eval_precision@eng.rst.gum': 0.3158504208158726, 'eval_recall@eng.rst.gum': 0.29754095510744566, 'eval_loss@eng.rst.gum': 1.8328585624694824, 'eval_runtime': 25.4146, 'eval_samples_per_second': 84.558, 'eval_steps_per_second': 2.676, 'epoch': 4.0}
{'train@eng.rst.gum_loss': 1.5925042629241943, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5268043462617831, 'train@eng.rst.gum_f1@eng.rst.gum': 0.33979164311361715, 'train@eng.rst.gum_precision@eng.rst.gum': 0.4811070825733914, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3509234405392749, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5925042629241943, 'train@eng.rst.gum_runtime': 162.4231, 'train@eng.rst.gum_samples_per_second': 85.561, 'train@eng.rst.gum_steps_per_second': 2.678, 'epoch': 5.0}
{'loss': 1.7068, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.7719171047210693, 'eval_accuracy@eng.rst.gum': 0.4881340158213122, 'eval_f1@eng.rst.gum': 0.32445263593106166, 'eval_precision@eng.rst.gum': 0.380559532833474, 'eval_recall@eng.rst.gum': 0.3383150685310546, 'eval_loss@eng.rst.gum': 1.7719168663024902, 'eval_runtime': 25.4385, 'eval_samples_per_second': 84.478, 'eval_steps_per_second': 2.673, 'epoch': 5.0}
{'train@eng.rst.gum_loss': 1.5402835607528687, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.540476361804706, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3737975562281666, 'train@eng.rst.gum_precision@eng.rst.gum': 0.4957477429968414, 'train@eng.rst.gum_recall@eng.rst.gum': 0.37787850153143426, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5402836799621582, 'train@eng.rst.gum_runtime': 162.182, 'train@eng.rst.gum_samples_per_second': 85.688, 'train@eng.rst.gum_steps_per_second': 2.682, 'epoch': 6.0}
{'loss': 1.6461, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.7333271503448486, 'eval_accuracy@eng.rst.gum': 0.49418334108887857, 'eval_f1@eng.rst.gum': 0.34208326991618776, 'eval_precision@eng.rst.gum': 0.3751006435810497, 'eval_recall@eng.rst.gum': 0.3540518666894439, 'eval_loss@eng.rst.gum': 1.7333271503448486, 'eval_runtime': 25.3852, 'eval_samples_per_second': 84.656, 'eval_steps_per_second': 2.679, 'epoch': 6.0}
{'train@eng.rst.gum_loss': 1.5036131143569946, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5490393610131683, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3911569219283931, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5286598877768534, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3953937190803892, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5036131143569946, 'train@eng.rst.gum_runtime': 162.2963, 'train@eng.rst.gum_samples_per_second': 85.627, 'train@eng.rst.gum_steps_per_second': 2.68, 'epoch': 7.0}
{'loss': 1.6007, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.7095632553100586, 'eval_accuracy@eng.rst.gum': 0.49790600279199626, 'eval_f1@eng.rst.gum': 0.35616351748459113, 'eval_precision@eng.rst.gum': 0.41095207306835096, 'eval_recall@eng.rst.gum': 0.3719318475941634, 'eval_loss@eng.rst.gum': 1.7095633745193481, 'eval_runtime': 25.4629, 'eval_samples_per_second': 84.397, 'eval_steps_per_second': 2.671, 'epoch': 7.0}
{'train@eng.rst.gum_loss': 1.4802577495574951, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5549399150895877, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3984048774118195, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5230329513696172, 'train@eng.rst.gum_recall@eng.rst.gum': 0.40276925727940155, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4802577495574951, 'train@eng.rst.gum_runtime': 162.3989, 'train@eng.rst.gum_samples_per_second': 85.573, 'train@eng.rst.gum_steps_per_second': 2.679, 'epoch': 8.0}
{'loss': 1.5656, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.695783257484436, 'eval_accuracy@eng.rst.gum': 0.504885993485342, 'eval_f1@eng.rst.gum': 0.3697627263843674, 'eval_precision@eng.rst.gum': 0.4528429445853028, 'eval_recall@eng.rst.gum': 0.38512852492054134, 'eval_loss@eng.rst.gum': 1.695783257484436, 'eval_runtime': 25.3987, 'eval_samples_per_second': 84.611, 'eval_steps_per_second': 2.677, 'epoch': 8.0}
{'train@eng.rst.gum_loss': 1.4598208665847778, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5629272504857163, 'train@eng.rst.gum_f1@eng.rst.gum': 0.41285118815092103, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5226325518083303, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4174576303699891, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4598209857940674, 'train@eng.rst.gum_runtime': 162.2778, 'train@eng.rst.gum_samples_per_second': 85.637, 'train@eng.rst.gum_steps_per_second': 2.681, 'epoch': 9.0}
{'loss': 1.5438, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.6786495447158813, 'eval_accuracy@eng.rst.gum': 0.5072126570497906, 'eval_f1@eng.rst.gum': 0.3774761983170075, 'eval_precision@eng.rst.gum': 0.4476560721253748, 'eval_recall@eng.rst.gum': 0.3956287654198905, 'eval_loss@eng.rst.gum': 1.6786495447158813, 'eval_runtime': 25.3929, 'eval_samples_per_second': 84.63, 'eval_steps_per_second': 2.678, 'epoch': 9.0}
{'train@eng.rst.gum_loss': 1.443932056427002, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5650140318054256, 'train@eng.rst.gum_f1@eng.rst.gum': 0.41621589713675766, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5241632941687502, 'train@eng.rst.gum_recall@eng.rst.gum': 0.41663362123033054, 'train@eng.rst.gum_loss@eng.rst.gum': 1.443932056427002, 'train@eng.rst.gum_runtime': 162.2835, 'train@eng.rst.gum_samples_per_second': 85.634, 'train@eng.rst.gum_steps_per_second': 2.68, 'epoch': 10.0}
{'loss': 1.5275, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.6690871715545654, 'eval_accuracy@eng.rst.gum': 0.5109353187529083, 'eval_f1@eng.rst.gum': 0.3766062701149465, 'eval_precision@eng.rst.gum': 0.4454498367234267, 'eval_recall@eng.rst.gum': 0.3917573399691295, 'eval_loss@eng.rst.gum': 1.6690871715545654, 'eval_runtime': 25.3621, 'eval_samples_per_second': 84.733, 'eval_steps_per_second': 2.681, 'epoch': 10.0}
{'train@eng.rst.gum_loss': 1.4365642070770264, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5692595524213859, 'train@eng.rst.gum_f1@eng.rst.gum': 0.42485811056975503, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5231526257097034, 'train@eng.rst.gum_recall@eng.rst.gum': 0.42723343881992454, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4365640878677368, 'train@eng.rst.gum_runtime': 162.3804, 'train@eng.rst.gum_samples_per_second': 85.583, 'train@eng.rst.gum_steps_per_second': 2.679, 'epoch': 11.0}
{'loss': 1.5218, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.6626927852630615, 'eval_accuracy@eng.rst.gum': 0.5132619823173569, 'eval_f1@eng.rst.gum': 0.38482940283079, 'eval_precision@eng.rst.gum': 0.4491242012282625, 'eval_recall@eng.rst.gum': 0.4009465234635076, 'eval_loss@eng.rst.gum': 1.662692666053772, 'eval_runtime': 25.4199, 'eval_samples_per_second': 84.54, 'eval_steps_per_second': 2.675, 'epoch': 11.0}
{'train@eng.rst.gum_loss': 1.4335075616836548, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.568755846585594, 'train@eng.rst.gum_f1@eng.rst.gum': 0.42406307811548827, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5248282035398837, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4252109283112804, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4335076808929443, 'train@eng.rst.gum_runtime': 162.3187, 'train@eng.rst.gum_samples_per_second': 85.616, 'train@eng.rst.gum_steps_per_second': 2.68, 'epoch': 12.0}
{'loss': 1.5063, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.6613975763320923, 'eval_accuracy@eng.rst.gum': 0.5132619823173569, 'eval_f1@eng.rst.gum': 0.3851035028581773, 'eval_precision@eng.rst.gum': 0.4562205529486078, 'eval_recall@eng.rst.gum': 0.39881243897557755, 'eval_loss@eng.rst.gum': 1.6613974571228027, 'eval_runtime': 25.4439, 'eval_samples_per_second': 84.46, 'eval_steps_per_second': 2.673, 'epoch': 12.0}
{'train_runtime': 6384.6127, 'train_samples_per_second': 26.12, 'train_steps_per_second': 0.818, 'train_loss': 1.8030578496355663, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.8031
  train_runtime            = 1:46:24.61
  train_samples_per_second =      26.12
  train_steps_per_second   =      0.818
{'train@fra.sdrt.annodis_loss': 2.3547415733337402, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.30068649885583526, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.09107154851347136, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.16170617457078418, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.10268987840856808, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.3547415733337402, 'train@fra.sdrt.annodis_runtime': 26.2805, 'train@fra.sdrt.annodis_samples_per_second': 83.141, 'train@fra.sdrt.annodis_steps_per_second': 2.626, 'epoch': 1.0}
{'loss': 2.8106, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.352309465408325, 'eval_accuracy@fra.sdrt.annodis': 0.2784090909090909, 'eval_f1@fra.sdrt.annodis': 0.0968787445813957, 'eval_precision@fra.sdrt.annodis': 0.16638078479833796, 'eval_recall@fra.sdrt.annodis': 0.10319527000626222, 'eval_loss@fra.sdrt.annodis': 2.3523097038269043, 'eval_runtime': 6.6232, 'eval_samples_per_second': 79.72, 'eval_steps_per_second': 2.567, 'epoch': 1.0}
{'train@fra.sdrt.annodis_loss': 2.0903263092041016, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.39359267734553777, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.14328093641039288, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.19368083740576805, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.15784211446042257, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.0903263092041016, 'train@fra.sdrt.annodis_runtime': 26.266, 'train@fra.sdrt.annodis_samples_per_second': 83.187, 'train@fra.sdrt.annodis_steps_per_second': 2.627, 'epoch': 2.0}
{'loss': 2.2555, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.1101596355438232, 'eval_accuracy@fra.sdrt.annodis': 0.3712121212121212, 'eval_f1@fra.sdrt.annodis': 0.14069726365636256, 'eval_precision@fra.sdrt.annodis': 0.19154481116117944, 'eval_recall@fra.sdrt.annodis': 0.1507793081311914, 'eval_loss@fra.sdrt.annodis': 2.1101598739624023, 'eval_runtime': 6.6331, 'eval_samples_per_second': 79.6, 'eval_steps_per_second': 2.563, 'epoch': 2.0}
{'train@fra.sdrt.annodis_loss': 1.9293192625045776, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4453089244851259, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.17015563562419567, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.17556398902939824, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.18943569241960095, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.9293195009231567, 'train@fra.sdrt.annodis_runtime': 26.2876, 'train@fra.sdrt.annodis_samples_per_second': 83.119, 'train@fra.sdrt.annodis_steps_per_second': 2.625, 'epoch': 3.0}
{'loss': 2.0715, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.9699418544769287, 'eval_accuracy@fra.sdrt.annodis': 0.38636363636363635, 'eval_f1@fra.sdrt.annodis': 0.15163915593122546, 'eval_precision@fra.sdrt.annodis': 0.2052550102801937, 'eval_recall@fra.sdrt.annodis': 0.15852169418282702, 'eval_loss@fra.sdrt.annodis': 1.9699420928955078, 'eval_runtime': 6.641, 'eval_samples_per_second': 79.506, 'eval_steps_per_second': 2.56, 'epoch': 3.0}
{'train@fra.sdrt.annodis_loss': 1.8177191019058228, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.46956521739130436, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.18502178753179727, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.20796591350568622, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.2131119135139844, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.8177192211151123, 'train@fra.sdrt.annodis_runtime': 26.287, 'train@fra.sdrt.annodis_samples_per_second': 83.121, 'train@fra.sdrt.annodis_steps_per_second': 2.625, 'epoch': 4.0}
{'loss': 1.9434, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.8785619735717773, 'eval_accuracy@fra.sdrt.annodis': 0.42234848484848486, 'eval_f1@fra.sdrt.annodis': 0.17263482480102935, 'eval_precision@fra.sdrt.annodis': 0.21598055630189103, 'eval_recall@fra.sdrt.annodis': 0.18691869654868912, 'eval_loss@fra.sdrt.annodis': 1.878562092781067, 'eval_runtime': 6.6139, 'eval_samples_per_second': 79.831, 'eval_steps_per_second': 2.57, 'epoch': 4.0}
{'train@fra.sdrt.annodis_loss': 1.742637038230896, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4842105263157895, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.21427183820098014, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.3554915610300957, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.23465983833404463, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.742637038230896, 'train@fra.sdrt.annodis_runtime': 26.2803, 'train@fra.sdrt.annodis_samples_per_second': 83.142, 'train@fra.sdrt.annodis_steps_per_second': 2.626, 'epoch': 5.0}
{'loss': 1.8378, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.8195675611495972, 'eval_accuracy@fra.sdrt.annodis': 0.4393939393939394, 'eval_f1@fra.sdrt.annodis': 0.1957950749943117, 'eval_precision@fra.sdrt.annodis': 0.27418338036045653, 'eval_recall@fra.sdrt.annodis': 0.2021025854388524, 'eval_loss@fra.sdrt.annodis': 1.8195676803588867, 'eval_runtime': 6.6026, 'eval_samples_per_second': 79.969, 'eval_steps_per_second': 2.575, 'epoch': 5.0}
{'train@fra.sdrt.annodis_loss': 1.6843801736831665, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.5075514874141877, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.2534280778480174, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.33823939603111003, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.2628502493224444, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.6843801736831665, 'train@fra.sdrt.annodis_runtime': 26.5873, 'train@fra.sdrt.annodis_samples_per_second': 82.182, 'train@fra.sdrt.annodis_steps_per_second': 2.595, 'epoch': 6.0}
{'loss': 1.786, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.7774362564086914, 'eval_accuracy@fra.sdrt.annodis': 0.4621212121212121, 'eval_f1@fra.sdrt.annodis': 0.21647791341197797, 'eval_precision@fra.sdrt.annodis': 0.28430750595542, 'eval_recall@fra.sdrt.annodis': 0.223434235099556, 'eval_loss@fra.sdrt.annodis': 1.7774362564086914, 'eval_runtime': 6.6317, 'eval_samples_per_second': 79.618, 'eval_steps_per_second': 2.563, 'epoch': 6.0}
{'train@fra.sdrt.annodis_loss': 1.64058518409729, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.5217391304347826, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.2799988022999196, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.3410656220633476, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.2861590754256647, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.6405853033065796, 'train@fra.sdrt.annodis_runtime': 26.276, 'train@fra.sdrt.annodis_samples_per_second': 83.156, 'train@fra.sdrt.annodis_steps_per_second': 2.626, 'epoch': 7.0}
{'loss': 1.7346, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.743106722831726, 'eval_accuracy@fra.sdrt.annodis': 0.4678030303030303, 'eval_f1@fra.sdrt.annodis': 0.2510824095075936, 'eval_precision@fra.sdrt.annodis': 0.37022139979437146, 'eval_recall@fra.sdrt.annodis': 0.24642695832525657, 'eval_loss@fra.sdrt.annodis': 1.7431068420410156, 'eval_runtime': 6.6099, 'eval_samples_per_second': 79.88, 'eval_steps_per_second': 2.572, 'epoch': 7.0}
{'train@fra.sdrt.annodis_loss': 1.609511375427246, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.5299771167048055, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.2901352366326328, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.34197305700839364, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.2988174159796878, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.609511375427246, 'train@fra.sdrt.annodis_runtime': 26.2773, 'train@fra.sdrt.annodis_samples_per_second': 83.152, 'train@fra.sdrt.annodis_steps_per_second': 2.626, 'epoch': 8.0}
{'loss': 1.7029, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.7185981273651123, 'eval_accuracy@fra.sdrt.annodis': 0.49053030303030304, 'eval_f1@fra.sdrt.annodis': 0.27015981059611704, 'eval_precision@fra.sdrt.annodis': 0.37630431520328184, 'eval_recall@fra.sdrt.annodis': 0.26597303808716677, 'eval_loss@fra.sdrt.annodis': 1.7185983657836914, 'eval_runtime': 6.6247, 'eval_samples_per_second': 79.701, 'eval_steps_per_second': 2.566, 'epoch': 8.0}
{'train@fra.sdrt.annodis_loss': 1.588544487953186, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.5363844393592677, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.30223521326381353, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.34286567273964014, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.3120695138927736, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.5885447263717651, 'train@fra.sdrt.annodis_runtime': 26.2746, 'train@fra.sdrt.annodis_samples_per_second': 83.16, 'train@fra.sdrt.annodis_steps_per_second': 2.626, 'epoch': 9.0}
{'loss': 1.6838, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.7040008306503296, 'eval_accuracy@fra.sdrt.annodis': 0.4943181818181818, 'eval_f1@fra.sdrt.annodis': 0.27324721304558336, 'eval_precision@fra.sdrt.annodis': 0.37555365996157536, 'eval_recall@fra.sdrt.annodis': 0.27106096373584015, 'eval_loss@fra.sdrt.annodis': 1.7040010690689087, 'eval_runtime': 6.6279, 'eval_samples_per_second': 79.663, 'eval_steps_per_second': 2.565, 'epoch': 9.0}
{'train@fra.sdrt.annodis_loss': 1.5727460384368896, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.54279176201373, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.3095913025986195, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.3421033902486199, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.31930552846508103, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.5727461576461792, 'train@fra.sdrt.annodis_runtime': 26.2833, 'train@fra.sdrt.annodis_samples_per_second': 83.133, 'train@fra.sdrt.annodis_steps_per_second': 2.625, 'epoch': 10.0}
{'loss': 1.6523, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.6939605474472046, 'eval_accuracy@fra.sdrt.annodis': 0.5, 'eval_f1@fra.sdrt.annodis': 0.28334331853471556, 'eval_precision@fra.sdrt.annodis': 0.38404546543733337, 'eval_recall@fra.sdrt.annodis': 0.27959765315625107, 'eval_loss@fra.sdrt.annodis': 1.693960428237915, 'eval_runtime': 6.5979, 'eval_samples_per_second': 80.026, 'eval_steps_per_second': 2.577, 'epoch': 10.0}
{'train@fra.sdrt.annodis_loss': 1.5647268295288086, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.5505720823798627, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.31850462446270944, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.34750587882847206, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.3286325581427204, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.5647269487380981, 'train@fra.sdrt.annodis_runtime': 26.2375, 'train@fra.sdrt.annodis_samples_per_second': 83.278, 'train@fra.sdrt.annodis_steps_per_second': 2.63, 'epoch': 11.0}
{'loss': 1.649, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.688080906867981, 'eval_accuracy@fra.sdrt.annodis': 0.5037878787878788, 'eval_f1@fra.sdrt.annodis': 0.28766962037933136, 'eval_precision@fra.sdrt.annodis': 0.38552418276974953, 'eval_recall@fra.sdrt.annodis': 0.28516921397351586, 'eval_loss@fra.sdrt.annodis': 1.68808114528656, 'eval_runtime': 6.612, 'eval_samples_per_second': 79.855, 'eval_steps_per_second': 2.571, 'epoch': 11.0}
{'train@fra.sdrt.annodis_loss': 1.561632752418518, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.5505720823798627, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.3178347143057075, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.34661719386012163, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.32884866007376135, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.5616326332092285, 'train@fra.sdrt.annodis_runtime': 26.2638, 'train@fra.sdrt.annodis_samples_per_second': 83.194, 'train@fra.sdrt.annodis_steps_per_second': 2.627, 'epoch': 12.0}
{'loss': 1.6331, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.6859768629074097, 'eval_accuracy@fra.sdrt.annodis': 0.5056818181818182, 'eval_f1@fra.sdrt.annodis': 0.293481959890603, 'eval_precision@fra.sdrt.annodis': 0.41208032078166046, 'eval_recall@fra.sdrt.annodis': 0.28568842477310047, 'eval_loss@fra.sdrt.annodis': 1.6859772205352783, 'eval_runtime': 6.6068, 'eval_samples_per_second': 79.917, 'eval_steps_per_second': 2.573, 'epoch': 12.0}
{'train_runtime': 1058.6878, 'train_samples_per_second': 24.767, 'train_steps_per_second': 0.782, 'train_loss': 1.8966986522582419, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.8031
  train_runtime            = 1:46:24.61
  train_samples_per_second =      26.12
  train_steps_per_second   =      0.818
-------------------------------------------------------------------
Lang1:  eng.rst.rstdt    Lang2:  fra.sdrt.annodis
Saving run to:  runs/full_shot/FullShot=v4_eng.rst.rstdt_fra.sdrt.annodis_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 16002 examples
read 1621 examples
read 2155 examples
read 2185 examples
read 528 examples
read 625 examples
Total prediction labels:  30
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (fr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=30, bias=True)
    )
  )
)
{'train@eng.rst.rstdt_loss': 1.7491710186004639, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.5086864141982252, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.08255530967092163, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.09097954475656478, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.1100484626669782, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.7491711378097534, 'train@eng.rst.rstdt_runtime': 186.6366, 'train@eng.rst.rstdt_samples_per_second': 85.739, 'train@eng.rst.rstdt_steps_per_second': 2.684, 'epoch': 1.0}
{'loss': 2.1424, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.7281062602996826, 'eval_accuracy@eng.rst.rstdt': 0.5181986428130784, 'eval_f1@eng.rst.rstdt': 0.08220260591181541, 'eval_precision@eng.rst.rstdt': 0.08565394878111925, 'eval_recall@eng.rst.rstdt': 0.10831224995658184, 'eval_loss@eng.rst.rstdt': 1.728106141090393, 'eval_runtime': 19.1966, 'eval_samples_per_second': 84.442, 'eval_steps_per_second': 2.657, 'epoch': 1.0}
{'train@eng.rst.rstdt_loss': 1.424622893333435, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6011123609548806, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.1997860529423512, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.2575246925306405, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.2093185362852951, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.424622893333435, 'train@eng.rst.rstdt_runtime': 186.7305, 'train@eng.rst.rstdt_samples_per_second': 85.696, 'train@eng.rst.rstdt_steps_per_second': 2.683, 'epoch': 2.0}
{'loss': 1.6111, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.4244263172149658, 'eval_accuracy@eng.rst.rstdt': 0.6064157927205429, 'eval_f1@eng.rst.rstdt': 0.201700128570577, 'eval_precision@eng.rst.rstdt': 0.24840033904706135, 'eval_recall@eng.rst.rstdt': 0.21481628173588685, 'eval_loss@eng.rst.rstdt': 1.4244263172149658, 'eval_runtime': 19.1859, 'eval_samples_per_second': 84.489, 'eval_steps_per_second': 2.658, 'epoch': 2.0}
{'train@eng.rst.rstdt_loss': 1.3212242126464844, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6250468691413573, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.2534744957617558, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.39094753070321314, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.24941930383687794, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.3212242126464844, 'train@eng.rst.rstdt_runtime': 186.9177, 'train@eng.rst.rstdt_samples_per_second': 85.61, 'train@eng.rst.rstdt_steps_per_second': 2.68, 'epoch': 3.0}
{'loss': 1.43, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.3424367904663086, 'eval_accuracy@eng.rst.rstdt': 0.6329426280074029, 'eval_f1@eng.rst.rstdt': 0.2526318444809934, 'eval_precision@eng.rst.rstdt': 0.3952670177410889, 'eval_recall@eng.rst.rstdt': 0.25263391360977344, 'eval_loss@eng.rst.rstdt': 1.3424367904663086, 'eval_runtime': 19.237, 'eval_samples_per_second': 84.265, 'eval_steps_per_second': 2.651, 'epoch': 3.0}
{'train@eng.rst.rstdt_loss': 1.25477135181427, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6420447444069491, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.32357706781896184, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.43979509084006246, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3061226087573108, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.25477135181427, 'train@eng.rst.rstdt_runtime': 186.7568, 'train@eng.rst.rstdt_samples_per_second': 85.684, 'train@eng.rst.rstdt_steps_per_second': 2.683, 'epoch': 4.0}
{'loss': 1.3383, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.3029401302337646, 'eval_accuracy@eng.rst.rstdt': 0.6384947563232573, 'eval_f1@eng.rst.rstdt': 0.31963923160531255, 'eval_precision@eng.rst.rstdt': 0.44419403567277504, 'eval_recall@eng.rst.rstdt': 0.3100720237494175, 'eval_loss@eng.rst.rstdt': 1.3029401302337646, 'eval_runtime': 19.2163, 'eval_samples_per_second': 84.355, 'eval_steps_per_second': 2.654, 'epoch': 4.0}
{'train@eng.rst.rstdt_loss': 1.2060152292251587, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6544181977252843, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.34844385595364913, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5151760313498367, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.325308842792422, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.2060152292251587, 'train@eng.rst.rstdt_runtime': 186.7876, 'train@eng.rst.rstdt_samples_per_second': 85.669, 'train@eng.rst.rstdt_steps_per_second': 2.682, 'epoch': 5.0}
{'loss': 1.2855, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.261121392250061, 'eval_accuracy@eng.rst.rstdt': 0.6421961752004935, 'eval_f1@eng.rst.rstdt': 0.32526739106518976, 'eval_precision@eng.rst.rstdt': 0.42840958016864117, 'eval_recall@eng.rst.rstdt': 0.31422835738463895, 'eval_loss@eng.rst.rstdt': 1.2611215114593506, 'eval_runtime': 19.2292, 'eval_samples_per_second': 84.299, 'eval_steps_per_second': 2.652, 'epoch': 5.0}
{'train@eng.rst.rstdt_loss': 1.1715967655181885, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6617922759655043, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.37126201841402773, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5172034230494512, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3407899981478564, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1715967655181885, 'train@eng.rst.rstdt_runtime': 186.7922, 'train@eng.rst.rstdt_samples_per_second': 85.667, 'train@eng.rst.rstdt_steps_per_second': 2.682, 'epoch': 6.0}
{'loss': 1.2404, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.233335256576538, 'eval_accuracy@eng.rst.rstdt': 0.6483652066625539, 'eval_f1@eng.rst.rstdt': 0.3461043067573053, 'eval_precision@eng.rst.rstdt': 0.49043499785761047, 'eval_recall@eng.rst.rstdt': 0.3361692067411877, 'eval_loss@eng.rst.rstdt': 1.2333351373672485, 'eval_runtime': 19.2438, 'eval_samples_per_second': 84.235, 'eval_steps_per_second': 2.65, 'epoch': 6.0}
{'train@eng.rst.rstdt_loss': 1.1509357690811157, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6659792525934258, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3843919469735995, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5830830274897454, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.350943417035232, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1509357690811157, 'train@eng.rst.rstdt_runtime': 186.7117, 'train@eng.rst.rstdt_samples_per_second': 85.704, 'train@eng.rst.rstdt_steps_per_second': 2.683, 'epoch': 7.0}
{'loss': 1.216, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.2169159650802612, 'eval_accuracy@eng.rst.rstdt': 0.6446637877853177, 'eval_f1@eng.rst.rstdt': 0.3530465681548916, 'eval_precision@eng.rst.rstdt': 0.5206673106102636, 'eval_recall@eng.rst.rstdt': 0.34115118613699974, 'eval_loss@eng.rst.rstdt': 1.2169159650802612, 'eval_runtime': 19.2164, 'eval_samples_per_second': 84.355, 'eval_steps_per_second': 2.654, 'epoch': 7.0}
{'train@eng.rst.rstdt_loss': 1.1337136030197144, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6667291588551431, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3908760391693148, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6360870411246881, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3595508463382743, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1337136030197144, 'train@eng.rst.rstdt_runtime': 186.7384, 'train@eng.rst.rstdt_samples_per_second': 85.692, 'train@eng.rst.rstdt_steps_per_second': 2.683, 'epoch': 8.0}
{'loss': 1.1935, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.2109053134918213, 'eval_accuracy@eng.rst.rstdt': 0.6465144972239358, 'eval_f1@eng.rst.rstdt': 0.3570238998124763, 'eval_precision@eng.rst.rstdt': 0.5242356336011083, 'eval_recall@eng.rst.rstdt': 0.346956974896067, 'eval_loss@eng.rst.rstdt': 1.2109053134918213, 'eval_runtime': 19.2289, 'eval_samples_per_second': 84.3, 'eval_steps_per_second': 2.652, 'epoch': 8.0}
{'train@eng.rst.rstdt_loss': 1.1218608617782593, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6706661667291589, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.39866548170002003, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.642500454835, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3641900693209735, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1218608617782593, 'train@eng.rst.rstdt_runtime': 186.7412, 'train@eng.rst.rstdt_samples_per_second': 85.691, 'train@eng.rst.rstdt_steps_per_second': 2.683, 'epoch': 9.0}
{'loss': 1.1795, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.2007691860198975, 'eval_accuracy@eng.rst.rstdt': 0.649599012954966, 'eval_f1@eng.rst.rstdt': 0.3596391022484219, 'eval_precision@eng.rst.rstdt': 0.5278894034061906, 'eval_recall@eng.rst.rstdt': 0.34781335136434255, 'eval_loss@eng.rst.rstdt': 1.2007691860198975, 'eval_runtime': 19.1903, 'eval_samples_per_second': 84.47, 'eval_steps_per_second': 2.658, 'epoch': 9.0}
{'train@eng.rst.rstdt_loss': 1.1166507005691528, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6684789401324834, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.40683584914485404, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6276363512151576, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.37324518011934227, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1166507005691528, 'train@eng.rst.rstdt_runtime': 186.787, 'train@eng.rst.rstdt_samples_per_second': 85.67, 'train@eng.rst.rstdt_steps_per_second': 2.682, 'epoch': 10.0}
{'loss': 1.1683, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.2054733037948608, 'eval_accuracy@eng.rst.rstdt': 0.6391116594694632, 'eval_f1@eng.rst.rstdt': 0.3636992359301351, 'eval_precision@eng.rst.rstdt': 0.5206569649951124, 'eval_recall@eng.rst.rstdt': 0.35306409857870696, 'eval_loss@eng.rst.rstdt': 1.2054733037948608, 'eval_runtime': 19.231, 'eval_samples_per_second': 84.291, 'eval_steps_per_second': 2.652, 'epoch': 10.0}
{'train@eng.rst.rstdt_loss': 1.1099495887756348, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6704786901637295, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4032607498261143, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6339360760322388, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.36908569953047526, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1099495887756348, 'train@eng.rst.rstdt_runtime': 186.9203, 'train@eng.rst.rstdt_samples_per_second': 85.609, 'train@eng.rst.rstdt_steps_per_second': 2.68, 'epoch': 11.0}
{'loss': 1.1629, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.1965974569320679, 'eval_accuracy@eng.rst.rstdt': 0.6428130783466995, 'eval_f1@eng.rst.rstdt': 0.3614021830272776, 'eval_precision@eng.rst.rstdt': 0.5240640707748949, 'eval_recall@eng.rst.rstdt': 0.34939422324339103, 'eval_loss@eng.rst.rstdt': 1.1965974569320679, 'eval_runtime': 19.1948, 'eval_samples_per_second': 84.45, 'eval_steps_per_second': 2.657, 'epoch': 11.0}
{'train@eng.rst.rstdt_loss': 1.1083526611328125, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6712285964254469, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.40432291240142254, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6327533364726906, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3698928129388757, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.108352541923523, 'train@eng.rst.rstdt_runtime': 186.8763, 'train@eng.rst.rstdt_samples_per_second': 85.629, 'train@eng.rst.rstdt_steps_per_second': 2.681, 'epoch': 12.0}
{'loss': 1.1592, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.1952826976776123, 'eval_accuracy@eng.rst.rstdt': 0.6428130783466995, 'eval_f1@eng.rst.rstdt': 0.3614499415187938, 'eval_precision@eng.rst.rstdt': 0.5241029643126747, 'eval_recall@eng.rst.rstdt': 0.34939422324339103, 'eval_loss@eng.rst.rstdt': 1.1952826976776123, 'eval_runtime': 19.2216, 'eval_samples_per_second': 84.332, 'eval_steps_per_second': 2.653, 'epoch': 12.0}
{'train_runtime': 7230.459, 'train_samples_per_second': 26.558, 'train_steps_per_second': 0.831, 'train_loss': 1.3439261004358471, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.3439
  train_runtime            = 2:00:30.45
  train_samples_per_second =     26.558
  train_steps_per_second   =      0.831
{'train@fra.sdrt.annodis_loss': 2.2631232738494873, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.2562929061784897, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.07574791959709726, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.11796175814585735, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.08800483384713635, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.2631235122680664, 'train@fra.sdrt.annodis_runtime': 27.0253, 'train@fra.sdrt.annodis_samples_per_second': 80.85, 'train@fra.sdrt.annodis_steps_per_second': 2.553, 'epoch': 1.0}
{'loss': 2.5133, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.327382802963257, 'eval_accuracy@fra.sdrt.annodis': 0.25946969696969696, 'eval_f1@fra.sdrt.annodis': 0.07989465252677891, 'eval_precision@fra.sdrt.annodis': 0.08615842234237689, 'eval_recall@fra.sdrt.annodis': 0.08991838341764088, 'eval_loss@fra.sdrt.annodis': 2.327383041381836, 'eval_runtime': 6.5598, 'eval_samples_per_second': 80.491, 'eval_steps_per_second': 2.592, 'epoch': 1.0}
{'train@fra.sdrt.annodis_loss': 2.042570114135742, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.351487414187643, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.1485740471597815, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.19315378819083362, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.15119833340244482, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.0425703525543213, 'train@fra.sdrt.annodis_runtime': 26.2408, 'train@fra.sdrt.annodis_samples_per_second': 83.267, 'train@fra.sdrt.annodis_steps_per_second': 2.629, 'epoch': 2.0}
{'loss': 2.1929, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.1072795391082764, 'eval_accuracy@fra.sdrt.annodis': 0.32007575757575757, 'eval_f1@fra.sdrt.annodis': 0.11506585788087821, 'eval_precision@fra.sdrt.annodis': 0.1415754355488233, 'eval_recall@fra.sdrt.annodis': 0.12179233070193549, 'eval_loss@fra.sdrt.annodis': 2.107279062271118, 'eval_runtime': 6.5792, 'eval_samples_per_second': 80.253, 'eval_steps_per_second': 2.584, 'epoch': 2.0}
{'train@fra.sdrt.annodis_loss': 1.9032891988754272, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.41556064073226545, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.1906575218222851, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.2115253902037123, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.19842112451202293, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.9032891988754272, 'train@fra.sdrt.annodis_runtime': 26.2525, 'train@fra.sdrt.annodis_samples_per_second': 83.23, 'train@fra.sdrt.annodis_steps_per_second': 2.628, 'epoch': 3.0}
{'loss': 2.0168, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.9786770343780518, 'eval_accuracy@fra.sdrt.annodis': 0.3522727272727273, 'eval_f1@fra.sdrt.annodis': 0.14282703818640435, 'eval_precision@fra.sdrt.annodis': 0.16640387642013899, 'eval_recall@fra.sdrt.annodis': 0.14504900073889743, 'eval_loss@fra.sdrt.annodis': 1.9786767959594727, 'eval_runtime': 6.6025, 'eval_samples_per_second': 79.969, 'eval_steps_per_second': 2.575, 'epoch': 3.0}
{'train@fra.sdrt.annodis_loss': 1.8008192777633667, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.45446224256292905, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.21401652495060391, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.21740867362703206, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.2333806841864627, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.8008191585540771, 'train@fra.sdrt.annodis_runtime': 26.225, 'train@fra.sdrt.annodis_samples_per_second': 83.317, 'train@fra.sdrt.annodis_steps_per_second': 2.631, 'epoch': 4.0}
{'loss': 1.9097, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.8782429695129395, 'eval_accuracy@fra.sdrt.annodis': 0.4109848484848485, 'eval_f1@fra.sdrt.annodis': 0.1894872851529138, 'eval_precision@fra.sdrt.annodis': 0.20236995817061176, 'eval_recall@fra.sdrt.annodis': 0.19443353786029602, 'eval_loss@fra.sdrt.annodis': 1.878243327140808, 'eval_runtime': 6.5822, 'eval_samples_per_second': 80.216, 'eval_steps_per_second': 2.583, 'epoch': 4.0}
{'train@fra.sdrt.annodis_loss': 1.7319778203964233, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4814645308924485, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.2358024367560352, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.23399297737108593, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.26296891922220694, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.7319778203964233, 'train@fra.sdrt.annodis_runtime': 26.2498, 'train@fra.sdrt.annodis_samples_per_second': 83.239, 'train@fra.sdrt.annodis_steps_per_second': 2.629, 'epoch': 5.0}
{'loss': 1.8387, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.8138951063156128, 'eval_accuracy@fra.sdrt.annodis': 0.44696969696969696, 'eval_f1@fra.sdrt.annodis': 0.21849144833209308, 'eval_precision@fra.sdrt.annodis': 0.2256453520548488, 'eval_recall@fra.sdrt.annodis': 0.22905572140729355, 'eval_loss@fra.sdrt.annodis': 1.8138951063156128, 'eval_runtime': 6.5842, 'eval_samples_per_second': 80.192, 'eval_steps_per_second': 2.582, 'epoch': 5.0}
{'train@fra.sdrt.annodis_loss': 1.6786768436431885, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.5011441647597255, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.2569551477777839, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.24790560654315424, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.28351112659732886, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.678676724433899, 'train@fra.sdrt.annodis_runtime': 26.2557, 'train@fra.sdrt.annodis_samples_per_second': 83.22, 'train@fra.sdrt.annodis_steps_per_second': 2.628, 'epoch': 6.0}
{'loss': 1.7641, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.771837830543518, 'eval_accuracy@fra.sdrt.annodis': 0.4393939393939394, 'eval_f1@fra.sdrt.annodis': 0.21681996331275602, 'eval_precision@fra.sdrt.annodis': 0.21703419660506315, 'eval_recall@fra.sdrt.annodis': 0.22902745635201446, 'eval_loss@fra.sdrt.annodis': 1.771837830543518, 'eval_runtime': 6.574, 'eval_samples_per_second': 80.317, 'eval_steps_per_second': 2.586, 'epoch': 6.0}
{'train@fra.sdrt.annodis_loss': 1.6384086608886719, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.5135011441647597, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.266694714168509, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.2567385987483782, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.29258151354012923, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.6384085416793823, 'train@fra.sdrt.annodis_runtime': 26.2081, 'train@fra.sdrt.annodis_samples_per_second': 83.371, 'train@fra.sdrt.annodis_steps_per_second': 2.633, 'epoch': 7.0}
{'loss': 1.7207, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.7366434335708618, 'eval_accuracy@fra.sdrt.annodis': 0.4621212121212121, 'eval_f1@fra.sdrt.annodis': 0.22671973096234696, 'eval_precision@fra.sdrt.annodis': 0.22783006309594336, 'eval_recall@fra.sdrt.annodis': 0.23848418405532434, 'eval_loss@fra.sdrt.annodis': 1.7366433143615723, 'eval_runtime': 6.5654, 'eval_samples_per_second': 80.421, 'eval_steps_per_second': 2.589, 'epoch': 7.0}
{'train@fra.sdrt.annodis_loss': 1.6102887392044067, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.5208237986270023, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.27322237783312275, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.26071397729982043, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.3000984853671497, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.6102886199951172, 'train@fra.sdrt.annodis_runtime': 26.2219, 'train@fra.sdrt.annodis_samples_per_second': 83.327, 'train@fra.sdrt.annodis_steps_per_second': 2.631, 'epoch': 8.0}
{'loss': 1.6861, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.7164090871810913, 'eval_accuracy@fra.sdrt.annodis': 0.4640151515151515, 'eval_f1@fra.sdrt.annodis': 0.22819170542903044, 'eval_precision@fra.sdrt.annodis': 0.22168521111412748, 'eval_recall@fra.sdrt.annodis': 0.24518178437143798, 'eval_loss@fra.sdrt.annodis': 1.7164092063903809, 'eval_runtime': 6.5458, 'eval_samples_per_second': 80.662, 'eval_steps_per_second': 2.597, 'epoch': 8.0}
{'train@fra.sdrt.annodis_loss': 1.5908312797546387, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.5254004576659039, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.27590016292674896, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.26356821104207073, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.30352522116013264, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.5908310413360596, 'train@fra.sdrt.annodis_runtime': 26.2175, 'train@fra.sdrt.annodis_samples_per_second': 83.341, 'train@fra.sdrt.annodis_steps_per_second': 2.632, 'epoch': 9.0}
{'loss': 1.6763, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.7028754949569702, 'eval_accuracy@fra.sdrt.annodis': 0.4715909090909091, 'eval_f1@fra.sdrt.annodis': 0.23060244900566898, 'eval_precision@fra.sdrt.annodis': 0.2247476883101822, 'eval_recall@fra.sdrt.annodis': 0.24782430249743392, 'eval_loss@fra.sdrt.annodis': 1.7028756141662598, 'eval_runtime': 6.5924, 'eval_samples_per_second': 80.093, 'eval_steps_per_second': 2.579, 'epoch': 9.0}
{'train@fra.sdrt.annodis_loss': 1.574800729751587, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.5308924485125858, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.2822774892327136, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.26855847321619275, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.30924373114156795, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.5748006105422974, 'train@fra.sdrt.annodis_runtime': 26.2525, 'train@fra.sdrt.annodis_samples_per_second': 83.23, 'train@fra.sdrt.annodis_steps_per_second': 2.628, 'epoch': 10.0}
{'loss': 1.6438, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.6920448541641235, 'eval_accuracy@fra.sdrt.annodis': 0.4810606060606061, 'eval_f1@fra.sdrt.annodis': 0.23314012447693777, 'eval_precision@fra.sdrt.annodis': 0.2209494841044469, 'eval_recall@fra.sdrt.annodis': 0.2529516400175054, 'eval_loss@fra.sdrt.annodis': 1.692044973373413, 'eval_runtime': 6.5892, 'eval_samples_per_second': 80.131, 'eval_steps_per_second': 2.58, 'epoch': 10.0}
{'train@fra.sdrt.annodis_loss': 1.5672268867492676, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.5327231121281465, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.28421791786689954, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.26929892696789737, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.3125519345886876, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.567226767539978, 'train@fra.sdrt.annodis_runtime': 26.2798, 'train@fra.sdrt.annodis_samples_per_second': 83.144, 'train@fra.sdrt.annodis_steps_per_second': 2.626, 'epoch': 11.0}
{'loss': 1.6417, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.68610680103302, 'eval_accuracy@fra.sdrt.annodis': 0.4791666666666667, 'eval_f1@fra.sdrt.annodis': 0.23189810082354378, 'eval_precision@fra.sdrt.annodis': 0.21837740114020704, 'eval_recall@fra.sdrt.annodis': 0.2524324292179208, 'eval_loss@fra.sdrt.annodis': 1.6861069202423096, 'eval_runtime': 6.5818, 'eval_samples_per_second': 80.222, 'eval_steps_per_second': 2.583, 'epoch': 11.0}
{'train@fra.sdrt.annodis_loss': 1.5641818046569824, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.5340961098398169, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.283968268468369, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.2685180278296181, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.312416433233674, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.5641816854476929, 'train@fra.sdrt.annodis_runtime': 26.2505, 'train@fra.sdrt.annodis_samples_per_second': 83.236, 'train@fra.sdrt.annodis_steps_per_second': 2.629, 'epoch': 12.0}
{'loss': 1.637, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.6837859153747559, 'eval_accuracy@fra.sdrt.annodis': 0.4810606060606061, 'eval_f1@fra.sdrt.annodis': 0.23256167290902294, 'eval_precision@fra.sdrt.annodis': 0.21908795524511618, 'eval_recall@fra.sdrt.annodis': 0.25293292971842135, 'eval_loss@fra.sdrt.annodis': 1.6837859153747559, 'eval_runtime': 6.6025, 'eval_samples_per_second': 79.97, 'eval_steps_per_second': 2.575, 'epoch': 12.0}
{'train_runtime': 1057.8055, 'train_samples_per_second': 24.787, 'train_steps_per_second': 0.783, 'train_loss': 1.8534247172627472, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.3439
  train_runtime            = 2:00:30.45
  train_samples_per_second =     26.558
  train_steps_per_second   =      0.831
-------------------------------------------------------------------
Lang1:  eng.sdrt.stac    Lang2:  fra.sdrt.annodis
Saving run to:  runs/full_shot/FullShot=v4_eng.sdrt.stac_fra.sdrt.annodis_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 9580 examples
read 1145 examples
read 1510 examples
read 2185 examples
read 528 examples
read 625 examples
Total prediction labels:  34
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (fr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=34, bias=True)
    )
  )
)
{'train@eng.sdrt.stac_loss': 2.101990222930908, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.3545929018789144, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.06553402196527583, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.10330581019935822, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.11036446416771414, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 2.101990222930908, 'train@eng.sdrt.stac_runtime': 111.9045, 'train@eng.sdrt.stac_samples_per_second': 85.609, 'train@eng.sdrt.stac_steps_per_second': 2.681, 'epoch': 1.0}
{'loss': 2.5439, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.060452699661255, 'eval_accuracy@eng.sdrt.stac': 0.3615720524017467, 'eval_f1@eng.sdrt.stac': 0.06551858064863846, 'eval_precision@eng.sdrt.stac': 0.06860563545542928, 'eval_recall@eng.sdrt.stac': 0.11217628194677982, 'eval_loss@eng.sdrt.stac': 2.060452938079834, 'eval_runtime': 13.7144, 'eval_samples_per_second': 83.489, 'eval_steps_per_second': 2.625, 'epoch': 1.0}
{'train@eng.sdrt.stac_loss': 1.8649039268493652, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4292275574112735, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.13237877718709296, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.14751454876421885, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.17761618625828102, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.8649039268493652, 'train@eng.sdrt.stac_runtime': 111.9162, 'train@eng.sdrt.stac_samples_per_second': 85.6, 'train@eng.sdrt.stac_steps_per_second': 2.681, 'epoch': 2.0}
{'loss': 2.0156, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.817284345626831, 'eval_accuracy@eng.sdrt.stac': 0.42707423580786025, 'eval_f1@eng.sdrt.stac': 0.12561432234788625, 'eval_precision@eng.sdrt.stac': 0.1241014407079763, 'eval_recall@eng.sdrt.stac': 0.17415052803793574, 'eval_loss@eng.sdrt.stac': 1.817284345626831, 'eval_runtime': 13.7072, 'eval_samples_per_second': 83.533, 'eval_steps_per_second': 2.626, 'epoch': 2.0}
{'train@eng.sdrt.stac_loss': 1.7536766529083252, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4454070981210856, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.1577215909701174, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.14109183096693204, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.19414723741474613, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.7536766529083252, 'train@eng.sdrt.stac_runtime': 111.8803, 'train@eng.sdrt.stac_samples_per_second': 85.627, 'train@eng.sdrt.stac_steps_per_second': 2.681, 'epoch': 3.0}
{'loss': 1.8423, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.7077380418777466, 'eval_accuracy@eng.sdrt.stac': 0.44366812227074237, 'eval_f1@eng.sdrt.stac': 0.147482448566418, 'eval_precision@eng.sdrt.stac': 0.13319288874656013, 'eval_recall@eng.sdrt.stac': 0.18709605524652184, 'eval_loss@eng.sdrt.stac': 1.7077381610870361, 'eval_runtime': 13.7115, 'eval_samples_per_second': 83.506, 'eval_steps_per_second': 2.626, 'epoch': 3.0}
{'train@eng.sdrt.stac_loss': 1.6800670623779297, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.47056367432150314, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.19193749978203706, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.2102439633827437, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2178300570128077, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.6800670623779297, 'train@eng.sdrt.stac_runtime': 111.884, 'train@eng.sdrt.stac_samples_per_second': 85.624, 'train@eng.sdrt.stac_steps_per_second': 2.681, 'epoch': 4.0}
{'loss': 1.7587, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.6328192949295044, 'eval_accuracy@eng.sdrt.stac': 0.4698689956331878, 'eval_f1@eng.sdrt.stac': 0.18065902864058397, 'eval_precision@eng.sdrt.stac': 0.1883630042686033, 'eval_recall@eng.sdrt.stac': 0.209233698160945, 'eval_loss@eng.sdrt.stac': 1.632819414138794, 'eval_runtime': 13.7519, 'eval_samples_per_second': 83.261, 'eval_steps_per_second': 2.618, 'epoch': 4.0}
{'train@eng.sdrt.stac_loss': 1.6326897144317627, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.48643006263048016, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.21392857287025266, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.27634834546603576, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.23683575348958968, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.6326897144317627, 'train@eng.sdrt.stac_runtime': 111.9508, 'train@eng.sdrt.stac_samples_per_second': 85.573, 'train@eng.sdrt.stac_steps_per_second': 2.68, 'epoch': 5.0}
{'loss': 1.7041, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.5920835733413696, 'eval_accuracy@eng.sdrt.stac': 0.4890829694323144, 'eval_f1@eng.sdrt.stac': 0.20271834662286292, 'eval_precision@eng.sdrt.stac': 0.21055175386108205, 'eval_recall@eng.sdrt.stac': 0.22654063604596977, 'eval_loss@eng.sdrt.stac': 1.5920836925506592, 'eval_runtime': 13.6635, 'eval_samples_per_second': 83.8, 'eval_steps_per_second': 2.635, 'epoch': 5.0}
{'train@eng.sdrt.stac_loss': 1.5942201614379883, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4899791231732777, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.21083619214702978, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.24558354241302938, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.23293949389169571, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5942200422286987, 'train@eng.sdrt.stac_runtime': 111.8305, 'train@eng.sdrt.stac_samples_per_second': 85.665, 'train@eng.sdrt.stac_steps_per_second': 2.683, 'epoch': 6.0}
{'loss': 1.6558, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.572019100189209, 'eval_accuracy@eng.sdrt.stac': 0.4873362445414847, 'eval_f1@eng.sdrt.stac': 0.20083061551801779, 'eval_precision@eng.sdrt.stac': 0.23709436293869726, 'eval_recall@eng.sdrt.stac': 0.22263094972066216, 'eval_loss@eng.sdrt.stac': 1.572019100189209, 'eval_runtime': 13.7184, 'eval_samples_per_second': 83.465, 'eval_steps_per_second': 2.624, 'epoch': 6.0}
{'train@eng.sdrt.stac_loss': 1.5731470584869385, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5011482254697286, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.22836986026019143, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.2362707402077155, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.25073451189927903, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5731472969055176, 'train@eng.sdrt.stac_runtime': 111.9418, 'train@eng.sdrt.stac_samples_per_second': 85.58, 'train@eng.sdrt.stac_steps_per_second': 2.68, 'epoch': 7.0}
{'loss': 1.6263, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.556353211402893, 'eval_accuracy@eng.sdrt.stac': 0.5100436681222708, 'eval_f1@eng.sdrt.stac': 0.22008621778667195, 'eval_precision@eng.sdrt.stac': 0.22270979404118804, 'eval_recall@eng.sdrt.stac': 0.240810488185998, 'eval_loss@eng.sdrt.stac': 1.5563533306121826, 'eval_runtime': 13.6843, 'eval_samples_per_second': 83.672, 'eval_steps_per_second': 2.631, 'epoch': 7.0}
{'train@eng.sdrt.stac_loss': 1.5374963283538818, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5131524008350731, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.25399206439541805, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.3642337362909591, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2709761079883387, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5374964475631714, 'train@eng.sdrt.stac_runtime': 112.0173, 'train@eng.sdrt.stac_samples_per_second': 85.522, 'train@eng.sdrt.stac_steps_per_second': 2.678, 'epoch': 8.0}
{'loss': 1.6047, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.5214003324508667, 'eval_accuracy@eng.sdrt.stac': 0.5161572052401747, 'eval_f1@eng.sdrt.stac': 0.2418949786531593, 'eval_precision@eng.sdrt.stac': 0.3219439201513197, 'eval_recall@eng.sdrt.stac': 0.2575377177074958, 'eval_loss@eng.sdrt.stac': 1.5214004516601562, 'eval_runtime': 13.7131, 'eval_samples_per_second': 83.497, 'eval_steps_per_second': 2.625, 'epoch': 8.0}
{'train@eng.sdrt.stac_loss': 1.5242242813110352, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.518580375782881, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.28365849396092574, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.3708201191888753, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.29244447955463704, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5242244005203247, 'train@eng.sdrt.stac_runtime': 111.9911, 'train@eng.sdrt.stac_samples_per_second': 85.543, 'train@eng.sdrt.stac_steps_per_second': 2.679, 'epoch': 9.0}
{'loss': 1.5773, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.514259934425354, 'eval_accuracy@eng.sdrt.stac': 0.5222707423580786, 'eval_f1@eng.sdrt.stac': 0.2476082671785383, 'eval_precision@eng.sdrt.stac': 0.3094899712150628, 'eval_recall@eng.sdrt.stac': 0.2638490023254023, 'eval_loss@eng.sdrt.stac': 1.514260172843933, 'eval_runtime': 13.7458, 'eval_samples_per_second': 83.298, 'eval_steps_per_second': 2.619, 'epoch': 9.0}
{'train@eng.sdrt.stac_loss': 1.5069234371185303, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5248434237995825, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.29157539790883047, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.3696224216126106, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.30222900804672254, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5069234371185303, 'train@eng.sdrt.stac_runtime': 111.9222, 'train@eng.sdrt.stac_samples_per_second': 85.595, 'train@eng.sdrt.stac_steps_per_second': 2.68, 'epoch': 10.0}
{'loss': 1.5672, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.5005214214324951, 'eval_accuracy@eng.sdrt.stac': 0.5275109170305677, 'eval_f1@eng.sdrt.stac': 0.25517513846425005, 'eval_precision@eng.sdrt.stac': 0.3076998259504361, 'eval_recall@eng.sdrt.stac': 0.27298494719494204, 'eval_loss@eng.sdrt.stac': 1.5005213022232056, 'eval_runtime': 13.7261, 'eval_samples_per_second': 83.418, 'eval_steps_per_second': 2.623, 'epoch': 10.0}
{'train@eng.sdrt.stac_loss': 1.498244285583496, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5281837160751566, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.29816943740249585, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.3720399984657321, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3096778909839562, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.498244285583496, 'train@eng.sdrt.stac_runtime': 112.0968, 'train@eng.sdrt.stac_samples_per_second': 85.462, 'train@eng.sdrt.stac_steps_per_second': 2.676, 'epoch': 11.0}
{'loss': 1.5516, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.4962708950042725, 'eval_accuracy@eng.sdrt.stac': 0.5310043668122271, 'eval_f1@eng.sdrt.stac': 0.2670547177852137, 'eval_precision@eng.sdrt.stac': 0.3251470066788137, 'eval_recall@eng.sdrt.stac': 0.28274317154109097, 'eval_loss@eng.sdrt.stac': 1.496270775794983, 'eval_runtime': 13.7733, 'eval_samples_per_second': 83.132, 'eval_steps_per_second': 2.614, 'epoch': 11.0}
{'train@eng.sdrt.stac_loss': 1.4975084066390991, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5288100208768267, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.299883608836568, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.3699749231388533, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.31145540091572366, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4975086450576782, 'train@eng.sdrt.stac_runtime': 111.8746, 'train@eng.sdrt.stac_samples_per_second': 85.632, 'train@eng.sdrt.stac_steps_per_second': 2.682, 'epoch': 12.0}
{'loss': 1.549, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.4949936866760254, 'eval_accuracy@eng.sdrt.stac': 0.5283842794759825, 'eval_f1@eng.sdrt.stac': 0.25958771447792056, 'eval_precision@eng.sdrt.stac': 0.3096972816808231, 'eval_recall@eng.sdrt.stac': 0.27807265255882946, 'eval_loss@eng.sdrt.stac': 1.4949936866760254, 'eval_runtime': 13.6977, 'eval_samples_per_second': 83.59, 'eval_steps_per_second': 2.628, 'epoch': 12.0}
{'train_runtime': 4356.6507, 'train_samples_per_second': 26.387, 'train_steps_per_second': 0.826, 'train_loss': 1.7497192213270398, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.7497
  train_runtime            = 1:12:36.65
  train_samples_per_second =     26.387
  train_steps_per_second   =      0.826
{'train@fra.sdrt.annodis_loss': 2.531203031539917, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.18947368421052632, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.038624357827553774, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.04002620883452976, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.061207504763771015, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.531203031539917, 'train@fra.sdrt.annodis_runtime': 26.3358, 'train@fra.sdrt.annodis_samples_per_second': 82.967, 'train@fra.sdrt.annodis_steps_per_second': 2.62, 'epoch': 1.0}
{'loss': 2.8971, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.5692436695098877, 'eval_accuracy@fra.sdrt.annodis': 0.1893939393939394, 'eval_f1@fra.sdrt.annodis': 0.03480986083547049, 'eval_precision@fra.sdrt.annodis': 0.046319370606651304, 'eval_recall@fra.sdrt.annodis': 0.05866059898597108, 'eval_loss@fra.sdrt.annodis': 2.5692436695098877, 'eval_runtime': 6.6239, 'eval_samples_per_second': 79.712, 'eval_steps_per_second': 2.566, 'epoch': 1.0}
{'train@fra.sdrt.annodis_loss': 2.3328051567077637, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.27185354691075514, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.06008728296072685, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.09059584281989003, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.08207606553126955, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.3328053951263428, 'train@fra.sdrt.annodis_runtime': 26.3074, 'train@fra.sdrt.annodis_samples_per_second': 83.056, 'train@fra.sdrt.annodis_steps_per_second': 2.623, 'epoch': 2.0}
{'loss': 2.4483, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.3572752475738525, 'eval_accuracy@fra.sdrt.annodis': 0.26325757575757575, 'eval_f1@fra.sdrt.annodis': 0.059682584509074786, 'eval_precision@fra.sdrt.annodis': 0.06651610843643768, 'eval_recall@fra.sdrt.annodis': 0.07840577947580522, 'eval_loss@fra.sdrt.annodis': 2.3572750091552734, 'eval_runtime': 6.6051, 'eval_samples_per_second': 79.938, 'eval_steps_per_second': 2.574, 'epoch': 2.0}
{'train@fra.sdrt.annodis_loss': 2.229388475418091, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.3139588100686499, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.07553320361849859, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.08381315526565941, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.0956422966468786, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.22938871383667, 'train@fra.sdrt.annodis_runtime': 26.2973, 'train@fra.sdrt.annodis_samples_per_second': 83.088, 'train@fra.sdrt.annodis_steps_per_second': 2.624, 'epoch': 3.0}
{'loss': 2.3097, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.251004219055176, 'eval_accuracy@fra.sdrt.annodis': 0.2916666666666667, 'eval_f1@fra.sdrt.annodis': 0.07916211457497441, 'eval_precision@fra.sdrt.annodis': 0.11793633278227117, 'eval_recall@fra.sdrt.annodis': 0.09319572951584178, 'eval_loss@fra.sdrt.annodis': 2.2510037422180176, 'eval_runtime': 6.6506, 'eval_samples_per_second': 79.392, 'eval_steps_per_second': 2.556, 'epoch': 3.0}
{'train@fra.sdrt.annodis_loss': 2.1505136489868164, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.34508009153318076, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.09519529697776007, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.10915449659249106, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1104908051640412, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.1505136489868164, 'train@fra.sdrt.annodis_runtime': 26.2779, 'train@fra.sdrt.annodis_samples_per_second': 83.15, 'train@fra.sdrt.annodis_steps_per_second': 2.626, 'epoch': 4.0}
{'loss': 2.224, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.17816162109375, 'eval_accuracy@fra.sdrt.annodis': 0.3162878787878788, 'eval_f1@fra.sdrt.annodis': 0.09480238147271518, 'eval_precision@fra.sdrt.annodis': 0.11512479277774704, 'eval_recall@fra.sdrt.annodis': 0.10511068550674861, 'eval_loss@fra.sdrt.annodis': 2.178161859512329, 'eval_runtime': 6.6159, 'eval_samples_per_second': 79.807, 'eval_steps_per_second': 2.57, 'epoch': 4.0}
{'train@fra.sdrt.annodis_loss': 2.0738468170166016, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.3739130434782609, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.10670271664910341, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.1122154518990861, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.12832855760511208, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.0738468170166016, 'train@fra.sdrt.annodis_runtime': 26.2842, 'train@fra.sdrt.annodis_samples_per_second': 83.13, 'train@fra.sdrt.annodis_steps_per_second': 2.625, 'epoch': 5.0}
{'loss': 2.1606, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.1122164726257324, 'eval_accuracy@fra.sdrt.annodis': 0.3428030303030303, 'eval_f1@fra.sdrt.annodis': 0.10638956932799706, 'eval_precision@fra.sdrt.annodis': 0.13351183842134068, 'eval_recall@fra.sdrt.annodis': 0.12140010215704554, 'eval_loss@fra.sdrt.annodis': 2.1122164726257324, 'eval_runtime': 6.611, 'eval_samples_per_second': 79.867, 'eval_steps_per_second': 2.571, 'epoch': 5.0}
{'train@fra.sdrt.annodis_loss': 2.012795925140381, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.40411899313501143, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.12474118040074877, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.11636448989284338, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1481732751878631, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.012795925140381, 'train@fra.sdrt.annodis_runtime': 26.3198, 'train@fra.sdrt.annodis_samples_per_second': 83.017, 'train@fra.sdrt.annodis_steps_per_second': 2.622, 'epoch': 6.0}
{'loss': 2.1022, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.0607988834381104, 'eval_accuracy@fra.sdrt.annodis': 0.35795454545454547, 'eval_f1@fra.sdrt.annodis': 0.1181032247113075, 'eval_precision@fra.sdrt.annodis': 0.1306529744459995, 'eval_recall@fra.sdrt.annodis': 0.13291491770143266, 'eval_loss@fra.sdrt.annodis': 2.0607991218566895, 'eval_runtime': 6.629, 'eval_samples_per_second': 79.651, 'eval_steps_per_second': 2.565, 'epoch': 6.0}
{'train@fra.sdrt.annodis_loss': 1.9607890844345093, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4237986270022883, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.13428068775016552, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12286941619186992, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.16191641569613685, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.9607890844345093, 'train@fra.sdrt.annodis_runtime': 26.2761, 'train@fra.sdrt.annodis_samples_per_second': 83.156, 'train@fra.sdrt.annodis_steps_per_second': 2.626, 'epoch': 7.0}
{'loss': 2.0485, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.0160915851593018, 'eval_accuracy@fra.sdrt.annodis': 0.36742424242424243, 'eval_f1@fra.sdrt.annodis': 0.12234874148029107, 'eval_precision@fra.sdrt.annodis': 0.13109659287208808, 'eval_recall@fra.sdrt.annodis': 0.1391137794018927, 'eval_loss@fra.sdrt.annodis': 2.0160915851593018, 'eval_runtime': 6.6006, 'eval_samples_per_second': 79.993, 'eval_steps_per_second': 2.576, 'epoch': 7.0}
{'train@fra.sdrt.annodis_loss': 1.9196604490280151, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.43386727688787186, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.14321458214845262, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.20834292387433326, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.17133360143765877, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.9196606874465942, 'train@fra.sdrt.annodis_runtime': 26.278, 'train@fra.sdrt.annodis_samples_per_second': 83.149, 'train@fra.sdrt.annodis_steps_per_second': 2.626, 'epoch': 8.0}
{'loss': 2.0102, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.982438087463379, 'eval_accuracy@fra.sdrt.annodis': 0.3731060606060606, 'eval_f1@fra.sdrt.annodis': 0.12660890813782266, 'eval_precision@fra.sdrt.annodis': 0.13210772353700623, 'eval_recall@fra.sdrt.annodis': 0.1444110229467356, 'eval_loss@fra.sdrt.annodis': 1.982438087463379, 'eval_runtime': 6.5819, 'eval_samples_per_second': 80.22, 'eval_steps_per_second': 2.583, 'epoch': 8.0}
{'train@fra.sdrt.annodis_loss': 1.8889943361282349, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.43661327231121283, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.1497640090011524, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.2731184948081459, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.17553758593074387, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.8889943361282349, 'train@fra.sdrt.annodis_runtime': 26.2928, 'train@fra.sdrt.annodis_samples_per_second': 83.103, 'train@fra.sdrt.annodis_steps_per_second': 2.624, 'epoch': 9.0}
{'loss': 1.965, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.9581726789474487, 'eval_accuracy@fra.sdrt.annodis': 0.38446969696969696, 'eval_f1@fra.sdrt.annodis': 0.14493398623381554, 'eval_precision@fra.sdrt.annodis': 0.23501935556097317, 'eval_recall@fra.sdrt.annodis': 0.1551063334096927, 'eval_loss@fra.sdrt.annodis': 1.9581726789474487, 'eval_runtime': 6.5778, 'eval_samples_per_second': 80.27, 'eval_steps_per_second': 2.584, 'epoch': 9.0}
{'train@fra.sdrt.annodis_loss': 1.8682026863098145, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4453089244851259, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.16111955117091936, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.24998767888951404, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.18304473551106415, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.868202805519104, 'train@fra.sdrt.annodis_runtime': 26.2831, 'train@fra.sdrt.annodis_samples_per_second': 83.133, 'train@fra.sdrt.annodis_steps_per_second': 2.625, 'epoch': 10.0}
{'loss': 1.9469, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.9422686100006104, 'eval_accuracy@fra.sdrt.annodis': 0.38636363636363635, 'eval_f1@fra.sdrt.annodis': 0.15064988238769322, 'eval_precision@fra.sdrt.annodis': 0.23373167307055295, 'eval_recall@fra.sdrt.annodis': 0.15902733357205814, 'eval_loss@fra.sdrt.annodis': 1.9422688484191895, 'eval_runtime': 6.6027, 'eval_samples_per_second': 79.968, 'eval_steps_per_second': 2.575, 'epoch': 10.0}
{'train@fra.sdrt.annodis_loss': 1.8563827276229858, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.45217391304347826, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.16983317324285888, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.2623962348463967, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.19098361231148206, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.8563827276229858, 'train@fra.sdrt.annodis_runtime': 26.2333, 'train@fra.sdrt.annodis_samples_per_second': 83.291, 'train@fra.sdrt.annodis_steps_per_second': 2.63, 'epoch': 11.0}
{'loss': 1.9222, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.9316117763519287, 'eval_accuracy@fra.sdrt.annodis': 0.38446969696969696, 'eval_f1@fra.sdrt.annodis': 0.1502912858786403, 'eval_precision@fra.sdrt.annodis': 0.23242022804747992, 'eval_recall@fra.sdrt.annodis': 0.158433281576137, 'eval_loss@fra.sdrt.annodis': 1.9316120147705078, 'eval_runtime': 6.608, 'eval_samples_per_second': 79.904, 'eval_steps_per_second': 2.573, 'epoch': 11.0}
{'train@fra.sdrt.annodis_loss': 1.852242112159729, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.45491990846681923, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.17322842997302013, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.23463712442482862, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.19348149834595346, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.852242112159729, 'train@fra.sdrt.annodis_runtime': 26.2597, 'train@fra.sdrt.annodis_samples_per_second': 83.207, 'train@fra.sdrt.annodis_steps_per_second': 2.628, 'epoch': 12.0}
{'loss': 1.9148, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.9283808469772339, 'eval_accuracy@fra.sdrt.annodis': 0.38636363636363635, 'eval_f1@fra.sdrt.annodis': 0.15106674016318106, 'eval_precision@fra.sdrt.annodis': 0.2332344610122388, 'eval_recall@fra.sdrt.annodis': 0.15895249237572162, 'eval_loss@fra.sdrt.annodis': 1.9283809661865234, 'eval_runtime': 6.5994, 'eval_samples_per_second': 80.008, 'eval_steps_per_second': 2.576, 'epoch': 12.0}
{'train_runtime': 1058.2001, 'train_samples_per_second': 24.778, 'train_steps_per_second': 0.782, 'train_loss': 2.1624503020503094, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.7497
  train_runtime            = 1:12:36.65
  train_samples_per_second =     26.387
  train_steps_per_second   =      0.826
-------------------------------------------------------------------
Lang1:  fas.rst.prstc    Lang2:  fra.sdrt.annodis
Saving run to:  runs/full_shot/FullShot=v4_fas.rst.prstc_fra.sdrt.annodis_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 4100 examples
read 499 examples
read 592 examples
read 2185 examples
read 528 examples
read 625 examples
Total prediction labels:  30
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (ar): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (fr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=30, bias=True)
    )
  )
)
{'train@fas.rst.prstc_loss': 2.393813371658325, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.23780487804878048, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.02260214430599826, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.013988522238163558, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.058823529411764705, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.393813133239746, 'train@fas.rst.prstc_runtime': 48.0351, 'train@fas.rst.prstc_samples_per_second': 85.354, 'train@fas.rst.prstc_steps_per_second': 2.686, 'epoch': 1.0}
{'loss': 2.6915, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.3161933422088623, 'eval_accuracy@fas.rst.prstc': 0.24448897795591182, 'eval_f1@fas.rst.prstc': 0.026194310252281264, 'eval_precision@fas.rst.prstc': 0.016299265197060788, 'eval_recall@fas.rst.prstc': 0.06666666666666667, 'eval_loss@fas.rst.prstc': 2.3161933422088623, 'eval_runtime': 6.1177, 'eval_samples_per_second': 81.566, 'eval_steps_per_second': 2.615, 'epoch': 1.0}
{'train@fas.rst.prstc_loss': 2.351475715637207, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.28, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04517197463117767, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03281340172636284, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07337312666474118, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.351475477218628, 'train@fas.rst.prstc_runtime': 48.0667, 'train@fas.rst.prstc_samples_per_second': 85.298, 'train@fas.rst.prstc_steps_per_second': 2.684, 'epoch': 2.0}
{'loss': 2.3911, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.2655386924743652, 'eval_accuracy@fas.rst.prstc': 0.30861723446893785, 'eval_f1@fas.rst.prstc': 0.05546055387004178, 'eval_precision@fas.rst.prstc': 0.04168914712595974, 'eval_recall@fas.rst.prstc': 0.08614872891423139, 'eval_loss@fas.rst.prstc': 2.265538454055786, 'eval_runtime': 6.1316, 'eval_samples_per_second': 81.382, 'eval_steps_per_second': 2.609, 'epoch': 2.0}
{'train@fas.rst.prstc_loss': 2.335223436355591, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.26, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.03583164510887671, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03353456632678628, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.06551886439245638, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.335223913192749, 'train@fas.rst.prstc_runtime': 48.1036, 'train@fas.rst.prstc_samples_per_second': 85.233, 'train@fas.rst.prstc_steps_per_second': 2.682, 'epoch': 3.0}
{'loss': 2.3541, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.250354290008545, 'eval_accuracy@fas.rst.prstc': 0.28256513026052105, 'eval_f1@fas.rst.prstc': 0.04638536686759509, 'eval_precision@fas.rst.prstc': 0.04665847665847666, 'eval_recall@fas.rst.prstc': 0.07784746970776907, 'eval_loss@fas.rst.prstc': 2.250354766845703, 'eval_runtime': 6.1147, 'eval_samples_per_second': 81.607, 'eval_steps_per_second': 2.617, 'epoch': 3.0}
{'train@fas.rst.prstc_loss': 2.318711042404175, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.24, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.0241728451410593, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03429022789056626, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.059467710706759526, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.318711280822754, 'train@fas.rst.prstc_runtime': 48.0632, 'train@fas.rst.prstc_samples_per_second': 85.304, 'train@fas.rst.prstc_steps_per_second': 2.684, 'epoch': 4.0}
{'loss': 2.343, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.2389979362487793, 'eval_accuracy@fas.rst.prstc': 0.2545090180360721, 'eval_f1@fas.rst.prstc': 0.03195742332817144, 'eval_precision@fas.rst.prstc': 0.06415021293070074, 'eval_recall@fas.rst.prstc': 0.06956521739130435, 'eval_loss@fas.rst.prstc': 2.2389976978302, 'eval_runtime': 6.1092, 'eval_samples_per_second': 81.681, 'eval_steps_per_second': 2.619, 'epoch': 4.0}
{'train@fas.rst.prstc_loss': 2.2945268154144287, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.29048780487804876, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.045437419678966445, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.0353539328239138, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07475305670549726, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.294527292251587, 'train@fas.rst.prstc_runtime': 48.0595, 'train@fas.rst.prstc_samples_per_second': 85.311, 'train@fas.rst.prstc_steps_per_second': 2.684, 'epoch': 5.0}
{'loss': 2.3273, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.21323561668396, 'eval_accuracy@fas.rst.prstc': 0.31462925851703405, 'eval_f1@fas.rst.prstc': 0.055491698595146875, 'eval_precision@fas.rst.prstc': 0.04518727345952477, 'eval_recall@fas.rst.prstc': 0.08728914231408887, 'eval_loss@fas.rst.prstc': 2.213235378265381, 'eval_runtime': 6.1217, 'eval_samples_per_second': 81.513, 'eval_steps_per_second': 2.614, 'epoch': 5.0}
{'train@fas.rst.prstc_loss': 2.2431352138519287, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3021951219512195, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04793231186127968, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.035325788309980156, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07844934373094574, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.2431349754333496, 'train@fas.rst.prstc_runtime': 48.0875, 'train@fas.rst.prstc_samples_per_second': 85.261, 'train@fas.rst.prstc_steps_per_second': 2.683, 'epoch': 6.0}
{'loss': 2.2906, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.1556708812713623, 'eval_accuracy@fas.rst.prstc': 0.3046092184368738, 'eval_f1@fas.rst.prstc': 0.053876102969745196, 'eval_precision@fas.rst.prstc': 0.0416352249685583, 'eval_recall@fas.rst.prstc': 0.08468995010691376, 'eval_loss@fas.rst.prstc': 2.1556711196899414, 'eval_runtime': 6.1088, 'eval_samples_per_second': 81.686, 'eval_steps_per_second': 2.619, 'epoch': 6.0}
{'train@fas.rst.prstc_loss': 2.2014267444610596, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.32121951219512196, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.05321568624733318, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03951821291888273, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08527111881304622, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.2014267444610596, 'train@fas.rst.prstc_runtime': 48.0985, 'train@fas.rst.prstc_samples_per_second': 85.242, 'train@fas.rst.prstc_steps_per_second': 2.682, 'epoch': 7.0}
{'loss': 2.2508, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.106421709060669, 'eval_accuracy@fas.rst.prstc': 0.33867735470941884, 'eval_f1@fas.rst.prstc': 0.061946161149659514, 'eval_precision@fas.rst.prstc': 0.0463762419478109, 'eval_recall@fas.rst.prstc': 0.09501069137562367, 'eval_loss@fas.rst.prstc': 2.106421709060669, 'eval_runtime': 7.2115, 'eval_samples_per_second': 69.195, 'eval_steps_per_second': 2.219, 'epoch': 7.0}
{'train@fas.rst.prstc_loss': 2.1735377311706543, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3309756097560976, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.055431358542738986, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.042072955357265275, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08819956569643678, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1735377311706543, 'train@fas.rst.prstc_runtime': 48.0883, 'train@fas.rst.prstc_samples_per_second': 85.26, 'train@fas.rst.prstc_steps_per_second': 2.683, 'epoch': 8.0}
{'loss': 2.218, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.0817296504974365, 'eval_accuracy@fas.rst.prstc': 0.35070140280561124, 'eval_f1@fas.rst.prstc': 0.06495529392725655, 'eval_precision@fas.rst.prstc': 0.04978070928807814, 'eval_recall@fas.rst.prstc': 0.09858873841767642, 'eval_loss@fas.rst.prstc': 2.0817296504974365, 'eval_runtime': 6.1585, 'eval_samples_per_second': 81.026, 'eval_steps_per_second': 2.598, 'epoch': 8.0}
{'train@fas.rst.prstc_loss': 2.154188632965088, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.34, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.057601469605377584, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.08140649627622075, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.0910060912779781, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.154189109802246, 'train@fas.rst.prstc_runtime': 48.0498, 'train@fas.rst.prstc_samples_per_second': 85.328, 'train@fas.rst.prstc_steps_per_second': 2.685, 'epoch': 9.0}
{'loss': 2.1929, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.0656425952911377, 'eval_accuracy@fas.rst.prstc': 0.3587174348697395, 'eval_f1@fas.rst.prstc': 0.06902726902726904, 'eval_precision@fas.rst.prstc': 0.116482346612305, 'eval_recall@fas.rst.prstc': 0.10208225481737129, 'eval_loss@fas.rst.prstc': 2.065643072128296, 'eval_runtime': 6.1164, 'eval_samples_per_second': 81.584, 'eval_steps_per_second': 2.616, 'epoch': 9.0}
{'train@fas.rst.prstc_loss': 2.1349902153015137, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3463414634146341, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.06257772186831871, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.07979780429912166, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.09415317808544149, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1349899768829346, 'train@fas.rst.prstc_runtime': 48.0713, 'train@fas.rst.prstc_samples_per_second': 85.29, 'train@fas.rst.prstc_steps_per_second': 2.684, 'epoch': 10.0}
{'loss': 2.1741, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.053680181503296, 'eval_accuracy@fas.rst.prstc': 0.3667334669338677, 'eval_f1@fas.rst.prstc': 0.0762116051258262, 'eval_precision@fas.rst.prstc': 0.11704308390022676, 'eval_recall@fas.rst.prstc': 0.10668392292205926, 'eval_loss@fas.rst.prstc': 2.053679943084717, 'eval_runtime': 6.1118, 'eval_samples_per_second': 81.645, 'eval_steps_per_second': 2.618, 'epoch': 10.0}
{'train@fas.rst.prstc_loss': 2.1239006519317627, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3519512195121951, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.06647305549611213, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.07721173527336239, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.09694253979347915, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.123900890350342, 'train@fas.rst.prstc_runtime': 48.0874, 'train@fas.rst.prstc_samples_per_second': 85.261, 'train@fas.rst.prstc_steps_per_second': 2.683, 'epoch': 11.0}
{'loss': 2.1558, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.046135425567627, 'eval_accuracy@fas.rst.prstc': 0.3687374749498998, 'eval_f1@fas.rst.prstc': 0.07662311027991438, 'eval_precision@fas.rst.prstc': 0.11743701781934868, 'eval_recall@fas.rst.prstc': 0.10726363306698679, 'eval_loss@fas.rst.prstc': 2.046135425567627, 'eval_runtime': 6.1577, 'eval_samples_per_second': 81.036, 'eval_steps_per_second': 2.598, 'epoch': 11.0}
{'train@fas.rst.prstc_loss': 2.123091220855713, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3526829268292683, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.06787005712998435, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.07415089207903529, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.09771909011967372, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.123090982437134, 'train@fas.rst.prstc_runtime': 48.1054, 'train@fas.rst.prstc_samples_per_second': 85.23, 'train@fas.rst.prstc_steps_per_second': 2.682, 'epoch': 12.0}
{'loss': 2.1515, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.046574592590332, 'eval_accuracy@fas.rst.prstc': 0.3667334669338677, 'eval_f1@fas.rst.prstc': 0.07588143210624652, 'eval_precision@fas.rst.prstc': 0.10012429378531072, 'eval_recall@fas.rst.prstc': 0.10668392292205926, 'eval_loss@fas.rst.prstc': 2.046574592590332, 'eval_runtime': 6.1692, 'eval_samples_per_second': 80.885, 'eval_steps_per_second': 2.594, 'epoch': 12.0}
{'train_runtime': 1871.4968, 'train_samples_per_second': 26.289, 'train_steps_per_second': 0.827, 'train_loss': 2.29506265408617, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.2951
  train_runtime            = 0:31:11.49
  train_samples_per_second =     26.289
  train_steps_per_second   =      0.827
{'train@fra.sdrt.annodis_loss': 2.411970376968384, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.2288329519450801, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.04195873614728393, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.041019735805576246, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.06481507921167108, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.411970615386963, 'train@fra.sdrt.annodis_runtime': 26.2357, 'train@fra.sdrt.annodis_samples_per_second': 83.283, 'train@fra.sdrt.annodis_steps_per_second': 2.63, 'epoch': 1.0}
{'loss': 2.6156, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.427748441696167, 'eval_accuracy@fra.sdrt.annodis': 0.19507575757575757, 'eval_f1@fra.sdrt.annodis': 0.0324813783704421, 'eval_precision@fra.sdrt.annodis': 0.02941844794467167, 'eval_recall@fra.sdrt.annodis': 0.05436467966755263, 'eval_loss@fra.sdrt.annodis': 2.427748441696167, 'eval_runtime': 6.5664, 'eval_samples_per_second': 80.41, 'eval_steps_per_second': 2.589, 'epoch': 1.0}
{'train@fra.sdrt.annodis_loss': 2.305633783340454, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.27414187643020593, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.059684136539563934, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.06877297429330861, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.08207053659944731, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.305633783340454, 'train@fra.sdrt.annodis_runtime': 26.2037, 'train@fra.sdrt.annodis_samples_per_second': 83.385, 'train@fra.sdrt.annodis_steps_per_second': 2.633, 'epoch': 2.0}
{'loss': 2.3808, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.33963942527771, 'eval_accuracy@fra.sdrt.annodis': 0.2537878787878788, 'eval_f1@fra.sdrt.annodis': 0.05401075327578179, 'eval_precision@fra.sdrt.annodis': 0.042541442733726634, 'eval_recall@fra.sdrt.annodis': 0.07438002211280162, 'eval_loss@fra.sdrt.annodis': 2.339639663696289, 'eval_runtime': 6.5697, 'eval_samples_per_second': 80.369, 'eval_steps_per_second': 2.588, 'epoch': 2.0}
{'train@fra.sdrt.annodis_loss': 2.2401320934295654, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.3025171624713959, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.06988510612011983, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.07903702579812341, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.09086492649139816, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.2401320934295654, 'train@fra.sdrt.annodis_runtime': 26.2471, 'train@fra.sdrt.annodis_samples_per_second': 83.247, 'train@fra.sdrt.annodis_steps_per_second': 2.629, 'epoch': 3.0}
{'loss': 2.2999, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.288193941116333, 'eval_accuracy@fra.sdrt.annodis': 0.26325757575757575, 'eval_f1@fra.sdrt.annodis': 0.05598386217715369, 'eval_precision@fra.sdrt.annodis': 0.04618235955397563, 'eval_recall@fra.sdrt.annodis': 0.07658835713525917, 'eval_loss@fra.sdrt.annodis': 2.288193702697754, 'eval_runtime': 6.5514, 'eval_samples_per_second': 80.593, 'eval_steps_per_second': 2.595, 'epoch': 3.0}
{'train@fra.sdrt.annodis_loss': 2.1854350566864014, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.31899313501144166, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.08080526458523356, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.08699986399712664, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.09856331048386648, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.1854352951049805, 'train@fra.sdrt.annodis_runtime': 26.2643, 'train@fra.sdrt.annodis_samples_per_second': 83.193, 'train@fra.sdrt.annodis_steps_per_second': 2.627, 'epoch': 4.0}
{'loss': 2.2497, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.2473862171173096, 'eval_accuracy@fra.sdrt.annodis': 0.2708333333333333, 'eval_f1@fra.sdrt.annodis': 0.059316204370758005, 'eval_precision@fra.sdrt.annodis': 0.049473165451443014, 'eval_recall@fra.sdrt.annodis': 0.07901549871089503, 'eval_loss@fra.sdrt.annodis': 2.2473864555358887, 'eval_runtime': 6.5739, 'eval_samples_per_second': 80.318, 'eval_steps_per_second': 2.586, 'epoch': 4.0}
{'train@fra.sdrt.annodis_loss': 2.135810136795044, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.34416475972540045, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.09190102729081175, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.11308934867323063, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.11182870798674362, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.135810375213623, 'train@fra.sdrt.annodis_runtime': 26.2156, 'train@fra.sdrt.annodis_samples_per_second': 83.347, 'train@fra.sdrt.annodis_steps_per_second': 2.632, 'epoch': 5.0}
{'loss': 2.2, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.213679075241089, 'eval_accuracy@fra.sdrt.annodis': 0.2878787878787879, 'eval_f1@fra.sdrt.annodis': 0.06192445307825694, 'eval_precision@fra.sdrt.annodis': 0.04893688012627257, 'eval_recall@fra.sdrt.annodis': 0.0866151717657429, 'eval_loss@fra.sdrt.annodis': 2.213679075241089, 'eval_runtime': 6.56, 'eval_samples_per_second': 80.488, 'eval_steps_per_second': 2.591, 'epoch': 5.0}
{'train@fra.sdrt.annodis_loss': 2.0926504135131836, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.3707093821510298, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.11439194479516596, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.11895437151449542, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1306230252605996, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.0926504135131836, 'train@fra.sdrt.annodis_runtime': 26.2553, 'train@fra.sdrt.annodis_samples_per_second': 83.221, 'train@fra.sdrt.annodis_steps_per_second': 2.628, 'epoch': 6.0}
{'loss': 2.1567, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.1796326637268066, 'eval_accuracy@fra.sdrt.annodis': 0.2916666666666667, 'eval_f1@fra.sdrt.annodis': 0.07199075149481654, 'eval_precision@fra.sdrt.annodis': 0.0785723924009211, 'eval_recall@fra.sdrt.annodis': 0.09203847123188412, 'eval_loss@fra.sdrt.annodis': 2.1796326637268066, 'eval_runtime': 6.559, 'eval_samples_per_second': 80.5, 'eval_steps_per_second': 2.592, 'epoch': 6.0}
{'train@fra.sdrt.annodis_loss': 2.053711175918579, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.3903890160183066, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.12458707118574013, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12296714759834032, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.14104482645099348, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.053711175918579, 'train@fra.sdrt.annodis_runtime': 26.2228, 'train@fra.sdrt.annodis_samples_per_second': 83.325, 'train@fra.sdrt.annodis_steps_per_second': 2.631, 'epoch': 7.0}
{'loss': 2.12, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.1503841876983643, 'eval_accuracy@fra.sdrt.annodis': 0.30303030303030304, 'eval_f1@fra.sdrt.annodis': 0.08190125939305837, 'eval_precision@fra.sdrt.annodis': 0.08459550834760567, 'eval_recall@fra.sdrt.annodis': 0.09874100077614775, 'eval_loss@fra.sdrt.annodis': 2.1503841876983643, 'eval_runtime': 6.5911, 'eval_samples_per_second': 80.109, 'eval_steps_per_second': 2.579, 'epoch': 7.0}
{'train@fra.sdrt.annodis_loss': 2.021191120147705, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.1301770433573935, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12122161213078668, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1499054602029062, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.021191358566284, 'train@fra.sdrt.annodis_runtime': 26.2125, 'train@fra.sdrt.annodis_samples_per_second': 83.357, 'train@fra.sdrt.annodis_steps_per_second': 2.632, 'epoch': 8.0}
{'loss': 2.0847, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.1269876956939697, 'eval_accuracy@fra.sdrt.annodis': 0.3181818181818182, 'eval_f1@fra.sdrt.annodis': 0.09121239923683852, 'eval_precision@fra.sdrt.annodis': 0.08950747675884498, 'eval_recall@fra.sdrt.annodis': 0.1075792468474739, 'eval_loss@fra.sdrt.annodis': 2.1269876956939697, 'eval_runtime': 6.5486, 'eval_samples_per_second': 80.628, 'eval_steps_per_second': 2.596, 'epoch': 8.0}
{'train@fra.sdrt.annodis_loss': 1.9969170093536377, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4064073226544622, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.13327608278459468, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12316411065625966, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.15505811156683066, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.9969167709350586, 'train@fra.sdrt.annodis_runtime': 26.2183, 'train@fra.sdrt.annodis_samples_per_second': 83.339, 'train@fra.sdrt.annodis_steps_per_second': 2.632, 'epoch': 9.0}
{'loss': 2.0586, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.1085407733917236, 'eval_accuracy@fra.sdrt.annodis': 0.32765151515151514, 'eval_f1@fra.sdrt.annodis': 0.09565042444170965, 'eval_precision@fra.sdrt.annodis': 0.09366634056075049, 'eval_recall@fra.sdrt.annodis': 0.1119785956429314, 'eval_loss@fra.sdrt.annodis': 2.1085407733917236, 'eval_runtime': 6.5911, 'eval_samples_per_second': 80.109, 'eval_steps_per_second': 2.579, 'epoch': 9.0}
{'train@fra.sdrt.annodis_loss': 1.9787462949752808, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.41510297482837527, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.13641813214420276, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12398851584909171, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.15941841906922857, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.9787462949752808, 'train@fra.sdrt.annodis_runtime': 26.2519, 'train@fra.sdrt.annodis_samples_per_second': 83.232, 'train@fra.sdrt.annodis_steps_per_second': 2.628, 'epoch': 10.0}
{'loss': 2.0371, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.0951130390167236, 'eval_accuracy@fra.sdrt.annodis': 0.32954545454545453, 'eval_f1@fra.sdrt.annodis': 0.09805218740996573, 'eval_precision@fra.sdrt.annodis': 0.09418270114776035, 'eval_recall@fra.sdrt.annodis': 0.11430110124005038, 'eval_loss@fra.sdrt.annodis': 2.0951130390167236, 'eval_runtime': 6.5792, 'eval_samples_per_second': 80.253, 'eval_steps_per_second': 2.584, 'epoch': 10.0}
{'train@fra.sdrt.annodis_loss': 1.9682013988494873, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.42013729977116704, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.1382166726178014, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12407198645195512, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.16294973625781117, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.9682013988494873, 'train@fra.sdrt.annodis_runtime': 26.2429, 'train@fra.sdrt.annodis_samples_per_second': 83.26, 'train@fra.sdrt.annodis_steps_per_second': 2.629, 'epoch': 11.0}
{'loss': 2.0103, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.086902141571045, 'eval_accuracy@fra.sdrt.annodis': 0.3333333333333333, 'eval_f1@fra.sdrt.annodis': 0.10023546434347735, 'eval_precision@fra.sdrt.annodis': 0.09489025731697424, 'eval_recall@fra.sdrt.annodis': 0.11673527047633254, 'eval_loss@fra.sdrt.annodis': 2.086902618408203, 'eval_runtime': 6.5546, 'eval_samples_per_second': 80.554, 'eval_steps_per_second': 2.594, 'epoch': 11.0}
{'train@fra.sdrt.annodis_loss': 1.9645442962646484, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4242562929061785, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.13925128532000638, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12492925825949228, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.16429225400575523, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.9645440578460693, 'train@fra.sdrt.annodis_runtime': 26.1706, 'train@fra.sdrt.annodis_samples_per_second': 83.491, 'train@fra.sdrt.annodis_steps_per_second': 2.637, 'epoch': 12.0}
{'loss': 2.0143, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.0843658447265625, 'eval_accuracy@fra.sdrt.annodis': 0.3333333333333333, 'eval_f1@fra.sdrt.annodis': 0.10115695371061315, 'eval_precision@fra.sdrt.annodis': 0.09583311984996032, 'eval_recall@fra.sdrt.annodis': 0.11742378914534697, 'eval_loss@fra.sdrt.annodis': 2.0843656063079834, 'eval_runtime': 6.5842, 'eval_samples_per_second': 80.192, 'eval_steps_per_second': 2.582, 'epoch': 12.0}
{'train_runtime': 1056.84, 'train_samples_per_second': 24.81, 'train_steps_per_second': 0.783, 'train_loss': 2.1856580909323577, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.2951
  train_runtime            = 0:31:11.49
  train_samples_per_second =     26.289
  train_steps_per_second   =      0.827
-------------------------------------------------------------------
Lang1:  nld.rst.nldt    Lang2:  fra.sdrt.annodis
Saving run to:  runs/full_shot/FullShot=v4_nld.rst.nldt_fra.sdrt.annodis_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 1608 examples
read 331 examples
read 326 examples
read 2185 examples
read 528 examples
read 625 examples
Total prediction labels:  48
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (fr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=48, bias=True)
    )
  )
)
{'train@nld.rst.nldt_loss': 3.352053165435791, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26430348258706465, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01306566650270536, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.00825948383084577, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03125, 'train@nld.rst.nldt_loss@nld.rst.nldt': 3.352052927017212, 'train@nld.rst.nldt_runtime': 19.4257, 'train@nld.rst.nldt_samples_per_second': 82.777, 'train@nld.rst.nldt_steps_per_second': 2.625, 'epoch': 1.0}
{'loss': 3.6363, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.307864189147949, 'eval_accuracy@nld.rst.nldt': 0.27794561933534745, 'eval_f1@nld.rst.nldt': 0.016110673321075214, 'eval_precision@nld.rst.nldt': 0.010294282197605462, 'eval_recall@nld.rst.nldt': 0.037037037037037035, 'eval_loss@nld.rst.nldt': 3.307864189147949, 'eval_runtime': 4.3401, 'eval_samples_per_second': 76.266, 'eval_steps_per_second': 2.535, 'epoch': 1.0}
{'train@nld.rst.nldt_loss': 2.9578511714935303, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26430348258706465, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01306566650270536, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.00825948383084577, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03125, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.9578511714935303, 'train@nld.rst.nldt_runtime': 19.5394, 'train@nld.rst.nldt_samples_per_second': 82.295, 'train@nld.rst.nldt_steps_per_second': 2.61, 'epoch': 2.0}
{'loss': 3.1379, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.8738112449645996, 'eval_accuracy@nld.rst.nldt': 0.27794561933534745, 'eval_f1@nld.rst.nldt': 0.016110673321075214, 'eval_precision@nld.rst.nldt': 0.010294282197605462, 'eval_recall@nld.rst.nldt': 0.037037037037037035, 'eval_loss@nld.rst.nldt': 2.8738110065460205, 'eval_runtime': 4.3515, 'eval_samples_per_second': 76.066, 'eval_steps_per_second': 2.528, 'epoch': 2.0}
{'train@nld.rst.nldt_loss': 2.8393797874450684, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26865671641791045, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01704180065670077, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.036239042462559413, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03333625116713352, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.8393795490264893, 'train@nld.rst.nldt_runtime': 19.5591, 'train@nld.rst.nldt_samples_per_second': 82.212, 'train@nld.rst.nldt_steps_per_second': 2.607, 'epoch': 3.0}
{'loss': 2.9223, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.761467456817627, 'eval_accuracy@nld.rst.nldt': 0.2809667673716012, 'eval_f1@nld.rst.nldt': 0.019929453262786598, 'eval_precision@nld.rst.nldt': 0.02890695573622403, 'eval_recall@nld.rst.nldt': 0.03909465020576132, 'eval_loss@nld.rst.nldt': 2.7614669799804688, 'eval_runtime': 4.3659, 'eval_samples_per_second': 75.815, 'eval_steps_per_second': 2.52, 'epoch': 3.0}
{'train@nld.rst.nldt_loss': 2.781054973602295, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.277363184079602, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.02350713085799832, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.02746413368228075, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.0383811858076564, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.781054735183716, 'train@nld.rst.nldt_runtime': 19.5586, 'train@nld.rst.nldt_samples_per_second': 82.214, 'train@nld.rst.nldt_steps_per_second': 2.608, 'epoch': 4.0}
{'loss': 2.8176, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.713482141494751, 'eval_accuracy@nld.rst.nldt': 0.2930513595166163, 'eval_f1@nld.rst.nldt': 0.030817946268691596, 'eval_precision@nld.rst.nldt': 0.02772610028130533, 'eval_recall@nld.rst.nldt': 0.0489801395598497, 'eval_loss@nld.rst.nldt': 2.713482141494751, 'eval_runtime': 4.3638, 'eval_samples_per_second': 75.851, 'eval_steps_per_second': 2.521, 'epoch': 4.0}
{'train@nld.rst.nldt_loss': 2.742918014526367, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2842039800995025, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.027473320324211355, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.02651969642023476, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.04291433239962651, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.742917776107788, 'train@nld.rst.nldt_runtime': 19.5333, 'train@nld.rst.nldt_samples_per_second': 82.321, 'train@nld.rst.nldt_steps_per_second': 2.611, 'epoch': 5.0}
{'loss': 2.7793, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.678521156311035, 'eval_accuracy@nld.rst.nldt': 0.2990936555891239, 'eval_f1@nld.rst.nldt': 0.0347250994091673, 'eval_precision@nld.rst.nldt': 0.03760673449417568, 'eval_recall@nld.rst.nldt': 0.052751578355443085, 'eval_loss@nld.rst.nldt': 2.678520917892456, 'eval_runtime': 4.4028, 'eval_samples_per_second': 75.18, 'eval_steps_per_second': 2.498, 'epoch': 5.0}
{'train@nld.rst.nldt_loss': 2.7065658569335938, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2935323383084577, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03160498027410544, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.027690079589010298, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.04873132586367881, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.706566095352173, 'train@nld.rst.nldt_runtime': 19.5128, 'train@nld.rst.nldt_samples_per_second': 82.408, 'train@nld.rst.nldt_steps_per_second': 2.614, 'epoch': 6.0}
{'loss': 2.7469, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.6492671966552734, 'eval_accuracy@nld.rst.nldt': 0.311178247734139, 'eval_f1@nld.rst.nldt': 0.03725089919315641, 'eval_precision@nld.rst.nldt': 0.034219210560041007, 'eval_recall@nld.rst.nldt': 0.05998261891981699, 'eval_loss@nld.rst.nldt': 2.6492671966552734, 'eval_runtime': 4.3533, 'eval_samples_per_second': 76.035, 'eval_steps_per_second': 2.527, 'epoch': 6.0}
{'train@nld.rst.nldt_loss': 2.6794252395629883, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2972636815920398, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03273374508354018, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.027046703956434513, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05120915032679739, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6794254779815674, 'train@nld.rst.nldt_runtime': 19.53, 'train@nld.rst.nldt_samples_per_second': 82.335, 'train@nld.rst.nldt_steps_per_second': 2.611, 'epoch': 7.0}
{'loss': 2.7156, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.6268577575683594, 'eval_accuracy@nld.rst.nldt': 0.31419939577039274, 'eval_f1@nld.rst.nldt': 0.03784936468322625, 'eval_precision@nld.rst.nldt': 0.03323276405956691, 'eval_recall@nld.rst.nldt': 0.06269585665720931, 'eval_loss@nld.rst.nldt': 2.6268579959869385, 'eval_runtime': 4.3912, 'eval_samples_per_second': 75.378, 'eval_steps_per_second': 2.505, 'epoch': 7.0}
{'train@nld.rst.nldt_loss': 2.6568477153778076, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30223880597014924, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03395112998781052, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.026590566878527427, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05459091970121382, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6568477153778076, 'train@nld.rst.nldt_runtime': 19.5554, 'train@nld.rst.nldt_samples_per_second': 82.228, 'train@nld.rst.nldt_steps_per_second': 2.608, 'epoch': 8.0}
{'loss': 2.701, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.610081672668457, 'eval_accuracy@nld.rst.nldt': 0.3202416918429003, 'eval_f1@nld.rst.nldt': 0.04096191646207035, 'eval_precision@nld.rst.nldt': 0.03642024802945181, 'eval_recall@nld.rst.nldt': 0.06546788334227947, 'eval_loss@nld.rst.nldt': 2.610081672668457, 'eval_runtime': 4.3649, 'eval_samples_per_second': 75.833, 'eval_steps_per_second': 2.52, 'epoch': 8.0}
{'train@nld.rst.nldt_loss': 2.642460346221924, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3034825870646766, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.034439635883633, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.026806078802623184, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05502859477124183, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.642460584640503, 'train@nld.rst.nldt_runtime': 19.5545, 'train@nld.rst.nldt_samples_per_second': 82.232, 'train@nld.rst.nldt_steps_per_second': 2.608, 'epoch': 9.0}
{'loss': 2.6768, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.598743200302124, 'eval_accuracy@nld.rst.nldt': 0.3202416918429003, 'eval_f1@nld.rst.nldt': 0.04096191646207035, 'eval_precision@nld.rst.nldt': 0.03642024802945181, 'eval_recall@nld.rst.nldt': 0.06546788334227947, 'eval_loss@nld.rst.nldt': 2.598743438720703, 'eval_runtime': 5.2133, 'eval_samples_per_second': 63.491, 'eval_steps_per_second': 2.11, 'epoch': 9.0}
{'train@nld.rst.nldt_loss': 2.632772922515869, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30286069651741293, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03434491072851905, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.026800521264843967, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.054780578898225955, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6327736377716064, 'train@nld.rst.nldt_runtime': 19.5838, 'train@nld.rst.nldt_samples_per_second': 82.109, 'train@nld.rst.nldt_steps_per_second': 2.604, 'epoch': 10.0}
{'loss': 2.661, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.5903284549713135, 'eval_accuracy@nld.rst.nldt': 0.3202416918429003, 'eval_f1@nld.rst.nldt': 0.04112064163748356, 'eval_precision@nld.rst.nldt': 0.036581134511417736, 'eval_recall@nld.rst.nldt': 0.06546788334227947, 'eval_loss@nld.rst.nldt': 2.5903284549713135, 'eval_runtime': 4.3781, 'eval_samples_per_second': 75.603, 'eval_steps_per_second': 2.512, 'epoch': 10.0}
{'train@nld.rst.nldt_loss': 2.625830888748169, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30659203980099503, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03551365486012739, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.02765298356767042, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.056268674136321195, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.625830888748169, 'train@nld.rst.nldt_runtime': 19.585, 'train@nld.rst.nldt_samples_per_second': 82.104, 'train@nld.rst.nldt_steps_per_second': 2.604, 'epoch': 11.0}
{'loss': 2.6551, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.584683895111084, 'eval_accuracy@nld.rst.nldt': 0.32326283987915405, 'eval_f1@nld.rst.nldt': 0.04182969036203913, 'eval_precision@nld.rst.nldt': 0.036272091447687164, 'eval_recall@nld.rst.nldt': 0.06752549651100376, 'eval_loss@nld.rst.nldt': 2.584683895111084, 'eval_runtime': 4.3715, 'eval_samples_per_second': 75.718, 'eval_steps_per_second': 2.516, 'epoch': 11.0}
{'train@nld.rst.nldt_loss': 2.623222589492798, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30970149253731344, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03632581782351747, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.02822855712566401, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.057742180205415494, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.623223066329956, 'train@nld.rst.nldt_runtime': 19.5363, 'train@nld.rst.nldt_samples_per_second': 82.308, 'train@nld.rst.nldt_steps_per_second': 2.611, 'epoch': 12.0}
{'loss': 2.6491, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.583162784576416, 'eval_accuracy@nld.rst.nldt': 0.32326283987915405, 'eval_f1@nld.rst.nldt': 0.04166673423551256, 'eval_precision@nld.rst.nldt': 0.03611241131460118, 'eval_recall@nld.rst.nldt': 0.06752549651100376, 'eval_loss@nld.rst.nldt': 2.583162784576416, 'eval_runtime': 4.3665, 'eval_samples_per_second': 75.805, 'eval_steps_per_second': 2.519, 'epoch': 12.0}
{'train_runtime': 775.5824, 'train_samples_per_second': 24.879, 'train_steps_per_second': 0.789, 'train_loss': 2.8415686164806093, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.8416
  train_runtime            = 0:12:55.58
  train_samples_per_second =     24.879
  train_steps_per_second   =      0.789
{'train@fra.sdrt.annodis_loss': 2.6285195350646973, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.2091533180778032, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.026529877794533793, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.02414050414050414, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.057898840972508116, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.6285195350646973, 'train@fra.sdrt.annodis_runtime': 26.3939, 'train@fra.sdrt.annodis_samples_per_second': 82.784, 'train@fra.sdrt.annodis_steps_per_second': 2.614, 'epoch': 1.0}
{'loss': 3.1964, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.6130735874176025, 'eval_accuracy@fra.sdrt.annodis': 0.20833333333333334, 'eval_f1@fra.sdrt.annodis': 0.024470739756902518, 'eval_precision@fra.sdrt.annodis': 0.025371828521434818, 'eval_recall@fra.sdrt.annodis': 0.05794648923759343, 'eval_loss@fra.sdrt.annodis': 2.6130735874176025, 'eval_runtime': 6.6958, 'eval_samples_per_second': 78.856, 'eval_steps_per_second': 2.539, 'epoch': 1.0}
{'train@fra.sdrt.annodis_loss': 2.3825137615203857, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.2759725400457666, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.05828649878461466, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.0516927402295143, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.08054054420613165, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.3825137615203857, 'train@fra.sdrt.annodis_runtime': 26.4105, 'train@fra.sdrt.annodis_samples_per_second': 82.732, 'train@fra.sdrt.annodis_steps_per_second': 2.613, 'epoch': 2.0}
{'loss': 2.5, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.3725497722625732, 'eval_accuracy@fra.sdrt.annodis': 0.2708333333333333, 'eval_f1@fra.sdrt.annodis': 0.05584991379102706, 'eval_precision@fra.sdrt.annodis': 0.04934938601955677, 'eval_recall@fra.sdrt.annodis': 0.076551283024111, 'eval_loss@fra.sdrt.annodis': 2.372549533843994, 'eval_runtime': 6.7297, 'eval_samples_per_second': 78.459, 'eval_steps_per_second': 2.526, 'epoch': 2.0}
{'train@fra.sdrt.annodis_loss': 2.3070437908172607, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.26636155606407325, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.05367527538574355, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.08417530871770953, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.07656815133092633, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.3070437908172607, 'train@fra.sdrt.annodis_runtime': 26.4026, 'train@fra.sdrt.annodis_samples_per_second': 82.757, 'train@fra.sdrt.annodis_steps_per_second': 2.613, 'epoch': 3.0}
{'loss': 2.3676, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.310638427734375, 'eval_accuracy@fra.sdrt.annodis': 0.26515151515151514, 'eval_f1@fra.sdrt.annodis': 0.052334818591769305, 'eval_precision@fra.sdrt.annodis': 0.051928545882375415, 'eval_recall@fra.sdrt.annodis': 0.0738083185296754, 'eval_loss@fra.sdrt.annodis': 2.310638666152954, 'eval_runtime': 6.7211, 'eval_samples_per_second': 78.559, 'eval_steps_per_second': 2.529, 'epoch': 3.0}
{'train@fra.sdrt.annodis_loss': 2.2504358291625977, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.2778032036613272, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.062033971831705954, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.09071341198908238, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.08157483438540143, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.2504360675811768, 'train@fra.sdrt.annodis_runtime': 26.3949, 'train@fra.sdrt.annodis_samples_per_second': 82.781, 'train@fra.sdrt.annodis_steps_per_second': 2.614, 'epoch': 4.0}
{'loss': 2.3143, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.2616522312164307, 'eval_accuracy@fra.sdrt.annodis': 0.26515151515151514, 'eval_f1@fra.sdrt.annodis': 0.05369237601414813, 'eval_precision@fra.sdrt.annodis': 0.05430492618460316, 'eval_recall@fra.sdrt.annodis': 0.07412292874390486, 'eval_loss@fra.sdrt.annodis': 2.2616522312164307, 'eval_runtime': 6.7032, 'eval_samples_per_second': 78.769, 'eval_steps_per_second': 2.536, 'epoch': 4.0}
{'train@fra.sdrt.annodis_loss': 2.1980020999908447, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.34508009153318076, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.08381753599358845, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.08970737905615915, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.10723929950159126, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.1980020999908447, 'train@fra.sdrt.annodis_runtime': 26.3741, 'train@fra.sdrt.annodis_samples_per_second': 82.847, 'train@fra.sdrt.annodis_steps_per_second': 2.616, 'epoch': 5.0}
{'loss': 2.2548, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.2209818363189697, 'eval_accuracy@fra.sdrt.annodis': 0.3162878787878788, 'eval_f1@fra.sdrt.annodis': 0.06759543996218012, 'eval_precision@fra.sdrt.annodis': 0.052996471718440324, 'eval_recall@fra.sdrt.annodis': 0.09408699111018254, 'eval_loss@fra.sdrt.annodis': 2.2209815979003906, 'eval_runtime': 6.6961, 'eval_samples_per_second': 78.852, 'eval_steps_per_second': 2.539, 'epoch': 5.0}
{'train@fra.sdrt.annodis_loss': 2.1463489532470703, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.3693363844393593, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.09695509201974435, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.1183489992894723, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.11884777402220169, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.1463489532470703, 'train@fra.sdrt.annodis_runtime': 26.4266, 'train@fra.sdrt.annodis_samples_per_second': 82.682, 'train@fra.sdrt.annodis_steps_per_second': 2.611, 'epoch': 6.0}
{'loss': 2.2115, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.1779935359954834, 'eval_accuracy@fra.sdrt.annodis': 0.3465909090909091, 'eval_f1@fra.sdrt.annodis': 0.0787666981531065, 'eval_precision@fra.sdrt.annodis': 0.1316866961493801, 'eval_recall@fra.sdrt.annodis': 0.10554965105340947, 'eval_loss@fra.sdrt.annodis': 2.1779932975769043, 'eval_runtime': 6.7084, 'eval_samples_per_second': 78.707, 'eval_steps_per_second': 2.534, 'epoch': 6.0}
{'train@fra.sdrt.annodis_loss': 2.097287178039551, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.39679633867276887, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.11988539573940596, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.13158357773905538, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.13660906845286075, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.097287178039551, 'train@fra.sdrt.annodis_runtime': 26.3909, 'train@fra.sdrt.annodis_samples_per_second': 82.794, 'train@fra.sdrt.annodis_steps_per_second': 2.615, 'epoch': 7.0}
{'loss': 2.1588, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.1391513347625732, 'eval_accuracy@fra.sdrt.annodis': 0.3465909090909091, 'eval_f1@fra.sdrt.annodis': 0.08638355701915312, 'eval_precision@fra.sdrt.annodis': 0.12892217715002524, 'eval_recall@fra.sdrt.annodis': 0.10846598954436337, 'eval_loss@fra.sdrt.annodis': 2.1391513347625732, 'eval_runtime': 6.6813, 'eval_samples_per_second': 79.027, 'eval_steps_per_second': 2.544, 'epoch': 7.0}
{'train@fra.sdrt.annodis_loss': 2.054361343383789, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.41189931350114417, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.1311058371239876, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12815671731040568, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.14991325100037278, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.054361343383789, 'train@fra.sdrt.annodis_runtime': 26.383, 'train@fra.sdrt.annodis_samples_per_second': 82.818, 'train@fra.sdrt.annodis_steps_per_second': 2.615, 'epoch': 8.0}
{'loss': 2.1263, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.10422945022583, 'eval_accuracy@fra.sdrt.annodis': 0.3560606060606061, 'eval_f1@fra.sdrt.annodis': 0.09205992522654287, 'eval_precision@fra.sdrt.annodis': 0.10787611573071101, 'eval_recall@fra.sdrt.annodis': 0.11355180755140386, 'eval_loss@fra.sdrt.annodis': 2.10422945022583, 'eval_runtime': 6.7195, 'eval_samples_per_second': 78.577, 'eval_steps_per_second': 2.53, 'epoch': 8.0}
{'train@fra.sdrt.annodis_loss': 2.021639347076416, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.41418764302059496, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.13322338499651704, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12622967614479683, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1555067134904718, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.021639347076416, 'train@fra.sdrt.annodis_runtime': 26.4134, 'train@fra.sdrt.annodis_samples_per_second': 82.723, 'train@fra.sdrt.annodis_steps_per_second': 2.612, 'epoch': 9.0}
{'loss': 2.0819, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.077484607696533, 'eval_accuracy@fra.sdrt.annodis': 0.3693181818181818, 'eval_f1@fra.sdrt.annodis': 0.10069657310907017, 'eval_precision@fra.sdrt.annodis': 0.10619584430807992, 'eval_recall@fra.sdrt.annodis': 0.12160796706440778, 'eval_loss@fra.sdrt.annodis': 2.0774848461151123, 'eval_runtime': 6.6916, 'eval_samples_per_second': 78.905, 'eval_steps_per_second': 2.541, 'epoch': 9.0}
{'train@fra.sdrt.annodis_loss': 1.9979199171066284, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.425629290617849, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.13777200463617, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12797390199145575, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.16215153373309887, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.9979201555252075, 'train@fra.sdrt.annodis_runtime': 26.4363, 'train@fra.sdrt.annodis_samples_per_second': 82.651, 'train@fra.sdrt.annodis_steps_per_second': 2.61, 'epoch': 10.0}
{'loss': 2.0522, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.0587961673736572, 'eval_accuracy@fra.sdrt.annodis': 0.3712121212121212, 'eval_f1@fra.sdrt.annodis': 0.10921224900650262, 'eval_precision@fra.sdrt.annodis': 0.11203736883084708, 'eval_recall@fra.sdrt.annodis': 0.1280194506133534, 'eval_loss@fra.sdrt.annodis': 2.0587961673736572, 'eval_runtime': 6.7163, 'eval_samples_per_second': 78.614, 'eval_steps_per_second': 2.531, 'epoch': 10.0}
{'train@fra.sdrt.annodis_loss': 1.983886957168579, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.43157894736842106, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.1393104137339599, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.1277111720588585, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.16590632183874998, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.9838871955871582, 'train@fra.sdrt.annodis_runtime': 26.4074, 'train@fra.sdrt.annodis_samples_per_second': 82.742, 'train@fra.sdrt.annodis_steps_per_second': 2.613, 'epoch': 11.0}
{'loss': 2.0305, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.046517848968506, 'eval_accuracy@fra.sdrt.annodis': 0.3693181818181818, 'eval_f1@fra.sdrt.annodis': 0.10989716606347535, 'eval_precision@fra.sdrt.annodis': 0.11040654725764015, 'eval_recall@fra.sdrt.annodis': 0.12887727715179756, 'eval_loss@fra.sdrt.annodis': 2.0465176105499268, 'eval_runtime': 6.7968, 'eval_samples_per_second': 77.683, 'eval_steps_per_second': 2.501, 'epoch': 11.0}
{'train@fra.sdrt.annodis_loss': 1.9790687561035156, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4302059496567506, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.13943160228130258, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12747611056792374, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.16605687927363816, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.9790687561035156, 'train@fra.sdrt.annodis_runtime': 26.415, 'train@fra.sdrt.annodis_samples_per_second': 82.718, 'train@fra.sdrt.annodis_steps_per_second': 2.612, 'epoch': 12.0}
{'loss': 2.0289, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.0429880619049072, 'eval_accuracy@fra.sdrt.annodis': 0.3731060606060606, 'eval_f1@fra.sdrt.annodis': 0.11079946161913375, 'eval_precision@fra.sdrt.annodis': 0.11108700732357407, 'eval_recall@fra.sdrt.annodis': 0.12985956785371444, 'eval_loss@fra.sdrt.annodis': 2.042987823486328, 'eval_runtime': 6.7483, 'eval_samples_per_second': 78.241, 'eval_steps_per_second': 2.519, 'epoch': 12.0}
{'train_runtime': 1060.8046, 'train_samples_per_second': 24.717, 'train_steps_per_second': 0.781, 'train_loss': 2.276929643419054, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.8416
  train_runtime            = 0:12:55.58
  train_samples_per_second =     24.879
  train_steps_per_second   =      0.789
-------------------------------------------------------------------
Lang1:  por.rst.cstn    Lang2:  fra.sdrt.annodis
Saving run to:  runs/full_shot/FullShot=v4_por.rst.cstn_fra.sdrt.annodis_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 4148 examples
read 573 examples
read 272 examples
read 2185 examples
read 528 examples
read 625 examples
Total prediction labels:  45
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (pt): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (fr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (pt): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=45, bias=True)
    )
  )
)
{'train@por.rst.cstn_loss': 2.5154483318328857, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.2774831243972999, 'train@por.rst.cstn_f1@por.rst.cstn': 0.013575674655595393, 'train@por.rst.cstn_precision@por.rst.cstn': 0.008671347637415621, 'train@por.rst.cstn_recall@por.rst.cstn': 0.03125, 'train@por.rst.cstn_loss@por.rst.cstn': 2.5154483318328857, 'train@por.rst.cstn_runtime': 49.6019, 'train@por.rst.cstn_samples_per_second': 83.626, 'train@por.rst.cstn_steps_per_second': 2.621, 'epoch': 1.0}
{'loss': 3.0829, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.592613935470581, 'eval_accuracy@por.rst.cstn': 0.28097731239092494, 'eval_f1@por.rst.cstn': 0.019940549913301957, 'eval_precision@por.rst.cstn': 0.012771696017769315, 'eval_recall@por.rst.cstn': 0.045454545454545456, 'eval_loss@por.rst.cstn': 2.592613935470581, 'eval_runtime': 7.2234, 'eval_samples_per_second': 79.325, 'eval_steps_per_second': 2.492, 'epoch': 1.0}
{'train@por.rst.cstn_loss': 2.2470321655273438, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.368129218900675, 'train@por.rst.cstn_f1@por.rst.cstn': 0.05436887932892836, 'train@por.rst.cstn_precision@por.rst.cstn': 0.0842108371210764, 'train@por.rst.cstn_recall@por.rst.cstn': 0.059832500131168496, 'train@por.rst.cstn_loss@por.rst.cstn': 2.2470321655273438, 'train@por.rst.cstn_runtime': 49.6448, 'train@por.rst.cstn_samples_per_second': 83.554, 'train@por.rst.cstn_steps_per_second': 2.619, 'epoch': 2.0}
{'loss': 2.4029, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.3467886447906494, 'eval_accuracy@por.rst.cstn': 0.31413612565445026, 'eval_f1@por.rst.cstn': 0.061853116975637286, 'eval_precision@por.rst.cstn': 0.10794419605895017, 'eval_recall@por.rst.cstn': 0.07305131765814105, 'eval_loss@por.rst.cstn': 2.3467886447906494, 'eval_runtime': 7.2177, 'eval_samples_per_second': 79.388, 'eval_steps_per_second': 2.494, 'epoch': 2.0}
{'train@por.rst.cstn_loss': 1.9999654293060303, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.46745419479267114, 'train@por.rst.cstn_f1@por.rst.cstn': 0.07529709063408507, 'train@por.rst.cstn_precision@por.rst.cstn': 0.0905695966897794, 'train@por.rst.cstn_recall@por.rst.cstn': 0.08984218100685357, 'train@por.rst.cstn_loss@por.rst.cstn': 1.9999654293060303, 'train@por.rst.cstn_runtime': 49.6188, 'train@por.rst.cstn_samples_per_second': 83.597, 'train@por.rst.cstn_steps_per_second': 2.62, 'epoch': 3.0}
{'loss': 2.1808, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.130204916000366, 'eval_accuracy@por.rst.cstn': 0.3944153577661431, 'eval_f1@por.rst.cstn': 0.09987430490166468, 'eval_precision@por.rst.cstn': 0.09822990633528673, 'eval_recall@por.rst.cstn': 0.13002284276987836, 'eval_loss@por.rst.cstn': 2.130204439163208, 'eval_runtime': 7.219, 'eval_samples_per_second': 79.374, 'eval_steps_per_second': 2.493, 'epoch': 3.0}
{'train@por.rst.cstn_loss': 1.8228282928466797, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5274831243972999, 'train@por.rst.cstn_f1@por.rst.cstn': 0.10733779230976198, 'train@por.rst.cstn_precision@por.rst.cstn': 0.11462778482900453, 'train@por.rst.cstn_recall@por.rst.cstn': 0.11764573382041746, 'train@por.rst.cstn_loss@por.rst.cstn': 1.8228281736373901, 'train@por.rst.cstn_runtime': 49.6811, 'train@por.rst.cstn_samples_per_second': 83.493, 'train@por.rst.cstn_steps_per_second': 2.617, 'epoch': 4.0}
{'loss': 1.9715, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.979191541671753, 'eval_accuracy@por.rst.cstn': 0.42757417102966844, 'eval_f1@por.rst.cstn': 0.13190615502203087, 'eval_precision@por.rst.cstn': 0.1319507604651605, 'eval_recall@por.rst.cstn': 0.15799416270344083, 'eval_loss@por.rst.cstn': 1.979191541671753, 'eval_runtime': 7.2116, 'eval_samples_per_second': 79.455, 'eval_steps_per_second': 2.496, 'epoch': 4.0}
{'train@por.rst.cstn_loss': 1.7184460163116455, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5443587270973963, 'train@por.rst.cstn_f1@por.rst.cstn': 0.11965559925275988, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13280572301392143, 'train@por.rst.cstn_recall@por.rst.cstn': 0.13256026654907227, 'train@por.rst.cstn_loss@por.rst.cstn': 1.718446135520935, 'train@por.rst.cstn_runtime': 49.6714, 'train@por.rst.cstn_samples_per_second': 83.509, 'train@por.rst.cstn_steps_per_second': 2.617, 'epoch': 5.0}
{'loss': 1.8301, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.8795850276947021, 'eval_accuracy@por.rst.cstn': 0.450261780104712, 'eval_f1@por.rst.cstn': 0.15060734894276737, 'eval_precision@por.rst.cstn': 0.16275618881057285, 'eval_recall@por.rst.cstn': 0.1707751607429161, 'eval_loss@por.rst.cstn': 1.8795850276947021, 'eval_runtime': 7.2218, 'eval_samples_per_second': 79.343, 'eval_steps_per_second': 2.492, 'epoch': 5.0}
{'train@por.rst.cstn_loss': 1.6517949104309082, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5612343297974928, 'train@por.rst.cstn_f1@por.rst.cstn': 0.126995641993428, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13581973981390025, 'train@por.rst.cstn_recall@por.rst.cstn': 0.13855234690908683, 'train@por.rst.cstn_loss@por.rst.cstn': 1.6517949104309082, 'train@por.rst.cstn_runtime': 49.6076, 'train@por.rst.cstn_samples_per_second': 83.616, 'train@por.rst.cstn_steps_per_second': 2.621, 'epoch': 6.0}
{'loss': 1.7476, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.8163666725158691, 'eval_accuracy@por.rst.cstn': 0.47643979057591623, 'eval_f1@por.rst.cstn': 0.16854519385907435, 'eval_precision@por.rst.cstn': 0.17536899085145324, 'eval_recall@por.rst.cstn': 0.18383821498289982, 'eval_loss@por.rst.cstn': 1.8163670301437378, 'eval_runtime': 7.203, 'eval_samples_per_second': 79.55, 'eval_steps_per_second': 2.499, 'epoch': 6.0}
{'train@por.rst.cstn_loss': 1.6076653003692627, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5730472516875603, 'train@por.rst.cstn_f1@por.rst.cstn': 0.13307161159730654, 'train@por.rst.cstn_precision@por.rst.cstn': 0.1331078517206825, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14498974440981305, 'train@por.rst.cstn_loss@por.rst.cstn': 1.6076651811599731, 'train@por.rst.cstn_runtime': 49.6156, 'train@por.rst.cstn_samples_per_second': 83.603, 'train@por.rst.cstn_steps_per_second': 2.62, 'epoch': 7.0}
{'loss': 1.6886, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.7752206325531006, 'eval_accuracy@por.rst.cstn': 0.49040139616055844, 'eval_f1@por.rst.cstn': 0.1788264986077565, 'eval_precision@por.rst.cstn': 0.1811608990563858, 'eval_recall@por.rst.cstn': 0.1937761256957221, 'eval_loss@por.rst.cstn': 1.7752203941345215, 'eval_runtime': 7.2206, 'eval_samples_per_second': 79.356, 'eval_steps_per_second': 2.493, 'epoch': 7.0}
{'train@por.rst.cstn_loss': 1.5803300142288208, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5788331726133076, 'train@por.rst.cstn_f1@por.rst.cstn': 0.13580789934597995, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13485957034190582, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14728609646695767, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5803300142288208, 'train@por.rst.cstn_runtime': 49.6048, 'train@por.rst.cstn_samples_per_second': 83.621, 'train@por.rst.cstn_steps_per_second': 2.621, 'epoch': 8.0}
{'loss': 1.6577, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.7522319555282593, 'eval_accuracy@por.rst.cstn': 0.49912739965095987, 'eval_f1@por.rst.cstn': 0.1868287619227228, 'eval_precision@por.rst.cstn': 0.18828908257681343, 'eval_recall@por.rst.cstn': 0.2003715833781363, 'eval_loss@por.rst.cstn': 1.7522320747375488, 'eval_runtime': 7.2011, 'eval_samples_per_second': 79.571, 'eval_steps_per_second': 2.5, 'epoch': 8.0}
{'train@por.rst.cstn_loss': 1.5606789588928223, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5836547733847638, 'train@por.rst.cstn_f1@por.rst.cstn': 0.138546666942703, 'train@por.rst.cstn_precision@por.rst.cstn': 0.1366801999588835, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14956722042215134, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5606790781021118, 'train@por.rst.cstn_runtime': 49.6096, 'train@por.rst.cstn_samples_per_second': 83.613, 'train@por.rst.cstn_steps_per_second': 2.62, 'epoch': 9.0}
{'loss': 1.6338, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.7349255084991455, 'eval_accuracy@por.rst.cstn': 0.5008726003490401, 'eval_f1@por.rst.cstn': 0.18885712335726162, 'eval_precision@por.rst.cstn': 0.189500249317149, 'eval_recall@por.rst.cstn': 0.20261450953410598, 'eval_loss@por.rst.cstn': 1.7349255084991455, 'eval_runtime': 7.2212, 'eval_samples_per_second': 79.349, 'eval_steps_per_second': 2.493, 'epoch': 9.0}
{'train@por.rst.cstn_loss': 1.544961929321289, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5891996142719382, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1409868955480255, 'train@por.rst.cstn_precision@por.rst.cstn': 0.1419864994393594, 'train@por.rst.cstn_recall@por.rst.cstn': 0.15327521062345484, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5449620485305786, 'train@por.rst.cstn_runtime': 49.6224, 'train@por.rst.cstn_samples_per_second': 83.591, 'train@por.rst.cstn_steps_per_second': 2.62, 'epoch': 10.0}
{'loss': 1.6069, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.7186803817749023, 'eval_accuracy@por.rst.cstn': 0.506108202443281, 'eval_f1@por.rst.cstn': 0.1939007552772897, 'eval_precision@por.rst.cstn': 0.20733464666765852, 'eval_recall@por.rst.cstn': 0.20900417365728374, 'eval_loss@por.rst.cstn': 1.718680500984192, 'eval_runtime': 7.214, 'eval_samples_per_second': 79.429, 'eval_steps_per_second': 2.495, 'epoch': 10.0}
{'train@por.rst.cstn_loss': 1.5371439456939697, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5904050144648023, 'train@por.rst.cstn_f1@por.rst.cstn': 0.14281461844192025, 'train@por.rst.cstn_precision@por.rst.cstn': 0.14691811977888963, 'train@por.rst.cstn_recall@por.rst.cstn': 0.15418445367410288, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5371439456939697, 'train@por.rst.cstn_runtime': 49.6177, 'train@por.rst.cstn_samples_per_second': 83.599, 'train@por.rst.cstn_steps_per_second': 2.62, 'epoch': 11.0}
{'loss': 1.601, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.7130342721939087, 'eval_accuracy@por.rst.cstn': 0.5043630017452007, 'eval_f1@por.rst.cstn': 0.19427975357815086, 'eval_precision@por.rst.cstn': 0.2090203560653496, 'eval_recall@por.rst.cstn': 0.20731120230808184, 'eval_loss@por.rst.cstn': 1.7130342721939087, 'eval_runtime': 7.2112, 'eval_samples_per_second': 79.46, 'eval_steps_per_second': 2.496, 'epoch': 11.0}
{'train@por.rst.cstn_loss': 1.5342907905578613, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5906460945033751, 'train@por.rst.cstn_f1@por.rst.cstn': 0.14296553841040743, 'train@por.rst.cstn_precision@por.rst.cstn': 0.14663379818739192, 'train@por.rst.cstn_recall@por.rst.cstn': 0.15445555368457042, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5342909097671509, 'train@por.rst.cstn_runtime': 49.6018, 'train@por.rst.cstn_samples_per_second': 83.626, 'train@por.rst.cstn_steps_per_second': 2.621, 'epoch': 12.0}
{'loss': 1.5919, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.709707260131836, 'eval_accuracy@por.rst.cstn': 0.5095986038394416, 'eval_f1@por.rst.cstn': 0.20109674396303542, 'eval_precision@por.rst.cstn': 0.2178034408954802, 'eval_recall@por.rst.cstn': 0.21209407892110202, 'eval_loss@por.rst.cstn': 1.7097073793411255, 'eval_runtime': 7.209, 'eval_samples_per_second': 79.484, 'eval_steps_per_second': 2.497, 'epoch': 12.0}
{'train_runtime': 1945.6431, 'train_samples_per_second': 25.583, 'train_steps_per_second': 0.802, 'train_loss': 1.9163045247395833, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.9163
  train_runtime            = 0:32:25.64
  train_samples_per_second =     25.583
  train_steps_per_second   =      0.802
{'train@fra.sdrt.annodis_loss': 2.6257455348968506, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.2613272311212815, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.05755902643120071, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.054343151929034855, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.07847638865180336, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.6257457733154297, 'train@fra.sdrt.annodis_runtime': 26.3208, 'train@fra.sdrt.annodis_samples_per_second': 83.014, 'train@fra.sdrt.annodis_steps_per_second': 2.622, 'epoch': 1.0}
{'loss': 3.2089, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.583570957183838, 'eval_accuracy@fra.sdrt.annodis': 0.2803030303030303, 'eval_f1@fra.sdrt.annodis': 0.057484147789652926, 'eval_precision@fra.sdrt.annodis': 0.05137239757477081, 'eval_recall@fra.sdrt.annodis': 0.08032573937731431, 'eval_loss@fra.sdrt.annodis': 2.583570718765259, 'eval_runtime': 6.6737, 'eval_samples_per_second': 79.117, 'eval_steps_per_second': 2.547, 'epoch': 1.0}
{'train@fra.sdrt.annodis_loss': 2.3183295726776123, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.3185354691075515, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.0713101614480007, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.057938879505723725, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.09626024767349362, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.3183295726776123, 'train@fra.sdrt.annodis_runtime': 26.3786, 'train@fra.sdrt.annodis_samples_per_second': 82.832, 'train@fra.sdrt.annodis_steps_per_second': 2.616, 'epoch': 2.0}
{'loss': 2.4722, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.285799741744995, 'eval_accuracy@fra.sdrt.annodis': 0.3125, 'eval_f1@fra.sdrt.annodis': 0.06823886929396064, 'eval_precision@fra.sdrt.annodis': 0.055251992262542876, 'eval_recall@fra.sdrt.annodis': 0.09201118737290422, 'eval_loss@fra.sdrt.annodis': 2.2858002185821533, 'eval_runtime': 6.6781, 'eval_samples_per_second': 79.065, 'eval_steps_per_second': 2.546, 'epoch': 2.0}
{'train@fra.sdrt.annodis_loss': 2.1906092166900635, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.34004576659038904, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.07897483698118653, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.14706018607912316, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1032988869576949, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.1906089782714844, 'train@fra.sdrt.annodis_runtime': 26.328, 'train@fra.sdrt.annodis_samples_per_second': 82.991, 'train@fra.sdrt.annodis_steps_per_second': 2.621, 'epoch': 3.0}
{'loss': 2.2952, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.166865110397339, 'eval_accuracy@fra.sdrt.annodis': 0.32954545454545453, 'eval_f1@fra.sdrt.annodis': 0.0735978228515542, 'eval_precision@fra.sdrt.annodis': 0.060357767250933, 'eval_recall@fra.sdrt.annodis': 0.09783199606667935, 'eval_loss@fra.sdrt.annodis': 2.166865110397339, 'eval_runtime': 6.6915, 'eval_samples_per_second': 78.906, 'eval_steps_per_second': 2.541, 'epoch': 3.0}
{'train@fra.sdrt.annodis_loss': 2.087859869003296, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.37757437070938216, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.10847635587047624, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.14369959601353888, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.12778423889532836, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.087860107421875, 'train@fra.sdrt.annodis_runtime': 26.34, 'train@fra.sdrt.annodis_samples_per_second': 82.954, 'train@fra.sdrt.annodis_steps_per_second': 2.62, 'epoch': 4.0}
{'loss': 2.1968, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.076840400695801, 'eval_accuracy@fra.sdrt.annodis': 0.3541666666666667, 'eval_f1@fra.sdrt.annodis': 0.0904408514315635, 'eval_precision@fra.sdrt.annodis': 0.11747300767919738, 'eval_recall@fra.sdrt.annodis': 0.11076785966835444, 'eval_loss@fra.sdrt.annodis': 2.076840877532959, 'eval_runtime': 6.675, 'eval_samples_per_second': 79.101, 'eval_steps_per_second': 2.547, 'epoch': 4.0}
{'train@fra.sdrt.annodis_loss': 2.0090811252593994, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.39633867276887874, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.11845908528807986, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.16720009264248625, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.14609096976955993, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.0090811252593994, 'train@fra.sdrt.annodis_runtime': 26.3324, 'train@fra.sdrt.annodis_samples_per_second': 82.978, 'train@fra.sdrt.annodis_steps_per_second': 2.62, 'epoch': 5.0}
{'loss': 2.1027, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.019068479537964, 'eval_accuracy@fra.sdrt.annodis': 0.375, 'eval_f1@fra.sdrt.annodis': 0.10595122780965671, 'eval_precision@fra.sdrt.annodis': 0.09623432357768329, 'eval_recall@fra.sdrt.annodis': 0.1296685686562094, 'eval_loss@fra.sdrt.annodis': 2.0190682411193848, 'eval_runtime': 9.4178, 'eval_samples_per_second': 56.064, 'eval_steps_per_second': 1.805, 'epoch': 5.0}
{'train@fra.sdrt.annodis_loss': 1.947568416595459, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4215102974828375, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.15079110439335075, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.18684896291528627, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.17560610699807, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.947568416595459, 'train@fra.sdrt.annodis_runtime': 26.3009, 'train@fra.sdrt.annodis_samples_per_second': 83.077, 'train@fra.sdrt.annodis_steps_per_second': 2.623, 'epoch': 6.0}
{'loss': 2.0356, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.9680143594741821, 'eval_accuracy@fra.sdrt.annodis': 0.39015151515151514, 'eval_f1@fra.sdrt.annodis': 0.12428791096621038, 'eval_precision@fra.sdrt.annodis': 0.1371377214416466, 'eval_recall@fra.sdrt.annodis': 0.14398242271717726, 'eval_loss@fra.sdrt.annodis': 1.9680143594741821, 'eval_runtime': 6.6937, 'eval_samples_per_second': 78.88, 'eval_steps_per_second': 2.54, 'epoch': 6.0}
{'train@fra.sdrt.annodis_loss': 1.9009228944778442, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4411899313501144, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.16906837548335896, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.17775009181755572, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.19610408671111415, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.9009226560592651, 'train@fra.sdrt.annodis_runtime': 26.3592, 'train@fra.sdrt.annodis_samples_per_second': 82.893, 'train@fra.sdrt.annodis_steps_per_second': 2.618, 'epoch': 7.0}
{'loss': 1.981, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.930702805519104, 'eval_accuracy@fra.sdrt.annodis': 0.4034090909090909, 'eval_f1@fra.sdrt.annodis': 0.13411935846946568, 'eval_precision@fra.sdrt.annodis': 0.13682628647771788, 'eval_recall@fra.sdrt.annodis': 0.1537649825272247, 'eval_loss@fra.sdrt.annodis': 1.9307029247283936, 'eval_runtime': 6.6764, 'eval_samples_per_second': 79.084, 'eval_steps_per_second': 2.546, 'epoch': 7.0}
{'train@fra.sdrt.annodis_loss': 1.8666956424713135, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.45308924485125857, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.17834371552741557, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.17849607416324853, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.20762637426773245, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.866695523262024, 'train@fra.sdrt.annodis_runtime': 26.3421, 'train@fra.sdrt.annodis_samples_per_second': 82.947, 'train@fra.sdrt.annodis_steps_per_second': 2.619, 'epoch': 8.0}
{'loss': 1.9432, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.9073405265808105, 'eval_accuracy@fra.sdrt.annodis': 0.4185606060606061, 'eval_f1@fra.sdrt.annodis': 0.15044893450596988, 'eval_precision@fra.sdrt.annodis': 0.1503533139515464, 'eval_recall@fra.sdrt.annodis': 0.16973346205069145, 'eval_loss@fra.sdrt.annodis': 1.9073405265808105, 'eval_runtime': 6.695, 'eval_samples_per_second': 78.865, 'eval_steps_per_second': 2.539, 'epoch': 8.0}
{'train@fra.sdrt.annodis_loss': 1.8436682224273682, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.45491990846681923, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.17878204639857534, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.17788453050903127, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.2086663036670172, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.8436682224273682, 'train@fra.sdrt.annodis_runtime': 26.3205, 'train@fra.sdrt.annodis_samples_per_second': 83.015, 'train@fra.sdrt.annodis_steps_per_second': 2.622, 'epoch': 9.0}
{'loss': 1.9248, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.8907535076141357, 'eval_accuracy@fra.sdrt.annodis': 0.42045454545454547, 'eval_f1@fra.sdrt.annodis': 0.1520707368847843, 'eval_precision@fra.sdrt.annodis': 0.14674996258997947, 'eval_recall@fra.sdrt.annodis': 0.1741736566187544, 'eval_loss@fra.sdrt.annodis': 1.8907532691955566, 'eval_runtime': 6.6927, 'eval_samples_per_second': 78.892, 'eval_steps_per_second': 2.54, 'epoch': 9.0}
{'train@fra.sdrt.annodis_loss': 1.8268072605133057, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4594965675057208, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.18072775378122796, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.17714412435987506, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.21025101712341387, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.8268072605133057, 'train@fra.sdrt.annodis_runtime': 26.3565, 'train@fra.sdrt.annodis_samples_per_second': 82.902, 'train@fra.sdrt.annodis_steps_per_second': 2.618, 'epoch': 10.0}
{'loss': 1.8959, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.8785189390182495, 'eval_accuracy@fra.sdrt.annodis': 0.42045454545454547, 'eval_f1@fra.sdrt.annodis': 0.15549461497258144, 'eval_precision@fra.sdrt.annodis': 0.15163125679425074, 'eval_recall@fra.sdrt.annodis': 0.17668284645769577, 'eval_loss@fra.sdrt.annodis': 1.878519058227539, 'eval_runtime': 6.7222, 'eval_samples_per_second': 78.546, 'eval_steps_per_second': 2.529, 'epoch': 10.0}
{'train@fra.sdrt.annodis_loss': 1.8176605701446533, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4627002288329519, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.18146403712232098, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.17407316275934184, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.21320946241933952, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.8176605701446533, 'train@fra.sdrt.annodis_runtime': 26.3818, 'train@fra.sdrt.annodis_samples_per_second': 82.822, 'train@fra.sdrt.annodis_steps_per_second': 2.615, 'epoch': 11.0}
{'loss': 1.8805, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.8713122606277466, 'eval_accuracy@fra.sdrt.annodis': 0.4185606060606061, 'eval_f1@fra.sdrt.annodis': 0.1542126400788565, 'eval_precision@fra.sdrt.annodis': 0.14964894031468154, 'eval_recall@fra.sdrt.annodis': 0.1763115856156838, 'eval_loss@fra.sdrt.annodis': 1.8713122606277466, 'eval_runtime': 6.6925, 'eval_samples_per_second': 78.895, 'eval_steps_per_second': 2.54, 'epoch': 11.0}
{'train@fra.sdrt.annodis_loss': 1.8143004179000854, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4640732265446224, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.18199410169769997, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.17498036694978403, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.21360285879232135, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.814300537109375, 'train@fra.sdrt.annodis_runtime': 26.3589, 'train@fra.sdrt.annodis_samples_per_second': 82.894, 'train@fra.sdrt.annodis_steps_per_second': 2.618, 'epoch': 12.0}
{'loss': 1.8798, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.8688029050827026, 'eval_accuracy@fra.sdrt.annodis': 0.42045454545454547, 'eval_f1@fra.sdrt.annodis': 0.1551830425024209, 'eval_precision@fra.sdrt.annodis': 0.1502916578817907, 'eval_recall@fra.sdrt.annodis': 0.17776020198804743, 'eval_loss@fra.sdrt.annodis': 1.8688030242919922, 'eval_runtime': 6.6912, 'eval_samples_per_second': 78.91, 'eval_steps_per_second': 2.541, 'epoch': 12.0}
{'train_runtime': 1064.2742, 'train_samples_per_second': 24.637, 'train_steps_per_second': 0.778, 'train_loss': 2.151403270481865, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.9163
  train_runtime            = 0:32:25.64
  train_samples_per_second =     25.583
  train_steps_per_second   =      0.802
-------------------------------------------------------------------
Lang1:  rus.rst.rrt    Lang2:  fra.sdrt.annodis
Saving run to:  runs/full_shot/FullShot=v4_rus.rst.rrt_fra.sdrt.annodis_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 28822 examples
read 2855 examples
read 2843 examples
read 2185 examples
read 528 examples
read 625 examples
Total prediction labels:  36
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (ru): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (fr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=36, bias=True)
    )
  )
)
{'train@rus.rst.rrt_loss': 1.737747073173523, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.49111789605162726, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.17598822518555063, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.22433711733903164, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.1928317898422284, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.737747073173523, 'train@rus.rst.rrt_runtime': 343.0252, 'train@rus.rst.rrt_samples_per_second': 84.023, 'train@rus.rst.rrt_steps_per_second': 2.627, 'epoch': 1.0}
{'loss': 2.1757, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.7738499641418457, 'eval_accuracy@rus.rst.rrt': 0.46654991243432575, 'eval_f1@rus.rst.rrt': 0.18978836885587186, 'eval_precision@rus.rst.rrt': 0.19004803522019217, 'eval_recall@rus.rst.rrt': 0.20894677599789183, 'eval_loss@rus.rst.rrt': 1.7738502025604248, 'eval_runtime': 34.3178, 'eval_samples_per_second': 83.193, 'eval_steps_per_second': 2.623, 'epoch': 1.0}
{'train@rus.rst.rrt_loss': 1.510913372039795, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5406633821386441, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.22141880441483067, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.27463586078044333, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.2285375995697849, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.510913372039795, 'train@rus.rst.rrt_runtime': 342.9614, 'train@rus.rst.rrt_samples_per_second': 84.039, 'train@rus.rst.rrt_steps_per_second': 2.627, 'epoch': 2.0}
{'loss': 1.6529, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.5737069845199585, 'eval_accuracy@rus.rst.rrt': 0.5162872154115586, 'eval_f1@rus.rst.rrt': 0.24380476258382258, 'eval_precision@rus.rst.rrt': 0.3150883644584672, 'eval_recall@rus.rst.rrt': 0.2530413931442131, 'eval_loss@rus.rst.rrt': 1.5737072229385376, 'eval_runtime': 34.3292, 'eval_samples_per_second': 83.165, 'eval_steps_per_second': 2.622, 'epoch': 2.0}
{'train@rus.rst.rrt_loss': 1.4277238845825195, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5646034279369926, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.2743086239305411, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4717106009317998, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.2700318107812289, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.4277238845825195, 'train@rus.rst.rrt_runtime': 343.1658, 'train@rus.rst.rrt_samples_per_second': 83.989, 'train@rus.rst.rrt_steps_per_second': 2.626, 'epoch': 3.0}
{'loss': 1.5173, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.4958553314208984, 'eval_accuracy@rus.rst.rrt': 0.5404553415061296, 'eval_f1@rus.rst.rrt': 0.30145883869195744, 'eval_precision@rus.rst.rrt': 0.3494459870597244, 'eval_recall@rus.rst.rrt': 0.299834744120562, 'eval_loss@rus.rst.rrt': 1.4958553314208984, 'eval_runtime': 34.3905, 'eval_samples_per_second': 83.017, 'eval_steps_per_second': 2.617, 'epoch': 3.0}
{'train@rus.rst.rrt_loss': 1.370792031288147, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5824023315522865, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3114277080752679, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4386583774699439, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.29495902659490314, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3707919120788574, 'train@rus.rst.rrt_runtime': 343.103, 'train@rus.rst.rrt_samples_per_second': 84.004, 'train@rus.rst.rrt_steps_per_second': 2.626, 'epoch': 4.0}
{'loss': 1.4509, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.4435194730758667, 'eval_accuracy@rus.rst.rrt': 0.5649737302977232, 'eval_f1@rus.rst.rrt': 0.36362561276723343, 'eval_precision@rus.rst.rrt': 0.5127775998459511, 'eval_recall@rus.rst.rrt': 0.34648959352012065, 'eval_loss@rus.rst.rrt': 1.4435195922851562, 'eval_runtime': 34.3374, 'eval_samples_per_second': 83.145, 'eval_steps_per_second': 2.621, 'epoch': 4.0}
{'train@rus.rst.rrt_loss': 1.3376294374465942, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5894455624175977, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3284448634664013, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4413631525196857, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.310275177410686, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3376295566558838, 'train@rus.rst.rrt_runtime': 343.0537, 'train@rus.rst.rrt_samples_per_second': 84.016, 'train@rus.rst.rrt_steps_per_second': 2.626, 'epoch': 5.0}
{'loss': 1.4104, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.417284369468689, 'eval_accuracy@rus.rst.rrt': 0.5667250437828372, 'eval_f1@rus.rst.rrt': 0.3788595529594489, 'eval_precision@rus.rst.rrt': 0.5047369756302545, 'eval_recall@rus.rst.rrt': 0.36231535057857767, 'eval_loss@rus.rst.rrt': 1.4172844886779785, 'eval_runtime': 34.3097, 'eval_samples_per_second': 83.213, 'eval_steps_per_second': 2.623, 'epoch': 5.0}
{'train@rus.rst.rrt_loss': 1.3133348226547241, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5961071403788772, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3398922430908533, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.44214693410890094, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.320954675298398, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3133347034454346, 'train@rus.rst.rrt_runtime': 343.0324, 'train@rus.rst.rrt_samples_per_second': 84.021, 'train@rus.rst.rrt_steps_per_second': 2.627, 'epoch': 6.0}
{'loss': 1.382, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.3966480493545532, 'eval_accuracy@rus.rst.rrt': 0.574430823117338, 'eval_f1@rus.rst.rrt': 0.3908713845218591, 'eval_precision@rus.rst.rrt': 0.5022875648563492, 'eval_recall@rus.rst.rrt': 0.3736148329863463, 'eval_loss@rus.rst.rrt': 1.3966479301452637, 'eval_runtime': 34.3702, 'eval_samples_per_second': 83.066, 'eval_steps_per_second': 2.619, 'epoch': 6.0}
{'train@rus.rst.rrt_loss': 1.2976666688919067, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6008604538199986, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3525664615510635, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.43343349954508187, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3342778732561196, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2976666688919067, 'train@rus.rst.rrt_runtime': 342.9071, 'train@rus.rst.rrt_samples_per_second': 84.052, 'train@rus.rst.rrt_steps_per_second': 2.628, 'epoch': 7.0}
{'loss': 1.3593, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.389305830001831, 'eval_accuracy@rus.rst.rrt': 0.5737302977232924, 'eval_f1@rus.rst.rrt': 0.3997363352786327, 'eval_precision@rus.rst.rrt': 0.5084685619799705, 'eval_recall@rus.rst.rrt': 0.3843485073928787, 'eval_loss@rus.rst.rrt': 1.389305591583252, 'eval_runtime': 34.3147, 'eval_samples_per_second': 83.2, 'eval_steps_per_second': 2.623, 'epoch': 7.0}
{'train@rus.rst.rrt_loss': 1.284722924232483, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6032197626812851, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3594560340756304, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4264205818897899, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.342743720632181, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2847230434417725, 'train@rus.rst.rrt_runtime': 342.994, 'train@rus.rst.rrt_samples_per_second': 84.031, 'train@rus.rst.rrt_steps_per_second': 2.627, 'epoch': 8.0}
{'loss': 1.3418, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.3778173923492432, 'eval_accuracy@rus.rst.rrt': 0.5782837127845885, 'eval_f1@rus.rst.rrt': 0.40631023079752143, 'eval_precision@rus.rst.rrt': 0.48767458918975193, 'eval_recall@rus.rst.rrt': 0.39287706547509443, 'eval_loss@rus.rst.rrt': 1.3778172731399536, 'eval_runtime': 34.3471, 'eval_samples_per_second': 83.122, 'eval_steps_per_second': 2.62, 'epoch': 8.0}
{'train@rus.rst.rrt_loss': 1.2726038694381714, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6056484629796683, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36201358347651924, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4331503126893521, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3436593996387086, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.272603988647461, 'train@rus.rst.rrt_runtime': 342.9506, 'train@rus.rst.rrt_samples_per_second': 84.041, 'train@rus.rst.rrt_steps_per_second': 2.627, 'epoch': 9.0}
{'loss': 1.332, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.3713316917419434, 'eval_accuracy@rus.rst.rrt': 0.5751313485113836, 'eval_f1@rus.rst.rrt': 0.40599683084507826, 'eval_precision@rus.rst.rrt': 0.4985707239027808, 'eval_recall@rus.rst.rrt': 0.39012464881870507, 'eval_loss@rus.rst.rrt': 1.3713316917419434, 'eval_runtime': 34.2946, 'eval_samples_per_second': 83.249, 'eval_steps_per_second': 2.624, 'epoch': 9.0}
{'train@rus.rst.rrt_loss': 1.2662550210952759, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6082506418707931, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3640662061155994, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.43981262193218934, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3432377342499262, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2662549018859863, 'train@rus.rst.rrt_runtime': 342.948, 'train@rus.rst.rrt_samples_per_second': 84.042, 'train@rus.rst.rrt_steps_per_second': 2.627, 'epoch': 10.0}
{'loss': 1.3222, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.3685212135314941, 'eval_accuracy@rus.rst.rrt': 0.5821366024518388, 'eval_f1@rus.rst.rrt': 0.41015255551252877, 'eval_precision@rus.rst.rrt': 0.5017024401001038, 'eval_recall@rus.rst.rrt': 0.3910734606967443, 'eval_loss@rus.rst.rrt': 1.3685210943222046, 'eval_runtime': 34.3439, 'eval_samples_per_second': 83.13, 'eval_steps_per_second': 2.621, 'epoch': 10.0}
{'train@rus.rst.rrt_loss': 1.2627475261688232, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6078342932482131, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3647783911063934, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.43228637085644817, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3462535991631014, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2627476453781128, 'train@rus.rst.rrt_runtime': 343.024, 'train@rus.rst.rrt_samples_per_second': 84.023, 'train@rus.rst.rrt_steps_per_second': 2.627, 'epoch': 11.0}
{'loss': 1.3153, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.3688459396362305, 'eval_accuracy@rus.rst.rrt': 0.5772329246935202, 'eval_f1@rus.rst.rrt': 0.41000190416971655, 'eval_precision@rus.rst.rrt': 0.49969656409526914, 'eval_recall@rus.rst.rrt': 0.3949996116344759, 'eval_loss@rus.rst.rrt': 1.368845820426941, 'eval_runtime': 34.3086, 'eval_samples_per_second': 83.215, 'eval_steps_per_second': 2.623, 'epoch': 11.0}
{'train@rus.rst.rrt_loss': 1.260970115661621, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6088404690861148, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36559572770953963, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4343395589936712, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34722792767888855, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.260970115661621, 'train@rus.rst.rrt_runtime': 343.0461, 'train@rus.rst.rrt_samples_per_second': 84.018, 'train@rus.rst.rrt_steps_per_second': 2.626, 'epoch': 12.0}
{'loss': 1.3114, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.366804599761963, 'eval_accuracy@rus.rst.rrt': 0.578984238178634, 'eval_f1@rus.rst.rrt': 0.4134775742435227, 'eval_precision@rus.rst.rrt': 0.4949650403149348, 'eval_recall@rus.rst.rrt': 0.3982198873171942, 'eval_loss@rus.rst.rrt': 1.3668044805526733, 'eval_runtime': 34.3317, 'eval_samples_per_second': 83.159, 'eval_steps_per_second': 2.621, 'epoch': 12.0}
{'train_runtime': 13272.9944, 'train_samples_per_second': 26.058, 'train_steps_per_second': 0.815, 'train_loss': 1.4642802191539204, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.4643
  train_runtime            = 3:41:12.99
  train_samples_per_second =     26.058
  train_steps_per_second   =      0.815
{'train@fra.sdrt.annodis_loss': 2.541555404663086, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.22654462242562928, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.07190067556486276, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.07038376888238587, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.08009960308354384, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.541555166244507, 'train@fra.sdrt.annodis_runtime': 26.2842, 'train@fra.sdrt.annodis_samples_per_second': 83.13, 'train@fra.sdrt.annodis_steps_per_second': 2.625, 'epoch': 1.0}
{'loss': 3.1309, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.580702304840088, 'eval_accuracy@fra.sdrt.annodis': 0.20265151515151514, 'eval_f1@fra.sdrt.annodis': 0.08081091469254122, 'eval_precision@fra.sdrt.annodis': 0.10205955156632666, 'eval_recall@fra.sdrt.annodis': 0.08142492008028641, 'eval_loss@fra.sdrt.annodis': 2.580702304840088, 'eval_runtime': 6.612, 'eval_samples_per_second': 79.855, 'eval_steps_per_second': 2.571, 'epoch': 1.0}
{'train@fra.sdrt.annodis_loss': 2.181044816970825, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.34553775743707094, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.11957429856913683, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.1314391543732192, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.13495229408664228, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.1810450553894043, 'train@fra.sdrt.annodis_runtime': 26.2907, 'train@fra.sdrt.annodis_samples_per_second': 83.109, 'train@fra.sdrt.annodis_steps_per_second': 2.625, 'epoch': 2.0}
{'loss': 2.3721, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.218385696411133, 'eval_accuracy@fra.sdrt.annodis': 0.2878787878787879, 'eval_f1@fra.sdrt.annodis': 0.08673729114379786, 'eval_precision@fra.sdrt.annodis': 0.0761679183848995, 'eval_recall@fra.sdrt.annodis': 0.10524377656563504, 'eval_loss@fra.sdrt.annodis': 2.218385934829712, 'eval_runtime': 6.6202, 'eval_samples_per_second': 79.756, 'eval_steps_per_second': 2.568, 'epoch': 2.0}
{'train@fra.sdrt.annodis_loss': 1.9858914613723755, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4077803203661327, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.14511564379687936, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.19893055530223136, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1591636598500494, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.9858916997909546, 'train@fra.sdrt.annodis_runtime': 26.2765, 'train@fra.sdrt.annodis_samples_per_second': 83.154, 'train@fra.sdrt.annodis_steps_per_second': 2.626, 'epoch': 3.0}
{'loss': 2.1264, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.030233860015869, 'eval_accuracy@fra.sdrt.annodis': 0.3428030303030303, 'eval_f1@fra.sdrt.annodis': 0.10754525205291149, 'eval_precision@fra.sdrt.annodis': 0.09936214445750274, 'eval_recall@fra.sdrt.annodis': 0.1261741546769685, 'eval_loss@fra.sdrt.annodis': 2.0302340984344482, 'eval_runtime': 6.6275, 'eval_samples_per_second': 79.668, 'eval_steps_per_second': 2.565, 'epoch': 3.0}
{'train@fra.sdrt.annodis_loss': 1.8548564910888672, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4576659038901602, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.19546676049673173, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.2635002897374438, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.20921661836357272, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.8548564910888672, 'train@fra.sdrt.annodis_runtime': 26.2892, 'train@fra.sdrt.annodis_samples_per_second': 83.114, 'train@fra.sdrt.annodis_steps_per_second': 2.625, 'epoch': 4.0}
{'loss': 1.9827, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.9113807678222656, 'eval_accuracy@fra.sdrt.annodis': 0.4185606060606061, 'eval_f1@fra.sdrt.annodis': 0.16641814639696548, 'eval_precision@fra.sdrt.annodis': 0.2140477316943571, 'eval_recall@fra.sdrt.annodis': 0.17843704268653554, 'eval_loss@fra.sdrt.annodis': 1.9113807678222656, 'eval_runtime': 6.614, 'eval_samples_per_second': 79.831, 'eval_steps_per_second': 2.57, 'epoch': 4.0}
{'train@fra.sdrt.annodis_loss': 1.76677405834198, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.494279176201373, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.24178822968441097, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.3115801961344983, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.254994965079766, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.76677405834198, 'train@fra.sdrt.annodis_runtime': 26.3272, 'train@fra.sdrt.annodis_samples_per_second': 82.994, 'train@fra.sdrt.annodis_steps_per_second': 2.621, 'epoch': 5.0}
{'loss': 1.8738, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.841812014579773, 'eval_accuracy@fra.sdrt.annodis': 0.4640151515151515, 'eval_f1@fra.sdrt.annodis': 0.21112166980342648, 'eval_precision@fra.sdrt.annodis': 0.20789490565190066, 'eval_recall@fra.sdrt.annodis': 0.22438045116396943, 'eval_loss@fra.sdrt.annodis': 1.8418121337890625, 'eval_runtime': 6.6276, 'eval_samples_per_second': 79.667, 'eval_steps_per_second': 2.565, 'epoch': 5.0}
{'train@fra.sdrt.annodis_loss': 1.7011072635650635, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.51441647597254, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.2646574602303908, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.31807006349662925, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.2754951714453022, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.701107382774353, 'train@fra.sdrt.annodis_runtime': 26.3137, 'train@fra.sdrt.annodis_samples_per_second': 83.037, 'train@fra.sdrt.annodis_steps_per_second': 2.622, 'epoch': 6.0}
{'loss': 1.7951, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.792051076889038, 'eval_accuracy@fra.sdrt.annodis': 0.49242424242424243, 'eval_f1@fra.sdrt.annodis': 0.24407559724705108, 'eval_precision@fra.sdrt.annodis': 0.27243750163834934, 'eval_recall@fra.sdrt.annodis': 0.25259535648694587, 'eval_loss@fra.sdrt.annodis': 1.792050838470459, 'eval_runtime': 6.6427, 'eval_samples_per_second': 79.485, 'eval_steps_per_second': 2.559, 'epoch': 6.0}
{'train@fra.sdrt.annodis_loss': 1.650026798248291, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.5267734553775744, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.2804844042644012, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.3292627405976398, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.29138343372698994, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.650026798248291, 'train@fra.sdrt.annodis_runtime': 26.3055, 'train@fra.sdrt.annodis_samples_per_second': 83.062, 'train@fra.sdrt.annodis_steps_per_second': 2.623, 'epoch': 7.0}
{'loss': 1.7315, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.7530369758605957, 'eval_accuracy@fra.sdrt.annodis': 0.4981060606060606, 'eval_f1@fra.sdrt.annodis': 0.24659498971482108, 'eval_precision@fra.sdrt.annodis': 0.2727220980784159, 'eval_recall@fra.sdrt.annodis': 0.2563812072160482, 'eval_loss@fra.sdrt.annodis': 1.7530367374420166, 'eval_runtime': 6.6005, 'eval_samples_per_second': 79.994, 'eval_steps_per_second': 2.576, 'epoch': 7.0}
{'train@fra.sdrt.annodis_loss': 1.6148343086242676, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.5345537757437071, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.2919408745025914, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.33169359230730805, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.30679220947523267, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.6148343086242676, 'train@fra.sdrt.annodis_runtime': 26.3333, 'train@fra.sdrt.annodis_samples_per_second': 82.975, 'train@fra.sdrt.annodis_steps_per_second': 2.62, 'epoch': 8.0}
{'loss': 1.7014, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.7297109365463257, 'eval_accuracy@fra.sdrt.annodis': 0.5132575757575758, 'eval_f1@fra.sdrt.annodis': 0.2911768755286657, 'eval_precision@fra.sdrt.annodis': 0.3155306866981071, 'eval_recall@fra.sdrt.annodis': 0.29469144594964153, 'eval_loss@fra.sdrt.annodis': 1.7297109365463257, 'eval_runtime': 6.6117, 'eval_samples_per_second': 79.858, 'eval_steps_per_second': 2.571, 'epoch': 8.0}
{'train@fra.sdrt.annodis_loss': 1.5910462141036987, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.5386727688787185, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.29372117155248173, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.3336051198213693, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.3087644840708501, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.5910459756851196, 'train@fra.sdrt.annodis_runtime': 26.3183, 'train@fra.sdrt.annodis_samples_per_second': 83.022, 'train@fra.sdrt.annodis_steps_per_second': 2.622, 'epoch': 9.0}
{'loss': 1.6818, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.7143261432647705, 'eval_accuracy@fra.sdrt.annodis': 0.5151515151515151, 'eval_f1@fra.sdrt.annodis': 0.2947452225759764, 'eval_precision@fra.sdrt.annodis': 0.3139327316693821, 'eval_recall@fra.sdrt.annodis': 0.30024700150519706, 'eval_loss@fra.sdrt.annodis': 1.71432626247406, 'eval_runtime': 6.6283, 'eval_samples_per_second': 79.658, 'eval_steps_per_second': 2.565, 'epoch': 9.0}
{'train@fra.sdrt.annodis_loss': 1.5748192071914673, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.5423340961098398, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.2976183843663213, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.33484678140764573, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.3121382656782642, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.5748192071914673, 'train@fra.sdrt.annodis_runtime': 26.2678, 'train@fra.sdrt.annodis_samples_per_second': 83.182, 'train@fra.sdrt.annodis_steps_per_second': 2.627, 'epoch': 10.0}
{'loss': 1.6479, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.7069456577301025, 'eval_accuracy@fra.sdrt.annodis': 0.5208333333333334, 'eval_f1@fra.sdrt.annodis': 0.29886721513604037, 'eval_precision@fra.sdrt.annodis': 0.31656542639213064, 'eval_recall@fra.sdrt.annodis': 0.3051418173046605, 'eval_loss@fra.sdrt.annodis': 1.7069456577301025, 'eval_runtime': 6.6336, 'eval_samples_per_second': 79.595, 'eval_steps_per_second': 2.563, 'epoch': 10.0}
{'train@fra.sdrt.annodis_loss': 1.5655242204666138, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.5482837528604119, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.30096969324633477, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.3385806687957139, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.3159490797972731, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.5655242204666138, 'train@fra.sdrt.annodis_runtime': 26.262, 'train@fra.sdrt.annodis_samples_per_second': 83.2, 'train@fra.sdrt.annodis_steps_per_second': 2.627, 'epoch': 11.0}
{'loss': 1.6407, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.7002878189086914, 'eval_accuracy@fra.sdrt.annodis': 0.5151515151515151, 'eval_f1@fra.sdrt.annodis': 0.29682445937597746, 'eval_precision@fra.sdrt.annodis': 0.31274032234773463, 'eval_recall@fra.sdrt.annodis': 0.30416857013264803, 'eval_loss@fra.sdrt.annodis': 1.700287938117981, 'eval_runtime': 6.6237, 'eval_samples_per_second': 79.713, 'eval_steps_per_second': 2.567, 'epoch': 11.0}
{'train@fra.sdrt.annodis_loss': 1.5622307062149048, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.5482837528604119, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.30069091926051683, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.33782798038782363, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.31593597210521424, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.5622307062149048, 'train@fra.sdrt.annodis_runtime': 26.2439, 'train@fra.sdrt.annodis_samples_per_second': 83.258, 'train@fra.sdrt.annodis_steps_per_second': 2.629, 'epoch': 12.0}
{'loss': 1.6273, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.697977900505066, 'eval_accuracy@fra.sdrt.annodis': 0.5189393939393939, 'eval_f1@fra.sdrt.annodis': 0.2995833076665742, 'eval_precision@fra.sdrt.annodis': 0.3157616092232498, 'eval_recall@fra.sdrt.annodis': 0.30658402906984616, 'eval_loss@fra.sdrt.annodis': 1.6979777812957764, 'eval_runtime': 6.589, 'eval_samples_per_second': 80.133, 'eval_steps_per_second': 2.58, 'epoch': 12.0}
{'train_runtime': 1058.543, 'train_samples_per_second': 24.77, 'train_steps_per_second': 0.782, 'train_loss': 1.942621083651188, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.4643
  train_runtime            = 3:41:12.99
  train_samples_per_second =     26.058
  train_steps_per_second   =      0.815
-------------------------------------------------------------------
Lang1:  spa.rst.rststb    Lang2:  fra.sdrt.annodis
Saving run to:  runs/full_shot/FullShot=v4_spa.rst.rststb_fra.sdrt.annodis_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2240 examples
read 383 examples
read 426 examples
read 2185 examples
read 528 examples
read 625 examples
Total prediction labels:  43
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (fr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=43, bias=True)
    )
  )
)
{'train@spa.rst.rststb_loss': 2.9361331462860107, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.2174107142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.02225753689453924, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.03907498614945424, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.04032132267116968, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.9361331462860107, 'train@spa.rst.rststb_runtime': 26.8496, 'train@spa.rst.rststb_samples_per_second': 83.428, 'train@spa.rst.rststb_steps_per_second': 2.607, 'epoch': 1.0}
{'loss': 3.3675, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.9765408039093018, 'eval_accuracy@spa.rst.rststb': 0.20887728459530025, 'eval_f1@spa.rst.rststb': 0.021094460062142, 'eval_precision@spa.rst.rststb': 0.015956223171764337, 'eval_recall@spa.rst.rststb': 0.044817190875561644, 'eval_loss@spa.rst.rststb': 2.9765408039093018, 'eval_runtime': 4.923, 'eval_samples_per_second': 77.798, 'eval_steps_per_second': 2.438, 'epoch': 1.0}
{'train@spa.rst.rststb_loss': 2.6081418991088867, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.2825892857142857, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.042216518610953754, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.0412288159095339, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.058653326458965024, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.6081416606903076, 'train@spa.rst.rststb_runtime': 26.9461, 'train@spa.rst.rststb_samples_per_second': 83.129, 'train@spa.rst.rststb_steps_per_second': 2.598, 'epoch': 2.0}
{'loss': 2.7751, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.7215585708618164, 'eval_accuracy@spa.rst.rststb': 0.2402088772845953, 'eval_f1@spa.rst.rststb': 0.03919191449427434, 'eval_precision@spa.rst.rststb': 0.05093708128576833, 'eval_recall@spa.rst.rststb': 0.056434256893846665, 'eval_loss@spa.rst.rststb': 2.7215585708618164, 'eval_runtime': 4.9402, 'eval_samples_per_second': 77.528, 'eval_steps_per_second': 2.429, 'epoch': 2.0}
{'train@spa.rst.rststb_loss': 2.47522234916687, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.31473214285714285, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.04692956823118318, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.03761353424742366, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.06874256447178528, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.47522234916687, 'train@spa.rst.rststb_runtime': 27.0289, 'train@spa.rst.rststb_samples_per_second': 82.874, 'train@spa.rst.rststb_steps_per_second': 2.59, 'epoch': 3.0}
{'loss': 2.5796, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.627760410308838, 'eval_accuracy@spa.rst.rststb': 0.28720626631853785, 'eval_f1@spa.rst.rststb': 0.05208137444511346, 'eval_precision@spa.rst.rststb': 0.04295070871157828, 'eval_recall@spa.rst.rststb': 0.07413382131221115, 'eval_loss@spa.rst.rststb': 2.627760410308838, 'eval_runtime': 4.9602, 'eval_samples_per_second': 77.214, 'eval_steps_per_second': 2.419, 'epoch': 3.0}
{'train@spa.rst.rststb_loss': 2.3713924884796143, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.33125, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.06092489748688622, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.09249219414904851, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.07749768002517883, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.3713924884796143, 'train@spa.rst.rststb_runtime': 27.0473, 'train@spa.rst.rststb_samples_per_second': 82.818, 'train@spa.rst.rststb_steps_per_second': 2.588, 'epoch': 4.0}
{'loss': 2.4767, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.5539400577545166, 'eval_accuracy@spa.rst.rststb': 0.3002610966057441, 'eval_f1@spa.rst.rststb': 0.06274740648324746, 'eval_precision@spa.rst.rststb': 0.0706121980072593, 'eval_recall@spa.rst.rststb': 0.08061484634522033, 'eval_loss@spa.rst.rststb': 2.5539400577545166, 'eval_runtime': 4.9687, 'eval_samples_per_second': 77.082, 'eval_steps_per_second': 2.415, 'epoch': 4.0}
{'train@spa.rst.rststb_loss': 2.279332160949707, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.3723214285714286, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.08390595853483149, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.08677909338030744, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.1009563386295731, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.279332160949707, 'train@spa.rst.rststb_runtime': 27.0301, 'train@spa.rst.rststb_samples_per_second': 82.87, 'train@spa.rst.rststb_steps_per_second': 2.59, 'epoch': 5.0}
{'loss': 2.3727, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.4880144596099854, 'eval_accuracy@spa.rst.rststb': 0.3289817232375979, 'eval_f1@spa.rst.rststb': 0.08566425804387465, 'eval_precision@spa.rst.rststb': 0.11306426452206504, 'eval_recall@spa.rst.rststb': 0.10085776170288939, 'eval_loss@spa.rst.rststb': 2.4880144596099854, 'eval_runtime': 4.9422, 'eval_samples_per_second': 77.496, 'eval_steps_per_second': 2.428, 'epoch': 5.0}
{'train@spa.rst.rststb_loss': 2.206145763397217, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.3888392857142857, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.09033917321756858, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.08825679544581622, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.11173320507978879, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.206145763397217, 'train@spa.rst.rststb_runtime': 27.0546, 'train@spa.rst.rststb_samples_per_second': 82.796, 'train@spa.rst.rststb_steps_per_second': 2.587, 'epoch': 6.0}
{'loss': 2.2935, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.432229995727539, 'eval_accuracy@spa.rst.rststb': 0.3524804177545692, 'eval_f1@spa.rst.rststb': 0.10824010653536108, 'eval_precision@spa.rst.rststb': 0.11372480239886693, 'eval_recall@spa.rst.rststb': 0.12485671656726124, 'eval_loss@spa.rst.rststb': 2.43222975730896, 'eval_runtime': 4.9331, 'eval_samples_per_second': 77.638, 'eval_steps_per_second': 2.433, 'epoch': 6.0}
{'train@spa.rst.rststb_loss': 2.1463005542755127, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.40714285714285714, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.09929623263878391, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.12475077562225696, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.12327949038926121, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.1463005542755127, 'train@spa.rst.rststb_runtime': 26.9806, 'train@spa.rst.rststb_samples_per_second': 83.022, 'train@spa.rst.rststb_steps_per_second': 2.594, 'epoch': 7.0}
{'loss': 2.235, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.3895819187164307, 'eval_accuracy@spa.rst.rststb': 0.3577023498694517, 'eval_f1@spa.rst.rststb': 0.10874716613641316, 'eval_precision@spa.rst.rststb': 0.10702655353256615, 'eval_recall@spa.rst.rststb': 0.13006575276061638, 'eval_loss@spa.rst.rststb': 2.389582395553589, 'eval_runtime': 4.9411, 'eval_samples_per_second': 77.514, 'eval_steps_per_second': 2.429, 'epoch': 7.0}
{'train@spa.rst.rststb_loss': 2.103130340576172, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4191964285714286, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10501714354554134, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.14492010933960664, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.1285148261251021, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.103130340576172, 'train@spa.rst.rststb_runtime': 27.0224, 'train@spa.rst.rststb_samples_per_second': 82.894, 'train@spa.rst.rststb_steps_per_second': 2.59, 'epoch': 8.0}
{'loss': 2.178, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.3578505516052246, 'eval_accuracy@spa.rst.rststb': 0.3681462140992167, 'eval_f1@spa.rst.rststb': 0.11111806569148654, 'eval_precision@spa.rst.rststb': 0.10667498326611088, 'eval_recall@spa.rst.rststb': 0.13333989587901646, 'eval_loss@spa.rst.rststb': 2.3578505516052246, 'eval_runtime': 4.9433, 'eval_samples_per_second': 77.479, 'eval_steps_per_second': 2.428, 'epoch': 8.0}
{'train@spa.rst.rststb_loss': 2.069073438644409, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4232142857142857, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10774652435869116, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.12979962096191464, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.133290500599428, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.06907320022583, 'train@spa.rst.rststb_runtime': 27.0142, 'train@spa.rst.rststb_samples_per_second': 82.919, 'train@spa.rst.rststb_steps_per_second': 2.591, 'epoch': 9.0}
{'loss': 2.1423, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.329472541809082, 'eval_accuracy@spa.rst.rststb': 0.381201044386423, 'eval_f1@spa.rst.rststb': 0.1111703918471787, 'eval_precision@spa.rst.rststb': 0.09872398040607465, 'eval_recall@spa.rst.rststb': 0.1391257729037734, 'eval_loss@spa.rst.rststb': 2.329472541809082, 'eval_runtime': 4.92, 'eval_samples_per_second': 77.846, 'eval_steps_per_second': 2.439, 'epoch': 9.0}
{'train@spa.rst.rststb_loss': 2.0472500324249268, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.42723214285714284, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10972326875432567, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.13002757596322737, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13446339500267093, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.047250270843506, 'train@spa.rst.rststb_runtime': 26.9477, 'train@spa.rst.rststb_samples_per_second': 83.124, 'train@spa.rst.rststb_steps_per_second': 2.598, 'epoch': 10.0}
{'loss': 2.1133, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.3152012825012207, 'eval_accuracy@spa.rst.rststb': 0.3838120104438642, 'eval_f1@spa.rst.rststb': 0.11219297255668051, 'eval_precision@spa.rst.rststb': 0.1004481435194851, 'eval_recall@spa.rst.rststb': 0.13967613063629952, 'eval_loss@spa.rst.rststb': 2.3152010440826416, 'eval_runtime': 4.9401, 'eval_samples_per_second': 77.529, 'eval_steps_per_second': 2.429, 'epoch': 10.0}
{'train@spa.rst.rststb_loss': 2.0344126224517822, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4299107142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.11087706726601805, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.13143291146093133, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13563135034540494, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0344128608703613, 'train@spa.rst.rststb_runtime': 26.9857, 'train@spa.rst.rststb_samples_per_second': 83.007, 'train@spa.rst.rststb_steps_per_second': 2.594, 'epoch': 11.0}
{'loss': 2.0985, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.3040525913238525, 'eval_accuracy@spa.rst.rststb': 0.38642297650130547, 'eval_f1@spa.rst.rststb': 0.11323736195709053, 'eval_precision@spa.rst.rststb': 0.1018525449870129, 'eval_recall@spa.rst.rststb': 0.14032505990300945, 'eval_loss@spa.rst.rststb': 2.3040525913238525, 'eval_runtime': 4.9247, 'eval_samples_per_second': 77.772, 'eval_steps_per_second': 2.437, 'epoch': 11.0}
{'train@spa.rst.rststb_loss': 2.030353546142578, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.43080357142857145, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.11186595455734179, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.12744820354410083, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13675357597633536, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.030353546142578, 'train@spa.rst.rststb_runtime': 26.9807, 'train@spa.rst.rststb_samples_per_second': 83.022, 'train@spa.rst.rststb_steps_per_second': 2.594, 'epoch': 12.0}
{'loss': 2.0801, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.3012847900390625, 'eval_accuracy@spa.rst.rststb': 0.38642297650130547, 'eval_f1@spa.rst.rststb': 0.11174951863394295, 'eval_precision@spa.rst.rststb': 0.09959995789030425, 'eval_recall@spa.rst.rststb': 0.14032505990300945, 'eval_loss@spa.rst.rststb': 2.3012847900390625, 'eval_runtime': 4.9256, 'eval_samples_per_second': 77.757, 'eval_steps_per_second': 2.436, 'epoch': 12.0}
{'train_runtime': 1062.2209, 'train_samples_per_second': 25.305, 'train_steps_per_second': 0.791, 'train_loss': 2.392702138991583, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.3927
  train_runtime            = 0:17:42.22
  train_samples_per_second =     25.305
  train_steps_per_second   =      0.791
{'train@fra.sdrt.annodis_loss': 2.5503592491149902, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.2160183066361556, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.04833693754523137, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.05590037249104629, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.06400002331341072, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.550359010696411, 'train@fra.sdrt.annodis_runtime': 26.2799, 'train@fra.sdrt.annodis_samples_per_second': 83.144, 'train@fra.sdrt.annodis_steps_per_second': 2.626, 'epoch': 1.0}
{'loss': 2.9491, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.543060302734375, 'eval_accuracy@fra.sdrt.annodis': 0.20454545454545456, 'eval_f1@fra.sdrt.annodis': 0.037219394312576565, 'eval_precision@fra.sdrt.annodis': 0.04962922879589546, 'eval_recall@fra.sdrt.annodis': 0.05605056122385724, 'eval_loss@fra.sdrt.annodis': 2.543060541152954, 'eval_runtime': 6.6842, 'eval_samples_per_second': 78.993, 'eval_steps_per_second': 2.543, 'epoch': 1.0}
{'train@fra.sdrt.annodis_loss': 2.3457577228546143, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.26636155606407325, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.062054489166018224, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.07168418572697884, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.08236294092753202, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.345757484436035, 'train@fra.sdrt.annodis_runtime': 26.2705, 'train@fra.sdrt.annodis_samples_per_second': 83.173, 'train@fra.sdrt.annodis_steps_per_second': 2.627, 'epoch': 2.0}
{'loss': 2.4613, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.3482985496520996, 'eval_accuracy@fra.sdrt.annodis': 0.24053030303030304, 'eval_f1@fra.sdrt.annodis': 0.050422963826475264, 'eval_precision@fra.sdrt.annodis': 0.03950914146722869, 'eval_recall@fra.sdrt.annodis': 0.06966901134433151, 'eval_loss@fra.sdrt.annodis': 2.3482987880706787, 'eval_runtime': 6.6582, 'eval_samples_per_second': 79.3, 'eval_steps_per_second': 2.553, 'epoch': 2.0}
{'train@fra.sdrt.annodis_loss': 2.248866081237793, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.30068649885583526, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.07268232387847348, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.07842473498191925, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.09161911453548832, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.248866081237793, 'train@fra.sdrt.annodis_runtime': 26.2905, 'train@fra.sdrt.annodis_samples_per_second': 83.11, 'train@fra.sdrt.annodis_steps_per_second': 2.625, 'epoch': 3.0}
{'loss': 2.3238, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.262234926223755, 'eval_accuracy@fra.sdrt.annodis': 0.26704545454545453, 'eval_f1@fra.sdrt.annodis': 0.056701489720357635, 'eval_precision@fra.sdrt.annodis': 0.04848362301382436, 'eval_recall@fra.sdrt.annodis': 0.0764584245027305, 'eval_loss@fra.sdrt.annodis': 2.262234926223755, 'eval_runtime': 6.6236, 'eval_samples_per_second': 79.714, 'eval_steps_per_second': 2.567, 'epoch': 3.0}
{'train@fra.sdrt.annodis_loss': 2.172194242477417, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.34553775743707094, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.10229741657439373, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.1259740423909324, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.11412055307693952, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.172194480895996, 'train@fra.sdrt.annodis_runtime': 26.3041, 'train@fra.sdrt.annodis_samples_per_second': 83.067, 'train@fra.sdrt.annodis_steps_per_second': 2.623, 'epoch': 4.0}
{'loss': 2.2526, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.1925785541534424, 'eval_accuracy@fra.sdrt.annodis': 0.2859848484848485, 'eval_f1@fra.sdrt.annodis': 0.06837404225565709, 'eval_precision@fra.sdrt.annodis': 0.07336004035026476, 'eval_recall@fra.sdrt.annodis': 0.08532656017493556, 'eval_loss@fra.sdrt.annodis': 2.1925785541534424, 'eval_runtime': 6.6622, 'eval_samples_per_second': 79.253, 'eval_steps_per_second': 2.552, 'epoch': 4.0}
{'train@fra.sdrt.annodis_loss': 2.101876735687256, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.40091533180778033, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.1272131160892867, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.13148611021766576, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.14770936524092296, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.1018764972686768, 'train@fra.sdrt.annodis_runtime': 26.3184, 'train@fra.sdrt.annodis_samples_per_second': 83.022, 'train@fra.sdrt.annodis_steps_per_second': 2.622, 'epoch': 5.0}
{'loss': 2.1769, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.135981798171997, 'eval_accuracy@fra.sdrt.annodis': 0.32386363636363635, 'eval_f1@fra.sdrt.annodis': 0.08803948679381285, 'eval_precision@fra.sdrt.annodis': 0.11963663461293719, 'eval_recall@fra.sdrt.annodis': 0.10693953078662165, 'eval_loss@fra.sdrt.annodis': 2.135981559753418, 'eval_runtime': 6.667, 'eval_samples_per_second': 79.197, 'eval_steps_per_second': 2.55, 'epoch': 5.0}
{'train@fra.sdrt.annodis_loss': 2.0342421531677246, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.43157894736842106, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.1398953215797937, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.13023884378190193, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1680955314894368, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.0342421531677246, 'train@fra.sdrt.annodis_runtime': 26.3343, 'train@fra.sdrt.annodis_samples_per_second': 82.972, 'train@fra.sdrt.annodis_steps_per_second': 2.62, 'epoch': 6.0}
{'loss': 2.1138, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.0783638954162598, 'eval_accuracy@fra.sdrt.annodis': 0.3560606060606061, 'eval_f1@fra.sdrt.annodis': 0.10738517266093388, 'eval_precision@fra.sdrt.annodis': 0.10703987997348431, 'eval_recall@fra.sdrt.annodis': 0.12630757566283357, 'eval_loss@fra.sdrt.annodis': 2.0783638954162598, 'eval_runtime': 6.6391, 'eval_samples_per_second': 79.529, 'eval_steps_per_second': 2.561, 'epoch': 6.0}
{'train@fra.sdrt.annodis_loss': 1.9744877815246582, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4379862700228833, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.1421152090504102, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12740826692296534, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.17332384832245956, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.9744877815246582, 'train@fra.sdrt.annodis_runtime': 26.3113, 'train@fra.sdrt.annodis_samples_per_second': 83.044, 'train@fra.sdrt.annodis_steps_per_second': 2.622, 'epoch': 7.0}
{'loss': 2.0569, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.0284767150878906, 'eval_accuracy@fra.sdrt.annodis': 0.3731060606060606, 'eval_f1@fra.sdrt.annodis': 0.11552679942472727, 'eval_precision@fra.sdrt.annodis': 0.10721564741482835, 'eval_recall@fra.sdrt.annodis': 0.13590418700240858, 'eval_loss@fra.sdrt.annodis': 2.0284767150878906, 'eval_runtime': 6.6942, 'eval_samples_per_second': 78.874, 'eval_steps_per_second': 2.54, 'epoch': 7.0}
{'train@fra.sdrt.annodis_loss': 1.927652359008789, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.448512585812357, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.14576759970933878, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.129315009167786, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1794466330853407, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.927652359008789, 'train@fra.sdrt.annodis_runtime': 28.614, 'train@fra.sdrt.annodis_samples_per_second': 76.361, 'train@fra.sdrt.annodis_steps_per_second': 2.411, 'epoch': 8.0}
{'loss': 2.0077, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.989749789237976, 'eval_accuracy@fra.sdrt.annodis': 0.3768939393939394, 'eval_f1@fra.sdrt.annodis': 0.11737980433241962, 'eval_precision@fra.sdrt.annodis': 0.10673034273866838, 'eval_recall@fra.sdrt.annodis': 0.13935753141435864, 'eval_loss@fra.sdrt.annodis': 1.989749789237976, 'eval_runtime': 6.6672, 'eval_samples_per_second': 79.194, 'eval_steps_per_second': 2.55, 'epoch': 8.0}
{'train@fra.sdrt.annodis_loss': 1.8935444355010986, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.45125858123569795, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.14758965082055178, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.18570645456256463, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.18051724294041405, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.8935449123382568, 'train@fra.sdrt.annodis_runtime': 26.3755, 'train@fra.sdrt.annodis_samples_per_second': 82.842, 'train@fra.sdrt.annodis_steps_per_second': 2.616, 'epoch': 9.0}
{'loss': 1.9632, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.9626811742782593, 'eval_accuracy@fra.sdrt.annodis': 0.38636363636363635, 'eval_f1@fra.sdrt.annodis': 0.12133692743822522, 'eval_precision@fra.sdrt.annodis': 0.1111633501555037, 'eval_recall@fra.sdrt.annodis': 0.1443497979259785, 'eval_loss@fra.sdrt.annodis': 1.9626811742782593, 'eval_runtime': 6.6542, 'eval_samples_per_second': 79.349, 'eval_steps_per_second': 2.555, 'epoch': 9.0}
{'train@fra.sdrt.annodis_loss': 1.8699363470077515, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.45308924485125857, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.14928410246627108, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.1860067516285959, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.18218393710551256, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.8699363470077515, 'train@fra.sdrt.annodis_runtime': 26.3118, 'train@fra.sdrt.annodis_samples_per_second': 83.043, 'train@fra.sdrt.annodis_steps_per_second': 2.622, 'epoch': 10.0}
{'loss': 1.9337, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.9440529346466064, 'eval_accuracy@fra.sdrt.annodis': 0.38446969696969696, 'eval_f1@fra.sdrt.annodis': 0.1218522378222021, 'eval_precision@fra.sdrt.annodis': 0.11037887377173092, 'eval_recall@fra.sdrt.annodis': 0.1445933324425161, 'eval_loss@fra.sdrt.annodis': 1.9440526962280273, 'eval_runtime': 6.663, 'eval_samples_per_second': 79.244, 'eval_steps_per_second': 2.551, 'epoch': 10.0}
{'train@fra.sdrt.annodis_loss': 1.856275200843811, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.45491990846681923, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.14966831812895048, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.18550081311479202, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.18342404128543527, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.856275200843811, 'train@fra.sdrt.annodis_runtime': 26.3477, 'train@fra.sdrt.annodis_samples_per_second': 82.929, 'train@fra.sdrt.annodis_steps_per_second': 2.619, 'epoch': 11.0}
{'loss': 1.9109, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.9315075874328613, 'eval_accuracy@fra.sdrt.annodis': 0.38636363636363635, 'eval_f1@fra.sdrt.annodis': 0.12312072303260349, 'eval_precision@fra.sdrt.annodis': 0.1102297664129072, 'eval_recall@fra.sdrt.annodis': 0.14650829087921366, 'eval_loss@fra.sdrt.annodis': 1.9315077066421509, 'eval_runtime': 6.6701, 'eval_samples_per_second': 79.159, 'eval_steps_per_second': 2.549, 'epoch': 11.0}
{'train@fra.sdrt.annodis_loss': 1.8516414165496826, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.45675057208237985, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.15208304507796308, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.18549068560380463, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.18493253755400485, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.8516414165496826, 'train@fra.sdrt.annodis_runtime': 26.2902, 'train@fra.sdrt.annodis_samples_per_second': 83.111, 'train@fra.sdrt.annodis_steps_per_second': 2.625, 'epoch': 12.0}
{'loss': 1.9131, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.92815101146698, 'eval_accuracy@fra.sdrt.annodis': 0.3958333333333333, 'eval_f1@fra.sdrt.annodis': 0.13081700159570478, 'eval_precision@fra.sdrt.annodis': 0.16823954481411516, 'eval_recall@fra.sdrt.annodis': 0.15192814493030765, 'eval_loss@fra.sdrt.annodis': 1.928151249885559, 'eval_runtime': 6.6426, 'eval_samples_per_second': 79.486, 'eval_steps_per_second': 2.559, 'epoch': 12.0}
{'train_runtime': 1061.0424, 'train_samples_per_second': 24.712, 'train_steps_per_second': 0.78, 'train_loss': 2.1719250517766833, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.3927
  train_runtime            = 0:17:42.22
  train_samples_per_second =     25.305
  train_steps_per_second   =      0.791
-------------------------------------------------------------------
Lang1:  spa.rst.sctb    Lang2:  fra.sdrt.annodis
Saving run to:  runs/full_shot/FullShot=v4_spa.rst.sctb_fra.sdrt.annodis_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 439 examples
read 94 examples
read 159 examples
read 2185 examples
read 528 examples
read 625 examples
Total prediction labels:  38
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (fr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=38, bias=True)
    )
  )
)
{'train@spa.rst.sctb_loss': 3.346691370010376, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.2619589977220957, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.04545277234504005, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.04111885513363998, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.06018900901965418, 'train@spa.rst.sctb_loss@spa.rst.sctb': 3.346691370010376, 'train@spa.rst.sctb_runtime': 5.5395, 'train@spa.rst.sctb_samples_per_second': 79.249, 'train@spa.rst.sctb_steps_per_second': 2.527, 'epoch': 1.0}
{'loss': 3.5174, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.353771686553955, 'eval_accuracy@spa.rst.sctb': 0.1595744680851064, 'eval_f1@spa.rst.sctb': 0.03752535496957404, 'eval_precision@spa.rst.sctb': 0.03870306292451621, 'eval_recall@spa.rst.sctb': 0.03780842480532883, 'eval_loss@spa.rst.sctb': 3.353771448135376, 'eval_runtime': 1.4606, 'eval_samples_per_second': 64.358, 'eval_steps_per_second': 2.054, 'epoch': 1.0}
{'train@spa.rst.sctb_loss': 3.0713436603546143, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.39863325740318906, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.04003747636925207, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03188943465530086, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05576164874551972, 'train@spa.rst.sctb_loss@spa.rst.sctb': 3.0713436603546143, 'train@spa.rst.sctb_runtime': 5.5457, 'train@spa.rst.sctb_samples_per_second': 79.161, 'train@spa.rst.sctb_steps_per_second': 2.524, 'epoch': 2.0}
{'loss': 3.2341, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.0781333446502686, 'eval_accuracy@spa.rst.sctb': 0.3404255319148936, 'eval_f1@spa.rst.sctb': 0.04634829541572726, 'eval_precision@spa.rst.sctb': 0.03728461081402258, 'eval_recall@spa.rst.sctb': 0.06492166244488225, 'eval_loss@spa.rst.sctb': 3.0781328678131104, 'eval_runtime': 1.4575, 'eval_samples_per_second': 64.495, 'eval_steps_per_second': 2.058, 'epoch': 2.0}
{'train@spa.rst.sctb_loss': 2.802377939224243, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.36674259681093396, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.033492497120264, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.028381303490780246, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.048467741935483866, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.802377939224243, 'train@spa.rst.sctb_runtime': 5.5376, 'train@spa.rst.sctb_samples_per_second': 79.276, 'train@spa.rst.sctb_steps_per_second': 2.528, 'epoch': 3.0}
{'loss': 2.9806, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.815239191055298, 'eval_accuracy@spa.rst.sctb': 0.3617021276595745, 'eval_f1@spa.rst.sctb': 0.043098754228199, 'eval_precision@spa.rst.sctb': 0.036944045911047343, 'eval_recall@spa.rst.sctb': 0.06454639271976734, 'eval_loss@spa.rst.sctb': 2.815239191055298, 'eval_runtime': 1.4497, 'eval_samples_per_second': 64.839, 'eval_steps_per_second': 2.069, 'epoch': 3.0}
{'train@spa.rst.sctb_loss': 2.5797810554504395, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3553530751708428, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02849625061855278, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.027609842265014683, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04503584229390681, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.5797808170318604, 'train@spa.rst.sctb_runtime': 5.5425, 'train@spa.rst.sctb_samples_per_second': 79.206, 'train@spa.rst.sctb_steps_per_second': 2.526, 'epoch': 4.0}
{'loss': 2.7204, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.605675458908081, 'eval_accuracy@spa.rst.sctb': 0.35106382978723405, 'eval_f1@spa.rst.sctb': 0.035819154107924155, 'eval_precision@spa.rst.sctb': 0.031194295900178252, 'eval_recall@spa.rst.sctb': 0.060136973449666946, 'eval_loss@spa.rst.sctb': 2.605675458908081, 'eval_runtime': 1.4617, 'eval_samples_per_second': 64.309, 'eval_steps_per_second': 2.052, 'epoch': 4.0}
{'train@spa.rst.sctb_loss': 2.427727222442627, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3439635535307517, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02390659747961453, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.020705172020961494, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.042455197132616494, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.427726984024048, 'train@spa.rst.sctb_runtime': 5.5452, 'train@spa.rst.sctb_samples_per_second': 79.168, 'train@spa.rst.sctb_steps_per_second': 2.525, 'epoch': 5.0}
{'loss': 2.5425, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.462703227996826, 'eval_accuracy@spa.rst.sctb': 0.3404255319148936, 'eval_f1@spa.rst.sctb': 0.03011764705882353, 'eval_precision@spa.rst.sctb': 0.020460358056265983, 'eval_recall@spa.rst.sctb': 0.05704099821746881, 'eval_loss@spa.rst.sctb': 2.462702751159668, 'eval_runtime': 1.441, 'eval_samples_per_second': 65.234, 'eval_steps_per_second': 2.082, 'epoch': 5.0}
{'train@spa.rst.sctb_loss': 2.33760404586792, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.33940774487471526, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02222809156169579, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.017485994397759105, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.041559139784946235, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.33760404586792, 'train@spa.rst.sctb_runtime': 5.5613, 'train@spa.rst.sctb_samples_per_second': 78.938, 'train@spa.rst.sctb_steps_per_second': 2.517, 'epoch': 6.0}
{'loss': 2.4191, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.378817319869995, 'eval_accuracy@spa.rst.sctb': 0.3404255319148936, 'eval_f1@spa.rst.sctb': 0.03011764705882353, 'eval_precision@spa.rst.sctb': 0.020460358056265983, 'eval_recall@spa.rst.sctb': 0.05704099821746881, 'eval_loss@spa.rst.sctb': 2.3788177967071533, 'eval_runtime': 1.4548, 'eval_samples_per_second': 64.613, 'eval_steps_per_second': 2.062, 'epoch': 6.0}
{'train@spa.rst.sctb_loss': 2.2831878662109375, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3553530751708428, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02801169335485841, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.02747555012224939, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04486559139784946, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.2831878662109375, 'train@spa.rst.sctb_runtime': 5.5419, 'train@spa.rst.sctb_samples_per_second': 79.214, 'train@spa.rst.sctb_steps_per_second': 2.526, 'epoch': 7.0}
{'loss': 2.3527, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.3297746181488037, 'eval_accuracy@spa.rst.sctb': 0.3617021276595745, 'eval_f1@spa.rst.sctb': 0.04066216650594663, 'eval_precision@spa.rst.sctb': 0.04467944481163252, 'eval_recall@spa.rst.sctb': 0.06323294868186509, 'eval_loss@spa.rst.sctb': 2.329775094985962, 'eval_runtime': 1.4693, 'eval_samples_per_second': 63.977, 'eval_steps_per_second': 2.042, 'epoch': 7.0}
{'train@spa.rst.sctb_loss': 2.2511250972747803, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3621867881548975, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.030350784832573672, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.029424793279958002, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04637992831541219, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.251124858856201, 'train@spa.rst.sctb_runtime': 5.5878, 'train@spa.rst.sctb_samples_per_second': 78.564, 'train@spa.rst.sctb_steps_per_second': 2.505, 'epoch': 8.0}
{'loss': 2.2937, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.302464485168457, 'eval_accuracy@spa.rst.sctb': 0.3617021276595745, 'eval_f1@spa.rst.sctb': 0.04052503646086533, 'eval_precision@spa.rst.sctb': 0.040998217468805706, 'eval_recall@spa.rst.sctb': 0.06323294868186509, 'eval_loss@spa.rst.sctb': 2.302464485168457, 'eval_runtime': 1.4774, 'eval_samples_per_second': 63.625, 'eval_steps_per_second': 2.031, 'epoch': 8.0}
{'train@spa.rst.sctb_loss': 2.229613780975342, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3826879271070615, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03553395368749764, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03288768183513614, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05058243727598566, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.229613780975342, 'train@spa.rst.sctb_runtime': 5.5596, 'train@spa.rst.sctb_samples_per_second': 78.962, 'train@spa.rst.sctb_steps_per_second': 2.518, 'epoch': 9.0}
{'loss': 2.276, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.2836954593658447, 'eval_accuracy@spa.rst.sctb': 0.39361702127659576, 'eval_f1@spa.rst.sctb': 0.05246094901267315, 'eval_precision@spa.rst.sctb': 0.05182072829131652, 'eval_recall@spa.rst.sctb': 0.07252087437845951, 'eval_loss@spa.rst.sctb': 2.2836952209472656, 'eval_runtime': 1.4784, 'eval_samples_per_second': 63.582, 'eval_steps_per_second': 2.029, 'epoch': 9.0}
{'train@spa.rst.sctb_loss': 2.2152295112609863, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.39635535307517084, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.038509021842355176, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03433215824520172, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05344086021505376, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.2152297496795654, 'train@spa.rst.sctb_runtime': 5.5389, 'train@spa.rst.sctb_samples_per_second': 79.257, 'train@spa.rst.sctb_steps_per_second': 2.528, 'epoch': 10.0}
{'loss': 2.2604, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.272214412689209, 'eval_accuracy@spa.rst.sctb': 0.39361702127659576, 'eval_f1@spa.rst.sctb': 0.050914483440435, 'eval_precision@spa.rst.sctb': 0.04343509555721022, 'eval_recall@spa.rst.sctb': 0.07252087437845951, 'eval_loss@spa.rst.sctb': 2.272214412689209, 'eval_runtime': 1.458, 'eval_samples_per_second': 64.474, 'eval_steps_per_second': 2.058, 'epoch': 10.0}
{'train@spa.rst.sctb_loss': 2.2073018550872803, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.39863325740318906, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03910722244830727, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03434221893126003, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05405913978494623, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.2073018550872803, 'train@spa.rst.sctb_runtime': 5.5872, 'train@spa.rst.sctb_samples_per_second': 78.572, 'train@spa.rst.sctb_steps_per_second': 2.506, 'epoch': 11.0}
{'loss': 2.2472, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.2654941082000732, 'eval_accuracy@spa.rst.sctb': 0.40425531914893614, 'eval_f1@spa.rst.sctb': 0.054084336437277614, 'eval_precision@spa.rst.sctb': 0.04619155354449472, 'eval_recall@spa.rst.sctb': 0.07561684961065766, 'eval_loss@spa.rst.sctb': 2.265493631362915, 'eval_runtime': 1.4664, 'eval_samples_per_second': 64.103, 'eval_steps_per_second': 2.046, 'epoch': 11.0}
{'train@spa.rst.sctb_loss': 2.2047276496887207, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4009111617312073, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03945121017156863, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.034316627203367533, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05450716845878136, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.2047276496887207, 'train@spa.rst.sctb_runtime': 5.5771, 'train@spa.rst.sctb_samples_per_second': 78.715, 'train@spa.rst.sctb_steps_per_second': 2.51, 'epoch': 12.0}
{'loss': 2.2392, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.2635772228240967, 'eval_accuracy@spa.rst.sctb': 0.40425531914893614, 'eval_f1@spa.rst.sctb': 0.054084336437277614, 'eval_precision@spa.rst.sctb': 0.04619155354449472, 'eval_recall@spa.rst.sctb': 0.07561684961065766, 'eval_loss@spa.rst.sctb': 2.263577461242676, 'eval_runtime': 1.4434, 'eval_samples_per_second': 65.124, 'eval_steps_per_second': 2.078, 'epoch': 12.0}
{'train_runtime': 217.4164, 'train_samples_per_second': 24.23, 'train_steps_per_second': 0.773, 'train_loss': 2.590266023363386, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.5903
  train_runtime            = 0:03:37.41
  train_samples_per_second =      24.23
  train_steps_per_second   =      0.773
{'train@fra.sdrt.annodis_loss': 2.6063809394836426, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.20869565217391303, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.04852651759749949, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.050887845623149124, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.06685717001632602, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.6063807010650635, 'train@fra.sdrt.annodis_runtime': 26.2745, 'train@fra.sdrt.annodis_samples_per_second': 83.161, 'train@fra.sdrt.annodis_steps_per_second': 2.626, 'epoch': 1.0}
{'loss': 3.1423, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.6094582080841064, 'eval_accuracy@fra.sdrt.annodis': 0.18181818181818182, 'eval_f1@fra.sdrt.annodis': 0.04315200105376514, 'eval_precision@fra.sdrt.annodis': 0.04444366515915979, 'eval_recall@fra.sdrt.annodis': 0.05902955116990363, 'eval_loss@fra.sdrt.annodis': 2.6094582080841064, 'eval_runtime': 6.6114, 'eval_samples_per_second': 79.862, 'eval_steps_per_second': 2.571, 'epoch': 1.0}
{'train@fra.sdrt.annodis_loss': 2.3734188079833984, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.2768878718535469, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.06004223519468213, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.10259957892442954, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.08258006653135633, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.3734188079833984, 'train@fra.sdrt.annodis_runtime': 26.2695, 'train@fra.sdrt.annodis_samples_per_second': 83.176, 'train@fra.sdrt.annodis_steps_per_second': 2.627, 'epoch': 2.0}
{'loss': 2.4913, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.3798539638519287, 'eval_accuracy@fra.sdrt.annodis': 0.26515151515151514, 'eval_f1@fra.sdrt.annodis': 0.058508928860842135, 'eval_precision@fra.sdrt.annodis': 0.05853959222380275, 'eval_recall@fra.sdrt.annodis': 0.07885044660595099, 'eval_loss@fra.sdrt.annodis': 2.3798537254333496, 'eval_runtime': 6.6299, 'eval_samples_per_second': 79.639, 'eval_steps_per_second': 2.564, 'epoch': 2.0}
{'train@fra.sdrt.annodis_loss': 2.2976067066192627, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.2723112128146453, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.057055167100570695, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.053609925499481334, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.07911134380358831, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.2976067066192627, 'train@fra.sdrt.annodis_runtime': 26.2624, 'train@fra.sdrt.annodis_samples_per_second': 83.199, 'train@fra.sdrt.annodis_steps_per_second': 2.627, 'epoch': 3.0}
{'loss': 2.3622, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.3126204013824463, 'eval_accuracy@fra.sdrt.annodis': 0.2518939393939394, 'eval_f1@fra.sdrt.annodis': 0.05499693779224227, 'eval_precision@fra.sdrt.annodis': 0.1017125004838145, 'eval_recall@fra.sdrt.annodis': 0.07421947433911856, 'eval_loss@fra.sdrt.annodis': 2.3126204013824463, 'eval_runtime': 6.6325, 'eval_samples_per_second': 79.608, 'eval_steps_per_second': 2.563, 'epoch': 3.0}
{'train@fra.sdrt.annodis_loss': 2.2409844398498535, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.297025171624714, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.06446206314511867, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.08424893528962865, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.0870421051629098, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.2409846782684326, 'train@fra.sdrt.annodis_runtime': 26.2937, 'train@fra.sdrt.annodis_samples_per_second': 83.1, 'train@fra.sdrt.annodis_steps_per_second': 2.624, 'epoch': 4.0}
{'loss': 2.3029, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.260719060897827, 'eval_accuracy@fra.sdrt.annodis': 0.2765151515151515, 'eval_f1@fra.sdrt.annodis': 0.06216807599337721, 'eval_precision@fra.sdrt.annodis': 0.0787563612598746, 'eval_recall@fra.sdrt.annodis': 0.08116983071834306, 'eval_loss@fra.sdrt.annodis': 2.260719060897827, 'eval_runtime': 6.6188, 'eval_samples_per_second': 79.772, 'eval_steps_per_second': 2.568, 'epoch': 4.0}
{'train@fra.sdrt.annodis_loss': 2.19114089012146, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.32814645308924484, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.07834155900207374, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.11778148735948211, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.10306661555748226, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.191140651702881, 'train@fra.sdrt.annodis_runtime': 26.2654, 'train@fra.sdrt.annodis_samples_per_second': 83.189, 'train@fra.sdrt.annodis_steps_per_second': 2.627, 'epoch': 5.0}
{'loss': 2.2482, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.22188663482666, 'eval_accuracy@fra.sdrt.annodis': 0.29545454545454547, 'eval_f1@fra.sdrt.annodis': 0.06980693030352024, 'eval_precision@fra.sdrt.annodis': 0.11421898306789045, 'eval_recall@fra.sdrt.annodis': 0.09220361815552675, 'eval_loss@fra.sdrt.annodis': 2.221886157989502, 'eval_runtime': 6.6245, 'eval_samples_per_second': 79.704, 'eval_steps_per_second': 2.566, 'epoch': 5.0}
{'train@fra.sdrt.annodis_loss': 2.1435353755950928, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.3826086956521739, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.1214400572397007, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12541899154903058, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.14330502531732672, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.1435353755950928, 'train@fra.sdrt.annodis_runtime': 26.2503, 'train@fra.sdrt.annodis_samples_per_second': 83.237, 'train@fra.sdrt.annodis_steps_per_second': 2.629, 'epoch': 6.0}
{'loss': 2.2034, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.182586431503296, 'eval_accuracy@fra.sdrt.annodis': 0.29734848484848486, 'eval_f1@fra.sdrt.annodis': 0.08120156506754934, 'eval_precision@fra.sdrt.annodis': 0.08207239857183622, 'eval_recall@fra.sdrt.annodis': 0.10113850441922617, 'eval_loss@fra.sdrt.annodis': 2.182586431503296, 'eval_runtime': 6.6087, 'eval_samples_per_second': 79.895, 'eval_steps_per_second': 2.572, 'epoch': 6.0}
{'train@fra.sdrt.annodis_loss': 2.0989737510681152, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4059496567505721, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.12969228800263077, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12435301532479798, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.15705754341148026, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.0989737510681152, 'train@fra.sdrt.annodis_runtime': 26.2652, 'train@fra.sdrt.annodis_samples_per_second': 83.19, 'train@fra.sdrt.annodis_steps_per_second': 2.627, 'epoch': 7.0}
{'loss': 2.1584, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.147705554962158, 'eval_accuracy@fra.sdrt.annodis': 0.32954545454545453, 'eval_f1@fra.sdrt.annodis': 0.09464887087457757, 'eval_precision@fra.sdrt.annodis': 0.08778935185185184, 'eval_recall@fra.sdrt.annodis': 0.11713328281907041, 'eval_loss@fra.sdrt.annodis': 2.147705316543579, 'eval_runtime': 6.615, 'eval_samples_per_second': 79.819, 'eval_steps_per_second': 2.57, 'epoch': 7.0}
{'train@fra.sdrt.annodis_loss': 2.060482978820801, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.41601830663615563, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.13235322899936294, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12456025596223233, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.16364605421611267, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.060482978820801, 'train@fra.sdrt.annodis_runtime': 26.256, 'train@fra.sdrt.annodis_samples_per_second': 83.219, 'train@fra.sdrt.annodis_steps_per_second': 2.628, 'epoch': 8.0}
{'loss': 2.1299, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.1166343688964844, 'eval_accuracy@fra.sdrt.annodis': 0.3352272727272727, 'eval_f1@fra.sdrt.annodis': 0.09888194259305548, 'eval_precision@fra.sdrt.annodis': 0.09004473839943827, 'eval_recall@fra.sdrt.annodis': 0.12352454273591773, 'eval_loss@fra.sdrt.annodis': 2.1166343688964844, 'eval_runtime': 6.6186, 'eval_samples_per_second': 79.775, 'eval_steps_per_second': 2.569, 'epoch': 8.0}
{'train@fra.sdrt.annodis_loss': 2.0315840244293213, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.42242562929061783, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.13450347683521688, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.1254053729121102, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.16727957152791478, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.031583786010742, 'train@fra.sdrt.annodis_runtime': 26.255, 'train@fra.sdrt.annodis_samples_per_second': 83.222, 'train@fra.sdrt.annodis_steps_per_second': 2.628, 'epoch': 9.0}
{'loss': 2.0877, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.0930004119873047, 'eval_accuracy@fra.sdrt.annodis': 0.3371212121212121, 'eval_f1@fra.sdrt.annodis': 0.0982941723523956, 'eval_precision@fra.sdrt.annodis': 0.08928212553212553, 'eval_recall@fra.sdrt.annodis': 0.12421041379215916, 'eval_loss@fra.sdrt.annodis': 2.0930001735687256, 'eval_runtime': 6.6298, 'eval_samples_per_second': 79.641, 'eval_steps_per_second': 2.564, 'epoch': 9.0}
{'train@fra.sdrt.annodis_loss': 2.0104167461395264, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4251716247139588, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.13560701224746, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12380948471507103, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.16865205434094638, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.0104167461395264, 'train@fra.sdrt.annodis_runtime': 26.2328, 'train@fra.sdrt.annodis_samples_per_second': 83.293, 'train@fra.sdrt.annodis_steps_per_second': 2.63, 'epoch': 10.0}
{'loss': 2.0603, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.075974941253662, 'eval_accuracy@fra.sdrt.annodis': 0.3465909090909091, 'eval_f1@fra.sdrt.annodis': 0.10198052036592566, 'eval_precision@fra.sdrt.annodis': 0.09046817768367404, 'eval_recall@fra.sdrt.annodis': 0.12855363169036427, 'eval_loss@fra.sdrt.annodis': 2.075974941253662, 'eval_runtime': 6.6244, 'eval_samples_per_second': 79.705, 'eval_steps_per_second': 2.566, 'epoch': 10.0}
{'train@fra.sdrt.annodis_loss': 1.9981125593185425, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4265446224256293, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.1360924038530098, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12319190381654804, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.16973199990067409, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.9981125593185425, 'train@fra.sdrt.annodis_runtime': 26.2428, 'train@fra.sdrt.annodis_samples_per_second': 83.261, 'train@fra.sdrt.annodis_steps_per_second': 2.629, 'epoch': 11.0}
{'loss': 2.0411, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.0652332305908203, 'eval_accuracy@fra.sdrt.annodis': 0.3409090909090909, 'eval_f1@fra.sdrt.annodis': 0.10002528047664423, 'eval_precision@fra.sdrt.annodis': 0.08820378837951819, 'eval_recall@fra.sdrt.annodis': 0.12720008014643544, 'eval_loss@fra.sdrt.annodis': 2.0652334690093994, 'eval_runtime': 6.6183, 'eval_samples_per_second': 79.778, 'eval_steps_per_second': 2.569, 'epoch': 11.0}
{'train@fra.sdrt.annodis_loss': 1.993790864944458, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4283752860411899, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.1368135718039223, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.1233082862841249, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1705315910392773, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.993790864944458, 'train@fra.sdrt.annodis_runtime': 26.2748, 'train@fra.sdrt.annodis_samples_per_second': 83.16, 'train@fra.sdrt.annodis_steps_per_second': 2.626, 'epoch': 12.0}
{'loss': 2.0404, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.0620319843292236, 'eval_accuracy@fra.sdrt.annodis': 0.3428030303030303, 'eval_f1@fra.sdrt.annodis': 0.10172992013975052, 'eval_precision@fra.sdrt.annodis': 0.09026216405945231, 'eval_recall@fra.sdrt.annodis': 0.12838909931595033, 'eval_loss@fra.sdrt.annodis': 2.062032461166382, 'eval_runtime': 6.6324, 'eval_samples_per_second': 79.61, 'eval_steps_per_second': 2.563, 'epoch': 12.0}
{'train_runtime': 1057.5821, 'train_samples_per_second': 24.792, 'train_steps_per_second': 0.783, 'train_loss': 2.272343548023758, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.5903
  train_runtime            = 0:03:37.41
  train_samples_per_second =      24.23
  train_steps_per_second   =      0.773
-------------------------------------------------------------------
Lang1:  tur.pdtb.tdb    Lang2:  fra.sdrt.annodis
Saving run to:  runs/full_shot/FullShot=v4_tur.pdtb.tdb_fra.sdrt.annodis_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2451 examples
read 312 examples
read 422 examples
read 2185 examples
read 528 examples
read 625 examples
Total prediction labels:  41
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (tr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (fr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (tr): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=41, bias=True)
    )
  )
)
{'train@tur.pdtb.tdb_loss': 2.86989164352417, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01741967515426626, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010891738953044898, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.869891405105591, 'train@tur.pdtb.tdb_runtime': 29.3763, 'train@tur.pdtb.tdb_samples_per_second': 83.434, 'train@tur.pdtb.tdb_steps_per_second': 2.621, 'epoch': 1.0}
{'loss': 3.295, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.7993693351745605, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.7993693351745605, 'eval_runtime': 4.0895, 'eval_samples_per_second': 76.292, 'eval_steps_per_second': 2.445, 'epoch': 1.0}
{'train@tur.pdtb.tdb_loss': 2.451303243637085, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01741967515426626, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010891738953044898, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.451303243637085, 'train@tur.pdtb.tdb_runtime': 29.4206, 'train@tur.pdtb.tdb_samples_per_second': 83.309, 'train@tur.pdtb.tdb_steps_per_second': 2.617, 'epoch': 2.0}
{'loss': 2.6345, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.3472375869750977, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.3472371101379395, 'eval_runtime': 4.117, 'eval_samples_per_second': 75.783, 'eval_steps_per_second': 2.429, 'epoch': 2.0}
{'train@tur.pdtb.tdb_loss': 2.3653573989868164, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01741967515426626, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010891738953044898, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.3653573989868164, 'train@tur.pdtb.tdb_runtime': 29.4579, 'train@tur.pdtb.tdb_samples_per_second': 83.204, 'train@tur.pdtb.tdb_steps_per_second': 2.614, 'epoch': 3.0}
{'loss': 2.4322, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.297896146774292, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.297895908355713, 'eval_runtime': 4.0885, 'eval_samples_per_second': 76.311, 'eval_steps_per_second': 2.446, 'epoch': 3.0}
{'train@tur.pdtb.tdb_loss': 2.3008038997650146, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.2762137902896777, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.035185940457190215, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.053163339187991536, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.055615039784501816, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.3008038997650146, 'train@tur.pdtb.tdb_runtime': 29.4626, 'train@tur.pdtb.tdb_samples_per_second': 83.19, 'train@tur.pdtb.tdb_steps_per_second': 2.613, 'epoch': 4.0}
{'loss': 2.3589, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.260359525680542, 'eval_accuracy@tur.pdtb.tdb': 0.26282051282051283, 'eval_f1@tur.pdtb.tdb': 0.02139819245082403, 'eval_precision@tur.pdtb.tdb': 0.01652892561983471, 'eval_recall@tur.pdtb.tdb': 0.04569615359835062, 'eval_loss@tur.pdtb.tdb': 2.260359525680542, 'eval_runtime': 5.0523, 'eval_samples_per_second': 61.755, 'eval_steps_per_second': 1.979, 'epoch': 4.0}
{'train@tur.pdtb.tdb_loss': 2.25331974029541, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3043655650754794, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.07440601317363355, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.08932069440197547, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.09069774878465883, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.2533202171325684, 'train@tur.pdtb.tdb_runtime': 29.5118, 'train@tur.pdtb.tdb_samples_per_second': 83.051, 'train@tur.pdtb.tdb_steps_per_second': 2.609, 'epoch': 5.0}
{'loss': 2.315, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.224963426589966, 'eval_accuracy@tur.pdtb.tdb': 0.2916666666666667, 'eval_f1@tur.pdtb.tdb': 0.06993506493506495, 'eval_precision@tur.pdtb.tdb': 0.06329800817160368, 'eval_recall@tur.pdtb.tdb': 0.09794704665363772, 'eval_loss@tur.pdtb.tdb': 2.2249631881713867, 'eval_runtime': 4.123, 'eval_samples_per_second': 75.674, 'eval_steps_per_second': 2.425, 'epoch': 5.0}
{'train@tur.pdtb.tdb_loss': 2.2101633548736572, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.31701346389228885, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.07894639090829685, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.08825273184790819, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.10121588147895243, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.210163116455078, 'train@tur.pdtb.tdb_runtime': 29.5, 'train@tur.pdtb.tdb_samples_per_second': 83.085, 'train@tur.pdtb.tdb_steps_per_second': 2.61, 'epoch': 6.0}
{'loss': 2.2713, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.198471784591675, 'eval_accuracy@tur.pdtb.tdb': 0.2948717948717949, 'eval_f1@tur.pdtb.tdb': 0.07268714742844042, 'eval_precision@tur.pdtb.tdb': 0.06538198205522439, 'eval_recall@tur.pdtb.tdb': 0.1000731983191232, 'eval_loss@tur.pdtb.tdb': 2.198472023010254, 'eval_runtime': 4.0872, 'eval_samples_per_second': 76.336, 'eval_steps_per_second': 2.447, 'epoch': 6.0}
{'train@tur.pdtb.tdb_loss': 2.181759834289551, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3263973888208894, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.08366292218312886, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.11143171589629065, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.10716373041060645, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.181759834289551, 'train@tur.pdtb.tdb_runtime': 29.4924, 'train@tur.pdtb.tdb_samples_per_second': 83.106, 'train@tur.pdtb.tdb_steps_per_second': 2.611, 'epoch': 7.0}
{'loss': 2.2384, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.181675910949707, 'eval_accuracy@tur.pdtb.tdb': 0.2916666666666667, 'eval_f1@tur.pdtb.tdb': 0.07009014777580026, 'eval_precision@tur.pdtb.tdb': 0.06571704810341174, 'eval_recall@tur.pdtb.tdb': 0.10082957576255577, 'eval_loss@tur.pdtb.tdb': 2.181676149368286, 'eval_runtime': 4.1069, 'eval_samples_per_second': 75.97, 'eval_steps_per_second': 2.435, 'epoch': 7.0}
{'train@tur.pdtb.tdb_loss': 2.16650390625, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3312933496532028, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.08656348545140155, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.10489658508815623, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.10892553798990287, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.16650390625, 'train@tur.pdtb.tdb_runtime': 29.5169, 'train@tur.pdtb.tdb_samples_per_second': 83.037, 'train@tur.pdtb.tdb_steps_per_second': 2.609, 'epoch': 8.0}
{'loss': 2.2153, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.1686620712280273, 'eval_accuracy@tur.pdtb.tdb': 0.2948717948717949, 'eval_f1@tur.pdtb.tdb': 0.06896538780596752, 'eval_precision@tur.pdtb.tdb': 0.06282063372312335, 'eval_recall@tur.pdtb.tdb': 0.10137722088851414, 'eval_loss@tur.pdtb.tdb': 2.1686620712280273, 'eval_runtime': 4.0965, 'eval_samples_per_second': 76.162, 'eval_steps_per_second': 2.441, 'epoch': 8.0}
{'train@tur.pdtb.tdb_loss': 2.144833564758301, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3312933496532028, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.08467126118717765, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.09329865711761916, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11005754855563336, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.144833564758301, 'train@tur.pdtb.tdb_runtime': 29.4658, 'train@tur.pdtb.tdb_samples_per_second': 83.181, 'train@tur.pdtb.tdb_steps_per_second': 2.613, 'epoch': 9.0}
{'loss': 2.1912, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.1539881229400635, 'eval_accuracy@tur.pdtb.tdb': 0.2916666666666667, 'eval_f1@tur.pdtb.tdb': 0.06606704020121044, 'eval_precision@tur.pdtb.tdb': 0.06339879722341034, 'eval_recall@tur.pdtb.tdb': 0.10090384789565143, 'eval_loss@tur.pdtb.tdb': 2.1539883613586426, 'eval_runtime': 4.0969, 'eval_samples_per_second': 76.156, 'eval_steps_per_second': 2.441, 'epoch': 9.0}
{'train@tur.pdtb.tdb_loss': 2.133291721343994, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3353733170134639, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.08725414711803091, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.09819843207875963, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11166642281572103, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.133291482925415, 'train@tur.pdtb.tdb_runtime': 29.4526, 'train@tur.pdtb.tdb_samples_per_second': 83.219, 'train@tur.pdtb.tdb_steps_per_second': 2.614, 'epoch': 10.0}
{'loss': 2.1784, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.145646810531616, 'eval_accuracy@tur.pdtb.tdb': 0.30448717948717946, 'eval_f1@tur.pdtb.tdb': 0.07190578259971488, 'eval_precision@tur.pdtb.tdb': 0.06447403606494516, 'eval_recall@tur.pdtb.tdb': 0.1061028972123478, 'eval_loss@tur.pdtb.tdb': 2.145646810531616, 'eval_runtime': 4.0981, 'eval_samples_per_second': 76.133, 'eval_steps_per_second': 2.44, 'epoch': 10.0}
{'train@tur.pdtb.tdb_loss': 2.130002498626709, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3325173398612811, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.08609601636971752, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.09604307432641464, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11034497900890465, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.130002498626709, 'train@tur.pdtb.tdb_runtime': 29.4854, 'train@tur.pdtb.tdb_samples_per_second': 83.126, 'train@tur.pdtb.tdb_steps_per_second': 2.611, 'epoch': 11.0}
{'loss': 2.1801, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.1437463760375977, 'eval_accuracy@tur.pdtb.tdb': 0.30128205128205127, 'eval_f1@tur.pdtb.tdb': 0.0701681442264355, 'eval_precision@tur.pdtb.tdb': 0.06582556465647778, 'eval_recall@tur.pdtb.tdb': 0.10397674554686233, 'eval_loss@tur.pdtb.tdb': 2.1437466144561768, 'eval_runtime': 4.1014, 'eval_samples_per_second': 76.071, 'eval_steps_per_second': 2.438, 'epoch': 11.0}
{'train@tur.pdtb.tdb_loss': 2.126817464828491, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3353733170134639, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.08806393022017459, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.09762947163231701, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11171268329516194, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.126817226409912, 'train@tur.pdtb.tdb_runtime': 29.4495, 'train@tur.pdtb.tdb_samples_per_second': 83.227, 'train@tur.pdtb.tdb_steps_per_second': 2.615, 'epoch': 12.0}
{'loss': 2.1705, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.1412694454193115, 'eval_accuracy@tur.pdtb.tdb': 0.3108974358974359, 'eval_f1@tur.pdtb.tdb': 0.07355474302955499, 'eval_precision@tur.pdtb.tdb': 0.06741114775548061, 'eval_recall@tur.pdtb.tdb': 0.10798744073402812, 'eval_loss@tur.pdtb.tdb': 2.1412694454193115, 'eval_runtime': 4.0998, 'eval_samples_per_second': 76.101, 'eval_steps_per_second': 2.439, 'epoch': 12.0}
{'train_runtime': 1148.8048, 'train_samples_per_second': 25.602, 'train_steps_per_second': 0.804, 'train_loss': 2.3733980913698933, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.3734
  train_runtime            = 0:19:08.80
  train_samples_per_second =     25.602
  train_steps_per_second   =      0.804
{'train@fra.sdrt.annodis_loss': 2.481205701828003, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.2160183066361556, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.03386543282330811, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.07452209752164259, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.059863062491976704, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.481205463409424, 'train@fra.sdrt.annodis_runtime': 26.3136, 'train@fra.sdrt.annodis_samples_per_second': 83.037, 'train@fra.sdrt.annodis_steps_per_second': 2.622, 'epoch': 1.0}
{'loss': 2.897, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.501917600631714, 'eval_accuracy@fra.sdrt.annodis': 0.21401515151515152, 'eval_f1@fra.sdrt.annodis': 0.03781605087741634, 'eval_precision@fra.sdrt.annodis': 0.06452782166052308, 'eval_recall@fra.sdrt.annodis': 0.05875744210808246, 'eval_loss@fra.sdrt.annodis': 2.501917600631714, 'eval_runtime': 6.6906, 'eval_samples_per_second': 78.917, 'eval_steps_per_second': 2.541, 'epoch': 1.0}
{'train@fra.sdrt.annodis_loss': 2.335400342941284, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.2897025171624714, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.06503036059721051, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.08094868785373176, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.08660049463649133, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.3354005813598633, 'train@fra.sdrt.annodis_runtime': 26.3335, 'train@fra.sdrt.annodis_samples_per_second': 82.974, 'train@fra.sdrt.annodis_steps_per_second': 2.62, 'epoch': 2.0}
{'loss': 2.4259, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.348083972930908, 'eval_accuracy@fra.sdrt.annodis': 0.2803030303030303, 'eval_f1@fra.sdrt.annodis': 0.05961920056306745, 'eval_precision@fra.sdrt.annodis': 0.047659828634080255, 'eval_recall@fra.sdrt.annodis': 0.0813665863856241, 'eval_loss@fra.sdrt.annodis': 2.348083734512329, 'eval_runtime': 6.632, 'eval_samples_per_second': 79.613, 'eval_steps_per_second': 2.563, 'epoch': 2.0}
{'train@fra.sdrt.annodis_loss': 2.2610023021698, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.29107551487414185, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.0691631395596713, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.09693719685059175, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.0864177621243851, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.2610023021698, 'train@fra.sdrt.annodis_runtime': 26.3069, 'train@fra.sdrt.annodis_samples_per_second': 83.058, 'train@fra.sdrt.annodis_steps_per_second': 2.623, 'epoch': 3.0}
{'loss': 2.3293, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.277827262878418, 'eval_accuracy@fra.sdrt.annodis': 0.2689393939393939, 'eval_f1@fra.sdrt.annodis': 0.05973091013552639, 'eval_precision@fra.sdrt.annodis': 0.05465727120899535, 'eval_recall@fra.sdrt.annodis': 0.07730922338052831, 'eval_loss@fra.sdrt.annodis': 2.277827024459839, 'eval_runtime': 6.6541, 'eval_samples_per_second': 79.349, 'eval_steps_per_second': 2.555, 'epoch': 3.0}
{'train@fra.sdrt.annodis_loss': 2.191082715988159, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.31990846681922197, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.08853697327660422, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.13517923390706788, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.10082526207531026, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.19108247756958, 'train@fra.sdrt.annodis_runtime': 26.3047, 'train@fra.sdrt.annodis_samples_per_second': 83.065, 'train@fra.sdrt.annodis_steps_per_second': 2.623, 'epoch': 4.0}
{'loss': 2.2626, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.2129640579223633, 'eval_accuracy@fra.sdrt.annodis': 0.30113636363636365, 'eval_f1@fra.sdrt.annodis': 0.07288707932545585, 'eval_precision@fra.sdrt.annodis': 0.14927678071818537, 'eval_recall@fra.sdrt.annodis': 0.08938434809186287, 'eval_loss@fra.sdrt.annodis': 2.2129642963409424, 'eval_runtime': 6.6696, 'eval_samples_per_second': 79.165, 'eval_steps_per_second': 2.549, 'epoch': 4.0}
{'train@fra.sdrt.annodis_loss': 2.1238670349121094, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.3881006864988558, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.1209359102351323, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.1276433186578533, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.14477866649707533, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.1238670349121094, 'train@fra.sdrt.annodis_runtime': 26.3259, 'train@fra.sdrt.annodis_samples_per_second': 82.998, 'train@fra.sdrt.annodis_steps_per_second': 2.621, 'epoch': 5.0}
{'loss': 2.1978, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.155836582183838, 'eval_accuracy@fra.sdrt.annodis': 0.36174242424242425, 'eval_f1@fra.sdrt.annodis': 0.10300848769187775, 'eval_precision@fra.sdrt.annodis': 0.12401349421978886, 'eval_recall@fra.sdrt.annodis': 0.12272515530972737, 'eval_loss@fra.sdrt.annodis': 2.155836820602417, 'eval_runtime': 6.6856, 'eval_samples_per_second': 78.976, 'eval_steps_per_second': 2.543, 'epoch': 5.0}
{'train@fra.sdrt.annodis_loss': 2.060962200164795, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4105263157894737, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.13118667963146294, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12247441923876885, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1616675424551167, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.060962200164795, 'train@fra.sdrt.annodis_runtime': 26.3612, 'train@fra.sdrt.annodis_samples_per_second': 82.887, 'train@fra.sdrt.annodis_steps_per_second': 2.617, 'epoch': 6.0}
{'loss': 2.1335, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.1019437313079834, 'eval_accuracy@fra.sdrt.annodis': 0.3541666666666667, 'eval_f1@fra.sdrt.annodis': 0.10358988560040724, 'eval_precision@fra.sdrt.annodis': 0.10127404613809013, 'eval_recall@fra.sdrt.annodis': 0.12577957197972797, 'eval_loss@fra.sdrt.annodis': 2.1019442081451416, 'eval_runtime': 6.654, 'eval_samples_per_second': 79.351, 'eval_steps_per_second': 2.555, 'epoch': 6.0}
{'train@fra.sdrt.annodis_loss': 2.0092294216156006, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4196796338672769, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.13599117372261343, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12207419892766191, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.16796746021008951, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.0092296600341797, 'train@fra.sdrt.annodis_runtime': 26.3194, 'train@fra.sdrt.annodis_samples_per_second': 83.019, 'train@fra.sdrt.annodis_steps_per_second': 2.622, 'epoch': 7.0}
{'loss': 2.0831, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.05747389793396, 'eval_accuracy@fra.sdrt.annodis': 0.3522727272727273, 'eval_f1@fra.sdrt.annodis': 0.10802055698272807, 'eval_precision@fra.sdrt.annodis': 0.1030875047605256, 'eval_recall@fra.sdrt.annodis': 0.12909117683732393, 'eval_loss@fra.sdrt.annodis': 2.05747389793396, 'eval_runtime': 6.66, 'eval_samples_per_second': 79.28, 'eval_steps_per_second': 2.553, 'epoch': 7.0}
{'train@fra.sdrt.annodis_loss': 1.969590425491333, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4251716247139588, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.13830884282341344, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12236997701615558, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.17131354092368348, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.969590425491333, 'train@fra.sdrt.annodis_runtime': 26.3233, 'train@fra.sdrt.annodis_samples_per_second': 83.006, 'train@fra.sdrt.annodis_steps_per_second': 2.621, 'epoch': 8.0}
{'loss': 2.0394, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.0229225158691406, 'eval_accuracy@fra.sdrt.annodis': 0.36363636363636365, 'eval_f1@fra.sdrt.annodis': 0.11255169345582562, 'eval_precision@fra.sdrt.annodis': 0.10619099371529352, 'eval_recall@fra.sdrt.annodis': 0.13386240102402208, 'eval_loss@fra.sdrt.annodis': 2.0229227542877197, 'eval_runtime': 6.6661, 'eval_samples_per_second': 79.207, 'eval_steps_per_second': 2.55, 'epoch': 8.0}
{'train@fra.sdrt.annodis_loss': 1.9406437873840332, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.43524027459954234, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.1426917293504033, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.18080204826064655, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.17591050916685302, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.9406437873840332, 'train@fra.sdrt.annodis_runtime': 26.3255, 'train@fra.sdrt.annodis_samples_per_second': 82.999, 'train@fra.sdrt.annodis_steps_per_second': 2.621, 'epoch': 9.0}
{'loss': 2.0063, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.9980945587158203, 'eval_accuracy@fra.sdrt.annodis': 0.3693181818181818, 'eval_f1@fra.sdrt.annodis': 0.11461265705327134, 'eval_precision@fra.sdrt.annodis': 0.10753781409668083, 'eval_recall@fra.sdrt.annodis': 0.1365909404485483, 'eval_loss@fra.sdrt.annodis': 1.9980943202972412, 'eval_runtime': 6.644, 'eval_samples_per_second': 79.47, 'eval_steps_per_second': 2.559, 'epoch': 9.0}
{'train@fra.sdrt.annodis_loss': 1.9198853969573975, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.43844393592677344, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.14588522975459695, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.18033444050885178, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1784899830689002, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.9198853969573975, 'train@fra.sdrt.annodis_runtime': 26.3042, 'train@fra.sdrt.annodis_samples_per_second': 83.067, 'train@fra.sdrt.annodis_steps_per_second': 2.623, 'epoch': 10.0}
{'loss': 1.977, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.9812191724777222, 'eval_accuracy@fra.sdrt.annodis': 0.3712121212121212, 'eval_f1@fra.sdrt.annodis': 0.11586607687327853, 'eval_precision@fra.sdrt.annodis': 0.10645104689195503, 'eval_recall@fra.sdrt.annodis': 0.13779866991714734, 'eval_loss@fra.sdrt.annodis': 1.9812191724777222, 'eval_runtime': 6.664, 'eval_samples_per_second': 79.232, 'eval_steps_per_second': 2.551, 'epoch': 10.0}
{'train@fra.sdrt.annodis_loss': 1.908568263053894, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4402745995423341, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.1465600147269121, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.18035999958108132, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.17946347744940677, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.908568263053894, 'train@fra.sdrt.annodis_runtime': 26.3022, 'train@fra.sdrt.annodis_samples_per_second': 83.073, 'train@fra.sdrt.annodis_steps_per_second': 2.623, 'epoch': 11.0}
{'loss': 1.9559, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.9703415632247925, 'eval_accuracy@fra.sdrt.annodis': 0.3768939393939394, 'eval_f1@fra.sdrt.annodis': 0.1193480171357056, 'eval_precision@fra.sdrt.annodis': 0.10969580755163957, 'eval_recall@fra.sdrt.annodis': 0.14270063070146108, 'eval_loss@fra.sdrt.annodis': 1.9703413248062134, 'eval_runtime': 6.6607, 'eval_samples_per_second': 79.27, 'eval_steps_per_second': 2.552, 'epoch': 11.0}
{'train@fra.sdrt.annodis_loss': 1.9045422077178955, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4416475972540046, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.14701222620495236, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.18046503012956316, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1799618727078753, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.9045422077178955, 'train@fra.sdrt.annodis_runtime': 26.2624, 'train@fra.sdrt.annodis_samples_per_second': 83.199, 'train@fra.sdrt.annodis_steps_per_second': 2.627, 'epoch': 12.0}
{'loss': 1.9552, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.9672273397445679, 'eval_accuracy@fra.sdrt.annodis': 0.3787878787878788, 'eval_f1@fra.sdrt.annodis': 0.11977107460010611, 'eval_precision@fra.sdrt.annodis': 0.11006045007402204, 'eval_recall@fra.sdrt.annodis': 0.14320113120196157, 'eval_loss@fra.sdrt.annodis': 1.9672271013259888, 'eval_runtime': 6.6303, 'eval_samples_per_second': 79.634, 'eval_steps_per_second': 2.564, 'epoch': 12.0}
{'train_runtime': 1061.3481, 'train_samples_per_second': 24.704, 'train_steps_per_second': 0.78, 'train_loss': 2.188579854181999, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.3734
  train_runtime            = 0:19:08.80
  train_samples_per_second =     25.602
  train_steps_per_second   =      0.804
-------------------------------------------------------------------
Lang1:  zho.rst.sctb    Lang2:  fra.sdrt.annodis
Saving run to:  runs/full_shot/FullShot=v4_zho.rst.sctb_fra.sdrt.annodis_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 439 examples
read 94 examples
read 159 examples
read 2185 examples
read 528 examples
read 625 examples
Total prediction labels:  39
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (zh): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (fr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=39, bias=True)
    )
  )
)
{'train@zho.rst.sctb_loss': 3.3474671840667725, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.3474674224853516, 'train@zho.rst.sctb_runtime': 5.4507, 'train@zho.rst.sctb_samples_per_second': 80.54, 'train@zho.rst.sctb_steps_per_second': 2.568, 'epoch': 1.0}
{'loss': 3.4963, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.3591606616973877, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 3.3591597080230713, 'eval_runtime': 1.4579, 'eval_samples_per_second': 64.477, 'eval_steps_per_second': 2.058, 'epoch': 1.0}
{'train@zho.rst.sctb_loss': 3.136791706085205, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.136791229248047, 'train@zho.rst.sctb_runtime': 5.4854, 'train@zho.rst.sctb_samples_per_second': 80.03, 'train@zho.rst.sctb_steps_per_second': 2.552, 'epoch': 2.0}
{'loss': 3.2697, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.161566972732544, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 3.161567211151123, 'eval_runtime': 1.4471, 'eval_samples_per_second': 64.959, 'eval_steps_per_second': 2.073, 'epoch': 2.0}
{'train@zho.rst.sctb_loss': 2.956737995147705, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.956738233566284, 'train@zho.rst.sctb_runtime': 5.475, 'train@zho.rst.sctb_samples_per_second': 80.183, 'train@zho.rst.sctb_steps_per_second': 2.557, 'epoch': 3.0}
{'loss': 3.0888, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.9916532039642334, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.9916536808013916, 'eval_runtime': 1.4295, 'eval_samples_per_second': 65.756, 'eval_steps_per_second': 2.099, 'epoch': 3.0}
{'train@zho.rst.sctb_loss': 2.8091378211975098, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.8091378211975098, 'train@zho.rst.sctb_runtime': 5.4702, 'train@zho.rst.sctb_samples_per_second': 80.253, 'train@zho.rst.sctb_steps_per_second': 2.559, 'epoch': 4.0}
{'loss': 2.9187, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.8530867099761963, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.8530871868133545, 'eval_runtime': 1.437, 'eval_samples_per_second': 65.412, 'eval_steps_per_second': 2.088, 'epoch': 4.0}
{'train@zho.rst.sctb_loss': 2.6914634704589844, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.6914637088775635, 'train@zho.rst.sctb_runtime': 5.464, 'train@zho.rst.sctb_samples_per_second': 80.344, 'train@zho.rst.sctb_steps_per_second': 2.562, 'epoch': 5.0}
{'loss': 2.7916, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.744107723236084, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.744107246398926, 'eval_runtime': 1.464, 'eval_samples_per_second': 64.206, 'eval_steps_per_second': 2.049, 'epoch': 5.0}
{'train@zho.rst.sctb_loss': 2.597196102142334, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.597196340560913, 'train@zho.rst.sctb_runtime': 5.4971, 'train@zho.rst.sctb_samples_per_second': 79.861, 'train@zho.rst.sctb_steps_per_second': 2.547, 'epoch': 6.0}
{'loss': 2.6886, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.6592485904693604, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.6592485904693604, 'eval_runtime': 1.4582, 'eval_samples_per_second': 64.463, 'eval_steps_per_second': 2.057, 'epoch': 6.0}
{'train@zho.rst.sctb_loss': 2.525787353515625, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.525787353515625, 'train@zho.rst.sctb_runtime': 5.5003, 'train@zho.rst.sctb_samples_per_second': 79.814, 'train@zho.rst.sctb_steps_per_second': 2.545, 'epoch': 7.0}
{'loss': 2.5994, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.596198320388794, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.596198320388794, 'eval_runtime': 1.4528, 'eval_samples_per_second': 64.7, 'eval_steps_per_second': 2.065, 'epoch': 7.0}
{'train@zho.rst.sctb_loss': 2.477107524871826, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.4771077632904053, 'train@zho.rst.sctb_runtime': 5.4873, 'train@zho.rst.sctb_samples_per_second': 80.003, 'train@zho.rst.sctb_steps_per_second': 2.551, 'epoch': 8.0}
{'loss': 2.5414, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.554720401763916, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.554720401763916, 'eval_runtime': 1.4518, 'eval_samples_per_second': 64.746, 'eval_steps_per_second': 2.066, 'epoch': 8.0}
{'train@zho.rst.sctb_loss': 2.443650245666504, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.443650245666504, 'train@zho.rst.sctb_runtime': 5.4858, 'train@zho.rst.sctb_samples_per_second': 80.025, 'train@zho.rst.sctb_steps_per_second': 2.552, 'epoch': 9.0}
{'loss': 2.4961, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.5268075466156006, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.5268075466156006, 'eval_runtime': 1.4374, 'eval_samples_per_second': 65.396, 'eval_steps_per_second': 2.087, 'epoch': 9.0}
{'train@zho.rst.sctb_loss': 2.421818256378174, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.421818256378174, 'train@zho.rst.sctb_runtime': 5.4714, 'train@zho.rst.sctb_samples_per_second': 80.236, 'train@zho.rst.sctb_steps_per_second': 2.559, 'epoch': 10.0}
{'loss': 2.4672, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.50994873046875, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.509948253631592, 'eval_runtime': 1.4466, 'eval_samples_per_second': 64.979, 'eval_steps_per_second': 2.074, 'epoch': 10.0}
{'train@zho.rst.sctb_loss': 2.4099910259246826, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.40999174118042, 'train@zho.rst.sctb_runtime': 5.4764, 'train@zho.rst.sctb_samples_per_second': 80.162, 'train@zho.rst.sctb_steps_per_second': 2.556, 'epoch': 11.0}
{'loss': 2.4436, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.500737428665161, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.500737190246582, 'eval_runtime': 1.4636, 'eval_samples_per_second': 64.224, 'eval_steps_per_second': 2.05, 'epoch': 11.0}
{'train@zho.rst.sctb_loss': 2.4061670303344727, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.4061670303344727, 'train@zho.rst.sctb_runtime': 5.4817, 'train@zho.rst.sctb_samples_per_second': 80.084, 'train@zho.rst.sctb_steps_per_second': 2.554, 'epoch': 12.0}
{'loss': 2.435, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.497797966003418, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.497798442840576, 'eval_runtime': 1.4642, 'eval_samples_per_second': 64.201, 'eval_steps_per_second': 2.049, 'epoch': 12.0}
{'train_runtime': 213.8194, 'train_samples_per_second': 24.638, 'train_steps_per_second': 0.786, 'train_loss': 2.7696974391028997, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.7697
  train_runtime            = 0:03:33.81
  train_samples_per_second =     24.638
  train_steps_per_second   =      0.786
{'train@fra.sdrt.annodis_loss': 2.623617172241211, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.21876430205949657, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.029210703039425117, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.02876207005261909, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.05987345141308556, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.623616933822632, 'train@fra.sdrt.annodis_runtime': 26.2859, 'train@fra.sdrt.annodis_samples_per_second': 83.124, 'train@fra.sdrt.annodis_steps_per_second': 2.625, 'epoch': 1.0}
{'loss': 3.152, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.6465420722961426, 'eval_accuracy@fra.sdrt.annodis': 0.2215909090909091, 'eval_f1@fra.sdrt.annodis': 0.03047030080928386, 'eval_precision@fra.sdrt.annodis': 0.03425475164605599, 'eval_recall@fra.sdrt.annodis': 0.06046700906513991, 'eval_loss@fra.sdrt.annodis': 2.6465418338775635, 'eval_runtime': 6.6086, 'eval_samples_per_second': 79.896, 'eval_steps_per_second': 2.572, 'epoch': 1.0}
{'train@fra.sdrt.annodis_loss': 2.3849189281463623, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.2791762013729977, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.06055314234379342, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.052042393562170004, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.08236016520956688, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.3849191665649414, 'train@fra.sdrt.annodis_runtime': 26.2556, 'train@fra.sdrt.annodis_samples_per_second': 83.22, 'train@fra.sdrt.annodis_steps_per_second': 2.628, 'epoch': 2.0}
{'loss': 2.5087, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.405240058898926, 'eval_accuracy@fra.sdrt.annodis': 0.26515151515151514, 'eval_f1@fra.sdrt.annodis': 0.05468781413384568, 'eval_precision@fra.sdrt.annodis': 0.04471511883164757, 'eval_recall@fra.sdrt.annodis': 0.07556847259166401, 'eval_loss@fra.sdrt.annodis': 2.405240297317505, 'eval_runtime': 6.6466, 'eval_samples_per_second': 79.44, 'eval_steps_per_second': 2.558, 'epoch': 2.0}
{'train@fra.sdrt.annodis_loss': 2.3107094764709473, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.29016018306636154, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.062056620456099956, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.06112750030233714, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.08422549665235228, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.3107097148895264, 'train@fra.sdrt.annodis_runtime': 26.3253, 'train@fra.sdrt.annodis_samples_per_second': 83.0, 'train@fra.sdrt.annodis_steps_per_second': 2.621, 'epoch': 3.0}
{'loss': 2.3754, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.3350260257720947, 'eval_accuracy@fra.sdrt.annodis': 0.2556818181818182, 'eval_f1@fra.sdrt.annodis': 0.050686829875431064, 'eval_precision@fra.sdrt.annodis': 0.044018543645409317, 'eval_recall@fra.sdrt.annodis': 0.0719928997879846, 'eval_loss@fra.sdrt.annodis': 2.335026264190674, 'eval_runtime': 6.6572, 'eval_samples_per_second': 79.313, 'eval_steps_per_second': 2.554, 'epoch': 3.0}
{'train@fra.sdrt.annodis_loss': 2.2608091831207275, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.29244851258581234, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.06188820669990788, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.06495151162085848, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.0846061758270938, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.2608089447021484, 'train@fra.sdrt.annodis_runtime': 26.2812, 'train@fra.sdrt.annodis_samples_per_second': 83.139, 'train@fra.sdrt.annodis_steps_per_second': 2.625, 'epoch': 4.0}
{'loss': 2.3236, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.2914793491363525, 'eval_accuracy@fra.sdrt.annodis': 0.25946969696969696, 'eval_f1@fra.sdrt.annodis': 0.051588227035924215, 'eval_precision@fra.sdrt.annodis': 0.04641354016354017, 'eval_recall@fra.sdrt.annodis': 0.07275239933599227, 'eval_loss@fra.sdrt.annodis': 2.2914793491363525, 'eval_runtime': 6.6654, 'eval_samples_per_second': 79.216, 'eval_steps_per_second': 2.55, 'epoch': 4.0}
{'train@fra.sdrt.annodis_loss': 2.2140798568725586, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.32906178489702514, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.07216784547544514, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.07659715066214708, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.09972578754911643, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.2140796184539795, 'train@fra.sdrt.annodis_runtime': 26.3078, 'train@fra.sdrt.annodis_samples_per_second': 83.055, 'train@fra.sdrt.annodis_steps_per_second': 2.623, 'epoch': 5.0}
{'loss': 2.2685, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.252427101135254, 'eval_accuracy@fra.sdrt.annodis': 0.29545454545454547, 'eval_f1@fra.sdrt.annodis': 0.06319946052279102, 'eval_precision@fra.sdrt.annodis': 0.050829114388436424, 'eval_recall@fra.sdrt.annodis': 0.08926683693038834, 'eval_loss@fra.sdrt.annodis': 2.252427339553833, 'eval_runtime': 6.6222, 'eval_samples_per_second': 79.732, 'eval_steps_per_second': 2.567, 'epoch': 5.0}
{'train@fra.sdrt.annodis_loss': 2.1742472648620605, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.34096109839816935, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.08188233040527182, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.14662737402104645, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.10663668079678174, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.1742470264434814, 'train@fra.sdrt.annodis_runtime': 26.3105, 'train@fra.sdrt.annodis_samples_per_second': 83.047, 'train@fra.sdrt.annodis_steps_per_second': 2.623, 'epoch': 6.0}
{'loss': 2.2322, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.21970534324646, 'eval_accuracy@fra.sdrt.annodis': 0.2935606060606061, 'eval_f1@fra.sdrt.annodis': 0.06749287168913821, 'eval_precision@fra.sdrt.annodis': 0.12366897946547656, 'eval_recall@fra.sdrt.annodis': 0.09121756195496522, 'eval_loss@fra.sdrt.annodis': 2.21970534324646, 'eval_runtime': 6.6434, 'eval_samples_per_second': 79.477, 'eval_steps_per_second': 2.559, 'epoch': 6.0}
{'train@fra.sdrt.annodis_loss': 2.136934518814087, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.3697940503432494, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.10686858797739125, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.1365676877594219, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1237223507558855, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.136934757232666, 'train@fra.sdrt.annodis_runtime': 26.3184, 'train@fra.sdrt.annodis_samples_per_second': 83.022, 'train@fra.sdrt.annodis_steps_per_second': 2.622, 'epoch': 7.0}
{'loss': 2.198, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.1903278827667236, 'eval_accuracy@fra.sdrt.annodis': 0.29734848484848486, 'eval_f1@fra.sdrt.annodis': 0.06896520999339296, 'eval_precision@fra.sdrt.annodis': 0.11496178947794368, 'eval_recall@fra.sdrt.annodis': 0.09210630116146143, 'eval_loss@fra.sdrt.annodis': 2.1903278827667236, 'eval_runtime': 6.6154, 'eval_samples_per_second': 79.814, 'eval_steps_per_second': 2.57, 'epoch': 7.0}
{'train@fra.sdrt.annodis_loss': 2.1046547889709473, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.38077803203661326, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.11624277514107008, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.1355278763098169, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.13215675349262637, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.1046550273895264, 'train@fra.sdrt.annodis_runtime': 26.2967, 'train@fra.sdrt.annodis_samples_per_second': 83.09, 'train@fra.sdrt.annodis_steps_per_second': 2.624, 'epoch': 8.0}
{'loss': 2.1687, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.1655664443969727, 'eval_accuracy@fra.sdrt.annodis': 0.3068181818181818, 'eval_f1@fra.sdrt.annodis': 0.07592244009415867, 'eval_precision@fra.sdrt.annodis': 0.11561179686179685, 'eval_recall@fra.sdrt.annodis': 0.09721082946758607, 'eval_loss@fra.sdrt.annodis': 2.1655664443969727, 'eval_runtime': 6.6439, 'eval_samples_per_second': 79.472, 'eval_steps_per_second': 2.559, 'epoch': 8.0}
{'train@fra.sdrt.annodis_loss': 2.0803165435791016, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.388558352402746, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.12169282441082828, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.13114218504754555, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.13990284094387462, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.0803165435791016, 'train@fra.sdrt.annodis_runtime': 26.2817, 'train@fra.sdrt.annodis_samples_per_second': 83.138, 'train@fra.sdrt.annodis_steps_per_second': 2.625, 'epoch': 9.0}
{'loss': 2.1364, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.1464593410491943, 'eval_accuracy@fra.sdrt.annodis': 0.3162878787878788, 'eval_f1@fra.sdrt.annodis': 0.08160830815178241, 'eval_precision@fra.sdrt.annodis': 0.09900337705070651, 'eval_recall@fra.sdrt.annodis': 0.10218438568012177, 'eval_loss@fra.sdrt.annodis': 2.1464593410491943, 'eval_runtime': 6.6412, 'eval_samples_per_second': 79.503, 'eval_steps_per_second': 2.56, 'epoch': 9.0}
{'train@fra.sdrt.annodis_loss': 2.0626964569091797, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.39862700228832953, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.1266096491090068, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.13012471393776254, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1457547849765474, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.0626964569091797, 'train@fra.sdrt.annodis_runtime': 26.3073, 'train@fra.sdrt.annodis_samples_per_second': 83.057, 'train@fra.sdrt.annodis_steps_per_second': 2.623, 'epoch': 10.0}
{'loss': 2.1172, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.133882999420166, 'eval_accuracy@fra.sdrt.annodis': 0.32575757575757575, 'eval_f1@fra.sdrt.annodis': 0.0853178722368436, 'eval_precision@fra.sdrt.annodis': 0.09524904047847377, 'eval_recall@fra.sdrt.annodis': 0.10591392610564901, 'eval_loss@fra.sdrt.annodis': 2.133883476257324, 'eval_runtime': 6.6549, 'eval_samples_per_second': 79.34, 'eval_steps_per_second': 2.555, 'epoch': 10.0}
{'train@fra.sdrt.annodis_loss': 2.051774740219116, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4022883295194508, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.1283235989403947, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.1285543417978699, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.14952028592204686, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.051774740219116, 'train@fra.sdrt.annodis_runtime': 26.2976, 'train@fra.sdrt.annodis_samples_per_second': 83.087, 'train@fra.sdrt.annodis_steps_per_second': 2.624, 'epoch': 11.0}
{'loss': 2.0936, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.1251211166381836, 'eval_accuracy@fra.sdrt.annodis': 0.3371212121212121, 'eval_f1@fra.sdrt.annodis': 0.09327205289568336, 'eval_precision@fra.sdrt.annodis': 0.094366880596968, 'eval_recall@fra.sdrt.annodis': 0.1136736659729775, 'eval_loss@fra.sdrt.annodis': 2.1251211166381836, 'eval_runtime': 6.6377, 'eval_samples_per_second': 79.545, 'eval_steps_per_second': 2.561, 'epoch': 11.0}
{'train@fra.sdrt.annodis_loss': 2.0479655265808105, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4064073226544622, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.13016676216375186, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.1283605662239023, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.15176191761436453, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.0479652881622314, 'train@fra.sdrt.annodis_runtime': 26.2868, 'train@fra.sdrt.annodis_samples_per_second': 83.122, 'train@fra.sdrt.annodis_steps_per_second': 2.625, 'epoch': 12.0}
{'loss': 2.0955, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.1224753856658936, 'eval_accuracy@fra.sdrt.annodis': 0.3333333333333333, 'eval_f1@fra.sdrt.annodis': 0.09259725817736596, 'eval_precision@fra.sdrt.annodis': 0.09343326427975963, 'eval_recall@fra.sdrt.annodis': 0.1126165340747241, 'eval_loss@fra.sdrt.annodis': 2.1224753856658936, 'eval_runtime': 6.6433, 'eval_samples_per_second': 79.479, 'eval_steps_per_second': 2.559, 'epoch': 12.0}
{'train_runtime': 1058.3608, 'train_samples_per_second': 24.774, 'train_steps_per_second': 0.782, 'train_loss': 2.305811822126453, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.7697
  train_runtime            = 0:03:33.81
  train_samples_per_second =     24.638
  train_steps_per_second   =      0.786

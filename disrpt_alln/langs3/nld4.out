0.3333333333333333
1.0
ASSIGN: 29
CustomBERTModel(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30073, 768, padding_idx=3)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (encoder): CustomPooler2(
    (_dropout): Dropout(p=0.0, inplace=False)
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (relation_decoder): Linear(in_features=768, out_features=29, bias=True)
)
['events.out.tfevents.1668153194.bc01bc43a6e4.23564.0']
Epoch 1: Best val_acc: 0.3628
Epoch 1: Best val_acc: 0.3628
Epoch 1: train_loss: 2.8162 train_acc: 0.2727 | val_loss: 2.4963 val_acc: 0.3628
00:02:04.38
train_size: 397
Epoch 2: Best val_acc: 0.3780
Epoch 2: Best val_acc: 0.3780
Epoch 2: train_loss: 2.5322 train_acc: 0.3331 | val_loss: 2.3444 val_acc: 0.3780
00:02:25.17
train_size: 397
Epoch 3: Best val_acc: 0.3811
Epoch 3: Best val_acc: 0.3811
Epoch 3: train_loss: 2.4129 train_acc: 0.3558 | val_loss: 2.2960 val_acc: 0.3811
00:02:25.34
train_size: 397
Epoch 4: Best val_acc: 0.4055
Epoch 4: Best val_acc: 0.4055
Epoch 4: train_loss: 2.3242 train_acc: 0.3854 | val_loss: 2.2345 val_acc: 0.4055
00:02:25.27
train_size: 397
Epoch 5: Best val_acc: 0.4207
Epoch 5: Best val_acc: 0.4207
Epoch 5: train_loss: 2.2425 train_acc: 0.4018 | val_loss: 2.1858 val_acc: 0.4207
00:02:25.37
train_size: 397
Epoch 6: Best val_acc: 0.4268
Epoch 6: Best val_acc: 0.4268
Epoch 6: train_loss: 2.1748 train_acc: 0.4030 | val_loss: 2.1703 val_acc: 0.4268
00:02:24.95
train_size: 397
Epoch 7: Best val_acc: 0.4329
Epoch 7: Best val_acc: 0.4329
Epoch 7: train_loss: 2.1087 train_acc: 0.4244 | val_loss: 2.1037 val_acc: 0.4329
00:02:21.46
train_size: 397
Epoch 8: Best val_acc: 0.4451
Epoch 8: Best val_acc: 0.4451
Epoch 8: train_loss: 2.0362 train_acc: 0.4339 | val_loss: 2.0804 val_acc: 0.4451
00:02:24.90
train_size: 397
Epoch 9: Best val_acc: 0.4634
Epoch 9: Best val_acc: 0.4634
Epoch 9: train_loss: 1.9785 train_acc: 0.4496 | val_loss: 2.0459 val_acc: 0.4634
00:02:24.86
train_size: 397
Epoch 10: Best val_acc: 0.4817
Epoch 10: Best val_acc: 0.4817
Epoch 10: train_loss: 1.9164 train_acc: 0.4717 | val_loss: 2.0373 val_acc: 0.4817
00:02:24.76
train_size: 397
Epoch 11: Best val_acc: 0.4909
Epoch 11: Best val_acc: 0.4909
Epoch 11: train_loss: 1.8604 train_acc: 0.4868 | val_loss: 2.0109 val_acc: 0.4909
00:02:24.99
train_size: 397
Epoch 12: Best val_acc: 0.4970
Epoch 12: Best val_acc: 0.4970
Epoch 12: train_loss: 1.8019 train_acc: 0.4994 | val_loss: 1.9998 val_acc: 0.4970
00:02:21.16
train_size: 397
Epoch 13: Best val_acc: 0.5030
Epoch 13: Best val_acc: 0.5030
Epoch 13: train_loss: 1.7367 train_acc: 0.5208 | val_loss: 1.9826 val_acc: 0.5030
00:02:24.97
train_size: 397
Epoch 00014: reducing learning rate of group 0 to 7.0000e-07.
Epoch 14: train_loss: 1.6764 train_acc: 0.5435 | val_loss: 1.9598 val_acc: 0.4939
00:02:23.21
train_size: 397
Epoch 15: Best val_acc: 0.5091
Epoch 15: Best val_acc: 0.5091
Epoch 15: train_loss: 1.6277 train_acc: 0.5504 | val_loss: 1.9465 val_acc: 0.5091
00:02:24.82
train_size: 397
Epoch 16: train_loss: 1.5919 train_acc: 0.5586 | val_loss: 1.9422 val_acc: 0.5030
00:02:23.15
train_size: 397
Epoch 17: Best val_acc: 0.5091
Epoch 17: train_loss: 1.5570 train_acc: 0.5806 | val_loss: 1.9410 val_acc: 0.5091
00:02:23.87
train_size: 397
Epoch 18: train_loss: 1.5179 train_acc: 0.5825 | val_loss: 1.9548 val_acc: 0.5061
00:02:19.48
train_size: 397
Epoch 19: Best val_acc: 0.5122
Epoch 19: Best val_acc: 0.5122
Epoch 19: train_loss: 1.4850 train_acc: 0.6001 | val_loss: 1.9484 val_acc: 0.5122
00:02:24.83
train_size: 397
Epoch 20: Best val_acc: 0.5152
Epoch 20: Best val_acc: 0.5152
Epoch 20: train_loss: 1.4393 train_acc: 0.6184 | val_loss: 1.9248 val_acc: 0.5152
00:02:24.63
train_size: 397
Epoch 21: Best val_acc: 0.5183
Epoch 21: Best val_acc: 0.5183
Epoch 21: train_loss: 1.4060 train_acc: 0.6291 | val_loss: 1.9096 val_acc: 0.5183
00:02:24.29
train_size: 397
Epoch 22: Best val_acc: 0.5213
Epoch 22: Best val_acc: 0.5213
Epoch 22: train_loss: 1.3710 train_acc: 0.6411 | val_loss: 1.9528 val_acc: 0.5213
00:02:24.90
train_size: 397
Epoch 23: Best val_acc: 0.5366
Epoch 23: Best val_acc: 0.5366
Epoch 23: train_loss: 1.3294 train_acc: 0.6543 | val_loss: 1.9149 val_acc: 0.5366
00:02:24.00
train_size: 397
Epoch 24: train_loss: 1.2959 train_acc: 0.6656 | val_loss: 1.9335 val_acc: 0.5122
00:02:23.12
train_size: 397
Epoch 25: train_loss: 1.2615 train_acc: 0.6776 | val_loss: 1.9480 val_acc: 0.5122
00:02:23.34
train_size: 397
Epoch 26: train_loss: 1.2259 train_acc: 0.6870 | val_loss: 1.9327 val_acc: 0.5183
00:02:23.31
train_size: 397
Epoch 00027: reducing learning rate of group 0 to 5.0000e-07.
Epoch 27: train_loss: 1.1988 train_acc: 0.6927 | val_loss: 1.9040 val_acc: 0.5274
00:02:23.19
train_size: 397
Epoch 28: train_loss: 1.1576 train_acc: 0.7084 | val_loss: 1.9023 val_acc: 0.5305
00:02:23.25
train_size: 397
Epoch 29: train_loss: 1.1430 train_acc: 0.7185 | val_loss: 1.9261 val_acc: 0.5213
00:02:19.60
train_size: 397
Epoch 30: train_loss: 1.1144 train_acc: 0.7229 | val_loss: 1.9083 val_acc: 0.5183
00:02:23.14
train_size: 397
Epoch 31: train_loss: 1.0908 train_acc: 0.7292 | val_loss: 1.9034 val_acc: 0.5335
00:02:23.18
train_size: 397
Epoch 32: train_loss: 1.0754 train_acc: 0.7399 | val_loss: 1.9110 val_acc: 0.5305
00:02:23.21
train_size: 397
Epoch 33: train_loss: 1.0477 train_acc: 0.7519 | val_loss: 1.9110 val_acc: 0.5213
00:02:22.96
train_size: 397
Epoch 34: train_loss: 1.0338 train_acc: 0.7487 | val_loss: 1.9079 val_acc: 0.5183
00:02:23.21
train_size: 397
Test_loss: 1.9656 test_acc: 0.4665
00:00:09.34
                      precision    recall  f1-score   support

          antithesis       0.00      0.00      0.00         2
          background       0.00      0.00      0.00         3
        circumstance       0.27      0.50      0.35        16
          concession       0.50      0.08      0.14        12
           condition       0.50      0.38      0.43         8
         conjunction       0.33      0.47      0.39        19
            contrast       0.00      0.00      0.00         7
         disjunction       1.00      0.25      0.40         4
         elaboration       0.59      0.86      0.70        95
          enablement       0.00      0.00      0.00         4
          evaluation       0.00      0.00      0.00         2
            evidence       0.00      0.00      0.00         6
      interpretation       0.10      0.10      0.10        10
               joint       0.00      0.00      0.00         3
             justify       0.38      0.60      0.46        10
                list       0.00      0.00      0.00        12
               means       0.00      0.00      0.00         4
          motivation       0.52      0.55      0.53        29
 nonvolitional-cause       0.40      0.31      0.35        13
nonvolitional-result       0.33      0.43      0.38        14
           otherwise       0.00      0.00      0.00         1
         preparation       0.45      0.68      0.54        19
             purpose       1.00      0.17      0.29         6
         restatement       0.00      0.00      0.00         2
            sequence       0.00      0.00      0.00        10
        solutionhood       0.00      0.00      0.00         8
                span       0.00      0.00      0.00         1
             summary       0.00      0.00      0.00         4
    volitional-cause       0.00      0.00      0.00         2

            accuracy                           0.46       326
           macro avg       0.22      0.19      0.17       326
        weighted avg       0.38      0.46      0.40       326

Test Loss: 1.966 |  Test Acc: 46.65%
Test_loss: 1.9831 test_acc: 0.4756
00:00:09.42
                      precision    recall  f1-score   support

          antithesis       0.00      0.00      0.00         2
          background       0.00      0.00      0.00         3
        circumstance       0.26      0.56      0.36        16
          concession       0.00      0.00      0.00        12
           condition       0.60      0.38      0.46         8
         conjunction       0.38      0.53      0.44        19
            contrast       0.00      0.00      0.00         7
         disjunction       1.00      0.25      0.40         4
         elaboration       0.58      0.92      0.71        95
          enablement       0.00      0.00      0.00         4
          evaluation       0.00      0.00      0.00         2
            evidence       0.00      0.00      0.00         6
      interpretation       0.17      0.10      0.12        10
               joint       0.00      0.00      0.00         3
             justify       0.42      0.50      0.45        10
                list       0.00      0.00      0.00        12
               means       0.00      0.00      0.00         4
          motivation       0.52      0.59      0.55        29
 nonvolitional-cause       0.40      0.31      0.35        13
nonvolitional-result       0.25      0.29      0.27        14
           otherwise       0.00      0.00      0.00         1
         preparation       0.40      0.63      0.49        19
             purpose       1.00      0.17      0.29         6
         restatement       0.00      0.00      0.00         2
            sequence       0.00      0.00      0.00        10
        solutionhood       0.00      0.00      0.00         8
                span       0.00      0.00      0.00         1
             summary       0.00      0.00      0.00         4
    volitional-cause       0.00      0.00      0.00         2

            accuracy                           0.47       326
           macro avg       0.21      0.18      0.17       326
        weighted avg       0.36      0.47      0.39       326

Latest Test Loss: 1.983 |  Latest Test Acc: 47.56%
Test_loss: 1.9831 test_acc: 0.4756
00:00:09.42
                      precision    recall  f1-score   support

          antithesis       0.00      0.00      0.00         2
          background       0.00      0.00      0.00         3
        circumstance       0.26      0.56      0.36        16
          concession       0.00      0.00      0.00        12
           condition       0.60      0.38      0.46         8
         conjunction       0.38      0.53      0.44        19
            contrast       0.00      0.00      0.00         7
         disjunction       1.00      0.25      0.40         4
         elaboration       0.58      0.92      0.71        95
          enablement       0.00      0.00      0.00         4
          evaluation       0.00      0.00      0.00         2
            evidence       0.00      0.00      0.00         6
      interpretation       0.17      0.10      0.12        10
               joint       0.00      0.00      0.00         3
             justify       0.42      0.50      0.45        10
                list       0.00      0.00      0.00        12
               means       0.00      0.00      0.00         4
          motivation       0.52      0.59      0.55        29
 nonvolitional-cause       0.40      0.31      0.35        13
nonvolitional-result       0.25      0.29      0.27        14
           otherwise       0.00      0.00      0.00         1
         preparation       0.40      0.63      0.49        19
             purpose       1.00      0.17      0.29         6
         restatement       0.00      0.00      0.00         2
            sequence       0.00      0.00      0.00        10
        solutionhood       0.00      0.00      0.00         8
                span       0.00      0.00      0.00         1
             summary       0.00      0.00      0.00         4
    volitional-cause       0.00      0.00      0.00         2

            accuracy                           0.47       326
           macro avg       0.21      0.18      0.17       326
        weighted avg       0.36      0.47      0.39       326

Best Test Loss: 1.983 |  Best Test Acc: 47.56%
Test_loss: 1.9190 test_acc: 0.5366
00:00:09.36
                      precision    recall  f1-score   support

          antithesis       0.00      0.00      0.00         5
          background       0.00      0.00      0.00         5
        circumstance       0.44      0.68      0.54        28
          concession       0.75      0.33      0.46         9
           condition       1.00      0.86      0.92         7
         conjunction       0.59      0.57      0.58        35
            contrast       0.00      0.00      0.00         4
         disjunction       1.00      0.10      0.18        10
         elaboration       0.56      0.84      0.67        92
          enablement       0.00      0.00      0.00         4
          evaluation       0.00      0.00      0.00         2
            evidence       0.00      0.00      0.00         6
      interpretation       0.33      0.10      0.15        10
               joint       0.00      0.00      0.00         9
             justify       0.73      0.53      0.62        15
                list       0.00      0.00      0.00         2
               means       0.00      0.00      0.00         2
          motivation       0.47      0.44      0.46        18
 nonvolitional-cause       0.50      0.38      0.43        16
nonvolitional-result       0.33      0.42      0.37        12
         preparation       0.50      0.86      0.63        21
         restatement       0.00      0.00      0.00         2
            sequence       0.00      0.00      0.00         2
        solutionhood       1.00      0.20      0.33         5
             summary       0.00      0.00      0.00         2
    volitional-cause       0.00      0.00      0.00         2

            accuracy                           0.53       325
           macro avg       0.32      0.24      0.24       325
        weighted avg       0.49      0.53      0.48       325

Val Loss: 1.919 |  Val Acc: 53.66%

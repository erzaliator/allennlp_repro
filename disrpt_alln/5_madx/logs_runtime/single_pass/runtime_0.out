-------------------------------------------------------------------
Lang1:  eng.pdtb.pdtb    Lang2:  deu.rst.pcc
Saving run to:  runs/SingleEpochPass=v4_eng.pdtb.pdtb_deu.rst.pcc_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 43920 examples
read 1674 examples
read 2257 examples
read 2164 examples
read 241 examples
read 260 examples
Total prediction labels:  49
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=49, bias=True)
    )
  )
)
{'train@eng.pdtb.pdtb_loss': 1.6505341529846191, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.5041211293260474, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.16462560941515872, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.20756156457410996, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.17104082115291613, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.6505340337753296, 'train@eng.pdtb.pdtb_runtime': 528.3099, 'train@eng.pdtb.pdtb_samples_per_second': 83.133, 'train@eng.pdtb.pdtb_steps_per_second': 2.599, 'epoch': 1.0}
{'loss': 2.0526, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 1.60077702999115, 'eval_accuracy@eng.pdtb.pdtb': 0.525089605734767, 'eval_f1@eng.pdtb.pdtb': 0.1981957384684879, 'eval_precision@eng.pdtb.pdtb': 0.25162961987049276, 'eval_recall@eng.pdtb.pdtb': 0.20407318954849768, 'eval_loss@eng.pdtb.pdtb': 1.6007766723632812, 'eval_runtime': 20.5887, 'eval_samples_per_second': 81.307, 'eval_steps_per_second': 2.574, 'epoch': 1.0}
{'train_runtime': 1661.2742, 'train_samples_per_second': 26.438, 'train_steps_per_second': 0.826, 'train_loss': 2.0525761192416243, 'epoch': 1.0}
***** train_log metrics *****
  epoch                    =        1.0
  train_loss               =     2.0526
  train_runtime            = 0:27:41.27
  train_samples_per_second =     26.438
  train_steps_per_second   =      0.826
{'train@deu.rst.pcc_loss': 3.486384868621826, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.06423290203327171, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.019060675013589552, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.016184928191784475, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.05117182217697439, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.4863851070404053, 'train@deu.rst.pcc_runtime': 26.8334, 'train@deu.rst.pcc_samples_per_second': 80.646, 'train@deu.rst.pcc_steps_per_second': 2.534, 'epoch': 1.0}
{'loss': 3.768, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 3.460430383682251, 'eval_accuracy@deu.rst.pcc': 0.07883817427385892, 'eval_f1@deu.rst.pcc': 0.019243063367079115, 'eval_precision@deu.rst.pcc': 0.014382823249102318, 'eval_recall@deu.rst.pcc': 0.04258241758241758, 'eval_loss@deu.rst.pcc': 3.4604310989379883, 'eval_runtime': 3.4575, 'eval_samples_per_second': 69.703, 'eval_steps_per_second': 2.314, 'epoch': 1.0}
{'train_runtime': 86.0798, 'train_samples_per_second': 25.139, 'train_steps_per_second': 0.79, 'train_loss': 3.767971263212316, 'epoch': 1.0}
***** train_logger metrics *****
  epoch                    =        1.0
  train_loss               =     2.0526
  train_runtime            = 0:27:41.27
  train_samples_per_second =     26.438
  train_steps_per_second   =      0.826
-------------------------------------------------------------------
Lang1:  eng.rst.gum    Lang2:  deu.rst.pcc
Saving run to:  runs/SingleEpochPass=v4_eng.rst.gum_deu.rst.pcc_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 13897 examples
read 2149 examples
read 2091 examples
read 2164 examples
read 241 examples
read 260 examples
Total prediction labels:  32
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=32, bias=True)
    )
  )
)
{'train@eng.rst.gum_loss': 2.6590375900268555, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.2259480463409369, 'train@eng.rst.gum_f1@eng.rst.gum': 0.021666219921693142, 'train@eng.rst.gum_precision@eng.rst.gum': 0.01861026414175901, 'train@eng.rst.gum_recall@eng.rst.gum': 0.04612070858845192, 'train@eng.rst.gum_loss@eng.rst.gum': 2.6590375900268555, 'train@eng.rst.gum_runtime': 167.1596, 'train@eng.rst.gum_samples_per_second': 83.136, 'train@eng.rst.gum_steps_per_second': 2.602, 'epoch': 1.0}
{'loss': 2.8283, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 2.7296266555786133, 'eval_accuracy@eng.rst.gum': 0.22522103303862262, 'eval_f1@eng.rst.gum': 0.02364317058377532, 'eval_precision@eng.rst.gum': 0.01950980747852418, 'eval_recall@eng.rst.gum': 0.04823450563936223, 'eval_loss@eng.rst.gum': 2.729626417160034, 'eval_runtime': 26.1513, 'eval_samples_per_second': 82.176, 'eval_steps_per_second': 2.6, 'epoch': 1.0}
{'train_runtime': 543.888, 'train_samples_per_second': 25.551, 'train_steps_per_second': 0.8, 'train_loss': 2.8283259249281607, 'epoch': 1.0}
***** train_log metrics *****
  epoch                    =        1.0
  train_loss               =     2.8283
  train_runtime            = 0:09:03.88
  train_samples_per_second =     25.551
  train_steps_per_second   =        0.8
{'train@deu.rst.pcc_loss': 3.238489866256714, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.06746765249537892, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.010082546144827682, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.011389484723622405, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.040999230672551054, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.2384896278381348, 'train@deu.rst.pcc_runtime': 26.7642, 'train@deu.rst.pcc_samples_per_second': 80.854, 'train@deu.rst.pcc_steps_per_second': 2.541, 'epoch': 1.0}
{'loss': 3.3417, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 3.2814207077026367, 'eval_accuracy@deu.rst.pcc': 0.07468879668049792, 'eval_f1@deu.rst.pcc': 0.013311999303378614, 'eval_precision@deu.rst.pcc': 0.022177810077519382, 'eval_recall@deu.rst.pcc': 0.042946024563671616, 'eval_loss@deu.rst.pcc': 3.281420946121216, 'eval_runtime': 3.3011, 'eval_samples_per_second': 73.006, 'eval_steps_per_second': 2.423, 'epoch': 1.0}
{'train_runtime': 86.0788, 'train_samples_per_second': 25.14, 'train_steps_per_second': 0.79, 'train_loss': 3.3417219274184284, 'epoch': 1.0}
***** train_logger metrics *****
  epoch                    =        1.0
  train_loss               =     2.8283
  train_runtime            = 0:09:03.88
  train_samples_per_second =     25.551
  train_steps_per_second   =        0.8
-------------------------------------------------------------------
Lang1:  eng.rst.rstdt    Lang2:  deu.rst.pcc
Saving run to:  runs/SingleEpochPass=v4_eng.rst.rstdt_deu.rst.pcc_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 16002 examples
read 1621 examples
read 2155 examples
read 2164 examples
read 241 examples
read 260 examples
Total prediction labels:  36
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=36, bias=True)
    )
  )
)
{'train@eng.rst.rstdt_loss': 2.0099058151245117, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.45269341332333457, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.06556531331256223, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.07290542641329993, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.08072489874856058, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 2.0099058151245117, 'train@eng.rst.rstdt_runtime': 192.4341, 'train@eng.rst.rstdt_samples_per_second': 83.156, 'train@eng.rst.rstdt_steps_per_second': 2.603, 'epoch': 1.0}
{'loss': 2.274, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 1.972913146018982, 'eval_accuracy@eng.rst.rstdt': 0.4540407156076496, 'eval_f1@eng.rst.rstdt': 0.06428269455962916, 'eval_precision@eng.rst.rstdt': 0.07260921341959058, 'eval_recall@eng.rst.rstdt': 0.07924846856026206, 'eval_loss@eng.rst.rstdt': 1.9729130268096924, 'eval_runtime': 19.8431, 'eval_samples_per_second': 81.691, 'eval_steps_per_second': 2.57, 'epoch': 1.0}
{'train_runtime': 616.1273, 'train_samples_per_second': 25.972, 'train_steps_per_second': 0.813, 'train_loss': 2.273960136367889, 'epoch': 1.0}
***** train_log metrics *****
  epoch                    =        1.0
  train_loss               =      2.274
  train_runtime            = 0:10:16.12
  train_samples_per_second =     25.972
  train_steps_per_second   =      0.813
{'train@deu.rst.pcc_loss': 3.5449769496917725, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.0046210720887245845, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.00035383200056613123, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.00017773354187402247, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.038461538461538464, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.5449769496917725, 'train@deu.rst.pcc_runtime': 26.8114, 'train@deu.rst.pcc_samples_per_second': 80.712, 'train@deu.rst.pcc_steps_per_second': 2.536, 'epoch': 1.0}
{'loss': 3.6979, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 3.488717794418335, 'eval_accuracy@deu.rst.pcc': 0.0, 'eval_f1@deu.rst.pcc': 0.0, 'eval_precision@deu.rst.pcc': 0.0, 'eval_recall@deu.rst.pcc': 0.0, 'eval_loss@deu.rst.pcc': 3.4887173175811768, 'eval_runtime': 3.3142, 'eval_samples_per_second': 72.718, 'eval_steps_per_second': 2.414, 'epoch': 1.0}
{'train_runtime': 85.9472, 'train_samples_per_second': 25.178, 'train_steps_per_second': 0.791, 'train_loss': 3.697856903076172, 'epoch': 1.0}
***** train_logger metrics *****
  epoch                    =        1.0
  train_loss               =      2.274
  train_runtime            = 0:10:16.12
  train_samples_per_second =     25.972
  train_steps_per_second   =      0.813
-------------------------------------------------------------------
Lang1:  eng.sdrt.stac    Lang2:  deu.rst.pcc
Saving run to:  runs/SingleEpochPass=v4_eng.sdrt.stac_deu.rst.pcc_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 9580 examples
read 1145 examples
read 1510 examples
read 2164 examples
read 241 examples
read 260 examples
Total prediction labels:  42
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=42, bias=True)
    )
  )
)
{'train@eng.sdrt.stac_loss': 2.3425185680389404, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.27755741127348643, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.04524272446742429, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.039201937138074114, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.0779713852209156, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 2.3425185680389404, 'train@eng.sdrt.stac_runtime': 114.923, 'train@eng.sdrt.stac_samples_per_second': 83.36, 'train@eng.sdrt.stac_steps_per_second': 2.61, 'epoch': 1.0}
{'loss': 2.6772, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 2.322416067123413, 'eval_accuracy@eng.sdrt.stac': 0.2794759825327511, 'eval_f1@eng.sdrt.stac': 0.040220641687592026, 'eval_precision@eng.sdrt.stac': 0.03404973649538867, 'eval_recall@eng.sdrt.stac': 0.07276283751682636, 'eval_loss@eng.sdrt.stac': 2.322416067123413, 'eval_runtime': 14.2552, 'eval_samples_per_second': 80.322, 'eval_steps_per_second': 2.525, 'epoch': 1.0}
{'train_runtime': 370.3882, 'train_samples_per_second': 25.865, 'train_steps_per_second': 0.81, 'train_loss': 2.6772471110026044, 'epoch': 1.0}
***** train_log metrics *****
  epoch                    =        1.0
  train_loss               =     2.6772
  train_runtime            = 0:06:10.38
  train_samples_per_second =     25.865
  train_steps_per_second   =       0.81
{'train@deu.rst.pcc_loss': 3.495795249938965, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.051756007393715345, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.007844339061393686, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.008805877416228533, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.04113486243303688, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.4957950115203857, 'train@deu.rst.pcc_runtime': 26.7988, 'train@deu.rst.pcc_samples_per_second': 80.75, 'train@deu.rst.pcc_steps_per_second': 2.537, 'epoch': 1.0}
{'loss': 3.6086, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 3.4877798557281494, 'eval_accuracy@deu.rst.pcc': 0.03734439834024896, 'eval_f1@deu.rst.pcc': 0.005205811138014527, 'eval_precision@deu.rst.pcc': 0.007414369256474519, 'eval_recall@deu.rst.pcc': 0.04315476190476191, 'eval_loss@deu.rst.pcc': 3.4877796173095703, 'eval_runtime': 3.3096, 'eval_samples_per_second': 72.819, 'eval_steps_per_second': 2.417, 'epoch': 1.0}
{'train_runtime': 85.7714, 'train_samples_per_second': 25.23, 'train_steps_per_second': 0.793, 'train_loss': 3.608626870548024, 'epoch': 1.0}
***** train_logger metrics *****
  epoch                    =        1.0
  train_loss               =     2.6772
  train_runtime            = 0:06:10.38
  train_samples_per_second =     25.865
  train_steps_per_second   =       0.81
-------------------------------------------------------------------
Lang1:  fas.rst.prstc    Lang2:  deu.rst.pcc
Saving run to:  runs/SingleEpochPass=v4_fas.rst.prstc_deu.rst.pcc_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 4100 examples
read 499 examples
read 592 examples
read 2164 examples
read 241 examples
read 260 examples
Total prediction labels:  36
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (ar): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=36, bias=True)
    )
  )
)
{'train@fas.rst.prstc_loss': 2.604099750518799, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2375609756097561, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.022583412553038557, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.013977584202746725, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.058763197586726997, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.604099750518799, 'train@fas.rst.prstc_runtime': 49.2982, 'train@fas.rst.prstc_samples_per_second': 83.167, 'train@fas.rst.prstc_steps_per_second': 2.617, 'epoch': 1.0}
{'loss': 2.9455, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 2.539728879928589, 'eval_accuracy@fas.rst.prstc': 0.24649298597194388, 'eval_f1@fas.rst.prstc': 0.027385984427141265, 'eval_precision@fas.rst.prstc': 0.08299866131191432, 'eval_recall@fas.rst.prstc': 0.0672463768115942, 'eval_loss@fas.rst.prstc': 2.539728879928589, 'eval_runtime': 6.354, 'eval_samples_per_second': 78.534, 'eval_steps_per_second': 2.518, 'epoch': 1.0}
{'train_runtime': 158.6894, 'train_samples_per_second': 25.837, 'train_steps_per_second': 0.813, 'train_loss': 2.945503648861434, 'epoch': 1.0}
***** train_log metrics *****
  epoch                    =        1.0
  train_loss               =     2.9455
  train_runtime            = 0:02:38.68
  train_samples_per_second =     25.837
  train_steps_per_second   =      0.813
{'train@deu.rst.pcc_loss': 3.387117385864258, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.09611829944547134, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.018569526490441763, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.01719262985576165, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.0407242541918469, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.387117385864258, 'train@deu.rst.pcc_runtime': 28.1939, 'train@deu.rst.pcc_samples_per_second': 76.754, 'train@deu.rst.pcc_steps_per_second': 2.412, 'epoch': 1.0}
{'loss': 3.4744, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 3.401181221008301, 'eval_accuracy@deu.rst.pcc': 0.07468879668049792, 'eval_f1@deu.rst.pcc': 0.018643762214866258, 'eval_precision@deu.rst.pcc': 0.013516107124929871, 'eval_recall@deu.rst.pcc': 0.049679487179487176, 'eval_loss@deu.rst.pcc': 3.40118145942688, 'eval_runtime': 3.3183, 'eval_samples_per_second': 72.628, 'eval_steps_per_second': 2.411, 'epoch': 1.0}
{'train_runtime': 87.1477, 'train_samples_per_second': 24.831, 'train_steps_per_second': 0.78, 'train_loss': 3.4744246987735523, 'epoch': 1.0}
***** train_logger metrics *****
  epoch                    =        1.0
  train_loss               =     2.9455
  train_runtime            = 0:02:38.68
  train_samples_per_second =     25.837
  train_steps_per_second   =      0.813
-------------------------------------------------------------------
Lang1:  fra.sdrt.annodis    Lang2:  deu.rst.pcc
Saving run to:  runs/SingleEpochPass=v4_fra.sdrt.annodis_deu.rst.pcc_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2185 examples
read 528 examples
read 625 examples
read 2164 examples
read 241 examples
read 260 examples
Total prediction labels:  40
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (fr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=40, bias=True)
    )
  )
)
{'train@fra.sdrt.annodis_loss': 3.1972389221191406, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.2151029748283753, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.044666509132201576, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.035543709217163204, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.06327297379730694, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 3.1972389221191406, 'train@fra.sdrt.annodis_runtime': 26.7997, 'train@fra.sdrt.annodis_samples_per_second': 81.531, 'train@fra.sdrt.annodis_steps_per_second': 2.575, 'epoch': 1.0}
{'loss': 3.3837, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 3.201608657836914, 'eval_accuracy@fra.sdrt.annodis': 0.24053030303030304, 'eval_f1@fra.sdrt.annodis': 0.04674267127493168, 'eval_precision@fra.sdrt.annodis': 0.03868251240645552, 'eval_recall@fra.sdrt.annodis': 0.06687320757933427, 'eval_loss@fra.sdrt.annodis': 3.201608419418335, 'eval_runtime': 6.768, 'eval_samples_per_second': 78.014, 'eval_steps_per_second': 2.512, 'epoch': 1.0}
{'train_runtime': 89.2841, 'train_samples_per_second': 24.472, 'train_steps_per_second': 0.773, 'train_loss': 3.383710778277853, 'epoch': 1.0}
***** train_log metrics *****
  epoch                    =        1.0
  train_loss               =     3.3837
  train_runtime            = 0:01:29.28
  train_samples_per_second =     24.472
  train_steps_per_second   =      0.773
{'train@deu.rst.pcc_loss': 3.416672706604004, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.09796672828096119, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.008267347651154673, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.02297008547008547, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.0392156862745098, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.416672468185425, 'train@deu.rst.pcc_runtime': 26.6713, 'train@deu.rst.pcc_samples_per_second': 81.136, 'train@deu.rst.pcc_steps_per_second': 2.55, 'epoch': 1.0}
{'loss': 3.5102, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 3.4299144744873047, 'eval_accuracy@deu.rst.pcc': 0.11618257261410789, 'eval_f1@deu.rst.pcc': 0.008674101610904586, 'eval_precision@deu.rst.pcc': 0.004840940525587829, 'eval_recall@deu.rst.pcc': 0.041666666666666664, 'eval_loss@deu.rst.pcc': 3.429914712905884, 'eval_runtime': 3.3768, 'eval_samples_per_second': 71.37, 'eval_steps_per_second': 2.369, 'epoch': 1.0}
{'train_runtime': 85.5828, 'train_samples_per_second': 25.285, 'train_steps_per_second': 0.795, 'train_loss': 3.5102038663976334, 'epoch': 1.0}
***** train_logger metrics *****
  epoch                    =        1.0
  train_loss               =     3.3837
  train_runtime            = 0:01:29.28
  train_samples_per_second =     24.472
  train_steps_per_second   =      0.773
-------------------------------------------------------------------
Lang1:  nld.rst.nldt    Lang2:  deu.rst.pcc
Saving run to:  runs/SingleEpochPass=v4_nld.rst.nldt_deu.rst.pcc_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 1608 examples
read 331 examples
read 326 examples
read 2164 examples
read 241 examples
read 260 examples
Total prediction labels:  39
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=39, bias=True)
    )
  )
)
{'train@nld.rst.nldt_loss': 3.4179630279541016, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26430348258706465, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01306566650270536, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.00825948383084577, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03125, 'train@nld.rst.nldt_loss@nld.rst.nldt': 3.4179635047912598, 'train@nld.rst.nldt_runtime': 19.837, 'train@nld.rst.nldt_samples_per_second': 81.061, 'train@nld.rst.nldt_steps_per_second': 2.571, 'epoch': 1.0}
{'loss': 3.5181, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 3.4056665897369385, 'eval_accuracy@nld.rst.nldt': 0.27794561933534745, 'eval_f1@nld.rst.nldt': 0.016110673321075214, 'eval_precision@nld.rst.nldt': 0.010294282197605462, 'eval_recall@nld.rst.nldt': 0.037037037037037035, 'eval_loss@nld.rst.nldt': 3.4056668281555176, 'eval_runtime': 4.4423, 'eval_samples_per_second': 74.51, 'eval_steps_per_second': 2.476, 'epoch': 1.0}
{'train_runtime': 65.4079, 'train_samples_per_second': 24.584, 'train_steps_per_second': 0.78, 'train_loss': 3.518146888882506, 'epoch': 1.0}
***** train_log metrics *****
  epoch                    =        1.0
  train_loss               =     3.5181
  train_runtime            = 0:01:05.40
  train_samples_per_second =     24.584
  train_steps_per_second   =       0.78
{'train@deu.rst.pcc_loss': 3.424682140350342, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.04251386321626617, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.003138325089544602, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.0016359045485259077, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.038461538461538464, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.424682140350342, 'train@deu.rst.pcc_runtime': 26.8297, 'train@deu.rst.pcc_samples_per_second': 80.657, 'train@deu.rst.pcc_steps_per_second': 2.535, 'epoch': 1.0}
{'loss': 3.5106, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 3.4409265518188477, 'eval_accuracy@deu.rst.pcc': 0.029045643153526972, 'eval_f1@deu.rst.pcc': 0.0023616734143049934, 'eval_precision@deu.rst.pcc': 0.0012152777777777778, 'eval_recall@deu.rst.pcc': 0.041666666666666664, 'eval_loss@deu.rst.pcc': 3.4409267902374268, 'eval_runtime': 3.3237, 'eval_samples_per_second': 72.51, 'eval_steps_per_second': 2.407, 'epoch': 1.0}
{'train_runtime': 85.7203, 'train_samples_per_second': 25.245, 'train_steps_per_second': 0.793, 'train_loss': 3.510637171128217, 'epoch': 1.0}
***** train_logger metrics *****
  epoch                    =        1.0
  train_loss               =     3.5181
  train_runtime            = 0:01:05.40
  train_samples_per_second =     24.584
  train_steps_per_second   =       0.78
-------------------------------------------------------------------
Lang1:  por.rst.cstn    Lang2:  deu.rst.pcc
Saving run to:  runs/SingleEpochPass=v4_por.rst.cstn_deu.rst.pcc_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 4148 examples
read 573 examples
read 272 examples
read 2164 examples
read 241 examples
read 260 examples
Total prediction labels:  42
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (pt): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (pt): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=42, bias=True)
    )
  )
)
{'train@por.rst.cstn_loss': 2.9597585201263428, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.2774831243972999, 'train@por.rst.cstn_f1@por.rst.cstn': 0.013575674655595393, 'train@por.rst.cstn_precision@por.rst.cstn': 0.008671347637415621, 'train@por.rst.cstn_recall@por.rst.cstn': 0.03125, 'train@por.rst.cstn_loss@por.rst.cstn': 2.959758758544922, 'train@por.rst.cstn_runtime': 50.7446, 'train@por.rst.cstn_samples_per_second': 81.743, 'train@por.rst.cstn_steps_per_second': 2.562, 'epoch': 1.0}
{'loss': 3.2453, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 2.9879746437072754, 'eval_accuracy@por.rst.cstn': 0.28097731239092494, 'eval_f1@por.rst.cstn': 0.019940549913301957, 'eval_precision@por.rst.cstn': 0.012771696017769315, 'eval_recall@por.rst.cstn': 0.045454545454545456, 'eval_loss@por.rst.cstn': 2.9879746437072754, 'eval_runtime': 7.4282, 'eval_samples_per_second': 77.138, 'eval_steps_per_second': 2.423, 'epoch': 1.0}
{'train_runtime': 164.3264, 'train_samples_per_second': 25.242, 'train_steps_per_second': 0.791, 'train_loss': 3.2452824519230767, 'epoch': 1.0}
***** train_log metrics *****
  epoch                    =        1.0
  train_loss               =     3.2453
  train_runtime            = 0:02:44.32
  train_samples_per_second =     25.242
  train_steps_per_second   =      0.791
{'train@deu.rst.pcc_loss': 3.457921266555786, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.03743068391866913, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.002776635129576306, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.001440307265549984, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.038461538461538464, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.457921028137207, 'train@deu.rst.pcc_runtime': 26.7764, 'train@deu.rst.pcc_samples_per_second': 80.817, 'train@deu.rst.pcc_steps_per_second': 2.54, 'epoch': 1.0}
{'loss': 3.5558, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 3.4799013137817383, 'eval_accuracy@deu.rst.pcc': 0.03734439834024896, 'eval_f1@deu.rst.pcc': 0.005398199143281755, 'eval_precision@deu.rst.pcc': 0.015289449112978524, 'eval_recall@deu.rst.pcc': 0.04315476190476191, 'eval_loss@deu.rst.pcc': 3.4799015522003174, 'eval_runtime': 3.3787, 'eval_samples_per_second': 71.33, 'eval_steps_per_second': 2.368, 'epoch': 1.0}
{'train_runtime': 85.9357, 'train_samples_per_second': 25.182, 'train_steps_per_second': 0.791, 'train_loss': 3.555842680089614, 'epoch': 1.0}
***** train_logger metrics *****
  epoch                    =        1.0
  train_loss               =     3.2453
  train_runtime            = 0:02:44.32
  train_samples_per_second =     25.242
  train_steps_per_second   =      0.791
-------------------------------------------------------------------
Lang1:  rus.rst.rrt    Lang2:  deu.rst.pcc
Saving run to:  runs/SingleEpochPass=v4_rus.rst.rrt_deu.rst.pcc_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 28822 examples
read 2855 examples
read 2843 examples
read 2164 examples
read 241 examples
read 260 examples
Total prediction labels:  34
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (ru): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=34, bias=True)
    )
  )
)
{'train@rus.rst.rrt_loss': 2.0206496715545654, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.4021580736937062, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.10481083459114293, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.16923021310682604, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.11679368480413467, 'train@rus.rst.rrt_loss@rus.rst.rrt': 2.0206496715545654, 'train@rus.rst.rrt_runtime': 353.9184, 'train@rus.rst.rrt_samples_per_second': 81.437, 'train@rus.rst.rrt_steps_per_second': 2.546, 'epoch': 1.0}
{'loss': 2.272, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 2.044785737991333, 'eval_accuracy@rus.rst.rrt': 0.3821366024518389, 'eval_f1@rus.rst.rrt': 0.11083010569988193, 'eval_precision@rus.rst.rrt': 0.1985243500390049, 'eval_recall@rus.rst.rrt': 0.125209800974407, 'eval_loss@rus.rst.rrt': 2.044785499572754, 'eval_runtime': 35.465, 'eval_samples_per_second': 80.502, 'eval_steps_per_second': 2.538, 'epoch': 1.0}
{'train_runtime': 1133.387, 'train_samples_per_second': 25.43, 'train_steps_per_second': 0.795, 'train_loss': 2.2719889142272476, 'epoch': 1.0}
***** train_log metrics *****
  epoch                    =        1.0
  train_loss               =      2.272
  train_runtime            = 0:18:53.38
  train_samples_per_second =      25.43
  train_steps_per_second   =      0.795
{'train@deu.rst.pcc_loss': 3.5429022312164307, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.02033271719038817, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.007861360484845663, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.008336854559405972, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.027536943702357233, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.5429024696350098, 'train@deu.rst.pcc_runtime': 26.9158, 'train@deu.rst.pcc_samples_per_second': 80.399, 'train@deu.rst.pcc_steps_per_second': 2.526, 'epoch': 1.0}
{'loss': 3.7933, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 3.5107367038726807, 'eval_accuracy@deu.rst.pcc': 0.03734439834024896, 'eval_f1@deu.rst.pcc': 0.013773169818386069, 'eval_precision@deu.rst.pcc': 0.01092619704744075, 'eval_recall@deu.rst.pcc': 0.06933170995670997, 'eval_loss@deu.rst.pcc': 3.5107369422912598, 'eval_runtime': 3.3854, 'eval_samples_per_second': 71.189, 'eval_steps_per_second': 2.363, 'epoch': 1.0}
{'train_runtime': 86.1439, 'train_samples_per_second': 25.121, 'train_steps_per_second': 0.789, 'train_loss': 3.793299506692325, 'epoch': 1.0}
***** train_logger metrics *****
  epoch                    =        1.0
  train_loss               =      2.272
  train_runtime            = 0:18:53.38
  train_samples_per_second =      25.43
  train_steps_per_second   =      0.795
-------------------------------------------------------------------
Lang1:  spa.rst.rststb    Lang2:  deu.rst.pcc
Saving run to:  runs/SingleEpochPass=v4_spa.rst.rststb_deu.rst.pcc_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2240 examples
read 383 examples
read 426 examples
read 2164 examples
read 241 examples
read 260 examples
Total prediction labels:  33
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=33, bias=True)
    )
  )
)
{'train@spa.rst.rststb_loss': 3.1128880977630615, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.2513392857142857, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.03485654959562489, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.03105944056084523, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.05203996396664044, 'train@spa.rst.rststb_loss@spa.rst.rststb': 3.1128883361816406, 'train@spa.rst.rststb_runtime': 27.5898, 'train@spa.rst.rststb_samples_per_second': 81.189, 'train@spa.rst.rststb_steps_per_second': 2.537, 'epoch': 1.0}
{'loss': 3.2549, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 3.1321346759796143, 'eval_accuracy@spa.rst.rststb': 0.2193211488250653, 'eval_f1@spa.rst.rststb': 0.032451960537514535, 'eval_precision@spa.rst.rststb': 0.03225279469765922, 'eval_recall@spa.rst.rststb': 0.05231553494846277, 'eval_loss@spa.rst.rststb': 3.132134199142456, 'eval_runtime': 5.0811, 'eval_samples_per_second': 75.378, 'eval_steps_per_second': 2.362, 'epoch': 1.0}
{'train_runtime': 90.0987, 'train_samples_per_second': 24.862, 'train_steps_per_second': 0.777, 'train_loss': 3.2549015590122767, 'epoch': 1.0}
***** train_log metrics *****
  epoch                    =        1.0
  train_loss               =     3.2549
  train_runtime            = 0:01:30.09
  train_samples_per_second =     24.862
  train_steps_per_second   =      0.777
{'train@deu.rst.pcc_loss': 3.268775463104248, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.07486136783733827, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.008111153070923065, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.011644416030631426, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.03735643153497638, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.268775701522827, 'train@deu.rst.pcc_runtime': 26.8738, 'train@deu.rst.pcc_samples_per_second': 80.524, 'train@deu.rst.pcc_steps_per_second': 2.53, 'epoch': 1.0}
{'loss': 3.3398, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 3.2712957859039307, 'eval_accuracy@deu.rst.pcc': 0.11618257261410789, 'eval_f1@deu.rst.pcc': 0.00907911802853437, 'eval_precision@deu.rst.pcc': 0.005094614264919942, 'eval_recall@deu.rst.pcc': 0.041666666666666664, 'eval_loss@deu.rst.pcc': 3.2712960243225098, 'eval_runtime': 3.3491, 'eval_samples_per_second': 71.959, 'eval_steps_per_second': 2.389, 'epoch': 1.0}
{'train_runtime': 86.2141, 'train_samples_per_second': 25.1, 'train_steps_per_second': 0.789, 'train_loss': 3.339780695298139, 'epoch': 1.0}
***** train_logger metrics *****
  epoch                    =        1.0
  train_loss               =     3.2549
  train_runtime            = 0:01:30.09
  train_samples_per_second =     24.862
  train_steps_per_second   =      0.777
-------------------------------------------------------------------
Lang1:  spa.rst.sctb    Lang2:  deu.rst.pcc
Saving run to:  runs/SingleEpochPass=v4_spa.rst.sctb_deu.rst.pcc_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 439 examples
read 94 examples
read 159 examples
read 2164 examples
read 241 examples
read 260 examples
Total prediction labels:  31
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=31, bias=True)
    )
  )
)
{'train@spa.rst.sctb_loss': 3.3273379802703857, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.20045558086560364, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03274003132833027, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.05124972376602102, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.07155473145547588, 'train@spa.rst.sctb_loss@spa.rst.sctb': 3.3273375034332275, 'train@spa.rst.sctb_runtime': 5.5809, 'train@spa.rst.sctb_samples_per_second': 78.661, 'train@spa.rst.sctb_steps_per_second': 2.509, 'epoch': 1.0}
{'loss': 3.3926, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 3.331416606903076, 'eval_accuracy@spa.rst.sctb': 0.19148936170212766, 'eval_f1@spa.rst.sctb': 0.040692875199917454, 'eval_precision@spa.rst.sctb': 0.04025214551530341, 'eval_recall@spa.rst.sctb': 0.05642135642135642, 'eval_loss@spa.rst.sctb': 3.3314170837402344, 'eval_runtime': 1.4934, 'eval_samples_per_second': 62.946, 'eval_steps_per_second': 2.009, 'epoch': 1.0}
{'train_runtime': 19.2656, 'train_samples_per_second': 22.787, 'train_steps_per_second': 0.727, 'train_loss': 3.3925503322056363, 'epoch': 1.0}
***** train_log metrics *****
  epoch                    =        1.0
  train_loss               =     3.3926
  train_runtime            = 0:00:19.26
  train_samples_per_second =     22.787
  train_steps_per_second   =      0.727
{'train@deu.rst.pcc_loss': 3.2561967372894287, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.08364140480591498, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.01219843939210789, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.009074621947231621, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.0388844350596616, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.2561967372894287, 'train@deu.rst.pcc_runtime': 26.8759, 'train@deu.rst.pcc_samples_per_second': 80.518, 'train@deu.rst.pcc_steps_per_second': 2.53, 'epoch': 1.0}
{'loss': 3.3267, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 3.2518539428710938, 'eval_accuracy@deu.rst.pcc': 0.0912863070539419, 'eval_f1@deu.rst.pcc': 0.015546449223392128, 'eval_precision@deu.rst.pcc': 0.015349794238683129, 'eval_recall@deu.rst.pcc': 0.04214997964997965, 'eval_loss@deu.rst.pcc': 3.2518532276153564, 'eval_runtime': 3.3136, 'eval_samples_per_second': 72.73, 'eval_steps_per_second': 2.414, 'epoch': 1.0}
{'train_runtime': 85.885, 'train_samples_per_second': 25.196, 'train_steps_per_second': 0.792, 'train_loss': 3.3266969568589153, 'epoch': 1.0}
***** train_logger metrics *****
  epoch                    =        1.0
  train_loss               =     3.3926
  train_runtime            = 0:00:19.26
  train_samples_per_second =     22.787
  train_steps_per_second   =      0.727
-------------------------------------------------------------------
Lang1:  tur.pdtb.tdb    Lang2:  deu.rst.pcc
Saving run to:  runs/SingleEpochPass=v4_tur.pdtb.tdb_deu.rst.pcc_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2451 examples
read 312 examples
read 422 examples
read 2164 examples
read 241 examples
read 260 examples
Total prediction labels:  49
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (tr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (tr): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=49, bias=True)
    )
  )
)
{'train@tur.pdtb.tdb_loss': 3.32271409034729, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01741967515426626, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010891738953044898, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 3.32271409034729, 'train@tur.pdtb.tdb_runtime': 30.5302, 'train@tur.pdtb.tdb_samples_per_second': 80.281, 'train@tur.pdtb.tdb_steps_per_second': 2.522, 'epoch': 1.0}
{'loss': 3.5111, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 3.2877116203308105, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 3.2877116203308105, 'eval_runtime': 4.3858, 'eval_samples_per_second': 71.139, 'eval_steps_per_second': 2.28, 'epoch': 1.0}
{'train_runtime': 97.8524, 'train_samples_per_second': 25.048, 'train_steps_per_second': 0.787, 'train_loss': 3.511109191101867, 'epoch': 1.0}
***** train_log metrics *****
  epoch                    =        1.0
  train_loss               =     3.5111
  train_runtime            = 0:01:37.85
  train_samples_per_second =     25.048
  train_steps_per_second   =      0.787
{'train@deu.rst.pcc_loss': 3.6362171173095703, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.02264325323475046, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.007027398719173796, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.031679239295498804, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.043125140768137706, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.6362171173095703, 'train@deu.rst.pcc_runtime': 27.0606, 'train@deu.rst.pcc_samples_per_second': 79.969, 'train@deu.rst.pcc_steps_per_second': 2.513, 'epoch': 1.0}
{'loss': 3.7419, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 3.637441635131836, 'eval_accuracy@deu.rst.pcc': 0.029045643153526972, 'eval_f1@deu.rst.pcc': 0.0115013872515748, 'eval_precision@deu.rst.pcc': 0.03136973180076628, 'eval_recall@deu.rst.pcc': 0.011243386243386243, 'eval_loss@deu.rst.pcc': 3.637441635131836, 'eval_runtime': 3.5323, 'eval_samples_per_second': 68.228, 'eval_steps_per_second': 2.265, 'epoch': 1.0}
{'train_runtime': 86.4106, 'train_samples_per_second': 25.043, 'train_steps_per_second': 0.787, 'train_loss': 3.7419009489171646, 'epoch': 1.0}
***** train_logger metrics *****
  epoch                    =        1.0
  train_loss               =     3.5111
  train_runtime            = 0:01:37.85
  train_samples_per_second =     25.048
  train_steps_per_second   =      0.787
-------------------------------------------------------------------
Lang1:  zho.rst.sctb    Lang2:  deu.rst.pcc
Saving run to:  runs/SingleEpochPass=v4_zho.rst.sctb_deu.rst.pcc_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 439 examples
read 94 examples
read 159 examples
read 2164 examples
read 241 examples
read 260 examples
Total prediction labels:  31
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (zh): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=31, bias=True)
    )
  )
)
{'train@zho.rst.sctb_loss': 3.333554983139038, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.20501138952164008, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.027698504214344895, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.029824149148010348, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04667586379013921, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.333555221557617, 'train@zho.rst.sctb_runtime': 5.4898, 'train@zho.rst.sctb_samples_per_second': 79.967, 'train@zho.rst.sctb_steps_per_second': 2.55, 'epoch': 1.0}
{'loss': 3.3905, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 3.346615791320801, 'eval_accuracy@zho.rst.sctb': 0.18085106382978725, 'eval_f1@zho.rst.sctb': 0.02587079057667293, 'eval_precision@zho.rst.sctb': 0.027910685805422643, 'eval_recall@zho.rst.sctb': 0.03417446524064171, 'eval_loss@zho.rst.sctb': 3.346615791320801, 'eval_runtime': 2.4824, 'eval_samples_per_second': 37.867, 'eval_steps_per_second': 1.209, 'epoch': 1.0}
{'train_runtime': 19.0203, 'train_samples_per_second': 23.081, 'train_steps_per_second': 0.736, 'train_loss': 3.3904923030308316, 'epoch': 1.0}
***** train_log metrics *****
  epoch                    =        1.0
  train_loss               =     3.3905
  train_runtime            = 0:00:19.02
  train_samples_per_second =     23.081
  train_steps_per_second   =      0.736
{'train@deu.rst.pcc_loss': 3.2524285316467285, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.09426987060998152, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.01179475046189011, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.009133207345773666, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.03888448517776993, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.2524285316467285, 'train@deu.rst.pcc_runtime': 26.8439, 'train@deu.rst.pcc_samples_per_second': 80.614, 'train@deu.rst.pcc_steps_per_second': 2.533, 'epoch': 1.0}
{'loss': 3.3213, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 3.2521004676818848, 'eval_accuracy@deu.rst.pcc': 0.06639004149377593, 'eval_f1@deu.rst.pcc': 0.009306693989071038, 'eval_precision@deu.rst.pcc': 0.006565656565656566, 'eval_recall@deu.rst.pcc': 0.046130952380952384, 'eval_loss@deu.rst.pcc': 3.252100706100464, 'eval_runtime': 3.3144, 'eval_samples_per_second': 72.714, 'eval_steps_per_second': 2.414, 'epoch': 1.0}
{'train_runtime': 85.8913, 'train_samples_per_second': 25.195, 'train_steps_per_second': 0.792, 'train_loss': 3.321328555836397, 'epoch': 1.0}
***** train_logger metrics *****
  epoch                    =        1.0
  train_loss               =     3.3905
  train_runtime            = 0:00:19.02
  train_samples_per_second =     23.081
  train_steps_per_second   =      0.736

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seeding for comparing experiment in part 2\n",
    "import torch\n",
    "import json\n",
    "SEED = 2025\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('cuda:7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNLI Bert\n",
    "## Second Tutorial\n",
    "https://towardsdatascience.com/fine-tuning-pre-trained-transformer-models-for-sentence-entailment-d87caf9ec9db\n",
    "Check his Github code for complete notebook. I never referred to it. Medium was enough.\n",
    "BERT in keras-tf: https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define macros\n",
    "BERT_MODEL = 'bert-base-german-cased'\n",
    "batch_size = 4\n",
    "batches_per_epoch = None\n",
    "\n",
    "save_path_suffix = 'cotraining_baseline_de_en_allshuffle_de_labelset_balanceen2000_absolute_'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# custom reader needed to handle quotechars\n",
    "def read_df_custom(file):\n",
    "    header = 'doc     unit1_toks      unit2_toks      unit1_txt       unit2_txt       s1_toks s2_toks unit1_sent      unit2_sent      dir     nuc_children    sat_children    genre   u1_discontinuous        u2_discontinuous       u1_issent        u2_issent       u1_length       u2_length       length_ratio    u1_speaker      u2_speaker      same_speaker    u1_func u1_pos  u1_depdir       u2_func u2_pos  u2_depdir       doclen  u1_position      u2_position     percent_distance        distance        lex_overlap_words       lex_overlap_length      unit1_case      unit2_case      label'\n",
    "    extracted_columns = ['unit1_txt', 'unit1_sent', 'unit2_txt', 'unit2_sent', 'dir', 'label', 'distance', 'u1_depdir', 'u2_depdir', 'u2_func', 'u1_position', 'u2_position', 'sat_children', 'nuc_children', 'genre', 'unit1_case', 'unit2_case',\n",
    "                            'u1_discontinuous', 'u2_discontinuous', 'same_speaker', 'lex_overlap_length', 'u1_func']\n",
    "    header = header.split()\n",
    "    df = pd.DataFrame(columns=extracted_columns)\n",
    "    file = open(file, 'r')\n",
    "\n",
    "    rows = []\n",
    "    count = 0 \n",
    "    for line in file:\n",
    "        line = line[:-1].split('\\t')\n",
    "        count+=1\n",
    "        if count ==1: continue\n",
    "        row = {}\n",
    "        for column in extracted_columns:\n",
    "            index = header.index(column)\n",
    "            try:\n",
    "                row[column] = line[index]\n",
    "            except:\n",
    "                print(count, line)\n",
    "            row[column] = line[index]\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame.from_records(rows)])\n",
    "    return df\n",
    "\n",
    "train_df_en = read_df_custom('../../processed/translated/eng.rst.rstdt_train_enriched_translated.rels')\n",
    "test_df_en = read_df_custom('../../processed/translated/eng.rst.rstdt_test_enriched_translated.rels')\n",
    "val_df_en = read_df_custom('../../processed/translated/eng.rst.rstdt_dev_enriched_translated.rels')\n",
    "train_df_de = read_df_custom('../../processed/deu.rst.pcc_train_enriched.rels')\n",
    "test_df_de = read_df_custom('../../processed/deu.rst.pcc_test_enriched.rels')\n",
    "val_df_de = read_df_custom('../../processed/deu.rst.pcc_dev_enriched.rels')\n",
    "\n",
    "lang='deu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "733\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>aux_new_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>joint</th>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elaboration</th>\n",
       "      <td>166</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>background</th>\n",
       "      <td>131</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>condition</th>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cause</th>\n",
       "      <td>92</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contrast</th>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             target  aux_new_count\n",
       "label                             \n",
       "joint           201            201\n",
       "elaboration     166            166\n",
       "background      131            131\n",
       "condition        99             99\n",
       "cause            92             92\n",
       "contrast         38             38\n",
       "summary           6              6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def dist_scaling(df_target, df_aux, plt=False):\n",
    "    dist1 = df_target.groupby('label').count()['dir'].reset_index().rename(columns={\"dir\": \"target\"})\n",
    "    dist1[\"target\"] = dist1[\"target\"]\n",
    "    dist2 = df_aux.groupby('label').count()['dir'].reset_index().rename(columns={\"dir\": \"aux\"})\n",
    "    j = pd.merge(dist1,dist2,on='label')\n",
    "    j.sort_values(by=[\"target\"], inplace=True, ascending=False)\n",
    "    j.set_index('label', inplace=True)\n",
    "    j['aux_new_count'] = j['target']\n",
    "    j = j[['target', 'aux_new_count']]\n",
    "\n",
    "    if plt:\n",
    "        ax = j.plot.bar(rot=0, title=\"New Label distribution for Aux and Target datasets\")\n",
    "        ax.set_xlabel(\"Labels\")\n",
    "        ax.set_ylabel(\"Percentages\")\n",
    "\n",
    "    print(j['aux_new_count'].sum())\n",
    "    return j\n",
    "\n",
    "dist_scaling(train_df_de, train_df_en, plt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "\n",
    "data = pd.DataFrame({'cols1':[4, 5, 5, 4, 321, 32, 5, 40, 50, 60],\n",
    "                     'cols2':[45, 66, 6, 6, 1, 432, 3, 40, 50, 60],\n",
    "                     'label':['A', 'B', 'C', 'C', 'A', 'B', 'B', 'A', 'D', 'F']})\n",
    "\n",
    "freq = pd.DataFrame({'label':['A', 'B', 'C', 'E'],\n",
    "                     'nostoextract':[2, 2, 2, 30], })\n",
    "\n",
    "def bootstrap(data, freq):\n",
    "    data_labels = list(data['label'].unique())\n",
    "    #drop the labels in freq that dont overlap\n",
    "    freq_bak = freq.copy(deep=True)\n",
    "    freq = freq[freq['label'].isin(data_labels)]\n",
    "\n",
    "    #save the labels in data that dont overlap\n",
    "    freq_labels = list(freq['label'].unique())\n",
    "    extra_data = data[~data.label.isin(freq_labels)]\n",
    "\n",
    "    #drop the labels in data that dont overlap\n",
    "    data = data[data['label'].isin(freq_labels)]\n",
    "\n",
    "    #bootstrap!\n",
    "    freq = freq.set_index('label')\n",
    "\n",
    "    def sampleClass(classgroup):\n",
    "        cls = classgroup['label'].iloc[0]\n",
    "        nDesired = freq.nostoextract[cls]\n",
    "        nRows = len(classgroup)\n",
    "\n",
    "        nSamples = min(nRows, nDesired)\n",
    "        return classgroup.sample(nSamples)\n",
    "\n",
    "    samples = data.groupby('label').apply(sampleClass)\n",
    "    samples.index = samples.index.get_level_values(1)\n",
    "\n",
    "    #add back the extra samples\n",
    "    samples = pd.concat([samples, extra_data])\n",
    "\n",
    "    return samples\n",
    "\n",
    "def get_distr(target_df, aux_df):\n",
    "    '''this function takes the target distribution and multiplies it by constant. aux_new_count denotes the target's scaled distribution that needs to be applies on aux_df'''\n",
    "    distr_combined = dist_scaling(target_df, aux_df)\n",
    "    distr_aux_new = distr_combined[['aux_new_count']]\n",
    "    distr_aux_new = distr_aux_new.rename(columns={\"aux_new_count\": \"nostoextract\"}).reset_index()\n",
    "    return distr_aux_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11430\n",
      "826\n",
      "before\n",
      "label\n",
      "background      711\n",
      "cause           563\n",
      "condition       265\n",
      "contrast        875\n",
      "elaboration    6454\n",
      "evaluation      459\n",
      "joint          1749\n",
      "means           183\n",
      "summary         171\n",
      "Name: dir, dtype: int64\n",
      "after\n",
      "label\n",
      "background     131\n",
      "cause           92\n",
      "condition       99\n",
      "contrast        38\n",
      "elaboration    166\n",
      "evaluation      85\n",
      "joint          201\n",
      "means            8\n",
      "summary          6\n",
      "Name: dir, dtype: int64\n",
      "826\n"
     ]
    }
   ],
   "source": [
    "def process_labels_and_merge_evaluation_for_german(df):\n",
    "    for delete_label in ['attribution', 'comparison', 'explanation', 'enablement', 'temporal', 'textual-organization', 'topic-change', 'topic-comment']:\n",
    "        df = df[df.label != delete_label]\n",
    "    df['label'] = df['label'].str.replace(\"manner-means\", \"means\")\n",
    "    df['label'] = df['label'].str.replace(\"evaluation-n\", \"evaluation\")\n",
    "    df['label'] = df['label'].str.replace(\"evaluation-s\", \"evaluation\")\n",
    "    return df\n",
    "\n",
    "def balance_dataset(df, distribution_df):\n",
    "    freq = get_distr(distribution_df, df)\n",
    "    print('before')\n",
    "    print(df.groupby('label').count()['dir'])\n",
    "    df = bootstrap(df, freq)\n",
    "    print('after')\n",
    "    print(df.groupby('label').count()['dir'])\n",
    "    return df\n",
    "\n",
    "train_df_de = process_labels_and_merge_evaluation_for_german(train_df_de)\n",
    "train_df_en = process_labels_and_merge_evaluation_for_german(train_df_en)\n",
    "test_df = process_labels_and_merge_evaluation_for_german(test_df_de)\n",
    "val_df = process_labels_and_merge_evaluation_for_german(val_df_de)\n",
    "\n",
    "print(len(train_df_en))\n",
    "train_df_en = balance_dataset(train_df_en, train_df_de)\n",
    "print(len(train_df_en))\n",
    "\n",
    "train_df = pd.concat([train_df_de, train_df_en])\n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2992 2990 748\n"
     ]
    }
   ],
   "source": [
    "def get_batches_per_epoch(train_df, batch_size=4):\n",
    "    size = len(train_df)\n",
    "    if size%batch_size!=0:\n",
    "        return int(size/batch_size)+1\n",
    "    else:\n",
    "        return int(size/batch_size)\n",
    "\n",
    "batches_per_epoch = get_batches_per_epoch(train_df, batch_size)\n",
    "print(batches_per_epoch*batch_size, len(train_df), batches_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping any empty values\n",
    "train_df.dropna(inplace=True)\n",
    "val_df.dropna(inplace=True)\n",
    "test_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare a dataset handler class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unit1_txt</th>\n",
       "      <th>unit1_sent</th>\n",
       "      <th>unit2_txt</th>\n",
       "      <th>unit2_sent</th>\n",
       "      <th>dir</th>\n",
       "      <th>label</th>\n",
       "      <th>distance</th>\n",
       "      <th>u1_depdir</th>\n",
       "      <th>u2_depdir</th>\n",
       "      <th>u2_func</th>\n",
       "      <th>...</th>\n",
       "      <th>sat_children</th>\n",
       "      <th>nuc_children</th>\n",
       "      <th>genre</th>\n",
       "      <th>unit1_case</th>\n",
       "      <th>unit2_case</th>\n",
       "      <th>u1_discontinuous</th>\n",
       "      <th>u2_discontinuous</th>\n",
       "      <th>same_speaker</th>\n",
       "      <th>lex_overlap_length</th>\n",
       "      <th>u1_func</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In Spremberg verfahren die Wärter offenbar nac...</td>\n",
       "      <td>In Spremberg verfahren die Wärter offenbar nac...</td>\n",
       "      <td>Hauptsache die Gesamtzahl stimmt ,</td>\n",
       "      <td>Hauptsache die Gesamtzahl stimmt , wenn abends...</td>\n",
       "      <td>1&lt;2</td>\n",
       "      <td>joint</td>\n",
       "      <td>1</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>root</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>news</td>\n",
       "      <td>cap_initial</td>\n",
       "      <td>cap_initial</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>root</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sowohl die G-20 Gruppe , in der die Industriel...</td>\n",
       "      <td>Sowohl die G-20 Gruppe , in der die Industriel...</td>\n",
       "      <td>aber immerhin .</td>\n",
       "      <td>Mehr als eine Willensbekundung ist das zwar no...</td>\n",
       "      <td>1&lt;2</td>\n",
       "      <td>evaluation</td>\n",
       "      <td>2</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>LEFT</td>\n",
       "      <td>conj</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>news</td>\n",
       "      <td>cap_initial</td>\n",
       "      <td>other</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>root</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Knapp einen Monat nach Ausbruch der Rathenower...</td>\n",
       "      <td>Knapp einen Monat nach Ausbruch der Rathenower...</td>\n",
       "      <td>nimmt das Verhalten von Bürgermeister Lünser l...</td>\n",
       "      <td>Während Timm mit diesem Verhalten Pro Rathenow...</td>\n",
       "      <td>1&gt;2</td>\n",
       "      <td>background</td>\n",
       "      <td>7</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>root</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>news</td>\n",
       "      <td>title</td>\n",
       "      <td>other</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>obl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Offenbar selbst unzureichend in Kenntnis geset...</td>\n",
       "      <td>Offenbar selbst unzureichend in Kenntnis geset...</td>\n",
       "      <td>bietet das Jugendamt unverbindliche Auskünfte -</td>\n",
       "      <td>Offenbar selbst unzureichend in Kenntnis geset...</td>\n",
       "      <td>1&gt;2</td>\n",
       "      <td>reason</td>\n",
       "      <td>1</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>root</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>news</td>\n",
       "      <td>cap_initial</td>\n",
       "      <td>other</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>acl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wer zum Beispiel an der \" Glücksspirale \" dreht ,</td>\n",
       "      <td>Wer zum Beispiel an der \" Glücksspirale \" dreh...</td>\n",
       "      <td>beweist sich indirekt als Wohltäter .</td>\n",
       "      <td>Wer zum Beispiel an der \" Glücksspirale \" dreh...</td>\n",
       "      <td>1&gt;2</td>\n",
       "      <td>condition</td>\n",
       "      <td>1</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>root</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>news</td>\n",
       "      <td>cap_initial</td>\n",
       "      <td>other</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>csubj</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           unit1_txt  \\\n",
       "0  In Spremberg verfahren die Wärter offenbar nac...   \n",
       "1  Sowohl die G-20 Gruppe , in der die Industriel...   \n",
       "2  Knapp einen Monat nach Ausbruch der Rathenower...   \n",
       "3  Offenbar selbst unzureichend in Kenntnis geset...   \n",
       "4  Wer zum Beispiel an der \" Glücksspirale \" dreht ,   \n",
       "\n",
       "                                          unit1_sent  \\\n",
       "0  In Spremberg verfahren die Wärter offenbar nac...   \n",
       "1  Sowohl die G-20 Gruppe , in der die Industriel...   \n",
       "2  Knapp einen Monat nach Ausbruch der Rathenower...   \n",
       "3  Offenbar selbst unzureichend in Kenntnis geset...   \n",
       "4  Wer zum Beispiel an der \" Glücksspirale \" dreh...   \n",
       "\n",
       "                                           unit2_txt  \\\n",
       "0                 Hauptsache die Gesamtzahl stimmt ,   \n",
       "1                                    aber immerhin .   \n",
       "2  nimmt das Verhalten von Bürgermeister Lünser l...   \n",
       "3    bietet das Jugendamt unverbindliche Auskünfte -   \n",
       "4              beweist sich indirekt als Wohltäter .   \n",
       "\n",
       "                                          unit2_sent  dir       label  \\\n",
       "0  Hauptsache die Gesamtzahl stimmt , wenn abends...  1<2       joint   \n",
       "1  Mehr als eine Willensbekundung ist das zwar no...  1<2  evaluation   \n",
       "2  Während Timm mit diesem Verhalten Pro Rathenow...  1>2  background   \n",
       "3  Offenbar selbst unzureichend in Kenntnis geset...  1>2      reason   \n",
       "4  Wer zum Beispiel an der \" Glücksspirale \" dreh...  1>2   condition   \n",
       "\n",
       "  distance u1_depdir u2_depdir u2_func  ... sat_children nuc_children genre  \\\n",
       "0        1      ROOT      ROOT    root  ...            1            1  news   \n",
       "1        2      ROOT      LEFT    conj  ...            1            2  news   \n",
       "2        7     RIGHT      ROOT    root  ...            2            5  news   \n",
       "3        1     RIGHT      ROOT    root  ...            0            2  news   \n",
       "4        1     RIGHT      ROOT    root  ...            0            5  news   \n",
       "\n",
       "    unit1_case   unit2_case u1_discontinuous u2_discontinuous same_speaker  \\\n",
       "0  cap_initial  cap_initial            False            False         True   \n",
       "1  cap_initial        other            False            False         True   \n",
       "2        title        other            False            False         True   \n",
       "3  cap_initial        other            False            False         True   \n",
       "4  cap_initial        other            False            False         True   \n",
       "\n",
       "  lex_overlap_length u1_func  \n",
       "0                  0    root  \n",
       "1                  0    root  \n",
       "2                  0     obl  \n",
       "3                  0     acl  \n",
       "4                  0   csubj  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['unit1_txt', 'unit1_sent', 'unit2_txt', 'unit2_sent', 'dir', 'label',\n",
       "       'distance', 'u1_depdir', 'u2_depdir', 'u2_func', 'u1_position',\n",
       "       'u2_position', 'sat_children', 'nuc_children', 'genre', 'unit1_case',\n",
       "       'unit2_case', 'u1_discontinuous', 'u2_discontinuous', 'same_speaker',\n",
       "       'lex_overlap_length', 'u1_func'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-24 19:04:38.238974: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-24 19:04:38.460146: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda-10.1/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-10.2/targets/x86_64-linux/lib/:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/home/VD/kaveri/anaconda3/envs/py310/lib/\n",
      "2023-02-24 19:04:38.460170: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-24 19:04:38.502154: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-02-24 19:04:39.430349: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda-10.1/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-10.2/targets/x86_64-linux/lib/:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/home/VD/kaveri/anaconda3/envs/py310/lib/\n",
      "2023-02-24 19:04:39.430445: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda-10.1/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-10.2/targets/x86_64-linux/lib/:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/home/VD/kaveri/anaconda3/envs/py310/lib/\n",
      "2023-02-24 19:04:39.430453: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing.sharedctypes import Value\n",
    "import torch\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, ConcatDataset\n",
    "from sys import path\n",
    "path.append('/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/allennlp/data/data_loaders/')\n",
    "from allennlp.data import allennlp_collate, Vocabulary\n",
    "from features_custom_original import get_vocab_feature_name\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertTokenizer, BertTokenizer\n",
    "\n",
    "class MNLIDataBert(Dataset):\n",
    "\n",
    "  def __init__(self, train_df, val_df, test_df):\n",
    "    self.lang = lang\n",
    "    self.num_labels = set()\n",
    "    self.train_df = train_df\n",
    "    self.val_df = val_df\n",
    "    self.test_df = test_df\n",
    "\n",
    "    self.tokenizer = BertTokenizer.from_pretrained(BERT_MODEL, do_lower_case=True) # Using a pre-trained BERT tokenizer to encode sentences\n",
    "    self.train_data = None\n",
    "    self.val_data = None\n",
    "    self.test_data = None\n",
    "    self.train_idx = None\n",
    "    self.val_idx = None\n",
    "    self.test_idx = None\n",
    "    self.vocab = Vocabulary(counter=None, max_vocab_size=100000)\n",
    "    self.init_data()\n",
    "\n",
    "  def init_data(self):\n",
    "    self.get_label_mapping()\n",
    "    self.init_feature_list()\n",
    "    self.init_feature_mappings_and_bins()\n",
    "    self.apply_bins()\n",
    "    self.calculate_unique_values()\n",
    "    self.train_data, self.train_idx = self.load_data(self.train_df)\n",
    "    self.val_data, self.val_idx = self.load_data(self.val_df)\n",
    "    self.test_data, self.test_idx = self.load_data(self.test_df)\n",
    "    \n",
    "\n",
    "  def combine_unique_column_values_to_dict(self, column_name):\n",
    "    ini_set = set([*self.train_df[column_name].unique(), *self.val_df[column_name].unique()])\n",
    "    res = dict.fromkeys(ini_set, 0)\n",
    "    return res\n",
    "\n",
    "  def get_label_mapping(self):\n",
    "    labels = {}\n",
    "    labels_list = list(set(list(self.train_df['label'].unique()) + list(self.test_df['label'].unique()) + list(self.val_df['label'].unique())))\n",
    "    for i in range(len(labels_list)):\n",
    "        labels[labels_list[i]] = i\n",
    "    self.label_dict = labels\n",
    "    # needed later for classification report object to generate precision and recall on test dataset\n",
    "    self.rev_label_dict = {self.label_dict[k]:k for k in self.label_dict.keys()} \n",
    "\n",
    "  def init_feature_mappings_and_bins(self):\n",
    "    self.feature_maps = { 'genre': self.combine_unique_column_values_to_dict('genre'),\n",
    "                          'unit1_case': self.combine_unique_column_values_to_dict('unit1_case'),\n",
    "                          'unit2_case': self.combine_unique_column_values_to_dict('unit2_case'),\n",
    "                          'u1_func': self.combine_unique_column_values_to_dict('u1_func'),\n",
    "                          'u2_func': self.combine_unique_column_values_to_dict('u2_func') }\n",
    "\n",
    "    self.bins = {\n",
    "      'distance': [[-1e9, -8], [-8, -2], [-2, 0], [0, 2], [2, 8], [8, 1e9]],\n",
    "      'u1_position': [[0.0, 0.1], [0.1, 0.2], [0.2, 0.3], [0.3, 0.4], [0.4, 0.5], [0.5, 0.6], [0.6, 0.7], [0.7, 0.8], [0.8, 0.9], [0.9, 1.0], [1.0, 1e9]],\n",
    "      'u2_position': [[0.0, 0.1], [0.1, 0.2], [0.2, 0.3], [0.3, 0.4], [0.4, 0.5], [0.5, 0.6], [0.6, 0.7], [0.7, 0.8], [0.8, 0.9], [0.9, 1.0], [1.0, 1e9]],\n",
    "      'lex_overlap_length': [[0, 2], [2, 7], [7, 1e9]]\n",
    "    }   \n",
    "\n",
    "  def add_directionality(self, premise, hypothesis, dir):\n",
    "    if dir == \"1<2\":\n",
    "        hypothesis = '< ' + hypothesis + ' {'\n",
    "    else:\n",
    "        premise = '} ' + premise + ' >'\n",
    "    return premise, hypothesis\n",
    "\n",
    "  def init_feature_list(self):\n",
    "    if self.lang=='nld':\n",
    "      self.feature_list = ['distance', 'u1_depdir', 'sat_children', 'genre', 'u1_position']\n",
    "    elif self.lang=='deu':\n",
    "      self.feature_list = ['distance', 'u1_depdir', 'u2_depdir', 'u2_func', 'u1_position', 'u2_position', 'sat_children', 'nuc_children']\n",
    "    elif self.lang=='eng.rst.gum':\n",
    "      self.feature_list = ['distance', 'same_speaker', 'u2_func', 'u2_depdir', 'unit1_case', 'unit2_case', 'nuc_children',\n",
    "                      'sat_children', 'genre', 'lex_overlap_length', 'u2_discontinuous', 'u1_discontinuous', 'u1_position', 'u2_position']\n",
    "    elif self.lang=='fas':\n",
    "      self.feature_list = ['distance', 'nuc_children', 'sat_children', 'u2_discontinuous', 'genre']\n",
    "    elif self.lang=='spa.rst.sctb':\n",
    "      self.feature_list = ['distance', 'u1_position', 'sat_children']\n",
    "    elif self.lang=='zho.rst.sctb':\n",
    "      self.feature_list = ['sat_children', 'nuc_children', 'genre', 'u2_discontinuous', 'u1_discontinuous', 'u1_depdir', 'u1_func']\n",
    "    else: \n",
    "      raise ValueError()\n",
    "\n",
    "  def get_mapping_from_dictionary(self, column_name, dict_val):\n",
    "    return self.feature_maps[column_name][dict_val]\n",
    "\n",
    "  def get_allen_features_list(self, features, feature_name):\n",
    "    if feature_name in ['distance', 'u1_depdir', 'u2_depdir', 'u1_func', 'u2_func', \n",
    "    'u1_position', 'u2_position', 'genre', 'same_speaker', 'unit1_case', 'unit2_case',\n",
    "    'lex_overlap_length', 'u2_discontinuous', 'u1_discontinuous', 'dir']: feature_value = self.apply_vocab(features[feature_name], feature_name) #for categorical values\n",
    "    elif feature_name in ['sat_children', 'nuc_children']: feature_value = float(features[feature_name]) #for identiy values\n",
    "    else: \n",
    "      print(feature_name)\n",
    "      raise ValueError()\n",
    "    return feature_value\n",
    "\n",
    "  def transform_feature(self, features):\n",
    "    assert len(features)==17\n",
    "    #after applying the vocab. we need to pass them as int\n",
    "    return {feature_name: torch.tensor(int(self.get_allen_features_list(features, feature_name))).to(device) for feature_name in self.feature_list+['dir']}\n",
    "\n",
    "  def calculate_unique_values(self):\n",
    "    for feature_name in self.feature_list+['dir']:\n",
    "      vocab_feature_name = get_vocab_feature_name(feature_name)\n",
    "      self.vocab.add_tokens_to_namespace(train_df[feature_name].apply(lambda x: str(x)), namespace=vocab_feature_name)\n",
    "      self.vocab.add_tokens_to_namespace(val_df[feature_name].apply(lambda x: str(x)), namespace=vocab_feature_name)\n",
    "\n",
    "  def apply_bins(self):\n",
    "    for df in [self.train_df, self.test_df, self.val_df]:\n",
    "      for feature_name in self.bins.keys():\n",
    "        if feature_name=='u2_func':\n",
    "          print(df[feature_name].unique())\n",
    "          raise ValueError()\n",
    "        df[feature_name] = df[feature_name].apply(lambda x: self.get_mapping_from_bin(feature_name, float(x)))\n",
    "\n",
    "  def get_mapping_from_bin(self, column_name, dict_val):\n",
    "    bins = self.bins[column_name]\n",
    "    for b,i in zip(bins, range(len(bins))):\n",
    "      left = b[0]\n",
    "      right = b[1]\n",
    "      if left<=dict_val and right>=dict_val: return i\n",
    "\n",
    "  def apply_vocab(self, feature_value, feature_name):\n",
    "    return self.vocab.get_token_index(str(feature_value), namespace=get_vocab_feature_name(feature_name))\n",
    "\n",
    "  def set_labels(self):\n",
    "    self.num_labels = len(self.num_labels)\n",
    "    \n",
    "  def load_data(self, df):\n",
    "    MAX_LEN = 512 \n",
    "    token_ids = []\n",
    "    mask_ids = []\n",
    "    # seg_ids = []\n",
    "    y = []\n",
    "    feats = []\n",
    "    idx = []\n",
    "    idx_map = {}\n",
    "\n",
    "    self.num_labels.update(df['label'].unique())\n",
    "\n",
    "    count=0\n",
    "    for row in df.iterrows():\n",
    "      row = row[1]\n",
    "      premise = row['unit1_txt']\n",
    "      hypothesis = row['unit2_txt']\n",
    "      label = row['label']\n",
    "      dir = row['dir']\n",
    "\n",
    "      features = {'distance': row['distance'],\n",
    "                'u1_depdir': row['u1_depdir'],\n",
    "                'u2_depdir': row['u2_depdir'],\n",
    "                'u1_func': row['u1_func'],\n",
    "                'u2_func': row['u2_func'],\n",
    "                'u1_position': row['u1_position'],\n",
    "                'u2_position': row['u2_position'],\n",
    "                'sat_children': row['sat_children'],\n",
    "                'nuc_children': row['nuc_children'],\n",
    "                'genre': row['genre'],\n",
    "                'unit1_case': row['unit1_case'],\n",
    "                'unit2_case': row['unit2_case'],\n",
    "                'u1_discontinuous': row['u1_discontinuous'],\n",
    "                'u2_discontinuous': row['u2_discontinuous'],\n",
    "                'same_speaker': row['same_speaker'],\n",
    "                'lex_overlap_length': row['lex_overlap_length'],\n",
    "                'dir': row['dir']}\n",
    "\n",
    "      premise, hypothesis = self.add_directionality(premise, hypothesis, dir)\n",
    "      encoded = self.tokenizer.encode_plus(premise, hypothesis, add_special_tokens = True, max_length=MAX_LEN, truncation=True, padding=False) #padding='max_length'\n",
    "      pair_token_ids = torch.tensor(encoded['input_ids'])\n",
    "\n",
    "      # segment_ids = torch.tensor(encoded['token_type_ids'])\n",
    "      attention_mask_ids = torch.tensor(encoded['attention_mask'])\n",
    "      assert len(pair_token_ids)==len(attention_mask_ids)\n",
    "\n",
    "      features = self.transform_feature(features)\n",
    "\n",
    "      token_ids.append(pair_token_ids)\n",
    "      # seg_ids.append(segment_ids)\n",
    "      mask_ids.append(attention_mask_ids)\n",
    "      y.append(self.label_dict[label])\n",
    "      feats.append(features)\n",
    "      \n",
    "      idx_map[count] = [premise, hypothesis]\n",
    "      idx.append(count)\n",
    "      count+=1\n",
    "      \n",
    "    token_ids = pad_sequence(token_ids, batch_first=True)\n",
    "    mask_ids = pad_sequence(mask_ids, batch_first=True)\n",
    "    # seg_ids = pad_sequence(seg_ids, batch_first=True)\n",
    "    y = torch.tensor(y)\n",
    "    idx = torch.tensor(idx)\n",
    "\n",
    "    class featureDataset(Dataset):\n",
    "      def __init__(self, token_ids, mask_ids, feats, y, idx):\n",
    "          self.token_ids = token_ids\n",
    "          self.mask_ids = mask_ids\n",
    "          # self.seg_ids = seg_ids\n",
    "          self.feats = feats\n",
    "          self.y = y\n",
    "          self.idx = idx\n",
    "\n",
    "      def __len__(self):\n",
    "          return len(self.feats)\n",
    "\n",
    "      def __getitem__(self, idx):\n",
    "          return self.token_ids[idx], self.mask_ids[idx], self.feats[idx], self.y[idx], self.idx[idx]\n",
    "          # return self.token_ids[idx], self.mask_ids[idx], self.seg_ids[idx], self.feats[idx], self.y[idx], self.idx[idx]\n",
    "\n",
    "    # dataset = featureDataset(token_ids, mask_ids, seg_ids, feats, y, idx)\n",
    "    dataset = featureDataset(token_ids, mask_ids, feats, y, idx)\n",
    "    return dataset, idx_map\n",
    "\n",
    "  def get_data_loaders(self, batch_size=4, batches_per_epoch=402, shuffle=True): #1609 samples / 64:25=1600 / 402:4=1608\n",
    "    self.set_labels()\n",
    "    train_loader_torch = DataLoader(\n",
    "      self.train_data,\n",
    "      shuffle=shuffle,\n",
    "      batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    val_loader_torch = DataLoader(\n",
    "      self.val_data,\n",
    "      shuffle=shuffle,\n",
    "      batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    test_loader_torch = DataLoader(\n",
    "      self.test_data,\n",
    "      shuffle=False,\n",
    "      batch_size=batch_size,\n",
    "    )\n",
    "    \n",
    "    train_loader = LoaderWrapper(train_loader_torch, n_step=batches_per_epoch)\n",
    "    val_loader = LoaderWrapper(val_loader_torch, n_step=batches_per_epoch)\n",
    "    test_loader = LoaderWrapper(test_loader_torch, n_step=batches_per_epoch)\n",
    "\n",
    "    return train_loader, val_loader_torch, test_loader_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoaderWrapper:\n",
    "    def __init__(self, loader, n_step):\n",
    "        self.step = n_step\n",
    "        self.idx = 0\n",
    "        self.iter_loader = iter(loader)\n",
    "        self.loader = loader\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.step\n",
    "\n",
    "    def __next__(self):\n",
    "        # if reached number of steps desired, stop\n",
    "        if self.idx == self.step:\n",
    "            self.idx = 0\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            self.idx += 1\n",
    "        # while True\n",
    "        try:\n",
    "            return next(self.iter_loader)\n",
    "        except StopIteration:\n",
    "            # reinstate iter_loader, then continue\n",
    "            self.iter_loader = iter(self.loader)\n",
    "            return next(self.iter_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mnliloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_dataset = MNLIDataBert(train_df, val_df, test_df)\n",
    "mnli_dataset_en = MNLIDataBert(train_df_en, val_df_en, test_df_en) #to apply bins because df passed by reference\n",
    "\n",
    "train_loader, val_loader, test_loader = mnli_dataset.get_data_loaders(batch_size=batch_size, batches_per_epoch=batches_per_epoch) #64X250\n",
    "label_dict = mnli_dataset.label_dict # required by custom func to calculate accuracy, bert model\n",
    "rev_label_dict = mnli_dataset.rev_label_dict # required by custom func to calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '3': 2, '4': 3, '5': 4}\n",
      "distance :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '3': 2, '4': 3, '5': 4}\n",
      "u1_depdir :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, 'ROOT': 2, 'RIGHT': 3, 'LEFT': 4}\n",
      "u1_depdir :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, 'ROOT': 2, 'RIGHT': 3, 'LEFT': 4}\n",
      "u2_depdir :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, 'ROOT': 2, 'RIGHT': 3, 'LEFT': 4}\n",
      "u2_depdir :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, 'ROOT': 2, 'RIGHT': 3, 'LEFT': 4}\n",
      "u2_func :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, 'root': 2, 'conj': 3, 'acl': 4, 'advcl': 5, 'csubj': 6, 'parataxis': 7, 'acl:relcl': 8, 'nmod': 9, 'compound': 10, 'csubj:pass': 11, 'ccomp': 12, 'amod': 13, 'advmod': 14, 'xcomp': 15, 'iobj': 16, 'punct': 17, 'appos': 18, 'dep': 19, 'cc': 20, 'nsubj': 21, 'flat': 22, 'obl': 23, 'obj': 24, 'aux': 25, 'list': 26, 'nummod': 27, 'nsubj:pass': 28}\n",
      "u2_func :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, 'root': 2, 'conj': 3, 'acl': 4, 'advcl': 5, 'csubj': 6, 'parataxis': 7, 'acl:relcl': 8, 'nmod': 9, 'compound': 10, 'csubj:pass': 11, 'ccomp': 12, 'amod': 13, 'advmod': 14, 'xcomp': 15, 'iobj': 16, 'punct': 17, 'appos': 18, 'dep': 19, 'cc': 20, 'nsubj': 21, 'flat': 22, 'obl': 23, 'obj': 24, 'aux': 25, 'list': 26, 'nummod': 27, 'nsubj:pass': 28, 'nmod:poss': 29, 'case': 30}\n",
      "u1_position :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '8': 2, '0': 3, '2': 4, '4': 5, '3': 6, '6': 7, '9': 8, '5': 9, '1': 10, '7': 11}\n",
      "u1_position :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '8': 2, '0': 3, '2': 4, '4': 5, '3': 6, '6': 7, '9': 8, '5': 9, '1': 10, '7': 11}\n",
      "u2_position :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '9': 2, '3': 3, '7': 4, '4': 5, '6': 6, '1': 7, '5': 8, '0': 9, '8': 10, '2': 11}\n",
      "u2_position :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '9': 2, '3': 3, '7': 4, '4': 5, '6': 6, '1': 7, '5': 8, '0': 9, '8': 10, '2': 11}\n",
      "sat_children :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '1': 2, '2': 3, '0': 4, '4': 5, '3': 6, '5': 7, '10': 8, '6': 9}\n",
      "sat_children :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '1': 2, '2': 3, '0': 4, '4': 5, '3': 6, '5': 7, '10': 8, '6': 9, '14': 10, '7': 11}\n",
      "nuc_children :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '1': 2, '2': 3, '5': 4, '4': 5, '6': 6, '3': 7, '16': 8, '10': 9, '8': 10, '7': 11, '9': 12, '11': 13, '12': 14, '13': 15, '21': 16}\n",
      "nuc_children :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '1': 2, '2': 3, '5': 4, '4': 5, '6': 6, '3': 7, '16': 8, '10': 9, '8': 10, '7': 11, '9': 12, '11': 13, '12': 14, '13': 15, '21': 16, '14': 17}\n",
      "dir :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '1<2': 2, '1>2': 3}\n"
     ]
    }
   ],
   "source": [
    "for feature in mnli_dataset.feature_list:\n",
    "    vocab_feature_name = get_vocab_feature_name(feature)\n",
    "    print(feature, ': ', mnli_dataset.vocab.get_token_to_index_vocabulary(vocab_feature_name))\n",
    "    mnli_dataset.vocab.add_tokens_to_namespace(val_df_en[feature].apply(lambda x: str(x)), namespace=vocab_feature_name)\n",
    "    print(feature, ': ', mnli_dataset.vocab.get_token_to_index_vocabulary(vocab_feature_name))\n",
    "print('dir', ': ', mnli_dataset.vocab.get_token_to_index_vocabulary('dir'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (pair_token_ids, mask_ids, feat, y, idx) in enumerate(train_loader):\n",
    "    assert len(feat)==len(mnli_dataset.feature_list)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from torch import optim\n",
    "import os\n",
    "path.append(os.path.join(os.getcwd(), '../utils/'))\n",
    "from CategoricalAccuracy import CategoricalAccuracy as CA\n",
    "import numpy as np\n",
    "\n",
    "ca = CA()\n",
    "\n",
    "x = torch.tensor(np.array([[[1,0,0], [1,0,0], [1,0,0]]]))\n",
    "y1 = torch.tensor(np.array([[0], [1], [1]]))\n",
    "y2 = torch.tensor(np.array([[0], [0], [0]]))\n",
    "\n",
    "ca(x,y1)\n",
    "print(ca.get_metric(reset=True))\n",
    "ca(x,y2)\n",
    "print(ca.get_metric(reset=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define evaulation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to evaluate model for train and test. And also use classification report for testing\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# helper function to calculate the batch accuracy\n",
    "def multi_acc(y_pred, y_test, allennlp=False):\n",
    "  if allennlp==False:\n",
    "    acc = (torch.log_softmax(y_pred, dim=1).argmax(dim=1) == y_test).sum().float() / float(y_test.size(0))\n",
    "    return acc\n",
    "\n",
    "# freeze model weights and measure validation / test \n",
    "def evaluate_accuracy(model, optimizer, data_loader, rev_label_dict, label_dict, is_training=True):\n",
    "  model.eval()\n",
    "  total_val_acc  = 0\n",
    "  total_val_loss = 0\n",
    "  \n",
    "  #for classification report\n",
    "  y_true = []\n",
    "  y_pred = []\n",
    "  idx_list = []\n",
    "  premise_list = []\n",
    "  hypo_list = []\n",
    "  idx_map = mnli_dataset.val_idx if is_training else mnli_dataset.test_idx\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for batch_idx, (pair_token_ids, mask_ids, feat, y, idx) in enumerate(data_loader):      \n",
    "      optimizer.zero_grad()\n",
    "      pair_token_ids = pair_token_ids.to(device)\n",
    "      mask_ids = mask_ids.to(device)\n",
    "      # seg_ids = seg_ids.to(device)\n",
    "      labels = y.to(device)\n",
    "      # feat = feat.to(device)\n",
    "      \n",
    "      outputs = model(pair_token_ids, \n",
    "                            # token_type_ids=seg_ids, \n",
    "                            attention_mask=mask_ids, \n",
    "                            feat=feat)\n",
    "      criterion = nn.CrossEntropyLoss()\n",
    "      loss = criterion(outputs, labels)\n",
    "      acc = multi_acc(outputs, labels)\n",
    "\n",
    "      total_val_loss += loss.item()\n",
    "      total_val_acc  += acc.item()\n",
    "\n",
    "      # log predictions for classification report\n",
    "      argmax_predictions = torch.argmax(outputs,dim=1).tolist()\n",
    "      labels_list = labels.tolist()\n",
    "      assert(len(labels_list)==len(argmax_predictions))\n",
    "      for p in argmax_predictions: y_pred.append(rev_label_dict[int(p)])\n",
    "      for l in labels_list: y_true.append(rev_label_dict[l])\n",
    "      for i in idx.tolist():\n",
    "        idx_list.append(i)\n",
    "        premise_list.append(idx_map[i][0])\n",
    "        hypo_list.append(idx_map[i][1])\n",
    "\n",
    "  val_acc  = total_val_acc/len(data_loader)\n",
    "  val_loss = total_val_loss/len(data_loader)\n",
    "  cr = classification_report(y_true, y_pred)\n",
    "\n",
    "  idx_json = {'idx': idx_list, 'gold_label': y_true, 'pred_label': y_pred, 'premise': premise_list, 'hypothesis': hypo_list}\n",
    "  \n",
    "  return val_acc, val_loss, cr, model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define custom bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASSIGN: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing FeaturefulBert: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing FeaturefulBert from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FeaturefulBert from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleDict(\n",
      "  (distance): Embedding(5, 3, padding_idx=0)\n",
      "  (u1_depdir): Embedding(5, 3, padding_idx=0)\n",
      "  (u2_depdir): Embedding(5, 3, padding_idx=0)\n",
      "  (u2_func): Embedding(31, 6, padding_idx=0)\n",
      "  (u1_position): Embedding(12, 4, padding_idx=0)\n",
      "  (u2_position): Embedding(12, 4, padding_idx=0)\n",
      "  (sat_children): Identity()\n",
      "  (nuc_children): Identity()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from typing import Any, Dict, Optional\n",
    "from transformers import BertModel, BertConfig\n",
    "import torch.nn as nn\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from featurefulbertembedder_custom_allennlp_dims import FeaturefulBertEmbedder\n",
    "from featureful_bert_custom_allennlp_dims import get_combined_feature_tensor_2 as get_combined_feature_tensor_forward\n",
    "from features_custom_allennlp_dims import get_feature_modules\n",
    "\n",
    "class CustomPooler2(nn.Module):\n",
    "    def __init__(self, *,\n",
    "                        requires_grad: bool = True,\n",
    "                        dropout: float = 0.0,\n",
    "                        transformer_kwargs: Optional[Dict[str, Any]] = None, ) -> None:\n",
    "        super().__init__()\n",
    "        bert = BertModel.from_pretrained(BERT_MODEL) #only used to pass config. BertAttentionClass used in FeatureFulBert\n",
    "        self._dropout = torch.nn.Dropout(p=dropout)\n",
    "        self.pooler = copy.deepcopy(bert.pooler)\n",
    "        for param in self.pooler.parameters():\n",
    "            param.requires_grad = requires_grad\n",
    "        self._embedding_dim = bert.config.hidden_size\n",
    "\n",
    "    def get_input_dim(self) -> int:\n",
    "        return self._embedding_dim\n",
    "\n",
    "    def get_output_dim(self) -> int:\n",
    "        return self._embedding_dim\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, mask: torch.BoolTensor = None, num_wrapping_dims: int = 0):\n",
    "        pooler = self.pooler\n",
    "        \n",
    "        for _ in range(num_wrapping_dims):\n",
    "            pooler = TimeDistributed(pooler)\n",
    "        pooled = pooler(tokens)\n",
    "        pooled = self._dropout(pooled)\n",
    "        return pooled\n",
    "\n",
    "class MyModule(nn.Module):    \n",
    "    def __init__(self, feature_list):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.feature_list = feature_list\n",
    "        self.feature_modules = nn.ModuleDict()\n",
    "        self.dims = 0\n",
    "        self.feature_modules, dims = get_feature_modules(feature_list, mnli_dataset.vocab, use_allennlp_dims=True)\n",
    "        self.dims += dims\n",
    "\n",
    "        print(self.feature_modules)\n",
    "        for feature in feature_list:\n",
    "            if feature not in ['distance', 'u1_depdir', 'u2_depdir', 'u2_func', 'u1_position', 'u2_position', \n",
    "                                'nuc_children', 'sat_children']: raise ValueError()\n",
    "            # elif 'genre' in feature_list:               self.modules['genre'] = nn.Embedding(mnli_dataset_en.distance_unique, 3)\n",
    "            # elif 'unit1_case' in feature_list:          self.modules['unit1_case'] = nn.Embedding(mnli_dataset_en.distance_unique, 3)\n",
    "            # elif 'unit2_case' in feature_list:          self.modules['unit2_case'] = nn.Embedding(mnli_dataset_en.distance_unique, 3)\n",
    "            # elif 'u1_discontinuous' in feature_list:    self.modules['u1_discontinuous'] = nn.Embedding(mnli_dataset_en.distance_unique, 3)\n",
    "            # elif 'u2_discontinuous' in feature_list:    self.modules['u2_discontinuous'] = nn.Embedding(mnli_dataset_en.distance_unique, 3)\n",
    "            # elif 'same_speaker' in feature_list:        self.modules['same_speaker'] = nn.Embedding(mnli_dataset_en.distance_unique, 3)\n",
    "            # elif 'lex_overlap_length' in feature_list:  self.modules['lex_overlap_length'] = nn.Embedding(mnli_dataset_en.distance_unique, 3)\n",
    "            # elif 'u1_func' in feature_list:             self.modules['u1_func'] = nn.Embedding(mnli_dataset_en.distance_unique, 3)\n",
    "\n",
    "    def forward(self, features):\n",
    "        feature_list = ['distance', 'u1_depdir', 'u2_depdir', 'u2_func', 'u1_position', 'u2_position', 'sat_children', 'nuc_children']\n",
    "        for i in range(len(feature_list)):\n",
    "            feature_name = feature_list[i]\n",
    "            feature_modules = self.feature_modules[feature_name]\n",
    "            feature = features[...,i]\n",
    "            if feature_name not in ['sat_children', 'nuc_children']:\n",
    "                if torch.max(feature)>feature_modules.num_embeddings:\n",
    "                    print(feature_name, feature)\n",
    "                    raise ValueError()\n",
    "\n",
    "        return get_combined_feature_tensor_forward(features, self.feature_list, self.feature_modules)\n",
    "\n",
    "class CustomBERTModel(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "          super(CustomBERTModel, self).__init__()\n",
    "          self.num_classes = num_labels\n",
    "          self.feature_list = mnli_dataset.feature_list\n",
    "          print('ASSIGN:', self.num_classes)\n",
    "\n",
    "          self.embedder = self.create_featureful_bert() #BERT MODEL\n",
    "          self.encoder = CustomPooler2()\n",
    "          self.module1 = MyModule(self.feature_list)\n",
    "          self.dropout1 = nn.Dropout(p=0.0)\n",
    "        #   self.dropout_decoder = nn.Dropout(p=0.5)\n",
    "          self.out_features = self.encoder.pooler.dense.out_features+self.module1.dims\n",
    "          self.relation_decoder = nn.Linear(self.out_features, self.num_classes)\n",
    "\n",
    "    def forward(self, pair_token_ids, attention_mask, feat):\n",
    "        direction_tensor = feat['dir'].to(device)\n",
    "        embedded_sentence = self.embedder(token_ids=pair_token_ids, #featurefulmebedder\n",
    "                        mask=attention_mask, \n",
    "                        # type_ids=token_type_ids,\n",
    "                        segment_concat_mask = None,\n",
    "                        direction_tensor = direction_tensor,\n",
    "                        feature_list = self.feature_list,\n",
    "                        features = feat)\n",
    "        # mask = token_type_ids\n",
    "        bertpooler_output = self.encoder(tokens=embedded_sentence, mask=None)\n",
    "        feat = self.convert_to_feature_list(feat)\n",
    "        feat = self.dropout1(feat)\n",
    "        feat = self.module1(feat)\n",
    "        try:\n",
    "            feat_concat = torch.concat((bertpooler_output, feat),-1)\n",
    "        except:\n",
    "            print(bertpooler_output.shape, feat.shape)\n",
    "            raise ValueError()\n",
    "        if feat_concat.shape[-1]!=self.module1.dims+bertpooler_output.shape[-1]: print(feat_concat.shape, self.module1.dims)\n",
    "        assert feat_concat.shape[-1] == self.module1.dims+bertpooler_output.shape[-1]\n",
    "        feat_concat = self.dropout1(feat_concat)\n",
    "        # feat_concat = self.dropout_decoder(feat_concat)\n",
    "        linear1_output = self.relation_decoder(feat_concat)\n",
    "        return linear1_output\n",
    "\n",
    "\n",
    "    def create_featureful_bert(self):\n",
    "        featureful_bert = FeaturefulBertEmbedder(model_name = BERT_MODEL,\n",
    "                                hidden_activation_allen = 'gelu',\n",
    "                                feature_list = self.feature_list, \n",
    "                                vocab=mnli_dataset.vocab,\n",
    "                                use_allen_dims=True)\n",
    "        return featureful_bert\n",
    "\n",
    "    def convert_to_feature_list(self, feat):\n",
    "        feature_linear = [feat[feature_name] for feature_name in self.feature_list]\n",
    "        feature_linear = torch.stack(feature_linear, dim=-1)\n",
    "        return feature_linear\n",
    "\n",
    "model = CustomBERTModel(mnli_dataset.num_labels)\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-6, correct_bias=False) # original 2e-5\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.8, mode='max', patience=35, min_lr=5e-7, verbose=True) #original factor=0.6, min_lr=5e-7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define training regime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prinintg model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomBERTModel(\n",
      "  (embedder): FeaturefulBertEmbedder(\n",
      "    (transformer_model): FeaturefulBert(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30000, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "      (feature_modules): ModuleDict(\n",
      "        (distance): Embedding(5, 3, padding_idx=0)\n",
      "        (u1_depdir): Embedding(5, 3, padding_idx=0)\n",
      "        (u2_depdir): Embedding(5, 3, padding_idx=0)\n",
      "        (u2_func): Embedding(31, 6, padding_idx=0)\n",
      "        (u1_position): Embedding(12, 4, padding_idx=0)\n",
      "        (u2_position): Embedding(12, 4, padding_idx=0)\n",
      "        (sat_children): Identity()\n",
      "        (nuc_children): Identity()\n",
      "      )\n",
      "      (feature_projector): Linear(in_features=26, out_features=768, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (encoder): CustomPooler2(\n",
      "    (_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (module1): MyModule(\n",
      "    (feature_modules): ModuleDict(\n",
      "      (distance): Embedding(5, 3, padding_idx=0)\n",
      "      (u1_depdir): Embedding(5, 3, padding_idx=0)\n",
      "      (u2_depdir): Embedding(5, 3, padding_idx=0)\n",
      "      (u2_func): Embedding(31, 6, padding_idx=0)\n",
      "      (u1_position): Embedding(12, 4, padding_idx=0)\n",
      "      (u2_position): Embedding(12, 4, padding_idx=0)\n",
      "      (sat_children): Identity()\n",
      "      (nuc_children): Identity()\n",
      "    )\n",
      "  )\n",
      "  (dropout1): Dropout(p=0.0, inplace=False)\n",
      "  (relation_decoder): Linear(in_features=793, out_features=25, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def writer_init(save_path_suffix):\n",
    "    writer_path = 'run1/'+save_path_suffix[:-1]+'/'\n",
    "    if os.path.isdir(writer_path):\n",
    "        filelist = [ f for f in os.listdir(writer_path) if 'events.out' in f ]\n",
    "        print(filelist)\n",
    "        for f in filelist:\n",
    "            os.remove(os.path.join(writer_path, f))\n",
    "    else:\n",
    "        os.mkdir(writer_path)\n",
    "    writer = SummaryWriter(log_dir=writer_path)\n",
    "    return writer\n",
    "\n",
    "writer = writer_init(save_path_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import traceback\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import Optional, Iterable, Dict, Any\n",
    "from EarlyStopperUtil import MetricTracker\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer, scheduler, rev_label_dict):  \n",
    "  EarlyStopper = MetricTracker(patience=20, metric_name='+accuracy')\n",
    "  best_val_acc = 0\n",
    "\n",
    "\n",
    "  for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_train_acc  = 0\n",
    "    \n",
    "    # logging for scheduler\n",
    "    losses = []\n",
    "    accuracies= []\n",
    "\n",
    "    train_size = 0\n",
    "\n",
    "    for batch_idx, (pair_token_ids, mask_ids, feat, y, idx) in enumerate(train_loader):\n",
    "      train_size+=1\n",
    "      optimizer.zero_grad()\n",
    "      pair_token_ids = pair_token_ids.to(device)\n",
    "      mask_ids = mask_ids.to(device)\n",
    "      # seg_ids = seg_ids.to(device)\n",
    "      labels = y.to(device)\n",
    "      # feat = feat.to(device)\n",
    "      outputs = model(pair_token_ids = pair_token_ids, \n",
    "                            # token_type_ids=seg_ids, \n",
    "                            attention_mask=mask_ids,\n",
    "                            feat=feat)\n",
    "      criterion = nn.CrossEntropyLoss()\n",
    "      loss = criterion(outputs, labels)\n",
    "      loss.backward()\n",
    "      acc = multi_acc(outputs, labels)\n",
    "      optimizer.step()\n",
    "      total_train_loss += loss.item()\n",
    "      total_train_acc  += acc.item()\n",
    "\n",
    "      losses.append(loss)\n",
    "      accuracies.append(acc)\n",
    "      \n",
    "    mean_loss = sum(losses)/len(losses)\n",
    "    scheduler.step(mean_loss)\n",
    "\n",
    "    train_acc  = total_train_acc/len(train_loader)\n",
    "    train_loss = total_train_loss/len(train_loader)\n",
    "\n",
    "    val_acc, val_loss, cr, model, optimizer = evaluate_accuracy(model, optimizer, val_loader, rev_label_dict, label_dict, None)\n",
    "    if val_acc>best_val_acc:\n",
    "      torch.save(model.state_dict(), 'run1/'+save_path_suffix+'_best.pt')\n",
    "      best_val_acc = val_acc\n",
    "      print(f'Epoch {epoch+1}: Best val_acc: {best_val_acc:.4f}')\n",
    "    if val_acc>=best_val_acc:\n",
    "      torch.save(model.state_dict(), 'run1/'+save_path_suffix+'_best_latest.pt')\n",
    "      best_val_acc = val_acc\n",
    "      print(f'Epoch {epoch+1}: Best val_acc: {best_val_acc:.4f}')\n",
    "    EarlyStopper.add_metric(val_acc)\n",
    "    if EarlyStopper.should_stop_early(): break\n",
    "\n",
    "    end = time.time()\n",
    "    hours, rem = divmod(end-start, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "    print(f'Epoch {epoch+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
    "    print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "    print(f'train_size: {train_size}')\n",
    "\n",
    "    writer.add_scalar('train_loss', train_loss, epoch)\n",
    "    writer.add_scalar('train_acc', train_acc, epoch)\n",
    "    writer.add_scalar('val_loss', val_loss, epoch)\n",
    "    writer.add_scalar('val_acc', val_acc, epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Best val_acc: 0.1721\n",
      "Epoch 1: Best val_acc: 0.1721\n",
      "Epoch 1: train_loss: 2.8089 train_acc: 0.1618 | val_loss: 2.7495 val_acc: 0.1721\n",
      "00:02:58.47\n",
      "train_size: 748\n",
      "Epoch 2: Best val_acc: 0.2336\n",
      "Epoch 2: Best val_acc: 0.2336\n",
      "Epoch 2: train_loss: 2.4891 train_acc: 0.2540 | val_loss: 2.5053 val_acc: 0.2336\n",
      "00:02:58.55\n",
      "train_size: 748\n",
      "Epoch 3: Best val_acc: 0.2459\n",
      "Epoch 3: Best val_acc: 0.2459\n",
      "Epoch 3: train_loss: 2.2858 train_acc: 0.3285 | val_loss: 2.3470 val_acc: 0.2459\n",
      "00:02:58.20\n",
      "train_size: 748\n",
      "Epoch 4: Best val_acc: 0.2910\n",
      "Epoch 4: Best val_acc: 0.2910\n",
      "Epoch 4: train_loss: 2.1249 train_acc: 0.3757 | val_loss: 2.2480 val_acc: 0.2910\n",
      "00:02:58.06\n",
      "train_size: 748\n",
      "Epoch 5: Best val_acc: 0.3197\n",
      "Epoch 5: Best val_acc: 0.3197\n",
      "Epoch 5: train_loss: 1.9826 train_acc: 0.4348 | val_loss: 2.1964 val_acc: 0.3197\n",
      "00:02:58.33\n",
      "train_size: 748\n",
      "Epoch 6: Best val_acc: 0.3320\n",
      "Epoch 6: Best val_acc: 0.3320\n",
      "Epoch 6: train_loss: 1.8422 train_acc: 0.4713 | val_loss: 2.1536 val_acc: 0.3320\n",
      "00:02:57.87\n",
      "train_size: 748\n",
      "Epoch 7: Best val_acc: 0.3648\n",
      "Epoch 7: Best val_acc: 0.3648\n",
      "Epoch 7: train_loss: 1.7145 train_acc: 0.5180 | val_loss: 2.0696 val_acc: 0.3648\n",
      "00:02:57.79\n",
      "train_size: 748\n",
      "Epoch 8: train_loss: 1.5813 train_acc: 0.5662 | val_loss: 2.0561 val_acc: 0.3566\n",
      "00:02:56.71\n",
      "train_size: 748\n",
      "Epoch 9: Best val_acc: 0.3852\n",
      "Epoch 9: Best val_acc: 0.3852\n",
      "Epoch 9: train_loss: 1.4624 train_acc: 0.6066 | val_loss: 2.0396 val_acc: 0.3852\n",
      "00:02:58.47\n",
      "train_size: 748\n",
      "Epoch 10: train_loss: 1.3312 train_acc: 0.6487 | val_loss: 2.0582 val_acc: 0.3811\n",
      "00:02:56.37\n",
      "train_size: 748\n",
      "Epoch 11: Best val_acc: 0.4057\n",
      "Epoch 11: Best val_acc: 0.4057\n",
      "Epoch 11: train_loss: 1.1950 train_acc: 0.6882 | val_loss: 2.0435 val_acc: 0.4057\n",
      "00:02:58.57\n",
      "train_size: 748\n",
      "Epoch 12: Best val_acc: 0.4098\n",
      "Epoch 12: Best val_acc: 0.4098\n",
      "Epoch 12: train_loss: 1.0843 train_acc: 0.7156 | val_loss: 2.0512 val_acc: 0.4098\n",
      "00:02:58.44\n",
      "train_size: 748\n",
      "Epoch 13: Best val_acc: 0.4139\n",
      "Epoch 13: Best val_acc: 0.4139\n",
      "Epoch 13: train_loss: 0.9704 train_acc: 0.7547 | val_loss: 2.0719 val_acc: 0.4139\n",
      "00:02:58.08\n",
      "train_size: 748\n",
      "Epoch 14: train_loss: 0.8711 train_acc: 0.7814 | val_loss: 2.1071 val_acc: 0.4057\n",
      "00:02:56.15\n",
      "train_size: 748\n",
      "Epoch 15: train_loss: 0.7787 train_acc: 0.8072 | val_loss: 2.1899 val_acc: 0.3811\n",
      "00:02:56.67\n",
      "train_size: 748\n",
      "Epoch 16: train_loss: 0.6829 train_acc: 0.8406 | val_loss: 2.2182 val_acc: 0.4016\n",
      "00:02:56.36\n",
      "train_size: 748\n",
      "Epoch 17: train_loss: 0.6123 train_acc: 0.8549 | val_loss: 2.2821 val_acc: 0.4016\n",
      "00:02:56.99\n",
      "train_size: 748\n",
      "Epoch 18: train_loss: 0.5328 train_acc: 0.8750 | val_loss: 2.3235 val_acc: 0.3811\n",
      "00:02:56.56\n",
      "train_size: 748\n",
      "Epoch 19: train_loss: 0.4789 train_acc: 0.8924 | val_loss: 2.4072 val_acc: 0.3689\n",
      "00:02:56.90\n",
      "train_size: 748\n",
      "Epoch 20: train_loss: 0.4135 train_acc: 0.9094 | val_loss: 2.4204 val_acc: 0.3893\n",
      "00:02:56.93\n",
      "train_size: 748\n",
      "Epoch 21: train_loss: 0.3698 train_acc: 0.9218 | val_loss: 2.4844 val_acc: 0.3852\n",
      "00:02:56.74\n",
      "train_size: 748\n",
      "Epoch 22: train_loss: 0.3162 train_acc: 0.9398 | val_loss: 2.4307 val_acc: 0.3893\n",
      "00:02:56.90\n",
      "train_size: 748\n",
      "Epoch 23: train_loss: 0.2834 train_acc: 0.9485 | val_loss: 2.6489 val_acc: 0.3197\n",
      "00:02:56.90\n",
      "train_size: 748\n",
      "Epoch 24: train_loss: 0.2565 train_acc: 0.9539 | val_loss: 2.6302 val_acc: 0.3566\n",
      "00:02:56.85\n",
      "train_size: 748\n",
      "Epoch 25: train_loss: 0.2210 train_acc: 0.9612 | val_loss: 2.6110 val_acc: 0.3648\n",
      "00:02:56.73\n",
      "train_size: 748\n",
      "Epoch 26: train_loss: 0.1837 train_acc: 0.9682 | val_loss: 2.6983 val_acc: 0.3238\n",
      "00:02:57.16\n",
      "train_size: 748\n",
      "Epoch 27: train_loss: 0.1729 train_acc: 0.9699 | val_loss: 2.7303 val_acc: 0.3689\n",
      "00:02:56.65\n",
      "train_size: 748\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    train(model, train_loader, val_loader, optimizer, scheduler, rev_label_dict) #6m30s 13m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/modeling_utils.py:768: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_loss: 3.8300 test_acc: 0.2500\n",
      "00:00:00.94\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    antithesis       0.17      0.11      0.13        18\n",
      "    background       0.25      0.35      0.29        17\n",
      "         cause       0.25      0.50      0.33         2\n",
      "  circumstance       0.00      0.00      0.00        15\n",
      "    concession       0.36      0.38      0.37        13\n",
      "     condition       0.35      0.67      0.46         9\n",
      "   conjunction       0.40      0.57      0.47         7\n",
      "      contrast       0.00      0.00      0.00         8\n",
      " e-elaboration       0.62      0.45      0.53        11\n",
      "   elaboration       0.00      0.00      0.00        10\n",
      "    evaluation       0.24      0.25      0.24        20\n",
      "      evidence       0.00      0.00      0.00        10\n",
      "interpretation       0.10      0.08      0.09        12\n",
      "         joint       0.20      0.24      0.22        29\n",
      "          list       0.44      0.31      0.36        26\n",
      "         means       0.50      0.50      0.50         2\n",
      "   preparation       0.67      0.50      0.57         4\n",
      "       purpose       0.67      0.67      0.67         3\n",
      "        reason       0.41      0.26      0.32        34\n",
      "   restatement       0.00      0.00      0.00         1\n",
      "        result       0.00      0.00      0.00         0\n",
      "      sequence       0.33      0.14      0.20         7\n",
      "  solutionhood       0.00      0.00      0.00         1\n",
      "       summary       0.00      0.00      0.00         1\n",
      "\n",
      "      accuracy                           0.25       260\n",
      "     macro avg       0.25      0.25      0.24       260\n",
      "  weighted avg       0.27      0.25      0.25       260\n",
      "\n",
      "Test Loss: 3.830 |  Test Acc: 25.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "def validate(model, test_loader, optimizer, rev_label_dict, label_dict):\n",
    "  start = time.time()\n",
    "  test_acc, test_loss, cr, model, optimizer = evaluate_accuracy(model, optimizer, test_loader, rev_label_dict, label_dict, is_training=False)\n",
    "  end = time.time()\n",
    "  hours, rem = divmod(end-start, 3600)\n",
    "  minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "  print(f'Test_loss: {test_loss:.4f} test_acc: {test_acc:.4f}')\n",
    "  print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "  print(cr)\n",
    "\n",
    "  return test_loss, test_acc\n",
    "\n",
    "\n",
    "test_loss, test_acc = validate(model, test_loader, optimizer, rev_label_dict, label_dict)\n",
    "writer.add_scalar('test_loss_latest', test_loss, 1)\n",
    "writer.add_scalar('test_acc_latest', test_acc, 1)\n",
    "print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### best earliest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/modeling_utils.py:768: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_loss: 2.7249 test_acc: 0.2962\n",
      "00:00:00.92\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    antithesis       0.67      0.11      0.19        18\n",
      "    background       0.24      0.29      0.26        17\n",
      "         cause       0.11      0.50      0.18         2\n",
      "  circumstance       0.10      0.07      0.08        15\n",
      "    concession       0.33      0.54      0.41        13\n",
      "     condition       0.40      0.67      0.50         9\n",
      "   conjunction       0.56      0.71      0.63         7\n",
      "      contrast       0.00      0.00      0.00         8\n",
      " e-elaboration       0.54      0.64      0.58        11\n",
      "   elaboration       0.05      0.10      0.07        10\n",
      "    evaluation       0.24      0.20      0.22        20\n",
      "      evidence       0.14      0.10      0.12        10\n",
      "interpretation       0.17      0.25      0.20        12\n",
      "         joint       0.25      0.34      0.29        29\n",
      "          list       0.33      0.23      0.27        26\n",
      "         means       0.00      0.00      0.00         2\n",
      "   preparation       0.60      0.75      0.67         4\n",
      "       purpose       1.00      0.67      0.80         3\n",
      "        reason       0.43      0.38      0.41        34\n",
      "   restatement       0.00      0.00      0.00         1\n",
      "      sequence       0.00      0.00      0.00         7\n",
      "  solutionhood       0.00      0.00      0.00         1\n",
      "       summary       0.00      0.00      0.00         1\n",
      "\n",
      "      accuracy                           0.30       260\n",
      "     macro avg       0.27      0.28      0.26       260\n",
      "  weighted avg       0.31      0.30      0.28       260\n",
      "\n",
      "Latest Test Loss: 2.725 |  Latest Test Acc: 29.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('run1/'+save_path_suffix+'_best.pt'))\n",
    "test_loss, test_acc = validate(model, test_loader, optimizer, rev_label_dict, label_dict)\n",
    "writer.add_scalar('test_loss_best_earliest', test_loss, 1)\n",
    "writer.add_scalar('test_acc_best_earliest', test_acc, 1)\n",
    "print(f'Latest Test Loss: {test_loss:.3f} |  Latest Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### best lastest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/modeling_utils.py:768: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_loss: 3.4249 test_acc: 0.2692\n",
      "00:00:00.91\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    antithesis       0.40      0.22      0.29        18\n",
      "    background       0.20      0.24      0.22        17\n",
      "         cause       0.12      0.50      0.20         2\n",
      "  circumstance       0.17      0.20      0.18        15\n",
      "    concession       0.43      0.46      0.44        13\n",
      "     condition       0.38      0.56      0.45         9\n",
      "   conjunction       0.40      0.57      0.47         7\n",
      "      contrast       0.00      0.00      0.00         8\n",
      " e-elaboration       0.60      0.55      0.57        11\n",
      "   elaboration       0.00      0.00      0.00        10\n",
      "    evaluation       0.25      0.15      0.19        20\n",
      "      evidence       0.10      0.20      0.13        10\n",
      "interpretation       0.15      0.17      0.16        12\n",
      "         joint       0.21      0.24      0.22        29\n",
      "          list       0.47      0.27      0.34        26\n",
      "         means       0.50      0.50      0.50         2\n",
      "   preparation       0.67      0.50      0.57         4\n",
      "       purpose       0.67      0.67      0.67         3\n",
      "        reason       0.36      0.29      0.32        34\n",
      "   restatement       0.00      0.00      0.00         1\n",
      "        result       0.00      0.00      0.00         0\n",
      "      sequence       0.50      0.14      0.22         7\n",
      "  solutionhood       0.00      0.00      0.00         1\n",
      "       summary       0.00      0.00      0.00         1\n",
      "\n",
      "      accuracy                           0.27       260\n",
      "     macro avg       0.27      0.27      0.26       260\n",
      "  weighted avg       0.30      0.27      0.27       260\n",
      "\n",
      "Best Test Loss: 3.425 |  Best Test Acc: 26.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('run1/'+save_path_suffix+'_best_latest.pt'))\n",
    "test_loss, test_acc = validate(model, test_loader, optimizer, rev_label_dict, label_dict)\n",
    "writer.add_scalar('test_loss_best_latest', test_loss, 1)\n",
    "writer.add_scalar('test_acc_best_latest', test_acc, 1)\n",
    "print(f'Best Test Loss: {test_loss:.3f} |  Best Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### best val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/modeling_utils.py:768: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_loss: 3.0151 test_acc: 0.3607\n",
      "00:00:00.78\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    antithesis       0.29      0.45      0.36        11\n",
      "    background       0.33      0.29      0.31        17\n",
      "         cause       0.25      0.14      0.18         7\n",
      "  circumstance       0.25      0.31      0.28        13\n",
      "    concession       0.43      0.55      0.48        11\n",
      "     condition       0.70      0.88      0.78         8\n",
      "   conjunction       0.67      0.75      0.71         8\n",
      "      contrast       0.09      0.33      0.14         3\n",
      " e-elaboration       0.69      0.69      0.69        13\n",
      "   elaboration       0.15      0.07      0.10        28\n",
      "    evaluation       0.00      0.00      0.00        13\n",
      "      evidence       0.21      0.62      0.31         8\n",
      "interpretation       0.08      0.08      0.08        13\n",
      "         joint       0.30      0.33      0.32        18\n",
      "          list       0.33      0.33      0.33        18\n",
      "         means       0.00      0.00      0.00         1\n",
      "   preparation       1.00      0.64      0.78        11\n",
      "       purpose       0.67      0.40      0.50         5\n",
      "        reason       0.54      0.54      0.54        28\n",
      "   restatement       0.00      0.00      0.00         1\n",
      "        result       0.00      0.00      0.00         3\n",
      "  solutionhood       0.00      0.00      0.00         1\n",
      "       summary       0.00      0.00      0.00         2\n",
      "\n",
      "      accuracy                           0.37       241\n",
      "     macro avg       0.30      0.32      0.30       241\n",
      "  weighted avg       0.36      0.37      0.35       241\n",
      "\n",
      "Val Loss: 3.015 |  Val Acc: 36.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('run1/'+save_path_suffix+'_best_latest.pt'))\n",
    "test_loss, test_acc = validate(model, val_loader, optimizer, rev_label_dict, label_dict)\n",
    "writer.add_scalar('val_loss_best_latest', test_loss, 1)\n",
    "writer.add_scalar('val_acc_best_latest', test_acc, 1)\n",
    "print(f'Val Loss: {test_loss:.3f} |  Val Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unit1_txt</th>\n",
       "      <th>unit1_sent</th>\n",
       "      <th>unit2_txt</th>\n",
       "      <th>unit2_sent</th>\n",
       "      <th>dir</th>\n",
       "      <th>distance</th>\n",
       "      <th>u1_depdir</th>\n",
       "      <th>u2_depdir</th>\n",
       "      <th>u2_func</th>\n",
       "      <th>u1_position</th>\n",
       "      <th>...</th>\n",
       "      <th>sat_children</th>\n",
       "      <th>nuc_children</th>\n",
       "      <th>genre</th>\n",
       "      <th>unit1_case</th>\n",
       "      <th>unit2_case</th>\n",
       "      <th>u1_discontinuous</th>\n",
       "      <th>u2_discontinuous</th>\n",
       "      <th>same_speaker</th>\n",
       "      <th>lex_overlap_length</th>\n",
       "      <th>u1_func</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>background</th>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>...</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cause</th>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "      <td>...</td>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>condition</th>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>...</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contrast</th>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>...</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elaboration</th>\n",
       "      <td>444</td>\n",
       "      <td>444</td>\n",
       "      <td>444</td>\n",
       "      <td>444</td>\n",
       "      <td>444</td>\n",
       "      <td>444</td>\n",
       "      <td>444</td>\n",
       "      <td>444</td>\n",
       "      <td>444</td>\n",
       "      <td>444</td>\n",
       "      <td>...</td>\n",
       "      <td>444</td>\n",
       "      <td>444</td>\n",
       "      <td>444</td>\n",
       "      <td>444</td>\n",
       "      <td>444</td>\n",
       "      <td>444</td>\n",
       "      <td>444</td>\n",
       "      <td>444</td>\n",
       "      <td>444</td>\n",
       "      <td>444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>evaluation</th>\n",
       "      <td>228</td>\n",
       "      <td>228</td>\n",
       "      <td>228</td>\n",
       "      <td>228</td>\n",
       "      <td>228</td>\n",
       "      <td>228</td>\n",
       "      <td>228</td>\n",
       "      <td>228</td>\n",
       "      <td>228</td>\n",
       "      <td>228</td>\n",
       "      <td>...</td>\n",
       "      <td>228</td>\n",
       "      <td>228</td>\n",
       "      <td>228</td>\n",
       "      <td>228</td>\n",
       "      <td>228</td>\n",
       "      <td>228</td>\n",
       "      <td>228</td>\n",
       "      <td>228</td>\n",
       "      <td>228</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joint</th>\n",
       "      <td>538</td>\n",
       "      <td>538</td>\n",
       "      <td>538</td>\n",
       "      <td>538</td>\n",
       "      <td>538</td>\n",
       "      <td>538</td>\n",
       "      <td>538</td>\n",
       "      <td>538</td>\n",
       "      <td>538</td>\n",
       "      <td>538</td>\n",
       "      <td>...</td>\n",
       "      <td>538</td>\n",
       "      <td>538</td>\n",
       "      <td>538</td>\n",
       "      <td>538</td>\n",
       "      <td>538</td>\n",
       "      <td>538</td>\n",
       "      <td>538</td>\n",
       "      <td>538</td>\n",
       "      <td>538</td>\n",
       "      <td>538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>means</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             unit1_txt  unit1_sent  unit2_txt  unit2_sent  dir  distance  \\\n",
       "label                                                                      \n",
       "background         351         351        351         351  351       351   \n",
       "cause              246         246        246         246  246       246   \n",
       "condition          265         265        265         265  265       265   \n",
       "contrast           102         102        102         102  102       102   \n",
       "elaboration        444         444        444         444  444       444   \n",
       "evaluation         228         228        228         228  228       228   \n",
       "joint              538         538        538         538  538       538   \n",
       "means               21          21         21          21   21        21   \n",
       "summary             16          16         16          16   16        16   \n",
       "\n",
       "             u1_depdir  u2_depdir  u2_func  u1_position  ...  sat_children  \\\n",
       "label                                                    ...                 \n",
       "background         351        351      351          351  ...           351   \n",
       "cause              246        246      246          246  ...           246   \n",
       "condition          265        265      265          265  ...           265   \n",
       "contrast           102        102      102          102  ...           102   \n",
       "elaboration        444        444      444          444  ...           444   \n",
       "evaluation         228        228      228          228  ...           228   \n",
       "joint              538        538      538          538  ...           538   \n",
       "means               21         21       21           21  ...            21   \n",
       "summary             16         16       16           16  ...            16   \n",
       "\n",
       "             nuc_children  genre  unit1_case  unit2_case  u1_discontinuous  \\\n",
       "label                                                                        \n",
       "background            351    351         351         351               351   \n",
       "cause                 246    246         246         246               246   \n",
       "condition             265    265         265         265               265   \n",
       "contrast              102    102         102         102               102   \n",
       "elaboration           444    444         444         444               444   \n",
       "evaluation            228    228         228         228               228   \n",
       "joint                 538    538         538         538               538   \n",
       "means                  21     21          21          21                21   \n",
       "summary                16     16          16          16                16   \n",
       "\n",
       "             u2_discontinuous  same_speaker  lex_overlap_length  u1_func  \n",
       "label                                                                     \n",
       "background                351           351                 351      351  \n",
       "cause                     246           246                 246      246  \n",
       "condition                 265           265                 265      265  \n",
       "contrast                  102           102                 102      102  \n",
       "elaboration               444           444                 444      444  \n",
       "evaluation                228           228                 228      228  \n",
       "joint                     538           538                 538      538  \n",
       "means                      21            21                  21       21  \n",
       "summary                    16            16                  16       16  \n",
       "\n",
       "[9 rows x 21 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_en.groupby(['label']).count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3409ea685db85227fbd9509d1b1ace14d085473eb2d57f3ba9dd0302d25f838"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

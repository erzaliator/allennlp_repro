{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL = 'bert-base-multilingual-cased'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CustomBert with adapters no discodisco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/discodapter/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASSIGN: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertAdapterModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomBERTModel(\n",
      "  (embedder): BertAdapterModel(\n",
      "    (shared_parameters): ModuleDict()\n",
      "    (bert): BertModel(\n",
      "      (shared_parameters): ModuleDict()\n",
      "      (invertible_adapters): ModuleDict()\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (key): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (value): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (prefix_gates): ModuleDict()\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (key): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (value): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (prefix_gates): ModuleDict()\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (key): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (value): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (prefix_gates): ModuleDict()\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (key): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (value): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (prefix_gates): ModuleDict()\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (key): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (value): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (prefix_gates): ModuleDict()\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (key): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (value): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (prefix_gates): ModuleDict()\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (key): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (value): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (prefix_gates): ModuleDict()\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (key): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (value): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (prefix_gates): ModuleDict()\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (key): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (value): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (prefix_gates): ModuleDict()\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (key): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (value): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (prefix_gates): ModuleDict()\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (key): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (value): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (prefix_gates): ModuleDict()\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (key): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (value): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (prefix_gates): ModuleDict()\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "      (prefix_tuning): PrefixTuningPool(\n",
      "        (prefix_tunings): ModuleDict()\n",
      "      )\n",
      "    )\n",
      "    (heads): ModuleDict()\n",
      "  )\n",
      "  (relation_decoder): Linear(in_features=792, out_features=23, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.bert.modeling_bert import BertModel\n",
    "import torch\n",
    "# import copy\n",
    "# from typing import Any, Dict, Optional\n",
    "# from transformers import XLMRobertaModel, XLMRobertaConfig\n",
    "import torch.nn as nn\n",
    "# from tensorflow.keras.layers import TimeDistributed\n",
    "# from featurefulbertembedder_custom_xlmr import FeaturefulBertEmbedder\n",
    "# from featureful_bert_custom_xlmr import get_combined_feature_tensor_2 as get_combined_feature_tensor_forward\n",
    "from transformers import AutoConfig, AutoAdapterModel\n",
    "\n",
    "\n",
    "class CustomBERTModel(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "          super(CustomBERTModel, self).__init__()\n",
    "          self.num_classes = num_labels\n",
    "        #   self.feature_list = mnli_dataset.feature_list\n",
    "          print('ASSIGN:', self.num_classes)\n",
    "\n",
    "          self.embedder = self.create_featureful_bert()\n",
    "        #   self.encoder = CustomPooler2()\n",
    "        #   self.module1 = MyModule(self.feature_list)\n",
    "        #   self.dropout1 = nn.Dropout(p=0.0)\n",
    "        #   self.dropout_decoder = nn.Dropout(p=0.5)\n",
    "          self.relation_decoder = nn.Linear(792, self.num_classes)\n",
    "\n",
    "    def forward(self, pair_token_ids, attention_mask, feat):\n",
    "        direction_tensor = feat['dir']\n",
    "        embedded_sentence = self.embedder(token_ids=pair_token_ids, #featurefulmebedder\n",
    "                        mask=attention_mask, \n",
    "                        # type_ids=token_type_ids,\n",
    "                        segment_concat_mask = None,\n",
    "                        direction_tensor = direction_tensor,\n",
    "                        feature_list = self.feature_list,\n",
    "                        features = feat)\n",
    "        # mask = token_type_ids\n",
    "        bertpooler_output = self.encoder(tokens=embedded_sentence, mask=None)\n",
    "        feat = self.convert_to_feature_list(feat)\n",
    "        feat = self.dropout1(feat)\n",
    "        feat = self.module1(feat)\n",
    "        try:\n",
    "            feat_concat = torch.concat((bertpooler_output, feat),-1)\n",
    "        except:\n",
    "            print(bertpooler_output.shape, feat.shape)\n",
    "            raise ValueError()\n",
    "        if feat_concat.shape[-1]!=792: print(feat_concat.shape)\n",
    "        assert feat_concat.shape[-1] == 792\n",
    "        feat_concat = self.dropout1(feat_concat)\n",
    "        # feat_concat = self.dropout_decoder(feat_concat)\n",
    "        linear1_output = self.relation_decoder(feat_concat)\n",
    "        return linear1_output\n",
    "\n",
    "\n",
    "    # def create_featureful_bert(self):\n",
    "    #     featureful_bert = FeaturefulBertEmbedder(model_name = BERT_MODEL,\n",
    "    #                             hidden_activation_allen = 'gelu',\n",
    "    #                             feature_list = self.feature_list, \n",
    "    #                             vocab=mnli_dataset.vocab)\n",
    "    #     return featureful_bert\n",
    "\n",
    "    def create_featureful_bert(self):\n",
    "        self.configadapter = AutoConfig.from_pretrained(\n",
    "            BERT_MODEL,\n",
    "        )\n",
    "        model = AutoAdapterModel.from_pretrained(\n",
    "            BERT_MODEL,\n",
    "            config=self.configadapter,\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def convert_to_feature_list(self, feat):\n",
    "        feature_linear = [feat[feature_name] for feature_name in self.feature_list]\n",
    "        feature_linear = torch.stack(feature_linear, dim=-1)\n",
    "        return feature_linear\n",
    "        \n",
    "\n",
    "model = CustomBERTModel(23)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomBERTModel(\n",
      "  (embedder): BertAdapterModel(\n",
      "    (shared_parameters): ModuleDict()\n",
      "    (bert): BertModel(\n",
      "      (shared_parameters): ModuleDict()\n",
      "      (invertible_adapters): ModuleDict(\n",
      "        (en): NICECouplingBlock(\n",
      "          (F): Sequential(\n",
      "            (0): Linear(in_features=384, out_features=192, bias=True)\n",
      "            (1): Activation_Function_Class(\n",
      "              (f): ReLU()\n",
      "            )\n",
      "            (2): Linear(in_features=192, out_features=384, bias=True)\n",
      "          )\n",
      "          (G): Sequential(\n",
      "            (0): Linear(in_features=384, out_features=192, bias=True)\n",
      "            (1): Activation_Function_Class(\n",
      "              (f): ReLU()\n",
      "            )\n",
      "            (2): Linear(in_features=192, out_features=384, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (de): NICECouplingBlock(\n",
      "          (F): Sequential(\n",
      "            (0): Linear(in_features=384, out_features=192, bias=True)\n",
      "            (1): Activation_Function_Class(\n",
      "              (f): NewGELUActivation()\n",
      "            )\n",
      "            (2): Linear(in_features=192, out_features=384, bias=True)\n",
      "          )\n",
      "          (G): Sequential(\n",
      "            (0): Linear(in_features=384, out_features=192, bias=True)\n",
      "            (1): Activation_Function_Class(\n",
      "              (f): NewGELUActivation()\n",
      "            )\n",
      "            (2): Linear(in_features=192, out_features=384, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (key): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (value): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (prefix_gates): ModuleDict()\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (en): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): ReLU()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "                )\n",
      "                (de): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): NewGELUActivation()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): NewGELUActivation()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (key): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (value): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (prefix_gates): ModuleDict()\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (en): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): ReLU()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "                )\n",
      "                (de): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): NewGELUActivation()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): NewGELUActivation()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (key): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (value): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (prefix_gates): ModuleDict()\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (en): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): ReLU()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "                )\n",
      "                (de): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): NewGELUActivation()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): NewGELUActivation()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (key): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (value): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (prefix_gates): ModuleDict()\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (en): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): ReLU()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "                )\n",
      "                (de): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): NewGELUActivation()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): NewGELUActivation()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (key): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (value): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (prefix_gates): ModuleDict()\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (en): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): ReLU()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "                )\n",
      "                (de): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): NewGELUActivation()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): NewGELUActivation()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (key): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (value): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (prefix_gates): ModuleDict()\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (en): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): ReLU()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "                )\n",
      "                (de): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): NewGELUActivation()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): NewGELUActivation()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (key): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (value): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (prefix_gates): ModuleDict()\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (en): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): ReLU()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "                )\n",
      "                (de): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): NewGELUActivation()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): NewGELUActivation()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (key): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (value): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (prefix_gates): ModuleDict()\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (en): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): ReLU()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "                )\n",
      "                (de): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): NewGELUActivation()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): NewGELUActivation()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (key): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (value): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (prefix_gates): ModuleDict()\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (en): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): ReLU()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "                )\n",
      "                (de): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): NewGELUActivation()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): NewGELUActivation()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (key): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (value): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (prefix_gates): ModuleDict()\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (en): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): ReLU()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "                )\n",
      "                (de): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): NewGELUActivation()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): NewGELUActivation()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (key): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (value): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (prefix_gates): ModuleDict()\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (en): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): ReLU()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "                )\n",
      "                (de): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): NewGELUActivation()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): NewGELUActivation()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (key): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (value): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningShim(\n",
      "                  (prefix_gates): ModuleDict()\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (en): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): ReLU()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "                )\n",
      "                (de): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): NewGELUActivation()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): NewGELUActivation()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "      (prefix_tuning): PrefixTuningPool(\n",
      "        (prefix_tunings): ModuleDict()\n",
      "      )\n",
      "    )\n",
      "    (heads): ModuleDict()\n",
      "  )\n",
      "  (relation_decoder): Linear(in_features=792, out_features=23, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdapterConfig\n",
    "lang_adapter_config = AdapterConfig.load(\"pfeiffer\", reduction_factor=2)\n",
    "\n",
    "model.embedder.load_adapter(\"en/wiki@ukp\", config=lang_adapter_config)\n",
    "model.embedder.load_adapter(\"de/wiki@ukp\", config=lang_adapter_config)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom BERT discodisco with adapter inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-18 16:31:22.143445: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-18 16:31:22.298731: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda-10.1/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-10.2/targets/x86_64-linux/lib/:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-01-18 16:31:22.298748: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-18 16:31:23.111861: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda-10.1/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-10.2/targets/x86_64-linux/lib/:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-01-18 16:31:23.111937: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda-10.1/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-10.2/targets/x86_64-linux/lib/:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-01-18 16:31:23.111945: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASSIGN: 23\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mnli_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 77\u001b[0m\n\u001b[1;32m     73\u001b[0m         feature_linear \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(feature_linear, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     74\u001b[0m         \u001b[39mreturn\u001b[39;00m feature_linear\n\u001b[0;32m---> 77\u001b[0m model \u001b[39m=\u001b[39m CustomBERTModel(\u001b[39m23\u001b[39;49m)\n\u001b[1;32m     78\u001b[0m \u001b[39mprint\u001b[39m(model)\n",
      "Cell \u001b[0;32mIn[4], line 20\u001b[0m, in \u001b[0;36mCustomBERTModel.__init__\u001b[0;34m(self, num_labels)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39m#   self.feature_list = mnli_dataset.feature_list\u001b[39;00m\n\u001b[1;32m     18\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mASSIGN:\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_classes)\n\u001b[0;32m---> 20\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedder \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_featureful_bert()\n\u001b[1;32m     21\u001b[0m \u001b[39m#   self.encoder = CustomPooler2()\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39m#   self.module1 = MyModule(self.feature_list)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m#   self.dropout1 = nn.Dropout(p=0.0)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39m#   self.dropout_decoder = nn.Dropout(p=0.5)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelation_decoder \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(\u001b[39m792\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_classes)\n",
      "Cell \u001b[0;32mIn[4], line 58\u001b[0m, in \u001b[0;36mCustomBERTModel.create_featureful_bert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_featureful_bert\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     55\u001b[0m     featureful_bert \u001b[39m=\u001b[39m FeaturefulBertEmbedder(model_name \u001b[39m=\u001b[39m BERT_MODEL,\n\u001b[1;32m     56\u001b[0m                             hidden_activation_allen \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mgelu\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     57\u001b[0m                             feature_list \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \n\u001b[0;32m---> 58\u001b[0m                             vocab\u001b[39m=\u001b[39mmnli_dataset\u001b[39m.\u001b[39mvocab)\n\u001b[1;32m     59\u001b[0m     \u001b[39mreturn\u001b[39;00m featureful_bert\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mnli_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers.models.bert.modeling_bert import BertModel\n",
    "import torch\n",
    "# import copy\n",
    "# from typing import Any, Dict, Optional\n",
    "# from transformers import XLMRobertaModel, XLMRobertaConfig\n",
    "import torch.nn as nn\n",
    "# from tensorflow.keras.layers import TimeDistributed\n",
    "from featurefulbertembedder_custom2 import FeaturefulBertEmbedder\n",
    "from featureful_bert_custom2 import get_combined_feature_tensor_2 as get_combined_feature_tensor_forward\n",
    "from transformers import AutoConfig, AutoAdapterModel\n",
    "\n",
    "\n",
    "class CustomBERTModel(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "          super(CustomBERTModel, self).__init__()\n",
    "          self.num_classes = num_labels\n",
    "        #   self.feature_list = mnli_dataset.feature_list\n",
    "          print('ASSIGN:', self.num_classes)\n",
    "\n",
    "          self.embedder = self.create_featureful_bert()\n",
    "        #   self.encoder = CustomPooler2()\n",
    "        #   self.module1 = MyModule(self.feature_list)\n",
    "        #   self.dropout1 = nn.Dropout(p=0.0)\n",
    "        #   self.dropout_decoder = nn.Dropout(p=0.5)\n",
    "          self.relation_decoder = nn.Linear(792, self.num_classes)\n",
    "\n",
    "    def forward(self, pair_token_ids, attention_mask, feat):\n",
    "        direction_tensor = feat['dir']\n",
    "        embedded_sentence = self.embedder(token_ids=pair_token_ids, #featurefulmebedder\n",
    "                        mask=attention_mask, \n",
    "                        # type_ids=token_type_ids,\n",
    "                        segment_concat_mask = None,\n",
    "                        direction_tensor = direction_tensor,\n",
    "                        feature_list = None,\n",
    "                        features = feat)\n",
    "        # mask = token_type_ids\n",
    "        bertpooler_output = self.encoder(tokens=embedded_sentence, mask=None)\n",
    "        feat = self.convert_to_feature_list(feat)\n",
    "        feat = self.dropout1(feat)\n",
    "        feat = self.module1(feat)\n",
    "        try:\n",
    "            feat_concat = torch.concat((bertpooler_output, feat),-1)\n",
    "        except:\n",
    "            print(bertpooler_output.shape, feat.shape)\n",
    "            raise ValueError()\n",
    "        if feat_concat.shape[-1]!=792: print(feat_concat.shape)\n",
    "        assert feat_concat.shape[-1] == 792\n",
    "        feat_concat = self.dropout1(feat_concat)\n",
    "        # feat_concat = self.dropout_decoder(feat_concat)\n",
    "        linear1_output = self.relation_decoder(feat_concat)\n",
    "        return linear1_output\n",
    "\n",
    "\n",
    "    def create_featureful_bert(self):\n",
    "        featureful_bert = FeaturefulBertEmbedder(model_name = BERT_MODEL,\n",
    "                                hidden_activation_allen = 'gelu',\n",
    "                                feature_list = None, \n",
    "                                vocab=mnli_dataset.vocab)\n",
    "        return featureful_bert\n",
    "\n",
    "    # def create_featureful_bert(self):\n",
    "    #     self.configadapter = AutoConfig.from_pretrained(\n",
    "    #         BERT_MODEL,\n",
    "    #     )\n",
    "    #     model = AutoAdapterModel.from_pretrained(\n",
    "    #         BERT_MODEL,\n",
    "    #         config=self.configadapter,\n",
    "    #     )\n",
    "    #     return model\n",
    "\n",
    "    def convert_to_feature_list(self, feat):\n",
    "        feature_linear = [feat[feature_name] for feature_name in self.feature_list]\n",
    "        feature_linear = torch.stack(feature_linear, dim=-1)\n",
    "        return feature_linear\n",
    "        \n",
    "\n",
    "model = CustomBERTModel(23)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (shared_parameters): ModuleDict()\n",
      "  (invertible_adapters): ModuleDict()\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (key): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (value): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (prefix_tuning): PrefixTuningShim(\n",
      "              (prefix_gates): ModuleDict()\n",
      "              (pool): PrefixTuningPool(\n",
      "                (prefix_tunings): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(\n",
      "            in_features=768, out_features=3072, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(\n",
      "            in_features=3072, out_features=768, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (adapters): ModuleDict()\n",
      "          (adapter_fusion_layer): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "      (1): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (key): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (value): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (prefix_tuning): PrefixTuningShim(\n",
      "              (prefix_gates): ModuleDict()\n",
      "              (pool): PrefixTuningPool(\n",
      "                (prefix_tunings): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(\n",
      "            in_features=768, out_features=3072, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(\n",
      "            in_features=3072, out_features=768, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (adapters): ModuleDict()\n",
      "          (adapter_fusion_layer): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "      (2): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (key): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (value): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (prefix_tuning): PrefixTuningShim(\n",
      "              (prefix_gates): ModuleDict()\n",
      "              (pool): PrefixTuningPool(\n",
      "                (prefix_tunings): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(\n",
      "            in_features=768, out_features=3072, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(\n",
      "            in_features=3072, out_features=768, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (adapters): ModuleDict()\n",
      "          (adapter_fusion_layer): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "      (3): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (key): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (value): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (prefix_tuning): PrefixTuningShim(\n",
      "              (prefix_gates): ModuleDict()\n",
      "              (pool): PrefixTuningPool(\n",
      "                (prefix_tunings): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(\n",
      "            in_features=768, out_features=3072, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(\n",
      "            in_features=3072, out_features=768, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (adapters): ModuleDict()\n",
      "          (adapter_fusion_layer): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "      (4): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (key): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (value): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (prefix_tuning): PrefixTuningShim(\n",
      "              (prefix_gates): ModuleDict()\n",
      "              (pool): PrefixTuningPool(\n",
      "                (prefix_tunings): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(\n",
      "            in_features=768, out_features=3072, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(\n",
      "            in_features=3072, out_features=768, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (adapters): ModuleDict()\n",
      "          (adapter_fusion_layer): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "      (5): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (key): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (value): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (prefix_tuning): PrefixTuningShim(\n",
      "              (prefix_gates): ModuleDict()\n",
      "              (pool): PrefixTuningPool(\n",
      "                (prefix_tunings): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(\n",
      "            in_features=768, out_features=3072, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(\n",
      "            in_features=3072, out_features=768, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (adapters): ModuleDict()\n",
      "          (adapter_fusion_layer): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "      (6): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (key): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (value): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (prefix_tuning): PrefixTuningShim(\n",
      "              (prefix_gates): ModuleDict()\n",
      "              (pool): PrefixTuningPool(\n",
      "                (prefix_tunings): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(\n",
      "            in_features=768, out_features=3072, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(\n",
      "            in_features=3072, out_features=768, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (adapters): ModuleDict()\n",
      "          (adapter_fusion_layer): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "      (7): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (key): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (value): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (prefix_tuning): PrefixTuningShim(\n",
      "              (prefix_gates): ModuleDict()\n",
      "              (pool): PrefixTuningPool(\n",
      "                (prefix_tunings): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(\n",
      "            in_features=768, out_features=3072, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(\n",
      "            in_features=3072, out_features=768, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (adapters): ModuleDict()\n",
      "          (adapter_fusion_layer): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "      (8): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (key): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (value): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (prefix_tuning): PrefixTuningShim(\n",
      "              (prefix_gates): ModuleDict()\n",
      "              (pool): PrefixTuningPool(\n",
      "                (prefix_tunings): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(\n",
      "            in_features=768, out_features=3072, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(\n",
      "            in_features=3072, out_features=768, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (adapters): ModuleDict()\n",
      "          (adapter_fusion_layer): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "      (9): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (key): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (value): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (prefix_tuning): PrefixTuningShim(\n",
      "              (prefix_gates): ModuleDict()\n",
      "              (pool): PrefixTuningPool(\n",
      "                (prefix_tunings): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(\n",
      "            in_features=768, out_features=3072, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(\n",
      "            in_features=3072, out_features=768, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (adapters): ModuleDict()\n",
      "          (adapter_fusion_layer): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "      (10): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (key): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (value): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (prefix_tuning): PrefixTuningShim(\n",
      "              (prefix_gates): ModuleDict()\n",
      "              (pool): PrefixTuningPool(\n",
      "                (prefix_tunings): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(\n",
      "            in_features=768, out_features=3072, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(\n",
      "            in_features=3072, out_features=768, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (adapters): ModuleDict()\n",
      "          (adapter_fusion_layer): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "      (11): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (key): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (value): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (prefix_tuning): PrefixTuningShim(\n",
      "              (prefix_gates): ModuleDict()\n",
      "              (pool): PrefixTuningPool(\n",
      "                (prefix_tunings): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(\n",
      "            in_features=768, out_features=3072, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(\n",
      "            in_features=3072, out_features=768, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (adapters): ModuleDict()\n",
      "          (adapter_fusion_layer): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      "  (prefix_tuning): PrefixTuningPool(\n",
      "    (prefix_tunings): ModuleDict()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained(BERT_MODEL)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.encoder = BertEncoder(config)\n",
    "# self.pooler = BertPooler(config) if add_pooling_layer else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing FeaturefulBert: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing FeaturefulBert from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FeaturefulBert from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeaturefulBert(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (key): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (value): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (prefix_tuning): PrefixTuningShim(\n",
      "              (prefix_gates): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(\n",
      "            in_features=768, out_features=3072, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(\n",
      "            in_features=3072, out_features=768, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (adapters): ModuleDict()\n",
      "          (adapter_fusion_layer): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "      (1): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (key): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (value): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (prefix_tuning): PrefixTuningShim(\n",
      "              (prefix_gates): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(\n",
      "            in_features=768, out_features=3072, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(\n",
      "            in_features=3072, out_features=768, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (adapters): ModuleDict()\n",
      "          (adapter_fusion_layer): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "      (2): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (key): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (value): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (prefix_tuning): PrefixTuningShim(\n",
      "              (prefix_gates): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(\n",
      "            in_features=768, out_features=3072, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(\n",
      "            in_features=3072, out_features=768, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (adapters): ModuleDict()\n",
      "          (adapter_fusion_layer): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "      (3): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (key): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (value): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (prefix_tuning): PrefixTuningShim(\n",
      "              (prefix_gates): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(\n",
      "            in_features=768, out_features=3072, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(\n",
      "            in_features=3072, out_features=768, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (adapters): ModuleDict()\n",
      "          (adapter_fusion_layer): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "      (4): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (key): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (value): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (prefix_tuning): PrefixTuningShim(\n",
      "              (prefix_gates): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(\n",
      "            in_features=768, out_features=3072, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(\n",
      "            in_features=3072, out_features=768, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (adapters): ModuleDict()\n",
      "          (adapter_fusion_layer): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "      (5): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (key): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (value): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (prefix_tuning): PrefixTuningShim(\n",
      "              (prefix_gates): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(\n",
      "            in_features=768, out_features=3072, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(\n",
      "            in_features=3072, out_features=768, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (adapters): ModuleDict()\n",
      "          (adapter_fusion_layer): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "      (6): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (key): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (value): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (prefix_tuning): PrefixTuningShim(\n",
      "              (prefix_gates): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(\n",
      "            in_features=768, out_features=3072, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(\n",
      "            in_features=3072, out_features=768, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (adapters): ModuleDict()\n",
      "          (adapter_fusion_layer): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "      (7): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (key): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (value): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (prefix_tuning): PrefixTuningShim(\n",
      "              (prefix_gates): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(\n",
      "            in_features=768, out_features=3072, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(\n",
      "            in_features=3072, out_features=768, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (adapters): ModuleDict()\n",
      "          (adapter_fusion_layer): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "      (8): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (key): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (value): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (prefix_tuning): PrefixTuningShim(\n",
      "              (prefix_gates): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(\n",
      "            in_features=768, out_features=3072, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(\n",
      "            in_features=3072, out_features=768, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (adapters): ModuleDict()\n",
      "          (adapter_fusion_layer): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "      (9): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (key): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (value): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (prefix_tuning): PrefixTuningShim(\n",
      "              (prefix_gates): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(\n",
      "            in_features=768, out_features=3072, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(\n",
      "            in_features=3072, out_features=768, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (adapters): ModuleDict()\n",
      "          (adapter_fusion_layer): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "      (10): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (key): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (value): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (prefix_tuning): PrefixTuningShim(\n",
      "              (prefix_gates): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(\n",
      "            in_features=768, out_features=3072, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(\n",
      "            in_features=3072, out_features=768, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (adapters): ModuleDict()\n",
      "          (adapter_fusion_layer): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "      (11): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (key): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (value): Linear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (prefix_tuning): PrefixTuningShim(\n",
      "              (prefix_gates): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(\n",
      "            in_features=768, out_features=3072, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(\n",
      "            in_features=3072, out_features=768, bias=True\n",
      "            (loras): ModuleDict()\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (adapters): ModuleDict()\n",
      "          (adapter_fusion_layer): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.bert.modeling_bert import BertEmbeddings, BertConfig\n",
    "from featurefulbertembedder_custom2 import FeaturefulBert\n",
    "\n",
    "transformer_model = FeaturefulBert.from_pretrained(pretrained_model_name_or_path=BERT_MODEL)\n",
    "print(transformer_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "discodapter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Jan 11 2023, 15:21:40) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5d8a2a8a5b9abc71c133cb1f832a117b3c491b0de04eda931ac6db8a0de0367f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

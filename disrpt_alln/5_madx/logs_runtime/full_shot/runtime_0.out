-------------------------------------------------------------------
Lang1:  eng.pdtb.pdtb    Lang2:  deu.rst.pcc
Saving run to:  runs/full_shot/FullShot=v4_eng.pdtb.pdtb_deu.rst.pcc_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 43920 examples
read 1674 examples
read 2257 examples
read 2164 examples
read 241 examples
read 260 examples
Total prediction labels:  49
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=49, bias=True)
    )
  )
)
{'train@eng.pdtb.pdtb_loss': 1.309112310409546, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.5874089253187614, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.24912215902094595, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.278529796753162, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.24670556029485888, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.3091120719909668, 'train@eng.pdtb.pdtb_runtime': 529.8838, 'train@eng.pdtb.pdtb_samples_per_second': 82.886, 'train@eng.pdtb.pdtb_steps_per_second': 2.591, 'epoch': 1.0}
{'loss': 1.8997, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.2289462089538574, 'eval_accuracy@eng.pdtb.pdtb': 0.6212664277180406, 'eval_f1@eng.pdtb.pdtb': 0.2960535891984247, 'eval_precision@eng.pdtb.pdtb': 0.33258660362271786, 'eval_recall@eng.pdtb.pdtb': 0.29597475338630463, 'eval_loss@eng.pdtb.pdtb': 1.228946328163147, 'eval_runtime': 20.8489, 'eval_samples_per_second': 80.292, 'eval_steps_per_second': 2.542, 'epoch': 1.0}
{'train@eng.pdtb.pdtb_loss': 1.120144248008728, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6313069216757742, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.3404473706223687, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.40836323102152866, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.33259218260571444, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.120144248008728, 'train@eng.pdtb.pdtb_runtime': 528.5684, 'train@eng.pdtb.pdtb_samples_per_second': 83.092, 'train@eng.pdtb.pdtb_steps_per_second': 2.598, 'epoch': 2.0}
{'loss': 1.254, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.0528616905212402, 'eval_accuracy@eng.pdtb.pdtb': 0.6600955794504182, 'eval_f1@eng.pdtb.pdtb': 0.38980230644652736, 'eval_precision@eng.pdtb.pdtb': 0.43852215989278137, 'eval_recall@eng.pdtb.pdtb': 0.3835633069935328, 'eval_loss@eng.pdtb.pdtb': 1.0528615713119507, 'eval_runtime': 20.7202, 'eval_samples_per_second': 80.791, 'eval_steps_per_second': 2.558, 'epoch': 2.0}
{'train@eng.pdtb.pdtb_loss': 1.067577838897705, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6462431693989071, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4054991768306596, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.4635399414856819, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.3875685112137184, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.067577838897705, 'train@eng.pdtb.pdtb_runtime': 529.0591, 'train@eng.pdtb.pdtb_samples_per_second': 83.015, 'train@eng.pdtb.pdtb_steps_per_second': 2.595, 'epoch': 3.0}
{'loss': 1.1425, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.0133827924728394, 'eval_accuracy@eng.pdtb.pdtb': 0.6708482676224612, 'eval_f1@eng.pdtb.pdtb': 0.442561543185123, 'eval_precision@eng.pdtb.pdtb': 0.492454854221836, 'eval_recall@eng.pdtb.pdtb': 0.4286943790382292, 'eval_loss@eng.pdtb.pdtb': 1.0133827924728394, 'eval_runtime': 20.7485, 'eval_samples_per_second': 80.68, 'eval_steps_per_second': 2.554, 'epoch': 3.0}
{'train@eng.pdtb.pdtb_loss': 1.0137684345245361, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6610428051001821, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.43840298627810703, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.4733229460590164, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4245250603528008, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.0137684345245361, 'train@eng.pdtb.pdtb_runtime': 529.6806, 'train@eng.pdtb.pdtb_samples_per_second': 82.918, 'train@eng.pdtb.pdtb_steps_per_second': 2.592, 'epoch': 4.0}
{'loss': 1.0893, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 0.9642879962921143, 'eval_accuracy@eng.pdtb.pdtb': 0.6774193548387096, 'eval_f1@eng.pdtb.pdtb': 0.47805227063052974, 'eval_precision@eng.pdtb.pdtb': 0.5301312281349904, 'eval_recall@eng.pdtb.pdtb': 0.4603317241662904, 'eval_loss@eng.pdtb.pdtb': 0.9642881155014038, 'eval_runtime': 20.7836, 'eval_samples_per_second': 80.544, 'eval_steps_per_second': 2.55, 'epoch': 4.0}
{'train@eng.pdtb.pdtb_loss': 0.9889627695083618, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6693989071038251, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4513785893440786, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5078973934938851, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.44376110071575886, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9889628887176514, 'train@eng.pdtb.pdtb_runtime': 528.0102, 'train@eng.pdtb.pdtb_samples_per_second': 83.18, 'train@eng.pdtb.pdtb_steps_per_second': 2.6, 'epoch': 5.0}
{'loss': 1.0561, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 0.9451942443847656, 'eval_accuracy@eng.pdtb.pdtb': 0.6851851851851852, 'eval_f1@eng.pdtb.pdtb': 0.5169024984168096, 'eval_precision@eng.pdtb.pdtb': 0.5999245197198076, 'eval_recall@eng.pdtb.pdtb': 0.4994530345539392, 'eval_loss@eng.pdtb.pdtb': 0.9451942443847656, 'eval_runtime': 20.6946, 'eval_samples_per_second': 80.891, 'eval_steps_per_second': 2.561, 'epoch': 5.0}
{'train@eng.pdtb.pdtb_loss': 0.9685097932815552, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6741347905282331, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.45883131733982924, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5107082520322124, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.45196510937727746, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9685097932815552, 'train@eng.pdtb.pdtb_runtime': 529.4118, 'train@eng.pdtb.pdtb_samples_per_second': 82.96, 'train@eng.pdtb.pdtb_steps_per_second': 2.593, 'epoch': 6.0}
{'loss': 1.0332, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 0.9368999600410461, 'eval_accuracy@eng.pdtb.pdtb': 0.6833930704898447, 'eval_f1@eng.pdtb.pdtb': 0.523711258354359, 'eval_precision@eng.pdtb.pdtb': 0.6173638071683869, 'eval_recall@eng.pdtb.pdtb': 0.49952989090345595, 'eval_loss@eng.pdtb.pdtb': 0.9368998408317566, 'eval_runtime': 22.0539, 'eval_samples_per_second': 75.905, 'eval_steps_per_second': 2.403, 'epoch': 6.0}
{'train@eng.pdtb.pdtb_loss': 0.956971287727356, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6764571948998178, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4613755376367746, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5314799615507146, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4506472697300748, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.956971287727356, 'train@eng.pdtb.pdtb_runtime': 529.4244, 'train@eng.pdtb.pdtb_samples_per_second': 82.958, 'train@eng.pdtb.pdtb_steps_per_second': 2.593, 'epoch': 7.0}
{'loss': 1.0168, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 0.9232616424560547, 'eval_accuracy@eng.pdtb.pdtb': 0.6917562724014337, 'eval_f1@eng.pdtb.pdtb': 0.5349785256005916, 'eval_precision@eng.pdtb.pdtb': 0.6195946257301386, 'eval_recall@eng.pdtb.pdtb': 0.513159074520231, 'eval_loss@eng.pdtb.pdtb': 0.9232616424560547, 'eval_runtime': 20.8905, 'eval_samples_per_second': 80.132, 'eval_steps_per_second': 2.537, 'epoch': 7.0}
{'train@eng.pdtb.pdtb_loss': 0.9444965720176697, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6807604735883425, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.468739438702846, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5161807846723824, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.46566851207467624, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9444965720176697, 'train@eng.pdtb.pdtb_runtime': 529.3624, 'train@eng.pdtb.pdtb_samples_per_second': 82.968, 'train@eng.pdtb.pdtb_steps_per_second': 2.594, 'epoch': 8.0}
{'loss': 1.0033, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 0.9190942049026489, 'eval_accuracy@eng.pdtb.pdtb': 0.6893667861409797, 'eval_f1@eng.pdtb.pdtb': 0.5447793968800133, 'eval_precision@eng.pdtb.pdtb': 0.6213397440753375, 'eval_recall@eng.pdtb.pdtb': 0.5283100790853781, 'eval_loss@eng.pdtb.pdtb': 0.9190942049026489, 'eval_runtime': 20.743, 'eval_samples_per_second': 80.702, 'eval_steps_per_second': 2.555, 'epoch': 8.0}
{'train@eng.pdtb.pdtb_loss': 0.9357678294181824, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6834699453551912, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4719525723244196, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5222449881213346, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4668090695287827, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9357678294181824, 'train@eng.pdtb.pdtb_runtime': 529.3589, 'train@eng.pdtb.pdtb_samples_per_second': 82.968, 'train@eng.pdtb.pdtb_steps_per_second': 2.594, 'epoch': 9.0}
{'loss': 0.9908, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 0.9158658385276794, 'eval_accuracy@eng.pdtb.pdtb': 0.6911589008363201, 'eval_f1@eng.pdtb.pdtb': 0.5455528335958963, 'eval_precision@eng.pdtb.pdtb': 0.6245429450971431, 'eval_recall@eng.pdtb.pdtb': 0.5242434835845139, 'eval_loss@eng.pdtb.pdtb': 0.9158658385276794, 'eval_runtime': 20.8228, 'eval_samples_per_second': 80.393, 'eval_steps_per_second': 2.545, 'epoch': 9.0}
{'train@eng.pdtb.pdtb_loss': 0.9313538074493408, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6850182149362477, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4731427361827248, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5261856374521623, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.46801700258220297, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9313538074493408, 'train@eng.pdtb.pdtb_runtime': 528.3654, 'train@eng.pdtb.pdtb_samples_per_second': 83.124, 'train@eng.pdtb.pdtb_steps_per_second': 2.599, 'epoch': 10.0}
{'loss': 0.9864, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 0.9095621109008789, 'eval_accuracy@eng.pdtb.pdtb': 0.6917562724014337, 'eval_f1@eng.pdtb.pdtb': 0.5472224776422467, 'eval_precision@eng.pdtb.pdtb': 0.6192541862876718, 'eval_recall@eng.pdtb.pdtb': 0.5357759481027433, 'eval_loss@eng.pdtb.pdtb': 0.9095621109008789, 'eval_runtime': 20.796, 'eval_samples_per_second': 80.496, 'eval_steps_per_second': 2.549, 'epoch': 10.0}
{'train@eng.pdtb.pdtb_loss': 0.9273300766944885, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6856557377049181, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4761637156185772, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5241806732551485, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4717029028924023, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9273301362991333, 'train@eng.pdtb.pdtb_runtime': 523.7655, 'train@eng.pdtb.pdtb_samples_per_second': 83.854, 'train@eng.pdtb.pdtb_steps_per_second': 2.621, 'epoch': 11.0}
{'loss': 0.9804, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.9099348783493042, 'eval_accuracy@eng.pdtb.pdtb': 0.6935483870967742, 'eval_f1@eng.pdtb.pdtb': 0.5489006652949259, 'eval_precision@eng.pdtb.pdtb': 0.6168022743794601, 'eval_recall@eng.pdtb.pdtb': 0.5382080752619547, 'eval_loss@eng.pdtb.pdtb': 0.9099347591400146, 'eval_runtime': 20.4513, 'eval_samples_per_second': 81.853, 'eval_steps_per_second': 2.592, 'epoch': 11.0}
{'train@eng.pdtb.pdtb_loss': 0.926041305065155, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6866347905282332, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4764907009797729, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5267205857212505, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.47108574697843325, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9260414242744446, 'train@eng.pdtb.pdtb_runtime': 523.5409, 'train@eng.pdtb.pdtb_samples_per_second': 83.89, 'train@eng.pdtb.pdtb_steps_per_second': 2.623, 'epoch': 12.0}
{'loss': 0.976, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.9082766175270081, 'eval_accuracy@eng.pdtb.pdtb': 0.6917562724014337, 'eval_f1@eng.pdtb.pdtb': 0.5513651223062421, 'eval_precision@eng.pdtb.pdtb': 0.6224819190290212, 'eval_recall@eng.pdtb.pdtb': 0.5376761443225817, 'eval_loss@eng.pdtb.pdtb': 0.9082764983177185, 'eval_runtime': 20.4202, 'eval_samples_per_second': 81.977, 'eval_steps_per_second': 2.595, 'epoch': 12.0}
{'train_runtime': 19971.0847, 'train_samples_per_second': 26.39, 'train_steps_per_second': 0.825, 'train_loss': 1.1190535028341413, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.1191
  train_runtime            = 5:32:51.08
  train_samples_per_second =      26.39
  train_steps_per_second   =      0.825
{'train@deu.rst.pcc_loss': 3.279353141784668, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.09981515711645102, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.04452466472996666, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.05126844969567511, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.056771325117706745, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.279353141784668, 'train@deu.rst.pcc_runtime': 26.6246, 'train@deu.rst.pcc_samples_per_second': 81.278, 'train@deu.rst.pcc_steps_per_second': 2.554, 'epoch': 1.0}
{'loss': 3.9363, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.3881723880767822, 'eval_accuracy@deu.rst.pcc': 0.07468879668049792, 'eval_f1@deu.rst.pcc': 0.035509698543450714, 'eval_precision@deu.rst.pcc': 0.03561533798059362, 'eval_recall@deu.rst.pcc': 0.04381302458225535, 'eval_loss@deu.rst.pcc': 3.3881728649139404, 'eval_runtime': 3.3668, 'eval_samples_per_second': 71.581, 'eval_steps_per_second': 2.376, 'epoch': 1.0}
{'train@deu.rst.pcc_loss': 2.8660101890563965, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.17051756007393715, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.07862312384801928, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.07620633291972045, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.0965515702422334, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.8660101890563965, 'train@deu.rst.pcc_runtime': 26.5954, 'train@deu.rst.pcc_samples_per_second': 81.367, 'train@deu.rst.pcc_steps_per_second': 2.557, 'epoch': 2.0}
{'loss': 3.0901, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.9795169830322266, 'eval_accuracy@deu.rst.pcc': 0.13278008298755187, 'eval_f1@deu.rst.pcc': 0.07013709114204256, 'eval_precision@deu.rst.pcc': 0.06747510864902169, 'eval_recall@deu.rst.pcc': 0.08692196692196694, 'eval_loss@deu.rst.pcc': 2.9795165061950684, 'eval_runtime': 3.3893, 'eval_samples_per_second': 71.106, 'eval_steps_per_second': 2.36, 'epoch': 2.0}
{'train@deu.rst.pcc_loss': 2.69775390625, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.21719038817005545, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.11040512637333698, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.10898243894062619, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.12776237444552646, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.697754144668579, 'train@deu.rst.pcc_runtime': 26.5776, 'train@deu.rst.pcc_samples_per_second': 81.422, 'train@deu.rst.pcc_steps_per_second': 2.559, 'epoch': 3.0}
{'loss': 2.83, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.837977170944214, 'eval_accuracy@deu.rst.pcc': 0.17842323651452283, 'eval_f1@deu.rst.pcc': 0.10463762459159895, 'eval_precision@deu.rst.pcc': 0.09657536890354226, 'eval_recall@deu.rst.pcc': 0.12814518814518816, 'eval_loss@deu.rst.pcc': 2.837977409362793, 'eval_runtime': 3.3683, 'eval_samples_per_second': 71.55, 'eval_steps_per_second': 2.375, 'epoch': 3.0}
{'train@deu.rst.pcc_loss': 2.6009418964385986, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.24491682070240295, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.12854736015339024, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.16606949205941315, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.1497158508677754, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6009418964385986, 'train@deu.rst.pcc_runtime': 26.6032, 'train@deu.rst.pcc_samples_per_second': 81.344, 'train@deu.rst.pcc_steps_per_second': 2.556, 'epoch': 4.0}
{'loss': 2.7252, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.754051685333252, 'eval_accuracy@deu.rst.pcc': 0.1908713692946058, 'eval_f1@deu.rst.pcc': 0.09878678864868123, 'eval_precision@deu.rst.pcc': 0.08115976702508959, 'eval_recall@deu.rst.pcc': 0.13514763014763015, 'eval_loss@deu.rst.pcc': 2.754051446914673, 'eval_runtime': 3.3671, 'eval_samples_per_second': 71.576, 'eval_steps_per_second': 2.376, 'epoch': 4.0}
{'train@deu.rst.pcc_loss': 2.535994291305542, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.26848428835489835, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.1484785377409324, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.18041287712939996, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.16845134808328383, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.535994291305542, 'train@deu.rst.pcc_runtime': 26.5915, 'train@deu.rst.pcc_samples_per_second': 81.379, 'train@deu.rst.pcc_steps_per_second': 2.557, 'epoch': 5.0}
{'loss': 2.6507, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.701575994491577, 'eval_accuracy@deu.rst.pcc': 0.22821576763485477, 'eval_f1@deu.rst.pcc': 0.1296197871097106, 'eval_precision@deu.rst.pcc': 0.1655231941547731, 'eval_recall@deu.rst.pcc': 0.1595837495837496, 'eval_loss@deu.rst.pcc': 2.7015762329101562, 'eval_runtime': 3.3508, 'eval_samples_per_second': 71.923, 'eval_steps_per_second': 2.387, 'epoch': 5.0}
{'train@deu.rst.pcc_loss': 2.4834887981414795, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.29390018484288355, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.16419987340385825, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.22074581387817163, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.18727147043119244, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.4834890365600586, 'train@deu.rst.pcc_runtime': 26.5717, 'train@deu.rst.pcc_samples_per_second': 81.44, 'train@deu.rst.pcc_steps_per_second': 2.559, 'epoch': 6.0}
{'loss': 2.5954, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.655287981033325, 'eval_accuracy@deu.rst.pcc': 0.23651452282157676, 'eval_f1@deu.rst.pcc': 0.13744293565360205, 'eval_precision@deu.rst.pcc': 0.16710075914423744, 'eval_recall@deu.rst.pcc': 0.16715173715173715, 'eval_loss@deu.rst.pcc': 2.655287265777588, 'eval_runtime': 3.3783, 'eval_samples_per_second': 71.337, 'eval_steps_per_second': 2.368, 'epoch': 6.0}
{'train@deu.rst.pcc_loss': 2.4425253868103027, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.30730129390018485, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.1729681445693885, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.22655991375064263, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.19679785382086928, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.442525625228882, 'train@deu.rst.pcc_runtime': 26.6033, 'train@deu.rst.pcc_samples_per_second': 81.343, 'train@deu.rst.pcc_steps_per_second': 2.556, 'epoch': 7.0}
{'loss': 2.5442, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.6262831687927246, 'eval_accuracy@deu.rst.pcc': 0.23236514522821577, 'eval_f1@deu.rst.pcc': 0.13896195272908154, 'eval_precision@deu.rst.pcc': 0.14864616316249193, 'eval_recall@deu.rst.pcc': 0.1708818958818959, 'eval_loss@deu.rst.pcc': 2.6262826919555664, 'eval_runtime': 3.383, 'eval_samples_per_second': 71.238, 'eval_steps_per_second': 2.365, 'epoch': 7.0}
{'train@deu.rst.pcc_loss': 2.4113657474517822, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.31377079482439924, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.17989913810304847, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.2377544114674799, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.20240488367748594, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.411365509033203, 'train@deu.rst.pcc_runtime': 26.5326, 'train@deu.rst.pcc_samples_per_second': 81.56, 'train@deu.rst.pcc_steps_per_second': 2.563, 'epoch': 8.0}
{'loss': 2.5066, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.599130153656006, 'eval_accuracy@deu.rst.pcc': 0.25311203319502074, 'eval_f1@deu.rst.pcc': 0.138382301033664, 'eval_precision@deu.rst.pcc': 0.14280195200605986, 'eval_recall@deu.rst.pcc': 0.17643745143745143, 'eval_loss@deu.rst.pcc': 2.599130868911743, 'eval_runtime': 3.353, 'eval_samples_per_second': 71.875, 'eval_steps_per_second': 2.386, 'epoch': 8.0}
{'train@deu.rst.pcc_loss': 2.387286424636841, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.32070240295748614, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.18496961479378676, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.24071916860619208, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.20662438188889432, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.3872861862182617, 'train@deu.rst.pcc_runtime': 26.5242, 'train@deu.rst.pcc_samples_per_second': 81.586, 'train@deu.rst.pcc_steps_per_second': 2.564, 'epoch': 9.0}
{'loss': 2.4914, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.5838983058929443, 'eval_accuracy@deu.rst.pcc': 0.24481327800829875, 'eval_f1@deu.rst.pcc': 0.13750881013640803, 'eval_precision@deu.rst.pcc': 0.142080257791633, 'eval_recall@deu.rst.pcc': 0.1738039738039738, 'eval_loss@deu.rst.pcc': 2.5838980674743652, 'eval_runtime': 3.3835, 'eval_samples_per_second': 71.228, 'eval_steps_per_second': 2.364, 'epoch': 9.0}
{'train@deu.rst.pcc_loss': 2.371575355529785, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.32393715341959334, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.1879739126377865, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.24473724133956765, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.20961447663594263, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.371575117111206, 'train@deu.rst.pcc_runtime': 26.5269, 'train@deu.rst.pcc_samples_per_second': 81.578, 'train@deu.rst.pcc_steps_per_second': 2.563, 'epoch': 10.0}
{'loss': 2.4612, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.5657052993774414, 'eval_accuracy@deu.rst.pcc': 0.26556016597510373, 'eval_f1@deu.rst.pcc': 0.14151485264180036, 'eval_precision@deu.rst.pcc': 0.14637259816207185, 'eval_recall@deu.rst.pcc': 0.18293095793095795, 'eval_loss@deu.rst.pcc': 2.5657055377960205, 'eval_runtime': 3.3276, 'eval_samples_per_second': 72.424, 'eval_steps_per_second': 2.404, 'epoch': 10.0}
{'train@deu.rst.pcc_loss': 2.3620107173919678, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.32532347504621073, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.18891384470107417, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.24294070028172035, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.2107030020846043, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.3620104789733887, 'train@deu.rst.pcc_runtime': 26.5408, 'train@deu.rst.pcc_samples_per_second': 81.535, 'train@deu.rst.pcc_steps_per_second': 2.562, 'epoch': 11.0}
{'loss': 2.4565, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.558999538421631, 'eval_accuracy@deu.rst.pcc': 0.26556016597510373, 'eval_f1@deu.rst.pcc': 0.14763847903182659, 'eval_precision@deu.rst.pcc': 0.18525622433981567, 'eval_recall@deu.rst.pcc': 0.1851387501387501, 'eval_loss@deu.rst.pcc': 2.558999538421631, 'eval_runtime': 3.3568, 'eval_samples_per_second': 71.794, 'eval_steps_per_second': 2.383, 'epoch': 11.0}
{'train@deu.rst.pcc_loss': 2.35886287689209, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.32532347504621073, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.18876895532352495, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.24234207349313686, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.21069745568061535, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.35886287689209, 'train@deu.rst.pcc_runtime': 26.6163, 'train@deu.rst.pcc_samples_per_second': 81.304, 'train@deu.rst.pcc_steps_per_second': 2.555, 'epoch': 12.0}
{'loss': 2.4452, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.5563888549804688, 'eval_accuracy@deu.rst.pcc': 0.26141078838174275, 'eval_f1@deu.rst.pcc': 0.14601470328039126, 'eval_precision@deu.rst.pcc': 0.18398177065756588, 'eval_recall@deu.rst.pcc': 0.1829165279165279, 'eval_loss@deu.rst.pcc': 2.5563888549804688, 'eval_runtime': 4.3758, 'eval_samples_per_second': 55.075, 'eval_steps_per_second': 1.828, 'epoch': 12.0}
{'train_runtime': 1022.3102, 'train_samples_per_second': 25.401, 'train_steps_per_second': 0.798, 'train_loss': 2.727734771429324, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.1191
  train_runtime            = 5:32:51.08
  train_samples_per_second =      26.39
  train_steps_per_second   =      0.825
-------------------------------------------------------------------
Lang1:  eng.rst.gum    Lang2:  deu.rst.pcc
Saving run to:  runs/full_shot/FullShot=v4_eng.rst.gum_deu.rst.pcc_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 13897 examples
read 2149 examples
read 2091 examples
read 2164 examples
read 241 examples
read 260 examples
Total prediction labels:  32
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=32, bias=True)
    )
  )
)
{'train@eng.rst.gum_loss': 2.522075891494751, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.24206663308627763, 'train@eng.rst.gum_f1@eng.rst.gum': 0.0379622544167205, 'train@eng.rst.gum_precision@eng.rst.gum': 0.06334911718543433, 'train@eng.rst.gum_recall@eng.rst.gum': 0.05617174852060093, 'train@eng.rst.gum_loss@eng.rst.gum': 2.522075891494751, 'train@eng.rst.gum_runtime': 165.3924, 'train@eng.rst.gum_samples_per_second': 84.024, 'train@eng.rst.gum_steps_per_second': 2.63, 'epoch': 1.0}
{'loss': 2.7817, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.601700782775879, 'eval_accuracy@eng.rst.gum': 0.24057701256398326, 'eval_f1@eng.rst.gum': 0.038942079789508935, 'eval_precision@eng.rst.gum': 0.04544610111765564, 'eval_recall@eng.rst.gum': 0.05897614513823742, 'eval_loss@eng.rst.gum': 2.601700782775879, 'eval_runtime': 25.9068, 'eval_samples_per_second': 82.951, 'eval_steps_per_second': 2.625, 'epoch': 1.0}
{'train@eng.rst.gum_loss': 2.089137077331543, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.393394257753472, 'train@eng.rst.gum_f1@eng.rst.gum': 0.15266020631092977, 'train@eng.rst.gum_precision@eng.rst.gum': 0.21387179508456192, 'train@eng.rst.gum_recall@eng.rst.gum': 0.16608638705984902, 'train@eng.rst.gum_loss@eng.rst.gum': 2.089137315750122, 'train@eng.rst.gum_runtime': 165.3076, 'train@eng.rst.gum_samples_per_second': 84.068, 'train@eng.rst.gum_steps_per_second': 2.631, 'epoch': 2.0}
{'loss': 2.3671, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.185955286026001, 'eval_accuracy@eng.rst.gum': 0.3634248487668683, 'eval_f1@eng.rst.gum': 0.1418344435556008, 'eval_precision@eng.rst.gum': 0.17648374285819696, 'eval_recall@eng.rst.gum': 0.1601875205031849, 'eval_loss@eng.rst.gum': 2.18595552444458, 'eval_runtime': 25.8641, 'eval_samples_per_second': 83.088, 'eval_steps_per_second': 2.629, 'epoch': 2.0}
{'train@eng.rst.gum_loss': 1.8086618185043335, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.47485068719867596, 'train@eng.rst.gum_f1@eng.rst.gum': 0.26497049741938355, 'train@eng.rst.gum_precision@eng.rst.gum': 0.33159187527377, 'train@eng.rst.gum_recall@eng.rst.gum': 0.27467148664714697, 'train@eng.rst.gum_loss@eng.rst.gum': 1.808661937713623, 'train@eng.rst.gum_runtime': 165.1719, 'train@eng.rst.gum_samples_per_second': 84.137, 'train@eng.rst.gum_steps_per_second': 2.634, 'epoch': 3.0}
{'loss': 2.0095, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.9307194948196411, 'eval_accuracy@eng.rst.gum': 0.4448580735225686, 'eval_f1@eng.rst.gum': 0.2589047905957108, 'eval_precision@eng.rst.gum': 0.32619216126495165, 'eval_recall@eng.rst.gum': 0.26975855832283896, 'eval_loss@eng.rst.gum': 1.9307194948196411, 'eval_runtime': 25.8128, 'eval_samples_per_second': 83.253, 'eval_steps_per_second': 2.634, 'epoch': 3.0}
{'train@eng.rst.gum_loss': 1.6796071529388428, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5033460459091891, 'train@eng.rst.gum_f1@eng.rst.gum': 0.305373446347662, 'train@eng.rst.gum_precision@eng.rst.gum': 0.43612812402834955, 'train@eng.rst.gum_recall@eng.rst.gum': 0.31431298033274435, 'train@eng.rst.gum_loss@eng.rst.gum': 1.6796071529388428, 'train@eng.rst.gum_runtime': 165.2706, 'train@eng.rst.gum_samples_per_second': 84.086, 'train@eng.rst.gum_steps_per_second': 2.632, 'epoch': 4.0}
{'loss': 1.8142, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.832224726676941, 'eval_accuracy@eng.rst.gum': 0.4667287110283853, 'eval_f1@eng.rst.gum': 0.28864334441027667, 'eval_precision@eng.rst.gum': 0.3322032812381039, 'eval_recall@eng.rst.gum': 0.2995871278814578, 'eval_loss@eng.rst.gum': 1.8322248458862305, 'eval_runtime': 25.8821, 'eval_samples_per_second': 83.03, 'eval_steps_per_second': 2.627, 'epoch': 4.0}
{'train@eng.rst.gum_loss': 1.6040793657302856, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5238540692235735, 'train@eng.rst.gum_f1@eng.rst.gum': 0.35625113094212973, 'train@eng.rst.gum_precision@eng.rst.gum': 0.4674532883906641, 'train@eng.rst.gum_recall@eng.rst.gum': 0.36116981086888084, 'train@eng.rst.gum_loss@eng.rst.gum': 1.6040794849395752, 'train@eng.rst.gum_runtime': 165.274, 'train@eng.rst.gum_samples_per_second': 84.085, 'train@eng.rst.gum_steps_per_second': 2.632, 'epoch': 5.0}
{'loss': 1.7145, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.7718390226364136, 'eval_accuracy@eng.rst.gum': 0.4862726849697534, 'eval_f1@eng.rst.gum': 0.3404781471462911, 'eval_precision@eng.rst.gum': 0.38241526220176764, 'eval_recall@eng.rst.gum': 0.34942231453678557, 'eval_loss@eng.rst.gum': 1.771838903427124, 'eval_runtime': 25.874, 'eval_samples_per_second': 83.056, 'eval_steps_per_second': 2.628, 'epoch': 5.0}
{'train@eng.rst.gum_loss': 1.5501577854156494, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5394689501331222, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3881124830160581, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5417954947002547, 'train@eng.rst.gum_recall@eng.rst.gum': 0.38980426064379187, 'train@eng.rst.gum_loss@eng.rst.gum': 1.550157904624939, 'train@eng.rst.gum_runtime': 165.278, 'train@eng.rst.gum_samples_per_second': 84.083, 'train@eng.rst.gum_steps_per_second': 2.632, 'epoch': 6.0}
{'loss': 1.6473, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.7331360578536987, 'eval_accuracy@eng.rst.gum': 0.49790600279199626, 'eval_f1@eng.rst.gum': 0.3587545603919069, 'eval_precision@eng.rst.gum': 0.43410281007645546, 'eval_recall@eng.rst.gum': 0.36818418582631024, 'eval_loss@eng.rst.gum': 1.7331359386444092, 'eval_runtime': 25.8378, 'eval_samples_per_second': 83.173, 'eval_steps_per_second': 2.632, 'epoch': 6.0}
{'train@eng.rst.gum_loss': 1.5138496160507202, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5468806217169173, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3964346117511622, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5492767284631802, 'train@eng.rst.gum_recall@eng.rst.gum': 0.39993454706149184, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5138496160507202, 'train@eng.rst.gum_runtime': 165.3821, 'train@eng.rst.gum_samples_per_second': 84.03, 'train@eng.rst.gum_steps_per_second': 2.63, 'epoch': 7.0}
{'loss': 1.6067, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.7109286785125732, 'eval_accuracy@eng.rst.gum': 0.5002326663564448, 'eval_f1@eng.rst.gum': 0.3707798028129701, 'eval_precision@eng.rst.gum': 0.4624034953584469, 'eval_recall@eng.rst.gum': 0.3842874038318218, 'eval_loss@eng.rst.gum': 1.7109285593032837, 'eval_runtime': 25.864, 'eval_samples_per_second': 83.089, 'eval_steps_per_second': 2.629, 'epoch': 7.0}
{'train@eng.rst.gum_loss': 1.4893912076950073, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5532848816291286, 'train@eng.rst.gum_f1@eng.rst.gum': 0.408048192602568, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5384259324820893, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4093638026188161, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4893912076950073, 'train@eng.rst.gum_runtime': 165.3659, 'train@eng.rst.gum_samples_per_second': 84.038, 'train@eng.rst.gum_steps_per_second': 2.631, 'epoch': 8.0}
{'loss': 1.5727, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.6953320503234863, 'eval_accuracy@eng.rst.gum': 0.5072126570497906, 'eval_f1@eng.rst.gum': 0.38141993883564373, 'eval_precision@eng.rst.gum': 0.4563260956919427, 'eval_recall@eng.rst.gum': 0.39274241483636885, 'eval_loss@eng.rst.gum': 1.6953321695327759, 'eval_runtime': 25.914, 'eval_samples_per_second': 82.928, 'eval_steps_per_second': 2.624, 'epoch': 8.0}
{'train@eng.rst.gum_loss': 1.4679601192474365, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5605526372598403, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4225896692373209, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5406878950490409, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4237286343782122, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4679601192474365, 'train@eng.rst.gum_runtime': 165.3509, 'train@eng.rst.gum_samples_per_second': 84.046, 'train@eng.rst.gum_steps_per_second': 2.631, 'epoch': 9.0}
{'loss': 1.5482, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.6784886121749878, 'eval_accuracy@eng.rst.gum': 0.5095393206142392, 'eval_f1@eng.rst.gum': 0.38959070797148027, 'eval_precision@eng.rst.gum': 0.4785294522238727, 'eval_recall@eng.rst.gum': 0.40188516614696335, 'eval_loss@eng.rst.gum': 1.6784886121749878, 'eval_runtime': 25.897, 'eval_samples_per_second': 82.983, 'eval_steps_per_second': 2.626, 'epoch': 9.0}
{'train@eng.rst.gum_loss': 1.4529308080673218, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5638627041807585, 'train@eng.rst.gum_f1@eng.rst.gum': 0.42601282559808434, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5279533355932188, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4234105864297579, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4529308080673218, 'train@eng.rst.gum_runtime': 165.3725, 'train@eng.rst.gum_samples_per_second': 84.035, 'train@eng.rst.gum_steps_per_second': 2.63, 'epoch': 10.0}
{'loss': 1.5303, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.6704862117767334, 'eval_accuracy@eng.rst.gum': 0.5095393206142392, 'eval_f1@eng.rst.gum': 0.3888969273373798, 'eval_precision@eng.rst.gum': 0.47681948764369464, 'eval_recall@eng.rst.gum': 0.39820700759245503, 'eval_loss@eng.rst.gum': 1.670486330986023, 'eval_runtime': 25.8728, 'eval_samples_per_second': 83.06, 'eval_steps_per_second': 2.628, 'epoch': 10.0}
{'train@eng.rst.gum_loss': 1.4456616640090942, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5673886450313017, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4323761691034894, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5327202496209922, 'train@eng.rst.gum_recall@eng.rst.gum': 0.43114740871880775, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4456616640090942, 'train@eng.rst.gum_runtime': 165.3494, 'train@eng.rst.gum_samples_per_second': 84.046, 'train@eng.rst.gum_steps_per_second': 2.631, 'epoch': 11.0}
{'loss': 1.5263, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.6648811101913452, 'eval_accuracy@eng.rst.gum': 0.5146579804560261, 'eval_f1@eng.rst.gum': 0.3957668031804711, 'eval_precision@eng.rst.gum': 0.4822873795595294, 'eval_recall@eng.rst.gum': 0.40626953391431936, 'eval_loss@eng.rst.gum': 1.6648809909820557, 'eval_runtime': 25.8621, 'eval_samples_per_second': 83.095, 'eval_steps_per_second': 2.629, 'epoch': 11.0}
{'train@eng.rst.gum_loss': 1.4426236152648926, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5683240987263438, 'train@eng.rst.gum_f1@eng.rst.gum': 0.43309171135200564, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5318940144511626, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4307762610946539, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4426236152648926, 'train@eng.rst.gum_runtime': 165.3238, 'train@eng.rst.gum_samples_per_second': 84.059, 'train@eng.rst.gum_steps_per_second': 2.631, 'epoch': 12.0}
{'loss': 1.5125, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.6634348630905151, 'eval_accuracy@eng.rst.gum': 0.5165193113075849, 'eval_f1@eng.rst.gum': 0.3943966209655344, 'eval_precision@eng.rst.gum': 0.48112992741349003, 'eval_recall@eng.rst.gum': 0.4046881518339119, 'eval_loss@eng.rst.gum': 1.6634348630905151, 'eval_runtime': 25.8734, 'eval_samples_per_second': 83.058, 'eval_steps_per_second': 2.628, 'epoch': 12.0}
{'train_runtime': 6459.3354, 'train_samples_per_second': 25.818, 'train_steps_per_second': 0.808, 'train_loss': 1.8025926611889367, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.8026
  train_runtime            = 1:47:39.33
  train_samples_per_second =     25.818
  train_steps_per_second   =      0.808
{'train@deu.rst.pcc_loss': 3.0100607872009277, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.10767097966728281, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.04009617535275207, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.03967935359736903, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.0552340971661985, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.0100607872009277, 'train@deu.rst.pcc_runtime': 26.4426, 'train@deu.rst.pcc_samples_per_second': 81.838, 'train@deu.rst.pcc_steps_per_second': 2.572, 'epoch': 1.0}
{'loss': 3.4271, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.0558884143829346, 'eval_accuracy@deu.rst.pcc': 0.1078838174273859, 'eval_f1@deu.rst.pcc': 0.04608079269183374, 'eval_precision@deu.rst.pcc': 0.04712130146657681, 'eval_recall@deu.rst.pcc': 0.06759386446886446, 'eval_loss@deu.rst.pcc': 3.0558884143829346, 'eval_runtime': 3.2439, 'eval_samples_per_second': 74.293, 'eval_steps_per_second': 2.466, 'epoch': 1.0}
{'train@deu.rst.pcc_loss': 2.7823474407196045, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.18484288354898337, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08356924022919282, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.10610619841781946, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.10815945910255749, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7823469638824463, 'train@deu.rst.pcc_runtime': 26.4681, 'train@deu.rst.pcc_samples_per_second': 81.759, 'train@deu.rst.pcc_steps_per_second': 2.569, 'epoch': 2.0}
{'loss': 2.9224, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.8357455730438232, 'eval_accuracy@deu.rst.pcc': 0.18672199170124482, 'eval_f1@deu.rst.pcc': 0.07601544555523297, 'eval_precision@deu.rst.pcc': 0.07010306560799172, 'eval_recall@deu.rst.pcc': 0.1136421448921449, 'eval_loss@deu.rst.pcc': 2.835745096206665, 'eval_runtime': 3.228, 'eval_samples_per_second': 74.659, 'eval_steps_per_second': 2.478, 'epoch': 2.0}
{'train@deu.rst.pcc_loss': 2.663480758666992, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.21164510166358594, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09867139398860648, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.12160497280537565, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.13049126685445597, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6634809970855713, 'train@deu.rst.pcc_runtime': 26.4688, 'train@deu.rst.pcc_samples_per_second': 81.757, 'train@deu.rst.pcc_steps_per_second': 2.569, 'epoch': 3.0}
{'loss': 2.7722, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.7379448413848877, 'eval_accuracy@deu.rst.pcc': 0.2074688796680498, 'eval_f1@deu.rst.pcc': 0.08715681059275278, 'eval_precision@deu.rst.pcc': 0.07086265814035991, 'eval_recall@deu.rst.pcc': 0.14028795278795278, 'eval_loss@deu.rst.pcc': 2.7379448413848877, 'eval_runtime': 3.2257, 'eval_samples_per_second': 74.712, 'eval_steps_per_second': 2.48, 'epoch': 3.0}
{'train@deu.rst.pcc_loss': 2.578786611557007, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.24075785582255083, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.12423264778742575, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.17696467629700185, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.1542854531260596, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.578786611557007, 'train@deu.rst.pcc_runtime': 26.4791, 'train@deu.rst.pcc_samples_per_second': 81.725, 'train@deu.rst.pcc_steps_per_second': 2.568, 'epoch': 4.0}
{'loss': 2.6939, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.663990020751953, 'eval_accuracy@deu.rst.pcc': 0.21161825726141079, 'eval_f1@deu.rst.pcc': 0.09965324546717187, 'eval_precision@deu.rst.pcc': 0.0884736112984786, 'eval_recall@deu.rst.pcc': 0.15098616661116662, 'eval_loss@deu.rst.pcc': 2.663990020751953, 'eval_runtime': 3.2397, 'eval_samples_per_second': 74.389, 'eval_steps_per_second': 2.469, 'epoch': 4.0}
{'train@deu.rst.pcc_loss': 2.5175509452819824, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2634011090573013, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.14179995818482147, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.18216599813707515, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.17164351007747852, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.5175509452819824, 'train@deu.rst.pcc_runtime': 26.4928, 'train@deu.rst.pcc_samples_per_second': 81.682, 'train@deu.rst.pcc_steps_per_second': 2.567, 'epoch': 5.0}
{'loss': 2.6158, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.6090431213378906, 'eval_accuracy@deu.rst.pcc': 0.24066390041493776, 'eval_f1@deu.rst.pcc': 0.12633208855796574, 'eval_precision@deu.rst.pcc': 0.12125951246886207, 'eval_recall@deu.rst.pcc': 0.17367586117586117, 'eval_loss@deu.rst.pcc': 2.6090428829193115, 'eval_runtime': 3.2402, 'eval_samples_per_second': 74.379, 'eval_steps_per_second': 2.469, 'epoch': 5.0}
{'train@deu.rst.pcc_loss': 2.469003200531006, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2809611829944547, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.15621508876950702, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.18603547213037264, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.18664748137894405, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.4690029621124268, 'train@deu.rst.pcc_runtime': 26.5078, 'train@deu.rst.pcc_samples_per_second': 81.636, 'train@deu.rst.pcc_steps_per_second': 2.565, 'epoch': 6.0}
{'loss': 2.5632, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.5689191818237305, 'eval_accuracy@deu.rst.pcc': 0.26556016597510373, 'eval_f1@deu.rst.pcc': 0.14820492678398092, 'eval_precision@deu.rst.pcc': 0.13397095035026071, 'eval_recall@deu.rst.pcc': 0.19600075850075852, 'eval_loss@deu.rst.pcc': 2.5689187049865723, 'eval_runtime': 3.2359, 'eval_samples_per_second': 74.477, 'eval_steps_per_second': 2.472, 'epoch': 6.0}
{'train@deu.rst.pcc_loss': 2.432098865509033, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2934380776340111, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.16680194407768265, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.19148631756652879, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.1960048741737506, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.432098627090454, 'train@deu.rst.pcc_runtime': 26.4732, 'train@deu.rst.pcc_samples_per_second': 81.743, 'train@deu.rst.pcc_steps_per_second': 2.569, 'epoch': 7.0}
{'loss': 2.5227, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.539235830307007, 'eval_accuracy@deu.rst.pcc': 0.2821576763485477, 'eval_f1@deu.rst.pcc': 0.15503188893459183, 'eval_precision@deu.rst.pcc': 0.13932304948918872, 'eval_recall@deu.rst.pcc': 0.204252923002923, 'eval_loss@deu.rst.pcc': 2.539236068725586, 'eval_runtime': 3.2446, 'eval_samples_per_second': 74.278, 'eval_steps_per_second': 2.466, 'epoch': 7.0}
{'train@deu.rst.pcc_loss': 2.4053680896759033, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.3036044362292052, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.17292120010130946, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.1973409292204076, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.20452460289461877, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.4053680896759033, 'train@deu.rst.pcc_runtime': 26.487, 'train@deu.rst.pcc_samples_per_second': 81.7, 'train@deu.rst.pcc_steps_per_second': 2.567, 'epoch': 8.0}
{'loss': 2.4934, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.5189919471740723, 'eval_accuracy@deu.rst.pcc': 0.29045643153526973, 'eval_f1@deu.rst.pcc': 0.15862445615471096, 'eval_precision@deu.rst.pcc': 0.1404890672599006, 'eval_recall@deu.rst.pcc': 0.21035561660561664, 'eval_loss@deu.rst.pcc': 2.5189919471740723, 'eval_runtime': 3.219, 'eval_samples_per_second': 74.867, 'eval_steps_per_second': 2.485, 'epoch': 8.0}
{'train@deu.rst.pcc_loss': 2.382777690887451, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.3096118299445471, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.17818879406650673, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.22272334050757625, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.2083652925227821, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.3827779293060303, 'train@deu.rst.pcc_runtime': 26.4784, 'train@deu.rst.pcc_samples_per_second': 81.727, 'train@deu.rst.pcc_steps_per_second': 2.568, 'epoch': 9.0}
{'loss': 2.4663, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.5066521167755127, 'eval_accuracy@deu.rst.pcc': 0.29045643153526973, 'eval_f1@deu.rst.pcc': 0.15759399198513846, 'eval_precision@deu.rst.pcc': 0.13476755862720777, 'eval_recall@deu.rst.pcc': 0.21207264957264957, 'eval_loss@deu.rst.pcc': 2.506652355194092, 'eval_runtime': 3.2333, 'eval_samples_per_second': 74.538, 'eval_steps_per_second': 2.474, 'epoch': 9.0}
{'train@deu.rst.pcc_loss': 2.3681321144104004, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.316543438077634, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.1831046167047891, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.20763657204981806, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.2124679977892409, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.3681318759918213, 'train@deu.rst.pcc_runtime': 26.4967, 'train@deu.rst.pcc_samples_per_second': 81.671, 'train@deu.rst.pcc_steps_per_second': 2.566, 'epoch': 10.0}
{'loss': 2.4451, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.4924368858337402, 'eval_accuracy@deu.rst.pcc': 0.2987551867219917, 'eval_f1@deu.rst.pcc': 0.16227978483525454, 'eval_precision@deu.rst.pcc': 0.14410397838029418, 'eval_recall@deu.rst.pcc': 0.21504884004884003, 'eval_loss@deu.rst.pcc': 2.4924371242523193, 'eval_runtime': 3.2106, 'eval_samples_per_second': 75.063, 'eval_steps_per_second': 2.492, 'epoch': 10.0}
{'train@deu.rst.pcc_loss': 2.359920024871826, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.316543438077634, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.18380244649393918, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.2240816459264949, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.21313143277223373, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.359920024871826, 'train@deu.rst.pcc_runtime': 26.477, 'train@deu.rst.pcc_samples_per_second': 81.731, 'train@deu.rst.pcc_steps_per_second': 2.568, 'epoch': 11.0}
{'loss': 2.4348, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.486483097076416, 'eval_accuracy@deu.rst.pcc': 0.2987551867219917, 'eval_f1@deu.rst.pcc': 0.16248073101811547, 'eval_precision@deu.rst.pcc': 0.14118487506645402, 'eval_recall@deu.rst.pcc': 0.21587555962555963, 'eval_loss@deu.rst.pcc': 2.486482858657837, 'eval_runtime': 3.2244, 'eval_samples_per_second': 74.742, 'eval_steps_per_second': 2.481, 'epoch': 11.0}
{'train@deu.rst.pcc_loss': 2.35713529586792, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.3183918669131238, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.18462636615769584, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.21935966689597666, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.21414983616157446, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.357135057449341, 'train@deu.rst.pcc_runtime': 26.4782, 'train@deu.rst.pcc_samples_per_second': 81.728, 'train@deu.rst.pcc_steps_per_second': 2.568, 'epoch': 12.0}
{'loss': 2.4271, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.4842159748077393, 'eval_accuracy@deu.rst.pcc': 0.2987551867219917, 'eval_f1@deu.rst.pcc': 0.16239195250603825, 'eval_precision@deu.rst.pcc': 0.14339528040185934, 'eval_recall@deu.rst.pcc': 0.21504884004884003, 'eval_loss@deu.rst.pcc': 2.4842159748077393, 'eval_runtime': 3.24, 'eval_samples_per_second': 74.383, 'eval_steps_per_second': 2.469, 'epoch': 12.0}
{'train_runtime': 1019.196, 'train_samples_per_second': 25.479, 'train_steps_per_second': 0.801, 'train_loss': 2.648651440938314, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.8026
  train_runtime            = 1:47:39.33
  train_samples_per_second =     25.818
  train_steps_per_second   =      0.808
-------------------------------------------------------------------
Lang1:  eng.rst.rstdt    Lang2:  deu.rst.pcc
Saving run to:  runs/full_shot/FullShot=v4_eng.rst.rstdt_deu.rst.pcc_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 16002 examples
read 1621 examples
read 2155 examples
read 2164 examples
read 241 examples
read 260 examples
Total prediction labels:  36
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=36, bias=True)
    )
  )
)
{'train@eng.rst.rstdt_loss': 1.7753747701644897, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.5067491563554556, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.08237550183952455, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.0875858808327398, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.10993446341577207, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.7753747701644897, 'train@eng.rst.rstdt_runtime': 190.2593, 'train@eng.rst.rstdt_samples_per_second': 84.106, 'train@eng.rst.rstdt_steps_per_second': 2.633, 'epoch': 1.0}
{'loss': 2.2104, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.7534499168395996, 'eval_accuracy@eng.rst.rstdt': 0.5206662553979026, 'eval_f1@eng.rst.rstdt': 0.08327325647638875, 'eval_precision@eng.rst.rstdt': 0.09484247520354841, 'eval_recall@eng.rst.rstdt': 0.1092191838173424, 'eval_loss@eng.rst.rstdt': 1.75344979763031, 'eval_runtime': 19.6333, 'eval_samples_per_second': 82.564, 'eval_steps_per_second': 2.598, 'epoch': 1.0}
{'train@eng.rst.rstdt_loss': 1.4313310384750366, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6022372203474565, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.19964648184028616, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.28917571856807206, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.21288637190492407, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.431330919265747, 'train@eng.rst.rstdt_runtime': 190.8604, 'train@eng.rst.rstdt_samples_per_second': 83.841, 'train@eng.rst.rstdt_steps_per_second': 2.625, 'epoch': 2.0}
{'loss': 1.6227, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.4304052591323853, 'eval_accuracy@eng.rst.rstdt': 0.6132017273288094, 'eval_f1@eng.rst.rstdt': 0.20250137272553417, 'eval_precision@eng.rst.rstdt': 0.28297255271547117, 'eval_recall@eng.rst.rstdt': 0.21335190251682704, 'eval_loss@eng.rst.rstdt': 1.4304052591323853, 'eval_runtime': 19.6485, 'eval_samples_per_second': 82.5, 'eval_steps_per_second': 2.596, 'epoch': 2.0}
{'train@eng.rst.rstdt_loss': 1.32119882106781, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6262967129108862, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.24760693856143423, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.3949927863026012, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.2482967932673604, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.32119882106781, 'train@eng.rst.rstdt_runtime': 190.2853, 'train@eng.rst.rstdt_samples_per_second': 84.095, 'train@eng.rst.rstdt_steps_per_second': 2.633, 'epoch': 3.0}
{'loss': 1.4328, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.3431652784347534, 'eval_accuracy@eng.rst.rstdt': 0.6310919185687847, 'eval_f1@eng.rst.rstdt': 0.24102362171122818, 'eval_precision@eng.rst.rstdt': 0.3129030353241711, 'eval_recall@eng.rst.rstdt': 0.24583574668670635, 'eval_loss@eng.rst.rstdt': 1.3431655168533325, 'eval_runtime': 19.729, 'eval_samples_per_second': 82.164, 'eval_steps_per_second': 2.585, 'epoch': 3.0}
{'train@eng.rst.rstdt_loss': 1.253199815750122, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6428571428571429, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3207484179865172, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.49601728082238583, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3078155783269462, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.2531996965408325, 'train@eng.rst.rstdt_runtime': 190.6739, 'train@eng.rst.rstdt_samples_per_second': 83.923, 'train@eng.rst.rstdt_steps_per_second': 2.628, 'epoch': 4.0}
{'loss': 1.3376, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.3010389804840088, 'eval_accuracy@eng.rst.rstdt': 0.6366440468846392, 'eval_f1@eng.rst.rstdt': 0.3046695793977781, 'eval_precision@eng.rst.rstdt': 0.421293639826494, 'eval_recall@eng.rst.rstdt': 0.306237916367125, 'eval_loss@eng.rst.rstdt': 1.3010390996932983, 'eval_runtime': 19.6422, 'eval_samples_per_second': 82.526, 'eval_steps_per_second': 2.596, 'epoch': 4.0}
{'train@eng.rst.rstdt_loss': 1.2019598484039307, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6555430571178603, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.35041956459641327, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5403220924035278, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3291574138680698, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.2019599676132202, 'train@eng.rst.rstdt_runtime': 190.6849, 'train@eng.rst.rstdt_samples_per_second': 83.919, 'train@eng.rst.rstdt_steps_per_second': 2.627, 'epoch': 5.0}
{'loss': 1.2789, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.2593623399734497, 'eval_accuracy@eng.rst.rstdt': 0.6421961752004935, 'eval_f1@eng.rst.rstdt': 0.31835096866760637, 'eval_precision@eng.rst.rstdt': 0.4178786929827377, 'eval_recall@eng.rst.rstdt': 0.3139810752641644, 'eval_loss@eng.rst.rstdt': 1.2593623399734497, 'eval_runtime': 19.6415, 'eval_samples_per_second': 82.529, 'eval_steps_per_second': 2.597, 'epoch': 5.0}
{'train@eng.rst.rstdt_loss': 1.1672642230987549, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6626671666041745, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3747563300282497, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5288680783847419, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3461469265057166, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1672641038894653, 'train@eng.rst.rstdt_runtime': 190.3096, 'train@eng.rst.rstdt_samples_per_second': 84.084, 'train@eng.rst.rstdt_steps_per_second': 2.633, 'epoch': 6.0}
{'loss': 1.2366, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.233336091041565, 'eval_accuracy@eng.rst.rstdt': 0.6477483035163479, 'eval_f1@eng.rst.rstdt': 0.3299914834762148, 'eval_precision@eng.rst.rstdt': 0.41552208164506216, 'eval_recall@eng.rst.rstdt': 0.32684664619491605, 'eval_loss@eng.rst.rstdt': 1.2333359718322754, 'eval_runtime': 19.5731, 'eval_samples_per_second': 82.818, 'eval_steps_per_second': 2.606, 'epoch': 6.0}
{'train@eng.rst.rstdt_loss': 1.1470446586608887, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6649793775778028, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3816460486934956, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5925704972867569, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3497968483749651, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1470445394515991, 'train@eng.rst.rstdt_runtime': 190.6704, 'train@eng.rst.rstdt_samples_per_second': 83.925, 'train@eng.rst.rstdt_steps_per_second': 2.628, 'epoch': 7.0}
{'loss': 1.2127, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.2153956890106201, 'eval_accuracy@eng.rst.rstdt': 0.6502159161011721, 'eval_f1@eng.rst.rstdt': 0.3457141945950955, 'eval_precision@eng.rst.rstdt': 0.4738443774106224, 'eval_recall@eng.rst.rstdt': 0.33589775220960505, 'eval_loss@eng.rst.rstdt': 1.2153956890106201, 'eval_runtime': 19.6749, 'eval_samples_per_second': 82.389, 'eval_steps_per_second': 2.592, 'epoch': 7.0}
{'train@eng.rst.rstdt_loss': 1.1299372911453247, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6681039870016248, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3940164446706953, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6415604588983224, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3621496473667252, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1299372911453247, 'train@eng.rst.rstdt_runtime': 190.7915, 'train@eng.rst.rstdt_samples_per_second': 83.872, 'train@eng.rst.rstdt_steps_per_second': 2.626, 'epoch': 8.0}
{'loss': 1.1904, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.2096130847930908, 'eval_accuracy@eng.rst.rstdt': 0.6477483035163479, 'eval_f1@eng.rst.rstdt': 0.3489978636555649, 'eval_precision@eng.rst.rstdt': 0.47356635494251137, 'eval_recall@eng.rst.rstdt': 0.33995414158776033, 'eval_loss@eng.rst.rstdt': 1.2096130847930908, 'eval_runtime': 19.6103, 'eval_samples_per_second': 82.661, 'eval_steps_per_second': 2.601, 'epoch': 8.0}
{'train@eng.rst.rstdt_loss': 1.1184407472610474, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6701037370328708, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.40010093071129693, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6334929913668008, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3662603399296177, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1184407472610474, 'train@eng.rst.rstdt_runtime': 190.2688, 'train@eng.rst.rstdt_samples_per_second': 84.102, 'train@eng.rst.rstdt_steps_per_second': 2.633, 'epoch': 9.0}
{'loss': 1.1752, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.200185775756836, 'eval_accuracy@eng.rst.rstdt': 0.6452806909315237, 'eval_f1@eng.rst.rstdt': 0.34716238908418784, 'eval_precision@eng.rst.rstdt': 0.4617391535887111, 'eval_recall@eng.rst.rstdt': 0.33895955426478985, 'eval_loss@eng.rst.rstdt': 1.2001858949661255, 'eval_runtime': 19.6115, 'eval_samples_per_second': 82.656, 'eval_steps_per_second': 2.601, 'epoch': 9.0}
{'train@eng.rst.rstdt_loss': 1.1129968166351318, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6704786901637295, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4114725879439951, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6241883290169667, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.37782807284501196, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1129968166351318, 'train@eng.rst.rstdt_runtime': 190.7583, 'train@eng.rst.rstdt_samples_per_second': 83.886, 'train@eng.rst.rstdt_steps_per_second': 2.626, 'epoch': 10.0}
{'loss': 1.1632, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.2047621011734009, 'eval_accuracy@eng.rst.rstdt': 0.6409623689080814, 'eval_f1@eng.rst.rstdt': 0.35529349575257746, 'eval_precision@eng.rst.rstdt': 0.45378178840969097, 'eval_recall@eng.rst.rstdt': 0.34983180266397995, 'eval_loss@eng.rst.rstdt': 1.2047622203826904, 'eval_runtime': 19.6602, 'eval_samples_per_second': 82.451, 'eval_steps_per_second': 2.594, 'epoch': 10.0}
{'train@eng.rst.rstdt_loss': 1.1061731576919556, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6718535183102112, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.40936597738083214, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6335817196598713, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3736816115449252, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1061732769012451, 'train@eng.rst.rstdt_runtime': 190.2697, 'train@eng.rst.rstdt_samples_per_second': 84.102, 'train@eng.rst.rstdt_steps_per_second': 2.633, 'epoch': 11.0}
{'loss': 1.1563, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.196331262588501, 'eval_accuracy@eng.rst.rstdt': 0.6465144972239358, 'eval_f1@eng.rst.rstdt': 0.35969447785289743, 'eval_precision@eng.rst.rstdt': 0.4857829308725608, 'eval_recall@eng.rst.rstdt': 0.3493699886312171, 'eval_loss@eng.rst.rstdt': 1.1963313817977905, 'eval_runtime': 19.5867, 'eval_samples_per_second': 82.76, 'eval_steps_per_second': 2.604, 'epoch': 11.0}
{'train@eng.rst.rstdt_loss': 1.1046127080917358, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6721034870641169, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.40953455627754565, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6340769113045264, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.37358771025317933, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1046127080917358, 'train@eng.rst.rstdt_runtime': 190.7423, 'train@eng.rst.rstdt_samples_per_second': 83.893, 'train@eng.rst.rstdt_steps_per_second': 2.627, 'epoch': 12.0}
{'loss': 1.1548, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.194517731666565, 'eval_accuracy@eng.rst.rstdt': 0.6458975940777298, 'eval_f1@eng.rst.rstdt': 0.3588582531378249, 'eval_precision@eng.rst.rstdt': 0.48551688270709903, 'eval_recall@eng.rst.rstdt': 0.34794383661162087, 'eval_loss@eng.rst.rstdt': 1.194517731666565, 'eval_runtime': 19.6714, 'eval_samples_per_second': 82.404, 'eval_steps_per_second': 2.593, 'epoch': 12.0}
{'train_runtime': 7320.8916, 'train_samples_per_second': 26.23, 'train_steps_per_second': 0.821, 'train_loss': 1.347624341567516, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.3476
  train_runtime            = 2:02:00.89
  train_samples_per_second =      26.23
  train_steps_per_second   =      0.821
{'train@deu.rst.pcc_loss': 3.0722076892852783, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.10166358595194085, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.026783478805425857, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.07711877419277251, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.04569323346764805, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.0722076892852783, 'train@deu.rst.pcc_runtime': 26.5084, 'train@deu.rst.pcc_samples_per_second': 81.635, 'train@deu.rst.pcc_steps_per_second': 2.565, 'epoch': 1.0}
{'loss': 3.4939, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.0481088161468506, 'eval_accuracy@deu.rst.pcc': 0.07468879668049792, 'eval_f1@deu.rst.pcc': 0.03159982135748891, 'eval_precision@deu.rst.pcc': 0.056749017266258646, 'eval_recall@deu.rst.pcc': 0.04540771728271728, 'eval_loss@deu.rst.pcc': 3.0481090545654297, 'eval_runtime': 3.2387, 'eval_samples_per_second': 74.413, 'eval_steps_per_second': 2.47, 'epoch': 1.0}
{'train@deu.rst.pcc_loss': 2.832735538482666, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.15434380776340112, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.055409349194501356, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.08570647328838552, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.08164467387725782, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.832735300064087, 'train@deu.rst.pcc_runtime': 26.5133, 'train@deu.rst.pcc_samples_per_second': 81.619, 'train@deu.rst.pcc_steps_per_second': 2.565, 'epoch': 2.0}
{'loss': 2.9872, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.8436875343322754, 'eval_accuracy@deu.rst.pcc': 0.15352697095435686, 'eval_f1@deu.rst.pcc': 0.05333581349206349, 'eval_precision@deu.rst.pcc': 0.05683475584591204, 'eval_recall@deu.rst.pcc': 0.09051307488807488, 'eval_loss@deu.rst.pcc': 2.8436875343322754, 'eval_runtime': 3.25, 'eval_samples_per_second': 74.155, 'eval_steps_per_second': 2.462, 'epoch': 2.0}
{'train@deu.rst.pcc_loss': 2.711089849472046, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.19593345656192238, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.07882719194717323, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.11369596264793465, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.11463306174969173, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.711089849472046, 'train@deu.rst.pcc_runtime': 26.5363, 'train@deu.rst.pcc_samples_per_second': 81.549, 'train@deu.rst.pcc_steps_per_second': 2.563, 'epoch': 3.0}
{'loss': 2.8224, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.7570908069610596, 'eval_accuracy@deu.rst.pcc': 0.16182572614107885, 'eval_f1@deu.rst.pcc': 0.06984233543385972, 'eval_precision@deu.rst.pcc': 0.06455551588224222, 'eval_recall@deu.rst.pcc': 0.11628764753764753, 'eval_loss@deu.rst.pcc': 2.7570905685424805, 'eval_runtime': 3.2591, 'eval_samples_per_second': 73.947, 'eval_steps_per_second': 2.455, 'epoch': 3.0}
{'train@deu.rst.pcc_loss': 2.6236209869384766, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.20609981515711645, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08681203969809939, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.1615173764797799, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.12324571322642336, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6236212253570557, 'train@deu.rst.pcc_runtime': 27.2708, 'train@deu.rst.pcc_samples_per_second': 79.352, 'train@deu.rst.pcc_steps_per_second': 2.494, 'epoch': 4.0}
{'loss': 2.7308, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.680176258087158, 'eval_accuracy@deu.rst.pcc': 0.1908713692946058, 'eval_f1@deu.rst.pcc': 0.0878171137243247, 'eval_precision@deu.rst.pcc': 0.11128670861937451, 'eval_recall@deu.rst.pcc': 0.1425281894031894, 'eval_loss@deu.rst.pcc': 2.680176019668579, 'eval_runtime': 3.2458, 'eval_samples_per_second': 74.25, 'eval_steps_per_second': 2.465, 'epoch': 4.0}
{'train@deu.rst.pcc_loss': 2.5617594718933105, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.22966728280961182, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.11095988001183144, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.17476132189281215, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.14011158258220258, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.5617592334747314, 'train@deu.rst.pcc_runtime': 26.5332, 'train@deu.rst.pcc_samples_per_second': 81.558, 'train@deu.rst.pcc_steps_per_second': 2.563, 'epoch': 5.0}
{'loss': 2.6613, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.6252970695495605, 'eval_accuracy@deu.rst.pcc': 0.21991701244813278, 'eval_f1@deu.rst.pcc': 0.12044458578019525, 'eval_precision@deu.rst.pcc': 0.17154010580734716, 'eval_recall@deu.rst.pcc': 0.16493171180671182, 'eval_loss@deu.rst.pcc': 2.6252965927124023, 'eval_runtime': 3.2658, 'eval_samples_per_second': 73.794, 'eval_steps_per_second': 2.45, 'epoch': 5.0}
{'train@deu.rst.pcc_loss': 2.510552167892456, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2555452865064695, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.14894642180935538, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.23113524198172597, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.1660199315295127, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.510552167892456, 'train@deu.rst.pcc_runtime': 26.5215, 'train@deu.rst.pcc_samples_per_second': 81.594, 'train@deu.rst.pcc_steps_per_second': 2.564, 'epoch': 6.0}
{'loss': 2.608, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.5790889263153076, 'eval_accuracy@deu.rst.pcc': 0.23651452282157676, 'eval_f1@deu.rst.pcc': 0.1374200387632962, 'eval_precision@deu.rst.pcc': 0.1612812823750324, 'eval_recall@deu.rst.pcc': 0.18146147833647833, 'eval_loss@deu.rst.pcc': 2.5790891647338867, 'eval_runtime': 3.2554, 'eval_samples_per_second': 74.031, 'eval_steps_per_second': 2.457, 'epoch': 6.0}
{'train@deu.rst.pcc_loss': 2.473174810409546, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2754158964879852, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.17482109301891968, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.23441105256386274, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.18773029166435468, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.473175048828125, 'train@deu.rst.pcc_runtime': 26.5207, 'train@deu.rst.pcc_samples_per_second': 81.597, 'train@deu.rst.pcc_steps_per_second': 2.564, 'epoch': 7.0}
{'loss': 2.5727, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.5494651794433594, 'eval_accuracy@deu.rst.pcc': 0.2572614107883817, 'eval_f1@deu.rst.pcc': 0.172614626092887, 'eval_precision@deu.rst.pcc': 0.22144635837212265, 'eval_recall@deu.rst.pcc': 0.20494632682132682, 'eval_loss@deu.rst.pcc': 2.549464702606201, 'eval_runtime': 3.2805, 'eval_samples_per_second': 73.464, 'eval_steps_per_second': 2.439, 'epoch': 7.0}
{'train@deu.rst.pcc_loss': 2.445178985595703, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2980591497227357, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.189469492944683, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.22850159869655243, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.20613129225393326, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.4451794624328613, 'train@deu.rst.pcc_runtime': 26.5124, 'train@deu.rst.pcc_samples_per_second': 81.622, 'train@deu.rst.pcc_steps_per_second': 2.565, 'epoch': 8.0}
{'loss': 2.5232, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.5259969234466553, 'eval_accuracy@deu.rst.pcc': 0.27385892116182575, 'eval_f1@deu.rst.pcc': 0.18520687246046433, 'eval_precision@deu.rst.pcc': 0.224582575737467, 'eval_recall@deu.rst.pcc': 0.2195335505997271, 'eval_loss@deu.rst.pcc': 2.5259971618652344, 'eval_runtime': 3.2731, 'eval_samples_per_second': 73.63, 'eval_steps_per_second': 2.444, 'epoch': 8.0}
{'train@deu.rst.pcc_loss': 2.4206857681274414, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.3036044362292052, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.19371978500917747, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.23129145003203314, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.2109731294100952, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.4206857681274414, 'train@deu.rst.pcc_runtime': 26.488, 'train@deu.rst.pcc_samples_per_second': 81.697, 'train@deu.rst.pcc_steps_per_second': 2.567, 'epoch': 9.0}
{'loss': 2.5016, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.510928153991699, 'eval_accuracy@deu.rst.pcc': 0.26141078838174275, 'eval_f1@deu.rst.pcc': 0.18052291957316466, 'eval_precision@deu.rst.pcc': 0.22473314604519054, 'eval_recall@deu.rst.pcc': 0.21410637973137972, 'eval_loss@deu.rst.pcc': 2.5109286308288574, 'eval_runtime': 3.2554, 'eval_samples_per_second': 74.032, 'eval_steps_per_second': 2.457, 'epoch': 9.0}
{'train@deu.rst.pcc_loss': 2.4049246311187744, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.3109981515711645, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.20017437175801972, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.23331073126723942, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.2181894410705361, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.4049246311187744, 'train@deu.rst.pcc_runtime': 26.4834, 'train@deu.rst.pcc_samples_per_second': 81.712, 'train@deu.rst.pcc_steps_per_second': 2.568, 'epoch': 10.0}
{'loss': 2.4872, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.4947428703308105, 'eval_accuracy@deu.rst.pcc': 0.26556016597510373, 'eval_f1@deu.rst.pcc': 0.18319890151172122, 'eval_precision@deu.rst.pcc': 0.2275423407002354, 'eval_recall@deu.rst.pcc': 0.21655736012353657, 'eval_loss@deu.rst.pcc': 2.4947423934936523, 'eval_runtime': 3.264, 'eval_samples_per_second': 73.836, 'eval_steps_per_second': 2.451, 'epoch': 10.0}
{'train@deu.rst.pcc_loss': 2.395700454711914, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.3123844731977819, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.20363842085354186, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.2354643876254047, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.22154889946221837, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.395700693130493, 'train@deu.rst.pcc_runtime': 26.4832, 'train@deu.rst.pcc_samples_per_second': 81.712, 'train@deu.rst.pcc_steps_per_second': 2.568, 'epoch': 11.0}
{'loss': 2.4732, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.4879097938537598, 'eval_accuracy@deu.rst.pcc': 0.26556016597510373, 'eval_f1@deu.rst.pcc': 0.18415195510270752, 'eval_precision@deu.rst.pcc': 0.22884138572659443, 'eval_recall@deu.rst.pcc': 0.21655736012353657, 'eval_loss@deu.rst.pcc': 2.4879097938537598, 'eval_runtime': 3.2465, 'eval_samples_per_second': 74.234, 'eval_steps_per_second': 2.464, 'epoch': 11.0}
{'train@deu.rst.pcc_loss': 2.3926172256469727, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.3133086876155268, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.2045312188674788, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.23667961072121615, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.22257678984702847, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.392617702484131, 'train@deu.rst.pcc_runtime': 26.4365, 'train@deu.rst.pcc_samples_per_second': 81.857, 'train@deu.rst.pcc_steps_per_second': 2.572, 'epoch': 12.0}
{'loss': 2.4557, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.4855141639709473, 'eval_accuracy@deu.rst.pcc': 0.26556016597510373, 'eval_f1@deu.rst.pcc': 0.18415195510270752, 'eval_precision@deu.rst.pcc': 0.22884138572659443, 'eval_recall@deu.rst.pcc': 0.21655736012353657, 'eval_loss@deu.rst.pcc': 2.485513687133789, 'eval_runtime': 3.3, 'eval_samples_per_second': 73.031, 'eval_steps_per_second': 2.424, 'epoch': 12.0}
{'train_runtime': 1020.2668, 'train_samples_per_second': 25.452, 'train_steps_per_second': 0.8, 'train_loss': 2.6930996015960096, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.3476
  train_runtime            = 2:02:00.89
  train_samples_per_second =      26.23
  train_steps_per_second   =      0.821
-------------------------------------------------------------------
Lang1:  eng.sdrt.stac    Lang2:  deu.rst.pcc
Saving run to:  runs/full_shot/FullShot=v4_eng.sdrt.stac_deu.rst.pcc_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 9580 examples
read 1145 examples
read 1510 examples
read 2164 examples
read 241 examples
read 260 examples
Total prediction labels:  42
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=42, bias=True)
    )
  )
)
{'train@eng.sdrt.stac_loss': 2.1124958992004395, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.3536534446764092, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.06433415610795507, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.07539785431471491, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.10965001068627504, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 2.1124956607818604, 'train@eng.sdrt.stac_runtime': 114.2171, 'train@eng.sdrt.stac_samples_per_second': 83.875, 'train@eng.sdrt.stac_steps_per_second': 2.627, 'epoch': 1.0}
{'loss': 2.5915, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.071579694747925, 'eval_accuracy@eng.sdrt.stac': 0.36069868995633186, 'eval_f1@eng.sdrt.stac': 0.06491310234305925, 'eval_precision@eng.sdrt.stac': 0.06254820191314948, 'eval_recall@eng.sdrt.stac': 0.11137331852924874, 'eval_loss@eng.sdrt.stac': 2.0715794563293457, 'eval_runtime': 13.9963, 'eval_samples_per_second': 81.807, 'eval_steps_per_second': 2.572, 'epoch': 1.0}
{'train@eng.sdrt.stac_loss': 1.8829195499420166, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4292275574112735, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.12858426622058722, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.13183025436462326, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.17471982515628692, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.8829195499420166, 'train@eng.sdrt.stac_runtime': 114.321, 'train@eng.sdrt.stac_samples_per_second': 83.799, 'train@eng.sdrt.stac_steps_per_second': 2.624, 'epoch': 2.0}
{'loss': 2.0301, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.8381401300430298, 'eval_accuracy@eng.sdrt.stac': 0.41397379912663756, 'eval_f1@eng.sdrt.stac': 0.1167886346222505, 'eval_precision@eng.sdrt.stac': 0.10820868840493944, 'eval_recall@eng.sdrt.stac': 0.16524603043104535, 'eval_loss@eng.sdrt.stac': 1.8381398916244507, 'eval_runtime': 14.7426, 'eval_samples_per_second': 77.666, 'eval_steps_per_second': 2.442, 'epoch': 2.0}
{'train@eng.sdrt.stac_loss': 1.7735798358917236, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4510438413361169, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.15199364639624158, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.13721408604600108, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.19229595303573777, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.7735798358917236, 'train@eng.sdrt.stac_runtime': 114.3516, 'train@eng.sdrt.stac_samples_per_second': 83.777, 'train@eng.sdrt.stac_steps_per_second': 2.623, 'epoch': 3.0}
{'loss': 1.8633, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.7325857877731323, 'eval_accuracy@eng.sdrt.stac': 0.45065502183406114, 'eval_f1@eng.sdrt.stac': 0.14460064026351957, 'eval_precision@eng.sdrt.stac': 0.1287579429028094, 'eval_recall@eng.sdrt.stac': 0.18715220386858783, 'eval_loss@eng.sdrt.stac': 1.7325857877731323, 'eval_runtime': 14.0198, 'eval_samples_per_second': 81.67, 'eval_steps_per_second': 2.568, 'epoch': 3.0}
{'train@eng.sdrt.stac_loss': 1.7004387378692627, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.47150313152400836, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.188033774776871, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.2086345281081427, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2171616957574103, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.7004387378692627, 'train@eng.sdrt.stac_runtime': 113.9993, 'train@eng.sdrt.stac_samples_per_second': 84.036, 'train@eng.sdrt.stac_steps_per_second': 2.632, 'epoch': 4.0}
{'loss': 1.7793, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.652619481086731, 'eval_accuracy@eng.sdrt.stac': 0.47685589519650656, 'eval_f1@eng.sdrt.stac': 0.1836133866559439, 'eval_precision@eng.sdrt.stac': 0.24528900291489653, 'eval_recall@eng.sdrt.stac': 0.21291419676275414, 'eval_loss@eng.sdrt.stac': 1.6526193618774414, 'eval_runtime': 13.98, 'eval_samples_per_second': 81.903, 'eval_steps_per_second': 2.575, 'epoch': 4.0}
{'train@eng.sdrt.stac_loss': 1.6446784734725952, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4878914405010438, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.21178450696256218, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.2789580146018983, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.23555140906705394, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.6446782350540161, 'train@eng.sdrt.stac_runtime': 114.0384, 'train@eng.sdrt.stac_samples_per_second': 84.007, 'train@eng.sdrt.stac_steps_per_second': 2.631, 'epoch': 5.0}
{'loss': 1.7171, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.599898099899292, 'eval_accuracy@eng.sdrt.stac': 0.4925764192139738, 'eval_f1@eng.sdrt.stac': 0.20579990812988824, 'eval_precision@eng.sdrt.stac': 0.21874194567492006, 'eval_recall@eng.sdrt.stac': 0.22980854896950942, 'eval_loss@eng.sdrt.stac': 1.599898338317871, 'eval_runtime': 13.9883, 'eval_samples_per_second': 81.854, 'eval_steps_per_second': 2.574, 'epoch': 5.0}
{'train@eng.sdrt.stac_loss': 1.5994924306869507, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.49164926931106473, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.21628782122598483, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.27448328030869695, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.23718452324949957, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5994925498962402, 'train@eng.sdrt.stac_runtime': 114.304, 'train@eng.sdrt.stac_samples_per_second': 83.812, 'train@eng.sdrt.stac_steps_per_second': 2.625, 'epoch': 6.0}
{'loss': 1.669, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.5707013607025146, 'eval_accuracy@eng.sdrt.stac': 0.4873362445414847, 'eval_f1@eng.sdrt.stac': 0.20558238686214175, 'eval_precision@eng.sdrt.stac': 0.22514992235826112, 'eval_recall@eng.sdrt.stac': 0.2267645777967044, 'eval_loss@eng.sdrt.stac': 1.5707013607025146, 'eval_runtime': 14.039, 'eval_samples_per_second': 81.558, 'eval_steps_per_second': 2.564, 'epoch': 6.0}
{'train@eng.sdrt.stac_loss': 1.5718837976455688, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5062630480167014, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.260793411809441, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.3425651117712837, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2743481606689304, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5718837976455688, 'train@eng.sdrt.stac_runtime': 114.328, 'train@eng.sdrt.stac_samples_per_second': 83.794, 'train@eng.sdrt.stac_steps_per_second': 2.624, 'epoch': 7.0}
{'loss': 1.6294, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.5464553833007812, 'eval_accuracy@eng.sdrt.stac': 0.509170305676856, 'eval_f1@eng.sdrt.stac': 0.24127565136824405, 'eval_precision@eng.sdrt.stac': 0.26983766951510746, 'eval_recall@eng.sdrt.stac': 0.25677962082461764, 'eval_loss@eng.sdrt.stac': 1.5464553833007812, 'eval_runtime': 14.019, 'eval_samples_per_second': 81.675, 'eval_steps_per_second': 2.568, 'epoch': 7.0}
{'train@eng.sdrt.stac_loss': 1.538107991218567, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5168058455114822, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.28196653651272124, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.35747945616107046, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2964738763449029, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.538107991218567, 'train@eng.sdrt.stac_runtime': 114.2532, 'train@eng.sdrt.stac_samples_per_second': 83.849, 'train@eng.sdrt.stac_steps_per_second': 2.626, 'epoch': 8.0}
{'loss': 1.6019, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.5137102603912354, 'eval_accuracy@eng.sdrt.stac': 0.5187772925764192, 'eval_f1@eng.sdrt.stac': 0.26011420968685484, 'eval_precision@eng.sdrt.stac': 0.27000334283473176, 'eval_recall@eng.sdrt.stac': 0.27670986030026967, 'eval_loss@eng.sdrt.stac': 1.5137100219726562, 'eval_runtime': 13.9819, 'eval_samples_per_second': 81.892, 'eval_steps_per_second': 2.575, 'epoch': 8.0}
{'train@eng.sdrt.stac_loss': 1.5222595930099487, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.522651356993737, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3009463474399762, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.3675264285511912, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.31348786578985566, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5222597122192383, 'train@eng.sdrt.stac_runtime': 113.9419, 'train@eng.sdrt.stac_samples_per_second': 84.078, 'train@eng.sdrt.stac_steps_per_second': 2.633, 'epoch': 9.0}
{'loss': 1.5802, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.5027260780334473, 'eval_accuracy@eng.sdrt.stac': 0.5275109170305677, 'eval_f1@eng.sdrt.stac': 0.2835212332387333, 'eval_precision@eng.sdrt.stac': 0.3914575266474132, 'eval_recall@eng.sdrt.stac': 0.29313982667925637, 'eval_loss@eng.sdrt.stac': 1.5027261972427368, 'eval_runtime': 13.9724, 'eval_samples_per_second': 81.948, 'eval_steps_per_second': 2.577, 'epoch': 9.0}
{'train@eng.sdrt.stac_loss': 1.504825472831726, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5268267223382046, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3096478352507429, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.36756108340524163, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.32240759674003594, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.504825472831726, 'train@eng.sdrt.stac_runtime': 114.3186, 'train@eng.sdrt.stac_samples_per_second': 83.801, 'train@eng.sdrt.stac_steps_per_second': 2.624, 'epoch': 10.0}
{'loss': 1.5623, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.4892889261245728, 'eval_accuracy@eng.sdrt.stac': 0.5336244541484716, 'eval_f1@eng.sdrt.stac': 0.28887367841598727, 'eval_precision@eng.sdrt.stac': 0.3931661083517243, 'eval_recall@eng.sdrt.stac': 0.2999514724305203, 'eval_loss@eng.sdrt.stac': 1.4892888069152832, 'eval_runtime': 14.0458, 'eval_samples_per_second': 81.519, 'eval_steps_per_second': 2.563, 'epoch': 10.0}
{'train@eng.sdrt.stac_loss': 1.4965465068817139, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5290187891440501, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3146909237670701, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.36935054199065176, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3270243993272159, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4965466260910034, 'train@eng.sdrt.stac_runtime': 114.268, 'train@eng.sdrt.stac_samples_per_second': 83.838, 'train@eng.sdrt.stac_steps_per_second': 2.625, 'epoch': 11.0}
{'loss': 1.5475, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.4852423667907715, 'eval_accuracy@eng.sdrt.stac': 0.5318777292576419, 'eval_f1@eng.sdrt.stac': 0.29459478958305496, 'eval_precision@eng.sdrt.stac': 0.392350780644219, 'eval_recall@eng.sdrt.stac': 0.30688305679337574, 'eval_loss@eng.sdrt.stac': 1.4852423667907715, 'eval_runtime': 14.0402, 'eval_samples_per_second': 81.552, 'eval_steps_per_second': 2.564, 'epoch': 11.0}
{'train@eng.sdrt.stac_loss': 1.4953341484069824, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5298538622129436, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3164019194647361, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.37086224664275824, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3285830643027121, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.495334267616272, 'train@eng.sdrt.stac_runtime': 114.3393, 'train@eng.sdrt.stac_samples_per_second': 83.786, 'train@eng.sdrt.stac_steps_per_second': 2.624, 'epoch': 12.0}
{'loss': 1.5441, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.4842195510864258, 'eval_accuracy@eng.sdrt.stac': 0.5379912663755458, 'eval_f1@eng.sdrt.stac': 0.298366787802188, 'eval_precision@eng.sdrt.stac': 0.39634304157884137, 'eval_recall@eng.sdrt.stac': 0.31140272530018664, 'eval_loss@eng.sdrt.stac': 1.4842194318771362, 'eval_runtime': 14.0563, 'eval_samples_per_second': 81.458, 'eval_steps_per_second': 2.561, 'epoch': 12.0}
{'train_runtime': 4413.4495, 'train_samples_per_second': 26.048, 'train_steps_per_second': 0.816, 'train_loss': 1.7596233961317274, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.7596
  train_runtime            = 1:13:33.44
  train_samples_per_second =     26.048
  train_steps_per_second   =      0.816
{'train@deu.rst.pcc_loss': 3.377939224243164, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.09380776340110905, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.018373711415785192, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.02372450118432325, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.04478212511598264, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.377939224243164, 'train@deu.rst.pcc_runtime': 26.5677, 'train@deu.rst.pcc_samples_per_second': 81.452, 'train@deu.rst.pcc_steps_per_second': 2.559, 'epoch': 1.0}
{'loss': 3.8368, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.394885301589966, 'eval_accuracy@deu.rst.pcc': 0.12033195020746888, 'eval_f1@deu.rst.pcc': 0.017900413711583924, 'eval_precision@deu.rst.pcc': 0.01656390372065429, 'eval_recall@deu.rst.pcc': 0.04741554741554741, 'eval_loss@deu.rst.pcc': 3.3948850631713867, 'eval_runtime': 3.3095, 'eval_samples_per_second': 72.821, 'eval_steps_per_second': 2.417, 'epoch': 1.0}
{'train@deu.rst.pcc_loss': 3.0090935230255127, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.13863216266173753, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.03285744070519646, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.026588074407184734, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.05855176566897221, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.0090935230255127, 'train@deu.rst.pcc_runtime': 26.5079, 'train@deu.rst.pcc_samples_per_second': 81.636, 'train@deu.rst.pcc_steps_per_second': 2.565, 'epoch': 2.0}
{'loss': 3.1873, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.065376043319702, 'eval_accuracy@deu.rst.pcc': 0.12033195020746888, 'eval_f1@deu.rst.pcc': 0.024833908893080483, 'eval_precision@deu.rst.pcc': 0.01657051282051282, 'eval_recall@deu.rst.pcc': 0.05428367928367928, 'eval_loss@deu.rst.pcc': 3.065375328063965, 'eval_runtime': 3.3113, 'eval_samples_per_second': 72.781, 'eval_steps_per_second': 2.416, 'epoch': 2.0}
{'train@deu.rst.pcc_loss': 2.869011163711548, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.16589648798521256, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.059118421925311755, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.07138778139526653, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.07931935722036958, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.869011163711548, 'train@deu.rst.pcc_runtime': 26.4971, 'train@deu.rst.pcc_samples_per_second': 81.669, 'train@deu.rst.pcc_steps_per_second': 2.566, 'epoch': 3.0}
{'loss': 2.9653, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.9586522579193115, 'eval_accuracy@deu.rst.pcc': 0.13278008298755187, 'eval_f1@deu.rst.pcc': 0.05616084086478823, 'eval_precision@deu.rst.pcc': 0.061248573503475456, 'eval_recall@deu.rst.pcc': 0.07617266992266992, 'eval_loss@deu.rst.pcc': 2.9586522579193115, 'eval_runtime': 3.3122, 'eval_samples_per_second': 72.762, 'eval_steps_per_second': 2.415, 'epoch': 3.0}
{'train@deu.rst.pcc_loss': 2.7847723960876465, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.1913123844731978, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.07761371140282046, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.12128498455768628, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.1003484852167357, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7847723960876465, 'train@deu.rst.pcc_runtime': 26.481, 'train@deu.rst.pcc_samples_per_second': 81.719, 'train@deu.rst.pcc_steps_per_second': 2.568, 'epoch': 4.0}
{'loss': 2.8847, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.8874447345733643, 'eval_accuracy@deu.rst.pcc': 0.14937759336099585, 'eval_f1@deu.rst.pcc': 0.05635253076705545, 'eval_precision@deu.rst.pcc': 0.046881097625778474, 'eval_recall@deu.rst.pcc': 0.08584528897028898, 'eval_loss@deu.rst.pcc': 2.8874454498291016, 'eval_runtime': 3.284, 'eval_samples_per_second': 73.386, 'eval_steps_per_second': 2.436, 'epoch': 4.0}
{'train@deu.rst.pcc_loss': 2.725403070449829, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.21487985212569316, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09523388315072948, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.11997615727340327, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.12253055719650977, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.72540283203125, 'train@deu.rst.pcc_runtime': 26.4986, 'train@deu.rst.pcc_samples_per_second': 81.665, 'train@deu.rst.pcc_steps_per_second': 2.566, 'epoch': 5.0}
{'loss': 2.7993, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.8433220386505127, 'eval_accuracy@deu.rst.pcc': 0.17012448132780084, 'eval_f1@deu.rst.pcc': 0.0702604154001205, 'eval_precision@deu.rst.pcc': 0.06800813008130081, 'eval_recall@deu.rst.pcc': 0.10587734025234025, 'eval_loss@deu.rst.pcc': 2.843322277069092, 'eval_runtime': 3.3021, 'eval_samples_per_second': 72.983, 'eval_steps_per_second': 2.423, 'epoch': 5.0}
{'train@deu.rst.pcc_loss': 2.6724305152893066, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.23983364140480593, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.11335462544724186, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.14690227523150023, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.14539151000747746, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6724302768707275, 'train@deu.rst.pcc_runtime': 26.5014, 'train@deu.rst.pcc_samples_per_second': 81.656, 'train@deu.rst.pcc_steps_per_second': 2.566, 'epoch': 6.0}
{'loss': 2.7489, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.8002564907073975, 'eval_accuracy@deu.rst.pcc': 0.2033195020746888, 'eval_f1@deu.rst.pcc': 0.10264723749045239, 'eval_precision@deu.rst.pcc': 0.08991839366225508, 'eval_recall@deu.rst.pcc': 0.14005034317534318, 'eval_loss@deu.rst.pcc': 2.8002569675445557, 'eval_runtime': 3.2888, 'eval_samples_per_second': 73.278, 'eval_steps_per_second': 2.432, 'epoch': 6.0}
{'train@deu.rst.pcc_loss': 2.6405673027038574, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.24491682070240295, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.11489041178842893, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.15707296047260844, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.1507439776413155, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6405675411224365, 'train@deu.rst.pcc_runtime': 26.4887, 'train@deu.rst.pcc_samples_per_second': 81.695, 'train@deu.rst.pcc_steps_per_second': 2.567, 'epoch': 7.0}
{'loss': 2.714, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.7776238918304443, 'eval_accuracy@deu.rst.pcc': 0.21991701244813278, 'eval_f1@deu.rst.pcc': 0.112171215568472, 'eval_precision@deu.rst.pcc': 0.10564481474407945, 'eval_recall@deu.rst.pcc': 0.15686628186628185, 'eval_loss@deu.rst.pcc': 2.7776238918304443, 'eval_runtime': 3.3329, 'eval_samples_per_second': 72.308, 'eval_steps_per_second': 2.4, 'epoch': 7.0}
{'train@deu.rst.pcc_loss': 2.6155755519866943, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2532347504621072, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.11853390412956098, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.16776918482365943, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.15621723400822635, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6155755519866943, 'train@deu.rst.pcc_runtime': 26.4789, 'train@deu.rst.pcc_samples_per_second': 81.725, 'train@deu.rst.pcc_steps_per_second': 2.568, 'epoch': 8.0}
{'loss': 2.6738, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.7638819217681885, 'eval_accuracy@deu.rst.pcc': 0.23651452282157676, 'eval_f1@deu.rst.pcc': 0.11727234753550543, 'eval_precision@deu.rst.pcc': 0.10936004847642777, 'eval_recall@deu.rst.pcc': 0.16948502886002884, 'eval_loss@deu.rst.pcc': 2.7638819217681885, 'eval_runtime': 3.2988, 'eval_samples_per_second': 73.057, 'eval_steps_per_second': 2.425, 'epoch': 8.0}
{'train@deu.rst.pcc_loss': 2.59511399269104, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2555452865064695, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.12392129151209705, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.15719593429884307, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.15950205972014378, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.59511399269104, 'train@deu.rst.pcc_runtime': 26.5485, 'train@deu.rst.pcc_samples_per_second': 81.511, 'train@deu.rst.pcc_steps_per_second': 2.561, 'epoch': 9.0}
{'loss': 2.667, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.750675678253174, 'eval_accuracy@deu.rst.pcc': 0.23651452282157676, 'eval_f1@deu.rst.pcc': 0.11983192446320982, 'eval_precision@deu.rst.pcc': 0.11740943651550849, 'eval_recall@deu.rst.pcc': 0.1717848124098124, 'eval_loss@deu.rst.pcc': 2.7506754398345947, 'eval_runtime': 3.3121, 'eval_samples_per_second': 72.764, 'eval_steps_per_second': 2.415, 'epoch': 9.0}
{'train@deu.rst.pcc_loss': 2.5832479000091553, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.25831792975970425, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.12432875246274543, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.1606418286722913, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.16144160153038334, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.5832481384277344, 'train@deu.rst.pcc_runtime': 26.6068, 'train@deu.rst.pcc_samples_per_second': 81.333, 'train@deu.rst.pcc_steps_per_second': 2.556, 'epoch': 10.0}
{'loss': 2.6455, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.741299629211426, 'eval_accuracy@deu.rst.pcc': 0.24481327800829875, 'eval_f1@deu.rst.pcc': 0.12519925332633916, 'eval_precision@deu.rst.pcc': 0.11973233011384149, 'eval_recall@deu.rst.pcc': 0.18078102453102454, 'eval_loss@deu.rst.pcc': 2.741299867630005, 'eval_runtime': 3.3123, 'eval_samples_per_second': 72.759, 'eval_steps_per_second': 2.415, 'epoch': 10.0}
{'train@deu.rst.pcc_loss': 2.574991226196289, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2620147874306839, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.12782958559491278, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.16254593343499074, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.16473989240007386, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.574991226196289, 'train@deu.rst.pcc_runtime': 26.6023, 'train@deu.rst.pcc_samples_per_second': 81.346, 'train@deu.rst.pcc_steps_per_second': 2.556, 'epoch': 11.0}
{'loss': 2.6262, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.7354068756103516, 'eval_accuracy@deu.rst.pcc': 0.24896265560165975, 'eval_f1@deu.rst.pcc': 0.12655707488842516, 'eval_precision@deu.rst.pcc': 0.12082304196358189, 'eval_recall@deu.rst.pcc': 0.18226911976911975, 'eval_loss@deu.rst.pcc': 2.7354073524475098, 'eval_runtime': 3.3022, 'eval_samples_per_second': 72.981, 'eval_steps_per_second': 2.423, 'epoch': 11.0}
{'train@deu.rst.pcc_loss': 2.572289228439331, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.26386321626617376, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.1286319250962637, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.16438284607833417, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.16571672237891893, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.57228946685791, 'train@deu.rst.pcc_runtime': 26.5901, 'train@deu.rst.pcc_samples_per_second': 81.384, 'train@deu.rst.pcc_steps_per_second': 2.557, 'epoch': 12.0}
{'loss': 2.6241, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.732637882232666, 'eval_accuracy@deu.rst.pcc': 0.25311203319502074, 'eval_f1@deu.rst.pcc': 0.1289259549520338, 'eval_precision@deu.rst.pcc': 0.12538783937377404, 'eval_recall@deu.rst.pcc': 0.183757215007215, 'eval_loss@deu.rst.pcc': 2.732637882232666, 'eval_runtime': 3.3315, 'eval_samples_per_second': 72.339, 'eval_steps_per_second': 2.401, 'epoch': 12.0}
{'train_runtime': 1019.8634, 'train_samples_per_second': 25.462, 'train_steps_per_second': 0.8, 'train_loss': 2.864405314127604, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.7596
  train_runtime            = 1:13:33.44
  train_samples_per_second =     26.048
  train_steps_per_second   =      0.816
-------------------------------------------------------------------
Lang1:  fas.rst.prstc    Lang2:  deu.rst.pcc
Saving run to:  runs/full_shot/FullShot=v4_fas.rst.prstc_deu.rst.pcc_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 4100 examples
read 499 examples
read 592 examples
read 2164 examples
read 241 examples
read 260 examples
Total prediction labels:  36
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (ar): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=36, bias=True)
    )
  )
)
{'train@fas.rst.prstc_loss': 2.4212067127227783, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.23780487804878048, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.02260214430599826, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.013988522238163558, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.058823529411764705, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.4212067127227783, 'train@fas.rst.prstc_runtime': 48.9476, 'train@fas.rst.prstc_samples_per_second': 83.763, 'train@fas.rst.prstc_steps_per_second': 2.635, 'epoch': 1.0}
{'loss': 2.8251, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.3393540382385254, 'eval_accuracy@fas.rst.prstc': 0.24448897795591182, 'eval_f1@fas.rst.prstc': 0.026194310252281264, 'eval_precision@fas.rst.prstc': 0.016299265197060788, 'eval_recall@fas.rst.prstc': 0.06666666666666667, 'eval_loss@fas.rst.prstc': 2.3393540382385254, 'eval_runtime': 6.2784, 'eval_samples_per_second': 79.479, 'eval_steps_per_second': 2.548, 'epoch': 1.0}
{'train@fas.rst.prstc_loss': 2.3639354705810547, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2685365853658537, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.043581541797860585, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.031484560466033026, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07083191596333024, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.363935708999634, 'train@fas.rst.prstc_runtime': 49.1621, 'train@fas.rst.prstc_samples_per_second': 83.397, 'train@fas.rst.prstc_steps_per_second': 2.624, 'epoch': 2.0}
{'loss': 2.408, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.2747251987457275, 'eval_accuracy@fas.rst.prstc': 0.2985971943887776, 'eval_f1@fas.rst.prstc': 0.05331815094490391, 'eval_precision@fas.rst.prstc': 0.03942033767805093, 'eval_recall@fas.rst.prstc': 0.08334996436208125, 'eval_loss@fas.rst.prstc': 2.2747254371643066, 'eval_runtime': 6.3201, 'eval_samples_per_second': 78.954, 'eval_steps_per_second': 2.532, 'epoch': 2.0}
{'train@fas.rst.prstc_loss': 2.3449695110321045, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.24146341463414633, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.025231635250805816, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.0356033033462701, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.05990329792582609, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3449695110321045, 'train@fas.rst.prstc_runtime': 49.1592, 'train@fas.rst.prstc_samples_per_second': 83.403, 'train@fas.rst.prstc_steps_per_second': 2.624, 'epoch': 3.0}
{'loss': 2.3653, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.2580254077911377, 'eval_accuracy@fas.rst.prstc': 0.24649298597194388, 'eval_f1@fas.rst.prstc': 0.027518037518037518, 'eval_precision@fas.rst.prstc': 0.029797570850202428, 'eval_recall@fas.rst.prstc': 0.0672463768115942, 'eval_loss@fas.rst.prstc': 2.2580254077911377, 'eval_runtime': 6.3138, 'eval_samples_per_second': 79.034, 'eval_steps_per_second': 2.534, 'epoch': 3.0}
{'train@fas.rst.prstc_loss': 2.3305625915527344, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.23780487804878048, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.02260214430599826, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.013988522238163558, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.058823529411764705, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3305623531341553, 'train@fas.rst.prstc_runtime': 49.0535, 'train@fas.rst.prstc_samples_per_second': 83.582, 'train@fas.rst.prstc_steps_per_second': 2.63, 'epoch': 4.0}
{'loss': 2.3487, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.248152494430542, 'eval_accuracy@fas.rst.prstc': 0.24448897795591182, 'eval_f1@fas.rst.prstc': 0.026194310252281264, 'eval_precision@fas.rst.prstc': 0.016299265197060788, 'eval_recall@fas.rst.prstc': 0.06666666666666667, 'eval_loss@fas.rst.prstc': 2.248152494430542, 'eval_runtime': 6.2855, 'eval_samples_per_second': 79.389, 'eval_steps_per_second': 2.546, 'epoch': 4.0}
{'train@fas.rst.prstc_loss': 2.3194353580474854, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2575609756097561, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.033716760430748505, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03199982673309703, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.06463956013392809, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3194353580474854, 'train@fas.rst.prstc_runtime': 48.9754, 'train@fas.rst.prstc_samples_per_second': 83.715, 'train@fas.rst.prstc_steps_per_second': 2.634, 'epoch': 5.0}
{'loss': 2.3391, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.2332067489624023, 'eval_accuracy@fas.rst.prstc': 0.2685370741482966, 'eval_f1@fas.rst.prstc': 0.03958173708659523, 'eval_precision@fas.rst.prstc': 0.04076252723311547, 'eval_recall@fas.rst.prstc': 0.07368971252078878, 'eval_loss@fas.rst.prstc': 2.2332067489624023, 'eval_runtime': 6.2632, 'eval_samples_per_second': 79.672, 'eval_steps_per_second': 2.555, 'epoch': 5.0}
{'train@fas.rst.prstc_loss': 2.3065977096557617, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2424390243902439, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.02566035154747846, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03537113910825574, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.06017222382679204, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.306597948074341, 'train@fas.rst.prstc_runtime': 48.9767, 'train@fas.rst.prstc_samples_per_second': 83.713, 'train@fas.rst.prstc_steps_per_second': 2.634, 'epoch': 6.0}
{'loss': 2.3262, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.22607421875, 'eval_accuracy@fas.rst.prstc': 0.24849699398797595, 'eval_f1@fas.rst.prstc': 0.028704182637258675, 'eval_precision@fas.rst.prstc': 0.03323150033944331, 'eval_recall@fas.rst.prstc': 0.06782608695652173, 'eval_loss@fas.rst.prstc': 2.22607421875, 'eval_runtime': 6.2712, 'eval_samples_per_second': 79.57, 'eval_steps_per_second': 2.551, 'epoch': 6.0}
{'train@fas.rst.prstc_loss': 2.284825325012207, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.27902439024390246, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04224855989611159, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.032043100359817236, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07147588331568307, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.284825325012207, 'train@fas.rst.prstc_runtime': 48.9367, 'train@fas.rst.prstc_samples_per_second': 83.782, 'train@fas.rst.prstc_steps_per_second': 2.636, 'epoch': 7.0}
{'loss': 2.3134, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.198225498199463, 'eval_accuracy@fas.rst.prstc': 0.2925851703406814, 'eval_f1@fas.rst.prstc': 0.04953886693017128, 'eval_precision@fas.rst.prstc': 0.0395606884057971, 'eval_recall@fas.rst.prstc': 0.08094559277738181, 'eval_loss@fas.rst.prstc': 2.198225736618042, 'eval_runtime': 6.2807, 'eval_samples_per_second': 79.449, 'eval_steps_per_second': 2.547, 'epoch': 7.0}
{'train@fas.rst.prstc_loss': 2.2513504028320312, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2978048780487805, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.0483424839693197, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03502649486025424, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07824652610635088, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.2513506412506104, 'train@fas.rst.prstc_runtime': 48.9222, 'train@fas.rst.prstc_samples_per_second': 83.807, 'train@fas.rst.prstc_steps_per_second': 2.637, 'epoch': 8.0}
{'loss': 2.2917, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.163174629211426, 'eval_accuracy@fas.rst.prstc': 0.3226452905811623, 'eval_f1@fas.rst.prstc': 0.05855443822978161, 'eval_precision@fas.rst.prstc': 0.043409454482988157, 'eval_recall@fas.rst.prstc': 0.0903064861012117, 'eval_loss@fas.rst.prstc': 2.163175344467163, 'eval_runtime': 6.2479, 'eval_samples_per_second': 79.867, 'eval_steps_per_second': 2.561, 'epoch': 8.0}
{'train@fas.rst.prstc_loss': 2.222196340560913, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2990243902439024, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04923035177791792, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03616066262694508, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.0789989623781864, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.222196340560913, 'train@fas.rst.prstc_runtime': 48.9843, 'train@fas.rst.prstc_samples_per_second': 83.7, 'train@fas.rst.prstc_steps_per_second': 2.633, 'epoch': 9.0}
{'loss': 2.2568, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.126511335372925, 'eval_accuracy@fas.rst.prstc': 0.32064128256513025, 'eval_f1@fas.rst.prstc': 0.05898294277060418, 'eval_precision@fas.rst.prstc': 0.04447684372135149, 'eval_recall@fas.rst.prstc': 0.08995961035875505, 'eval_loss@fas.rst.prstc': 2.1265110969543457, 'eval_runtime': 6.2557, 'eval_samples_per_second': 79.767, 'eval_steps_per_second': 2.558, 'epoch': 9.0}
{'train@fas.rst.prstc_loss': 2.2051377296447754, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.30634146341463414, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.05079217375880345, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03770871431305283, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08120449707433437, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.2051374912261963, 'train@fas.rst.prstc_runtime': 49.0866, 'train@fas.rst.prstc_samples_per_second': 83.526, 'train@fas.rst.prstc_steps_per_second': 2.628, 'epoch': 10.0}
{'loss': 2.2412, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.106677293777466, 'eval_accuracy@fas.rst.prstc': 0.3226452905811623, 'eval_f1@fas.rst.prstc': 0.05978003843793078, 'eval_precision@fas.rst.prstc': 0.045627488237423096, 'eval_recall@fas.rst.prstc': 0.09060584461867428, 'eval_loss@fas.rst.prstc': 2.106677293777466, 'eval_runtime': 6.2638, 'eval_samples_per_second': 79.664, 'eval_steps_per_second': 2.554, 'epoch': 10.0}
{'train@fas.rst.prstc_loss': 2.193554162979126, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.30902439024390244, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.05164464622803769, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.038846814059120646, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08207973642266508, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.193554162979126, 'train@fas.rst.prstc_runtime': 49.0971, 'train@fas.rst.prstc_samples_per_second': 83.508, 'train@fas.rst.prstc_steps_per_second': 2.627, 'epoch': 11.0}
{'loss': 2.2228, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.092653512954712, 'eval_accuracy@fas.rst.prstc': 0.33266533066132264, 'eval_f1@fas.rst.prstc': 0.06189083820662768, 'eval_precision@fas.rst.prstc': 0.04770224517858588, 'eval_recall@fas.rst.prstc': 0.09350439534331194, 'eval_loss@fas.rst.prstc': 2.092653512954712, 'eval_runtime': 6.2896, 'eval_samples_per_second': 79.338, 'eval_steps_per_second': 2.544, 'epoch': 11.0}
{'train@fas.rst.prstc_loss': 2.193897247314453, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.30878048780487805, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.051565263383284404, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03873120376594091, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08199180599681224, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.193897008895874, 'train@fas.rst.prstc_runtime': 49.0802, 'train@fas.rst.prstc_samples_per_second': 83.537, 'train@fas.rst.prstc_steps_per_second': 2.628, 'epoch': 12.0}
{'loss': 2.2167, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.09291410446167, 'eval_accuracy@fas.rst.prstc': 0.3306613226452906, 'eval_f1@fas.rst.prstc': 0.06140988942291874, 'eval_precision@fas.rst.prstc': 0.04717163022895507, 'eval_recall@fas.rst.prstc': 0.09292468519838441, 'eval_loss@fas.rst.prstc': 2.092913866043091, 'eval_runtime': 6.2846, 'eval_samples_per_second': 79.4, 'eval_steps_per_second': 2.546, 'epoch': 12.0}
{'train_runtime': 1894.5066, 'train_samples_per_second': 25.97, 'train_steps_per_second': 0.817, 'train_loss': 2.3462474340000203, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.3462
  train_runtime            = 0:31:34.50
  train_samples_per_second =      25.97
  train_steps_per_second   =      0.817
{'train@deu.rst.pcc_loss': 3.18574857711792, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.09426987060998152, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.019761385134987527, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.018086218823635127, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.04379511523089078, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.18574857711792, 'train@deu.rst.pcc_runtime': 26.5463, 'train@deu.rst.pcc_samples_per_second': 81.518, 'train@deu.rst.pcc_steps_per_second': 2.562, 'epoch': 1.0}
{'loss': 3.4065, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.199324131011963, 'eval_accuracy@deu.rst.pcc': 0.0954356846473029, 'eval_f1@deu.rst.pcc': 0.024354374137753224, 'eval_precision@deu.rst.pcc': 0.027674514022493917, 'eval_recall@deu.rst.pcc': 0.04049018111518111, 'eval_loss@deu.rst.pcc': 3.199324131011963, 'eval_runtime': 3.2712, 'eval_samples_per_second': 73.674, 'eval_steps_per_second': 2.446, 'epoch': 1.0}
{'train@deu.rst.pcc_loss': 2.9766433238983154, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.10951940850277264, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.020189772486441212, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.03048430884754388, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.04437207243665839, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.9766430854797363, 'train@deu.rst.pcc_runtime': 26.5295, 'train@deu.rst.pcc_samples_per_second': 81.57, 'train@deu.rst.pcc_steps_per_second': 2.563, 'epoch': 2.0}
{'loss': 3.0774, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.0147993564605713, 'eval_accuracy@deu.rst.pcc': 0.12448132780082988, 'eval_f1@deu.rst.pcc': 0.02544104136878242, 'eval_precision@deu.rst.pcc': 0.026735624123422157, 'eval_recall@deu.rst.pcc': 0.051669973544973546, 'eval_loss@deu.rst.pcc': 3.0147993564605713, 'eval_runtime': 3.2681, 'eval_samples_per_second': 73.743, 'eval_steps_per_second': 2.448, 'epoch': 2.0}
{'train@deu.rst.pcc_loss': 2.9099514484405518, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.12523105360443623, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.032563247071678056, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.061698584493954496, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.05428284967436896, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.9099514484405518, 'train@deu.rst.pcc_runtime': 26.5405, 'train@deu.rst.pcc_samples_per_second': 81.536, 'train@deu.rst.pcc_steps_per_second': 2.562, 'epoch': 3.0}
{'loss': 2.9648, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.9613826274871826, 'eval_accuracy@deu.rst.pcc': 0.13278008298755187, 'eval_f1@deu.rst.pcc': 0.02819366286819043, 'eval_precision@deu.rst.pcc': 0.024345423232212584, 'eval_recall@deu.rst.pcc': 0.05842999592999593, 'eval_loss@deu.rst.pcc': 2.9613826274871826, 'eval_runtime': 3.2508, 'eval_samples_per_second': 74.134, 'eval_steps_per_second': 2.461, 'epoch': 3.0}
{'train@deu.rst.pcc_loss': 2.860680103302002, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.1534195933456562, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.04712448333379809, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.10386997685007149, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.07435471896282639, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.860680341720581, 'train@deu.rst.pcc_runtime': 26.5076, 'train@deu.rst.pcc_samples_per_second': 81.637, 'train@deu.rst.pcc_steps_per_second': 2.565, 'epoch': 4.0}
{'loss': 2.9163, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.922457456588745, 'eval_accuracy@deu.rst.pcc': 0.14107883817427386, 'eval_f1@deu.rst.pcc': 0.035664878346721385, 'eval_precision@deu.rst.pcc': 0.03229166666666666, 'eval_recall@deu.rst.pcc': 0.06967338217338218, 'eval_loss@deu.rst.pcc': 2.922457218170166, 'eval_runtime': 3.2681, 'eval_samples_per_second': 73.743, 'eval_steps_per_second': 2.448, 'epoch': 4.0}
{'train@deu.rst.pcc_loss': 2.8191471099853516, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.16959334565619225, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.061359316096639596, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.10576305331359238, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.09092931153535098, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.8191471099853516, 'train@deu.rst.pcc_runtime': 26.4834, 'train@deu.rst.pcc_samples_per_second': 81.711, 'train@deu.rst.pcc_steps_per_second': 2.568, 'epoch': 5.0}
{'loss': 2.88, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.8897595405578613, 'eval_accuracy@deu.rst.pcc': 0.15767634854771784, 'eval_f1@deu.rst.pcc': 0.04454335988272825, 'eval_precision@deu.rst.pcc': 0.06819529972515047, 'eval_recall@deu.rst.pcc': 0.08367038054538055, 'eval_loss@deu.rst.pcc': 2.8897597789764404, 'eval_runtime': 3.2381, 'eval_samples_per_second': 74.427, 'eval_steps_per_second': 2.471, 'epoch': 5.0}
{'train@deu.rst.pcc_loss': 2.787076950073242, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.1913123844731978, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.07951267131849714, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.1029329495936675, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.10812037289548722, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7870771884918213, 'train@deu.rst.pcc_runtime': 26.4501, 'train@deu.rst.pcc_samples_per_second': 81.814, 'train@deu.rst.pcc_steps_per_second': 2.571, 'epoch': 6.0}
{'loss': 2.8455, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.862903356552124, 'eval_accuracy@deu.rst.pcc': 0.18672199170124482, 'eval_f1@deu.rst.pcc': 0.06884741662643196, 'eval_precision@deu.rst.pcc': 0.07633833938980998, 'eval_recall@deu.rst.pcc': 0.10839565527065527, 'eval_loss@deu.rst.pcc': 2.862903118133545, 'eval_runtime': 3.2298, 'eval_samples_per_second': 74.617, 'eval_steps_per_second': 2.477, 'epoch': 6.0}
{'train@deu.rst.pcc_loss': 2.7599167823791504, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.20841035120147874, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08850663696447555, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.10565268910282614, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.12353409443900222, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7599167823791504, 'train@deu.rst.pcc_runtime': 26.448, 'train@deu.rst.pcc_samples_per_second': 81.821, 'train@deu.rst.pcc_steps_per_second': 2.571, 'epoch': 7.0}
{'loss': 2.8184, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.842421293258667, 'eval_accuracy@deu.rst.pcc': 0.17842323651452283, 'eval_f1@deu.rst.pcc': 0.07437111520758137, 'eval_precision@deu.rst.pcc': 0.07929831855136733, 'eval_recall@deu.rst.pcc': 0.112859940984941, 'eval_loss@deu.rst.pcc': 2.842421770095825, 'eval_runtime': 3.2396, 'eval_samples_per_second': 74.393, 'eval_steps_per_second': 2.469, 'epoch': 7.0}
{'train@deu.rst.pcc_loss': 2.7392847537994385, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2199630314232902, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09313414334325644, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.14431885153273322, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.1318189104217542, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7392849922180176, 'train@deu.rst.pcc_runtime': 26.4584, 'train@deu.rst.pcc_samples_per_second': 81.789, 'train@deu.rst.pcc_steps_per_second': 2.57, 'epoch': 8.0}
{'loss': 2.7889, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.827634334564209, 'eval_accuracy@deu.rst.pcc': 0.1991701244813278, 'eval_f1@deu.rst.pcc': 0.08099902498916654, 'eval_precision@deu.rst.pcc': 0.08118212197159565, 'eval_recall@deu.rst.pcc': 0.12774089336589337, 'eval_loss@deu.rst.pcc': 2.82763409614563, 'eval_runtime': 3.2572, 'eval_samples_per_second': 73.991, 'eval_steps_per_second': 2.456, 'epoch': 8.0}
{'train@deu.rst.pcc_loss': 2.7218775749206543, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.22134935304990758, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09557708365212333, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.14636580933215268, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.1339697897134884, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7218778133392334, 'train@deu.rst.pcc_runtime': 26.4237, 'train@deu.rst.pcc_samples_per_second': 81.896, 'train@deu.rst.pcc_steps_per_second': 2.573, 'epoch': 9.0}
{'loss': 2.7781, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.816195011138916, 'eval_accuracy@deu.rst.pcc': 0.1908713692946058, 'eval_f1@deu.rst.pcc': 0.07906824362854763, 'eval_precision@deu.rst.pcc': 0.07887272505420902, 'eval_recall@deu.rst.pcc': 0.12476470288970289, 'eval_loss@deu.rst.pcc': 2.816194772720337, 'eval_runtime': 3.2513, 'eval_samples_per_second': 74.123, 'eval_steps_per_second': 2.461, 'epoch': 9.0}
{'train@deu.rst.pcc_loss': 2.7096641063690186, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.22504621072088724, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09718532866380897, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.14821278713583938, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.13637820459984842, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7096641063690186, 'train@deu.rst.pcc_runtime': 26.4525, 'train@deu.rst.pcc_samples_per_second': 81.807, 'train@deu.rst.pcc_steps_per_second': 2.571, 'epoch': 10.0}
{'loss': 2.7573, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.8043997287750244, 'eval_accuracy@deu.rst.pcc': 0.1991701244813278, 'eval_f1@deu.rst.pcc': 0.079863182988183, 'eval_precision@deu.rst.pcc': 0.07783756777564826, 'eval_recall@deu.rst.pcc': 0.12774089336589337, 'eval_loss@deu.rst.pcc': 2.804399251937866, 'eval_runtime': 3.2684, 'eval_samples_per_second': 73.736, 'eval_steps_per_second': 2.448, 'epoch': 10.0}
{'train@deu.rst.pcc_loss': 2.7026686668395996, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.22597042513863216, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09892584166383628, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.1539600663539078, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.13743437033471795, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7026684284210205, 'train@deu.rst.pcc_runtime': 26.446, 'train@deu.rst.pcc_samples_per_second': 81.827, 'train@deu.rst.pcc_steps_per_second': 2.571, 'epoch': 11.0}
{'loss': 2.7496, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.799529552459717, 'eval_accuracy@deu.rst.pcc': 0.1991701244813278, 'eval_f1@deu.rst.pcc': 0.07970028899408944, 'eval_precision@deu.rst.pcc': 0.07807686499546965, 'eval_recall@deu.rst.pcc': 0.12774089336589337, 'eval_loss@deu.rst.pcc': 2.799529552459717, 'eval_runtime': 3.2656, 'eval_samples_per_second': 73.8, 'eval_steps_per_second': 2.45, 'epoch': 11.0}
{'train@deu.rst.pcc_loss': 2.6997556686401367, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.22643253234750463, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.0990331292155773, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.1534163579795154, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.13786569784976, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6997556686401367, 'train@deu.rst.pcc_runtime': 26.4854, 'train@deu.rst.pcc_samples_per_second': 81.706, 'train@deu.rst.pcc_steps_per_second': 2.567, 'epoch': 12.0}
{'loss': 2.7346, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.796616792678833, 'eval_accuracy@deu.rst.pcc': 0.1991701244813278, 'eval_f1@deu.rst.pcc': 0.07983380386041226, 'eval_precision@deu.rst.pcc': 0.07763897116528695, 'eval_recall@deu.rst.pcc': 0.12774089336589337, 'eval_loss@deu.rst.pcc': 2.7966160774230957, 'eval_runtime': 3.2656, 'eval_samples_per_second': 73.8, 'eval_steps_per_second': 2.45, 'epoch': 12.0}
{'train_runtime': 1018.6496, 'train_samples_per_second': 25.493, 'train_steps_per_second': 0.801, 'train_loss': 2.893113940369849, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.3462
  train_runtime            = 0:31:34.50
  train_samples_per_second =      25.97
  train_steps_per_second   =      0.817
-------------------------------------------------------------------
Lang1:  fra.sdrt.annodis    Lang2:  deu.rst.pcc
Saving run to:  runs/full_shot/FullShot=v4_fra.sdrt.annodis_deu.rst.pcc_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2185 examples
read 528 examples
read 625 examples
read 2164 examples
read 241 examples
read 260 examples
Total prediction labels:  40
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (fr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=40, bias=True)
    )
  )
)
{'train@fra.sdrt.annodis_loss': 2.6617472171783447, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.23295194508009154, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.04158660073499027, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.047779508838282886, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.06545908022249222, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.6617469787597656, 'train@fra.sdrt.annodis_runtime': 26.6132, 'train@fra.sdrt.annodis_samples_per_second': 82.102, 'train@fra.sdrt.annodis_steps_per_second': 2.593, 'epoch': 1.0}
{'loss': 3.1783, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.683342218399048, 'eval_accuracy@fra.sdrt.annodis': 0.2215909090909091, 'eval_f1@fra.sdrt.annodis': 0.035838665632083634, 'eval_precision@fra.sdrt.annodis': 0.03247771473538728, 'eval_recall@fra.sdrt.annodis': 0.06048225449402327, 'eval_loss@fra.sdrt.annodis': 2.6833419799804688, 'eval_runtime': 6.7175, 'eval_samples_per_second': 78.601, 'eval_steps_per_second': 2.531, 'epoch': 1.0}
{'train@fra.sdrt.annodis_loss': 2.3930652141571045, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.28695652173913044, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.061861072618092074, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.05069631146943302, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.08510308618421712, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.3930654525756836, 'train@fra.sdrt.annodis_runtime': 26.7663, 'train@fra.sdrt.annodis_samples_per_second': 81.632, 'train@fra.sdrt.annodis_steps_per_second': 2.578, 'epoch': 2.0}
{'loss': 2.5273, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.411207675933838, 'eval_accuracy@fra.sdrt.annodis': 0.2537878787878788, 'eval_f1@fra.sdrt.annodis': 0.052841177729137385, 'eval_precision@fra.sdrt.annodis': 0.04295597891098871, 'eval_recall@fra.sdrt.annodis': 0.07289879010197459, 'eval_loss@fra.sdrt.annodis': 2.411207914352417, 'eval_runtime': 9.3957, 'eval_samples_per_second': 56.196, 'eval_steps_per_second': 1.809, 'epoch': 2.0}
{'train@fra.sdrt.annodis_loss': 2.30836820602417, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.28421052631578947, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.06122818897477736, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.11288779157296044, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.0828413979160579, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.308368444442749, 'train@fra.sdrt.annodis_runtime': 26.7863, 'train@fra.sdrt.annodis_samples_per_second': 81.572, 'train@fra.sdrt.annodis_steps_per_second': 2.576, 'epoch': 3.0}
{'loss': 2.3751, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.330933094024658, 'eval_accuracy@fra.sdrt.annodis': 0.26136363636363635, 'eval_f1@fra.sdrt.annodis': 0.05263546375319242, 'eval_precision@fra.sdrt.annodis': 0.045744236954897835, 'eval_recall@fra.sdrt.annodis': 0.07405051295670888, 'eval_loss@fra.sdrt.annodis': 2.3309333324432373, 'eval_runtime': 6.7696, 'eval_samples_per_second': 77.996, 'eval_steps_per_second': 2.511, 'epoch': 3.0}
{'train@fra.sdrt.annodis_loss': 2.2489676475524902, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.30480549199084667, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.06717948910128971, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.07677572273425251, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.08940490306785467, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.248967409133911, 'train@fra.sdrt.annodis_runtime': 26.8097, 'train@fra.sdrt.annodis_samples_per_second': 81.5, 'train@fra.sdrt.annodis_steps_per_second': 2.574, 'epoch': 4.0}
{'loss': 2.318, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.275782823562622, 'eval_accuracy@fra.sdrt.annodis': 0.2708333333333333, 'eval_f1@fra.sdrt.annodis': 0.057330889559781745, 'eval_precision@fra.sdrt.annodis': 0.05022301962605412, 'eval_recall@fra.sdrt.annodis': 0.07725534464890643, 'eval_loss@fra.sdrt.annodis': 2.275782585144043, 'eval_runtime': 6.7612, 'eval_samples_per_second': 78.093, 'eval_steps_per_second': 2.514, 'epoch': 4.0}
{'train@fra.sdrt.annodis_loss': 2.198432445526123, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.3528604118993135, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.08203439066800869, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.11178748431641525, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1087313659598482, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.198432445526123, 'train@fra.sdrt.annodis_runtime': 26.799, 'train@fra.sdrt.annodis_samples_per_second': 81.533, 'train@fra.sdrt.annodis_steps_per_second': 2.575, 'epoch': 5.0}
{'loss': 2.2578, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.235126495361328, 'eval_accuracy@fra.sdrt.annodis': 0.3143939393939394, 'eval_f1@fra.sdrt.annodis': 0.07226268643327835, 'eval_precision@fra.sdrt.annodis': 0.10802074600610655, 'eval_recall@fra.sdrt.annodis': 0.09594701496031083, 'eval_loss@fra.sdrt.annodis': 2.235126495361328, 'eval_runtime': 6.7739, 'eval_samples_per_second': 77.947, 'eval_steps_per_second': 2.51, 'epoch': 5.0}
{'train@fra.sdrt.annodis_loss': 2.1507575511932373, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.38123569794050344, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.10860411269773088, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12337451861574723, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1317424693948057, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.1507577896118164, 'train@fra.sdrt.annodis_runtime': 26.8041, 'train@fra.sdrt.annodis_samples_per_second': 81.517, 'train@fra.sdrt.annodis_steps_per_second': 2.574, 'epoch': 6.0}
{'loss': 2.215, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.1944525241851807, 'eval_accuracy@fra.sdrt.annodis': 0.3314393939393939, 'eval_f1@fra.sdrt.annodis': 0.08628446724843529, 'eval_precision@fra.sdrt.annodis': 0.11923736651384706, 'eval_recall@fra.sdrt.annodis': 0.10710081739292104, 'eval_loss@fra.sdrt.annodis': 2.194453001022339, 'eval_runtime': 6.7767, 'eval_samples_per_second': 77.914, 'eval_steps_per_second': 2.509, 'epoch': 6.0}
{'train@fra.sdrt.annodis_loss': 2.105365514755249, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4032036613272311, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.12257239601864105, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12443276649421403, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1505082545065585, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.10536527633667, 'train@fra.sdrt.annodis_runtime': 26.8553, 'train@fra.sdrt.annodis_samples_per_second': 81.362, 'train@fra.sdrt.annodis_steps_per_second': 2.569, 'epoch': 7.0}
{'loss': 2.1706, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.1570329666137695, 'eval_accuracy@fra.sdrt.annodis': 0.3371212121212121, 'eval_f1@fra.sdrt.annodis': 0.09156290206003109, 'eval_precision@fra.sdrt.annodis': 0.09364700279731478, 'eval_recall@fra.sdrt.annodis': 0.11294916161399754, 'eval_loss@fra.sdrt.annodis': 2.1570329666137695, 'eval_runtime': 6.7755, 'eval_samples_per_second': 77.928, 'eval_steps_per_second': 2.509, 'epoch': 7.0}
{'train@fra.sdrt.annodis_loss': 2.0661768913269043, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.41327231121281466, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.1281287642009519, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.125891785526709, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1589178426879331, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.066176652908325, 'train@fra.sdrt.annodis_runtime': 26.808, 'train@fra.sdrt.annodis_samples_per_second': 81.506, 'train@fra.sdrt.annodis_steps_per_second': 2.574, 'epoch': 8.0}
{'loss': 2.1401, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.126433849334717, 'eval_accuracy@fra.sdrt.annodis': 0.3409090909090909, 'eval_f1@fra.sdrt.annodis': 0.09420211436107936, 'eval_precision@fra.sdrt.annodis': 0.08413165902068881, 'eval_recall@fra.sdrt.annodis': 0.11779620420562914, 'eval_loss@fra.sdrt.annodis': 2.126434087753296, 'eval_runtime': 6.7988, 'eval_samples_per_second': 77.661, 'eval_steps_per_second': 2.5, 'epoch': 8.0}
{'train@fra.sdrt.annodis_loss': 2.0366909503936768, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.41098398169336386, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.1264589861516089, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.1250124032210922, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.15887602890492883, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.0366909503936768, 'train@fra.sdrt.annodis_runtime': 26.8434, 'train@fra.sdrt.annodis_samples_per_second': 81.398, 'train@fra.sdrt.annodis_steps_per_second': 2.57, 'epoch': 9.0}
{'loss': 2.0994, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.1020474433898926, 'eval_accuracy@fra.sdrt.annodis': 0.3446969696969697, 'eval_f1@fra.sdrt.annodis': 0.09629333036877766, 'eval_precision@fra.sdrt.annodis': 0.08571581672105233, 'eval_recall@fra.sdrt.annodis': 0.12110159866000653, 'eval_loss@fra.sdrt.annodis': 2.1020472049713135, 'eval_runtime': 6.7653, 'eval_samples_per_second': 78.045, 'eval_steps_per_second': 2.513, 'epoch': 9.0}
{'train@fra.sdrt.annodis_loss': 2.0147714614868164, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.41876430205949655, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.13085583144561214, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12421598835196253, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1627044779372454, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.0147714614868164, 'train@fra.sdrt.annodis_runtime': 26.8181, 'train@fra.sdrt.annodis_samples_per_second': 81.475, 'train@fra.sdrt.annodis_steps_per_second': 2.573, 'epoch': 10.0}
{'loss': 2.0694, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.0846331119537354, 'eval_accuracy@fra.sdrt.annodis': 0.3446969696969697, 'eval_f1@fra.sdrt.annodis': 0.09591400160073554, 'eval_precision@fra.sdrt.annodis': 0.08407243074278314, 'eval_recall@fra.sdrt.annodis': 0.12123083831849508, 'eval_loss@fra.sdrt.annodis': 2.0846328735351562, 'eval_runtime': 6.7519, 'eval_samples_per_second': 78.201, 'eval_steps_per_second': 2.518, 'epoch': 10.0}
{'train@fra.sdrt.annodis_loss': 2.001897096633911, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.422883295194508, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.1332032260982094, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.1259056091207683, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1656096719792977, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.0018973350524902, 'train@fra.sdrt.annodis_runtime': 26.792, 'train@fra.sdrt.annodis_samples_per_second': 81.554, 'train@fra.sdrt.annodis_steps_per_second': 2.575, 'epoch': 11.0}
{'loss': 2.0498, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.0736348628997803, 'eval_accuracy@fra.sdrt.annodis': 0.3503787878787879, 'eval_f1@fra.sdrt.annodis': 0.09994079788086767, 'eval_precision@fra.sdrt.annodis': 0.08958241731035847, 'eval_recall@fra.sdrt.annodis': 0.1257252519423874, 'eval_loss@fra.sdrt.annodis': 2.0736351013183594, 'eval_runtime': 6.7457, 'eval_samples_per_second': 78.272, 'eval_steps_per_second': 2.52, 'epoch': 11.0}
{'train@fra.sdrt.annodis_loss': 1.997462511062622, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4265446224256293, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.13505778880905842, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12708516937151196, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.16723017066534518, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.997462511062622, 'train@fra.sdrt.annodis_runtime': 26.7594, 'train@fra.sdrt.annodis_samples_per_second': 81.653, 'train@fra.sdrt.annodis_steps_per_second': 2.579, 'epoch': 12.0}
{'loss': 2.0454, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.0704641342163086, 'eval_accuracy@fra.sdrt.annodis': 0.3484848484848485, 'eval_f1@fra.sdrt.annodis': 0.09936349601560927, 'eval_precision@fra.sdrt.annodis': 0.08888217986412345, 'eval_recall@fra.sdrt.annodis': 0.12520604114280276, 'eval_loss@fra.sdrt.annodis': 2.0704634189605713, 'eval_runtime': 6.7602, 'eval_samples_per_second': 78.105, 'eval_steps_per_second': 2.515, 'epoch': 12.0}
{'train_runtime': 1073.3829, 'train_samples_per_second': 24.427, 'train_steps_per_second': 0.771, 'train_loss': 2.2871822412463203, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.2872
  train_runtime            = 0:17:53.38
  train_samples_per_second =     24.427
  train_steps_per_second   =      0.771
{'train@deu.rst.pcc_loss': 3.3383846282958984, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.09658040665434381, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.008772273168806344, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.05192501321232664, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.03933699253700486, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.3383846282958984, 'train@deu.rst.pcc_runtime': 26.4837, 'train@deu.rst.pcc_samples_per_second': 81.711, 'train@deu.rst.pcc_steps_per_second': 2.568, 'epoch': 1.0}
{'loss': 3.6438, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.3513388633728027, 'eval_accuracy@deu.rst.pcc': 0.05394190871369295, 'eval_f1@deu.rst.pcc': 0.004350736278447121, 'eval_precision@deu.rst.pcc': 0.0022951977401129945, 'eval_recall@deu.rst.pcc': 0.041666666666666664, 'eval_loss@deu.rst.pcc': 3.3513386249542236, 'eval_runtime': 3.3408, 'eval_samples_per_second': 72.138, 'eval_steps_per_second': 2.395, 'epoch': 1.0}
{'train@deu.rst.pcc_loss': 3.0320451259613037, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.1321626617375231, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.036830337507881956, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.035775236469089325, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.07353949830010502, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.0320451259613037, 'train@deu.rst.pcc_runtime': 26.4935, 'train@deu.rst.pcc_samples_per_second': 81.68, 'train@deu.rst.pcc_steps_per_second': 2.567, 'epoch': 2.0}
{'loss': 3.1959, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.070842742919922, 'eval_accuracy@deu.rst.pcc': 0.1078838174273859, 'eval_f1@deu.rst.pcc': 0.04396666867623267, 'eval_precision@deu.rst.pcc': 0.0916566204540888, 'eval_recall@deu.rst.pcc': 0.09904100529100529, 'eval_loss@deu.rst.pcc': 3.070842742919922, 'eval_runtime': 3.2837, 'eval_samples_per_second': 73.392, 'eval_steps_per_second': 2.436, 'epoch': 2.0}
{'train@deu.rst.pcc_loss': 2.8660998344421387, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.17929759704251386, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.057422058701782915, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.0657416888353716, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.10708590692324854, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.8661000728607178, 'train@deu.rst.pcc_runtime': 26.4823, 'train@deu.rst.pcc_samples_per_second': 81.715, 'train@deu.rst.pcc_steps_per_second': 2.568, 'epoch': 3.0}
{'loss': 2.9624, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.9146206378936768, 'eval_accuracy@deu.rst.pcc': 0.14937759336099585, 'eval_f1@deu.rst.pcc': 0.04738770729074673, 'eval_precision@deu.rst.pcc': 0.03306573633469452, 'eval_recall@deu.rst.pcc': 0.11389016076516077, 'eval_loss@deu.rst.pcc': 2.9146206378936768, 'eval_runtime': 3.2927, 'eval_samples_per_second': 73.193, 'eval_steps_per_second': 2.43, 'epoch': 3.0}
{'train@deu.rst.pcc_loss': 2.791943073272705, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.19963031423290203, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.06838444137018902, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.06960604225766685, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.11742151081668281, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.791943311691284, 'train@deu.rst.pcc_runtime': 26.4608, 'train@deu.rst.pcc_samples_per_second': 81.781, 'train@deu.rst.pcc_steps_per_second': 2.57, 'epoch': 4.0}
{'loss': 2.8605, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.8484723567962646, 'eval_accuracy@deu.rst.pcc': 0.17427385892116182, 'eval_f1@deu.rst.pcc': 0.0518594524228323, 'eval_precision@deu.rst.pcc': 0.03919097310641428, 'eval_recall@deu.rst.pcc': 0.11849435286935285, 'eval_loss@deu.rst.pcc': 2.8484725952148438, 'eval_runtime': 3.2767, 'eval_samples_per_second': 73.55, 'eval_steps_per_second': 2.442, 'epoch': 4.0}
{'train@deu.rst.pcc_loss': 2.7430789470672607, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2088724584103512, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.07716957071467245, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.08667521230085953, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.12651614344751255, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7430789470672607, 'train@deu.rst.pcc_runtime': 26.5265, 'train@deu.rst.pcc_samples_per_second': 81.579, 'train@deu.rst.pcc_steps_per_second': 2.563, 'epoch': 5.0}
{'loss': 2.8058, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.8043603897094727, 'eval_accuracy@deu.rst.pcc': 0.1908713692946058, 'eval_f1@deu.rst.pcc': 0.07209068081949438, 'eval_precision@deu.rst.pcc': 0.09004116556620478, 'eval_recall@deu.rst.pcc': 0.13278792966292965, 'eval_loss@deu.rst.pcc': 2.8043603897094727, 'eval_runtime': 3.2915, 'eval_samples_per_second': 73.219, 'eval_steps_per_second': 2.43, 'epoch': 5.0}
{'train@deu.rst.pcc_loss': 2.7046620845794678, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.21857670979667282, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08718192059086939, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.09504353918862653, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.1344395562600299, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7046618461608887, 'train@deu.rst.pcc_runtime': 26.5338, 'train@deu.rst.pcc_samples_per_second': 81.556, 'train@deu.rst.pcc_steps_per_second': 2.563, 'epoch': 6.0}
{'loss': 2.7677, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.770927906036377, 'eval_accuracy@deu.rst.pcc': 0.18672199170124482, 'eval_f1@deu.rst.pcc': 0.07042779439474216, 'eval_precision@deu.rst.pcc': 0.07525819088319088, 'eval_recall@deu.rst.pcc': 0.13129983442483442, 'eval_loss@deu.rst.pcc': 2.770927906036377, 'eval_runtime': 3.309, 'eval_samples_per_second': 72.831, 'eval_steps_per_second': 2.418, 'epoch': 6.0}
{'train@deu.rst.pcc_loss': 2.6757171154022217, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.23059149722735675, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09551153441183306, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.09711444580777348, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.14249030081970251, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6757166385650635, 'train@deu.rst.pcc_runtime': 26.5374, 'train@deu.rst.pcc_samples_per_second': 81.545, 'train@deu.rst.pcc_steps_per_second': 2.562, 'epoch': 7.0}
{'loss': 2.7407, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.745981454849243, 'eval_accuracy@deu.rst.pcc': 0.1908713692946058, 'eval_f1@deu.rst.pcc': 0.07608723023847297, 'eval_precision@deu.rst.pcc': 0.07291176331757751, 'eval_recall@deu.rst.pcc': 0.13450496262996262, 'eval_loss@deu.rst.pcc': 2.745981454849243, 'eval_runtime': 3.8057, 'eval_samples_per_second': 63.326, 'eval_steps_per_second': 2.102, 'epoch': 7.0}
{'train@deu.rst.pcc_loss': 2.655471086502075, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.23197781885397412, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09938911803411878, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.13299099690098332, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.14480082880873335, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6554715633392334, 'train@deu.rst.pcc_runtime': 26.5683, 'train@deu.rst.pcc_samples_per_second': 81.45, 'train@deu.rst.pcc_steps_per_second': 2.559, 'epoch': 8.0}
{'loss': 2.7043, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.7281084060668945, 'eval_accuracy@deu.rst.pcc': 0.2074688796680498, 'eval_f1@deu.rst.pcc': 0.08659860164224842, 'eval_precision@deu.rst.pcc': 0.08521645021645023, 'eval_recall@deu.rst.pcc': 0.14358384670884672, 'eval_loss@deu.rst.pcc': 2.7281088829040527, 'eval_runtime': 3.2782, 'eval_samples_per_second': 73.516, 'eval_steps_per_second': 2.44, 'epoch': 8.0}
{'train@deu.rst.pcc_loss': 2.6348490715026855, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.23706099815157117, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.10319231283401102, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.14759508959910866, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.14809785030527034, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6348488330841064, 'train@deu.rst.pcc_runtime': 26.5788, 'train@deu.rst.pcc_samples_per_second': 81.418, 'train@deu.rst.pcc_steps_per_second': 2.558, 'epoch': 9.0}
{'loss': 2.6898, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.7154102325439453, 'eval_accuracy@deu.rst.pcc': 0.1991701244813278, 'eval_f1@deu.rst.pcc': 0.0841984146497331, 'eval_precision@deu.rst.pcc': 0.0777939104515389, 'eval_recall@deu.rst.pcc': 0.14060765623265623, 'eval_loss@deu.rst.pcc': 2.715409994125366, 'eval_runtime': 3.2856, 'eval_samples_per_second': 73.35, 'eval_steps_per_second': 2.435, 'epoch': 9.0}
{'train@deu.rst.pcc_loss': 2.622184991836548, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.24075785582255083, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.10697029286390641, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.14539768897597055, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.15164170266512123, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.622184991836548, 'train@deu.rst.pcc_runtime': 26.5301, 'train@deu.rst.pcc_samples_per_second': 81.568, 'train@deu.rst.pcc_steps_per_second': 2.563, 'epoch': 10.0}
{'loss': 2.6773, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.7021164894104004, 'eval_accuracy@deu.rst.pcc': 0.21161825726141079, 'eval_f1@deu.rst.pcc': 0.0982287073177716, 'eval_precision@deu.rst.pcc': 0.12442779743476336, 'eval_recall@deu.rst.pcc': 0.15049822862322862, 'eval_loss@deu.rst.pcc': 2.702116012573242, 'eval_runtime': 3.2681, 'eval_samples_per_second': 73.742, 'eval_steps_per_second': 2.448, 'epoch': 10.0}
{'train@deu.rst.pcc_loss': 2.6150259971618652, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.24399260628465805, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.1092401895959486, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.14920688150807962, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.15363721723780424, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6150259971618652, 'train@deu.rst.pcc_runtime': 26.5206, 'train@deu.rst.pcc_samples_per_second': 81.597, 'train@deu.rst.pcc_steps_per_second': 2.564, 'epoch': 11.0}
{'loss': 2.664, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.696014165878296, 'eval_accuracy@deu.rst.pcc': 0.2074688796680498, 'eval_f1@deu.rst.pcc': 0.09607369448854956, 'eval_precision@deu.rst.pcc': 0.10224556649239046, 'eval_recall@deu.rst.pcc': 0.14729310041810043, 'eval_loss@deu.rst.pcc': 2.696014165878296, 'eval_runtime': 3.2923, 'eval_samples_per_second': 73.201, 'eval_steps_per_second': 2.43, 'epoch': 11.0}
{'train@deu.rst.pcc_loss': 2.612577199935913, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.24399260628465805, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.10986931021382643, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.14662636631238807, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.15396896152639472, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.612577438354492, 'train@deu.rst.pcc_runtime': 26.493, 'train@deu.rst.pcc_samples_per_second': 81.682, 'train@deu.rst.pcc_steps_per_second': 2.567, 'epoch': 12.0}
{'loss': 2.6563, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.6934783458709717, 'eval_accuracy@deu.rst.pcc': 0.2074688796680498, 'eval_f1@deu.rst.pcc': 0.09636455070876283, 'eval_precision@deu.rst.pcc': 0.10307015845571538, 'eval_recall@deu.rst.pcc': 0.14729310041810043, 'eval_loss@deu.rst.pcc': 2.693478584289551, 'eval_runtime': 3.2625, 'eval_samples_per_second': 73.87, 'eval_steps_per_second': 2.452, 'epoch': 12.0}
{'train_runtime': 1020.6527, 'train_samples_per_second': 25.443, 'train_steps_per_second': 0.799, 'train_loss': 2.8640439463596716, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.2872
  train_runtime            = 0:17:53.38
  train_samples_per_second =     24.427
  train_steps_per_second   =      0.771
-------------------------------------------------------------------
Lang1:  nld.rst.nldt    Lang2:  deu.rst.pcc
Saving run to:  runs/full_shot/FullShot=v4_nld.rst.nldt_deu.rst.pcc_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 1608 examples
read 331 examples
read 326 examples
read 2164 examples
read 241 examples
read 260 examples
Total prediction labels:  39
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=39, bias=True)
    )
  )
)
{'train@nld.rst.nldt_loss': 3.1586034297943115, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26430348258706465, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01306566650270536, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.00825948383084577, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03125, 'train@nld.rst.nldt_loss@nld.rst.nldt': 3.1586036682128906, 'train@nld.rst.nldt_runtime': 19.5813, 'train@nld.rst.nldt_samples_per_second': 82.119, 'train@nld.rst.nldt_steps_per_second': 2.605, 'epoch': 1.0}
{'loss': 3.4412, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.123213529586792, 'eval_accuracy@nld.rst.nldt': 0.27794561933534745, 'eval_f1@nld.rst.nldt': 0.016110673321075214, 'eval_precision@nld.rst.nldt': 0.010294282197605462, 'eval_recall@nld.rst.nldt': 0.037037037037037035, 'eval_loss@nld.rst.nldt': 3.12321400642395, 'eval_runtime': 4.34, 'eval_samples_per_second': 76.268, 'eval_steps_per_second': 2.535, 'epoch': 1.0}
{'train@nld.rst.nldt_loss': 2.87304425239563, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26430348258706465, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01306566650270536, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.00825948383084577, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03125, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.87304425239563, 'train@nld.rst.nldt_runtime': 19.7679, 'train@nld.rst.nldt_samples_per_second': 81.344, 'train@nld.rst.nldt_steps_per_second': 2.58, 'epoch': 2.0}
{'loss': 3.0167, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.8099026679992676, 'eval_accuracy@nld.rst.nldt': 0.27794561933534745, 'eval_f1@nld.rst.nldt': 0.016110673321075214, 'eval_precision@nld.rst.nldt': 0.010294282197605462, 'eval_recall@nld.rst.nldt': 0.037037037037037035, 'eval_loss@nld.rst.nldt': 2.8099026679992676, 'eval_runtime': 4.348, 'eval_samples_per_second': 76.126, 'eval_steps_per_second': 2.53, 'epoch': 2.0}
{'train@nld.rst.nldt_loss': 2.7816121578216553, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.27052238805970147, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.018255111308705163, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.03678314513355497, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.0340219421101774, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.781611919403076, 'train@nld.rst.nldt_runtime': 19.7985, 'train@nld.rst.nldt_samples_per_second': 81.218, 'train@nld.rst.nldt_steps_per_second': 2.576, 'epoch': 3.0}
{'loss': 2.8541, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.721632242202759, 'eval_accuracy@nld.rst.nldt': 0.2930513595166163, 'eval_f1@nld.rst.nldt': 0.029898775661487523, 'eval_precision@nld.rst.nldt': 0.04765201338410061, 'eval_recall@nld.rst.nldt': 0.046325690770135215, 'eval_loss@nld.rst.nldt': 2.721632242202759, 'eval_runtime': 4.3509, 'eval_samples_per_second': 76.076, 'eval_steps_per_second': 2.528, 'epoch': 3.0}
{'train@nld.rst.nldt_loss': 2.734285831451416, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.28171641791044777, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.025727949398793457, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.025688307309743474, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.04064192343604108, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.734285354614258, 'train@nld.rst.nldt_runtime': 19.8019, 'train@nld.rst.nldt_samples_per_second': 81.204, 'train@nld.rst.nldt_steps_per_second': 2.576, 'epoch': 4.0}
{'loss': 2.7653, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.6817328929901123, 'eval_accuracy@nld.rst.nldt': 0.30513595166163143, 'eval_f1@nld.rst.nldt': 0.03644991375395397, 'eval_precision@nld.rst.nldt': 0.03730876502194331, 'eval_recall@nld.rst.nldt': 0.05686680469289165, 'eval_loss@nld.rst.nldt': 2.6817331314086914, 'eval_runtime': 4.3721, 'eval_samples_per_second': 75.707, 'eval_steps_per_second': 2.516, 'epoch': 4.0}
{'train@nld.rst.nldt_loss': 2.6993842124938965, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2966417910447761, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03158684683272649, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.02810917472036209, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.04776027077497666, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6993839740753174, 'train@nld.rst.nldt_runtime': 19.8331, 'train@nld.rst.nldt_samples_per_second': 81.076, 'train@nld.rst.nldt_steps_per_second': 2.571, 'epoch': 5.0}
{'loss': 2.7378, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.6494715213775635, 'eval_accuracy@nld.rst.nldt': 0.311178247734139, 'eval_f1@nld.rst.nldt': 0.03812695922196728, 'eval_precision@nld.rst.nldt': 0.03588677118088883, 'eval_recall@nld.rst.nldt': 0.06063824348848503, 'eval_loss@nld.rst.nldt': 2.6494715213775635, 'eval_runtime': 4.3886, 'eval_samples_per_second': 75.422, 'eval_steps_per_second': 2.506, 'epoch': 5.0}
{'train@nld.rst.nldt_loss': 2.666245222091675, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31156716417910446, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03690227853398529, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.030131898143450346, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05610060690943044, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.666245460510254, 'train@nld.rst.nldt_runtime': 19.8222, 'train@nld.rst.nldt_samples_per_second': 81.121, 'train@nld.rst.nldt_steps_per_second': 2.573, 'epoch': 6.0}
{'loss': 2.7052, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.622750997543335, 'eval_accuracy@nld.rst.nldt': 0.31722054380664655, 'eval_f1@nld.rst.nldt': 0.040335177426635296, 'eval_precision@nld.rst.nldt': 0.035720761708833634, 'eval_recall@nld.rst.nldt': 0.06440968228407842, 'eval_loss@nld.rst.nldt': 2.622750997543335, 'eval_runtime': 4.3831, 'eval_samples_per_second': 75.517, 'eval_steps_per_second': 2.51, 'epoch': 6.0}
{'train@nld.rst.nldt_loss': 2.6388425827026367, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31156716417910446, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.036920784148896435, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.02869695587374422, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05796276844070962, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.638842821121216, 'train@nld.rst.nldt_runtime': 19.8531, 'train@nld.rst.nldt_samples_per_second': 80.995, 'train@nld.rst.nldt_steps_per_second': 2.569, 'epoch': 7.0}
{'loss': 2.674, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.6000688076019287, 'eval_accuracy@nld.rst.nldt': 0.32628398791540786, 'eval_f1@nld.rst.nldt': 0.04318613842684039, 'eval_precision@nld.rst.nldt': 0.037381034285059046, 'eval_recall@nld.rst.nldt': 0.07023873424839609, 'eval_loss@nld.rst.nldt': 2.6000685691833496, 'eval_runtime': 4.3732, 'eval_samples_per_second': 75.688, 'eval_steps_per_second': 2.515, 'epoch': 7.0}
{'train@nld.rst.nldt_loss': 2.6171720027923584, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31156716417910446, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.037177184819972425, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.02802879595623856, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05999941643323996, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6171720027923584, 'train@nld.rst.nldt_runtime': 19.7875, 'train@nld.rst.nldt_samples_per_second': 81.263, 'train@nld.rst.nldt_steps_per_second': 2.577, 'epoch': 8.0}
{'loss': 2.6568, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.5841434001922607, 'eval_accuracy@nld.rst.nldt': 0.32326283987915405, 'eval_f1@nld.rst.nldt': 0.04227134622347974, 'eval_precision@nld.rst.nldt': 0.036011153198653195, 'eval_recall@nld.rst.nldt': 0.06983615775886308, 'eval_loss@nld.rst.nldt': 2.5841431617736816, 'eval_runtime': 4.3993, 'eval_samples_per_second': 75.239, 'eval_steps_per_second': 2.5, 'epoch': 8.0}
{'train@nld.rst.nldt_loss': 2.603029727935791, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31405472636815923, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03791288398867875, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.028391752685403056, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.060991479925303446, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.603029727935791, 'train@nld.rst.nldt_runtime': 19.765, 'train@nld.rst.nldt_samples_per_second': 81.356, 'train@nld.rst.nldt_steps_per_second': 2.58, 'epoch': 9.0}
{'loss': 2.6381, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.5724992752075195, 'eval_accuracy@nld.rst.nldt': 0.32326283987915405, 'eval_f1@nld.rst.nldt': 0.04227134622347974, 'eval_precision@nld.rst.nldt': 0.036011153198653195, 'eval_recall@nld.rst.nldt': 0.06983615775886308, 'eval_loss@nld.rst.nldt': 2.5724987983703613, 'eval_runtime': 4.3882, 'eval_samples_per_second': 75.429, 'eval_steps_per_second': 2.507, 'epoch': 9.0}
{'train@nld.rst.nldt_loss': 2.592954158782959, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31467661691542287, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03815724089121972, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.028577302631578948, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.06123949579831932, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.592954397201538, 'train@nld.rst.nldt_runtime': 19.79, 'train@nld.rst.nldt_samples_per_second': 81.253, 'train@nld.rst.nldt_steps_per_second': 2.577, 'epoch': 10.0}
{'loss': 2.6248, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.563950538635254, 'eval_accuracy@nld.rst.nldt': 0.32326283987915405, 'eval_f1@nld.rst.nldt': 0.04227134622347974, 'eval_precision@nld.rst.nldt': 0.036011153198653195, 'eval_recall@nld.rst.nldt': 0.06983615775886308, 'eval_loss@nld.rst.nldt': 2.563950777053833, 'eval_runtime': 4.4035, 'eval_samples_per_second': 75.167, 'eval_steps_per_second': 2.498, 'epoch': 10.0}
{'train@nld.rst.nldt_loss': 2.5858263969421387, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31467661691542287, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03776878053025739, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.02810314354140387, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.06123949579831932, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.5858266353607178, 'train@nld.rst.nldt_runtime': 19.8336, 'train@nld.rst.nldt_samples_per_second': 81.074, 'train@nld.rst.nldt_steps_per_second': 2.571, 'epoch': 11.0}
{'loss': 2.6137, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.5586981773376465, 'eval_accuracy@nld.rst.nldt': 0.3202416918429003, 'eval_f1@nld.rst.nldt': 0.041932369886260375, 'eval_precision@nld.rst.nldt': 0.03575837742504409, 'eval_recall@nld.rst.nldt': 0.06943358126933007, 'eval_loss@nld.rst.nldt': 2.5586984157562256, 'eval_runtime': 4.3587, 'eval_samples_per_second': 75.94, 'eval_steps_per_second': 2.524, 'epoch': 11.0}
{'train@nld.rst.nldt_loss': 2.5831408500671387, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31716417910447764, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03844588511943084, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.028582511768290214, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.062289915966386555, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.5831410884857178, 'train@nld.rst.nldt_runtime': 19.8065, 'train@nld.rst.nldt_samples_per_second': 81.186, 'train@nld.rst.nldt_steps_per_second': 2.575, 'epoch': 12.0}
{'loss': 2.6136, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.5572361946105957, 'eval_accuracy@nld.rst.nldt': 0.3202416918429003, 'eval_f1@nld.rst.nldt': 0.041932369886260375, 'eval_precision@nld.rst.nldt': 0.03575837742504409, 'eval_recall@nld.rst.nldt': 0.06943358126933007, 'eval_loss@nld.rst.nldt': 2.557236433029175, 'eval_runtime': 4.3773, 'eval_samples_per_second': 75.617, 'eval_steps_per_second': 2.513, 'epoch': 12.0}
{'train_runtime': 780.6151, 'train_samples_per_second': 24.719, 'train_steps_per_second': 0.784, 'train_loss': 2.7784186967837265, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.7784
  train_runtime            = 0:13:00.61
  train_samples_per_second =     24.719
  train_steps_per_second   =      0.784
{'train@deu.rst.pcc_loss': 3.1648879051208496, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.11876155268022182, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.02536082654865484, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.031093792553562476, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.05077188183258986, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.1648876667022705, 'train@deu.rst.pcc_runtime': 26.5335, 'train@deu.rst.pcc_samples_per_second': 81.557, 'train@deu.rst.pcc_steps_per_second': 2.563, 'epoch': 1.0}
{'loss': 3.5528, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.1860110759735107, 'eval_accuracy@deu.rst.pcc': 0.1037344398340249, 'eval_f1@deu.rst.pcc': 0.015090090090090089, 'eval_precision@deu.rst.pcc': 0.011353252923976606, 'eval_recall@deu.rst.pcc': 0.03968253968253968, 'eval_loss@deu.rst.pcc': 3.1860108375549316, 'eval_runtime': 3.3048, 'eval_samples_per_second': 72.924, 'eval_steps_per_second': 2.421, 'epoch': 1.0}
{'train@deu.rst.pcc_loss': 2.944887638092041, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.1377079482439926, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.04046906726469846, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.04545904566313072, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.0639502360901431, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.944887399673462, 'train@deu.rst.pcc_runtime': 26.4961, 'train@deu.rst.pcc_samples_per_second': 81.672, 'train@deu.rst.pcc_steps_per_second': 2.566, 'epoch': 2.0}
{'loss': 3.061, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.9852046966552734, 'eval_accuracy@deu.rst.pcc': 0.12448132780082988, 'eval_f1@deu.rst.pcc': 0.04111973023672189, 'eval_precision@deu.rst.pcc': 0.040780935141776165, 'eval_recall@deu.rst.pcc': 0.06330764143264143, 'eval_loss@deu.rst.pcc': 2.9852044582366943, 'eval_runtime': 3.2815, 'eval_samples_per_second': 73.443, 'eval_steps_per_second': 2.438, 'epoch': 2.0}
{'train@deu.rst.pcc_loss': 2.863028049468994, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.1779112754158965, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.05803190620605039, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.044693758655662545, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.09703513964176597, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.8630282878875732, 'train@deu.rst.pcc_runtime': 26.5349, 'train@deu.rst.pcc_samples_per_second': 81.553, 'train@deu.rst.pcc_steps_per_second': 2.563, 'epoch': 3.0}
{'loss': 2.9361, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.921388626098633, 'eval_accuracy@deu.rst.pcc': 0.16182572614107885, 'eval_f1@deu.rst.pcc': 0.057882021234228954, 'eval_precision@deu.rst.pcc': 0.04207099304852054, 'eval_recall@deu.rst.pcc': 0.11085037647537649, 'eval_loss@deu.rst.pcc': 2.921389102935791, 'eval_runtime': 3.2729, 'eval_samples_per_second': 73.636, 'eval_steps_per_second': 2.444, 'epoch': 3.0}
{'train@deu.rst.pcc_loss': 2.810349225997925, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.1954713493530499, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.06287064940219754, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.05093731616772558, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.11109122198704065, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.8103487491607666, 'train@deu.rst.pcc_runtime': 26.4782, 'train@deu.rst.pcc_samples_per_second': 81.728, 'train@deu.rst.pcc_steps_per_second': 2.568, 'epoch': 4.0}
{'loss': 2.8684, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.8799734115600586, 'eval_accuracy@deu.rst.pcc': 0.16182572614107885, 'eval_f1@deu.rst.pcc': 0.05267546478483978, 'eval_precision@deu.rst.pcc': 0.037101711988147414, 'eval_recall@deu.rst.pcc': 0.11085037647537647, 'eval_loss@deu.rst.pcc': 2.879973888397217, 'eval_runtime': 3.2813, 'eval_samples_per_second': 73.447, 'eval_steps_per_second': 2.438, 'epoch': 4.0}
{'train@deu.rst.pcc_loss': 2.7740116119384766, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.19685767097966728, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.06246366215415947, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.054802257898558485, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.11292313742939361, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7740116119384766, 'train@deu.rst.pcc_runtime': 26.488, 'train@deu.rst.pcc_samples_per_second': 81.697, 'train@deu.rst.pcc_steps_per_second': 2.567, 'epoch': 5.0}
{'loss': 2.8231, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.852055788040161, 'eval_accuracy@deu.rst.pcc': 0.17012448132780084, 'eval_f1@deu.rst.pcc': 0.05186770383471995, 'eval_precision@deu.rst.pcc': 0.03787548818306427, 'eval_recall@deu.rst.pcc': 0.11500305250305248, 'eval_loss@deu.rst.pcc': 2.852055549621582, 'eval_runtime': 3.3093, 'eval_samples_per_second': 72.825, 'eval_steps_per_second': 2.417, 'epoch': 5.0}
{'train@deu.rst.pcc_loss': 2.7439444065093994, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.19731977818853974, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.06191812721881766, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.04883055819187091, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.11471838966007643, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.743943929672241, 'train@deu.rst.pcc_runtime': 26.5347, 'train@deu.rst.pcc_samples_per_second': 81.554, 'train@deu.rst.pcc_steps_per_second': 2.563, 'epoch': 6.0}
{'loss': 2.7881, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.8276519775390625, 'eval_accuracy@deu.rst.pcc': 0.16182572614107885, 'eval_f1@deu.rst.pcc': 0.05376155515600595, 'eval_precision@deu.rst.pcc': 0.03850257570495665, 'eval_recall@deu.rst.pcc': 0.11829085266585267, 'eval_loss@deu.rst.pcc': 2.8276519775390625, 'eval_runtime': 3.2849, 'eval_samples_per_second': 73.365, 'eval_steps_per_second': 2.435, 'epoch': 6.0}
{'train@deu.rst.pcc_loss': 2.7209665775299072, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.19824399260628467, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.062236583871468985, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.05146914882200605, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.11505524007323881, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7209668159484863, 'train@deu.rst.pcc_runtime': 26.5473, 'train@deu.rst.pcc_samples_per_second': 81.515, 'train@deu.rst.pcc_steps_per_second': 2.561, 'epoch': 7.0}
{'loss': 2.7647, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.811384916305542, 'eval_accuracy@deu.rst.pcc': 0.17012448132780084, 'eval_f1@deu.rst.pcc': 0.05333688657467944, 'eval_precision@deu.rst.pcc': 0.03768811435478102, 'eval_recall@deu.rst.pcc': 0.12044032356532357, 'eval_loss@deu.rst.pcc': 2.811384677886963, 'eval_runtime': 3.2785, 'eval_samples_per_second': 73.509, 'eval_steps_per_second': 2.44, 'epoch': 7.0}
{'train@deu.rst.pcc_loss': 2.7033028602600098, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2010166358595194, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.06396382775710464, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.06089055005274782, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.11660444298183663, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7033028602600098, 'train@deu.rst.pcc_runtime': 26.5454, 'train@deu.rst.pcc_samples_per_second': 81.521, 'train@deu.rst.pcc_steps_per_second': 2.562, 'epoch': 8.0}
{'loss': 2.744, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.798630952835083, 'eval_accuracy@deu.rst.pcc': 0.17427385892116182, 'eval_f1@deu.rst.pcc': 0.05289156049846839, 'eval_precision@deu.rst.pcc': 0.03749032371569317, 'eval_recall@deu.rst.pcc': 0.12021138583638584, 'eval_loss@deu.rst.pcc': 2.798630952835083, 'eval_runtime': 3.2639, 'eval_samples_per_second': 73.838, 'eval_steps_per_second': 2.451, 'epoch': 8.0}
{'train@deu.rst.pcc_loss': 2.6888020038604736, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.20378927911275416, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.06791126845959693, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.09167574591180903, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.11850804928175075, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6888020038604736, 'train@deu.rst.pcc_runtime': 26.5617, 'train@deu.rst.pcc_samples_per_second': 81.471, 'train@deu.rst.pcc_steps_per_second': 2.56, 'epoch': 9.0}
{'loss': 2.7299, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.790947675704956, 'eval_accuracy@deu.rst.pcc': 0.17012448132780084, 'eval_f1@deu.rst.pcc': 0.053772682159781336, 'eval_precision@deu.rst.pcc': 0.037933625543615186, 'eval_recall@deu.rst.pcc': 0.12044032356532357, 'eval_loss@deu.rst.pcc': 2.790947675704956, 'eval_runtime': 3.2887, 'eval_samples_per_second': 73.281, 'eval_steps_per_second': 2.433, 'epoch': 9.0}
{'train@deu.rst.pcc_loss': 2.679314136505127, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.20794824399260628, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.07177561779915814, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.10187848573113568, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.12111740892531267, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.679313898086548, 'train@deu.rst.pcc_runtime': 26.5409, 'train@deu.rst.pcc_samples_per_second': 81.534, 'train@deu.rst.pcc_steps_per_second': 2.562, 'epoch': 10.0}
{'loss': 2.7143, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.7817039489746094, 'eval_accuracy@deu.rst.pcc': 0.17012448132780084, 'eval_f1@deu.rst.pcc': 0.05337820745428699, 'eval_precision@deu.rst.pcc': 0.037751484609438314, 'eval_recall@deu.rst.pcc': 0.12044032356532357, 'eval_loss@deu.rst.pcc': 2.7817046642303467, 'eval_runtime': 3.2849, 'eval_samples_per_second': 73.366, 'eval_steps_per_second': 2.435, 'epoch': 10.0}
{'train@deu.rst.pcc_loss': 2.6733808517456055, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.20702402957486138, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.07238824208142472, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.08950827696348176, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.12086965655493742, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6733808517456055, 'train@deu.rst.pcc_runtime': 26.4998, 'train@deu.rst.pcc_samples_per_second': 81.661, 'train@deu.rst.pcc_steps_per_second': 2.566, 'epoch': 11.0}
{'loss': 2.7048, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.7774887084960938, 'eval_accuracy@deu.rst.pcc': 0.16597510373443983, 'eval_f1@deu.rst.pcc': 0.05210380396807069, 'eval_precision@deu.rst.pcc': 0.03702358047596143, 'eval_recall@deu.rst.pcc': 0.11723519536019537, 'eval_loss@deu.rst.pcc': 2.7774887084960938, 'eval_runtime': 3.2797, 'eval_samples_per_second': 73.483, 'eval_steps_per_second': 2.439, 'epoch': 11.0}
{'train@deu.rst.pcc_loss': 2.6714115142822266, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2088724584103512, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.07429166697361018, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.09212488704014127, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.12235350436049869, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6714115142822266, 'train@deu.rst.pcc_runtime': 26.497, 'train@deu.rst.pcc_samples_per_second': 81.67, 'train@deu.rst.pcc_steps_per_second': 2.566, 'epoch': 12.0}
{'loss': 2.7039, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.7756147384643555, 'eval_accuracy@deu.rst.pcc': 0.17012448132780084, 'eval_f1@deu.rst.pcc': 0.053880259434115, 'eval_precision@deu.rst.pcc': 0.038299949609473416, 'eval_recall@deu.rst.pcc': 0.12044032356532357, 'eval_loss@deu.rst.pcc': 2.7756147384643555, 'eval_runtime': 3.2877, 'eval_samples_per_second': 73.305, 'eval_steps_per_second': 2.433, 'epoch': 12.0}
{'train_runtime': 1018.4968, 'train_samples_per_second': 25.496, 'train_steps_per_second': 0.801, 'train_loss': 2.865928275912416, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.7784
  train_runtime            = 0:13:00.61
  train_samples_per_second =     24.719
  train_steps_per_second   =      0.784
-------------------------------------------------------------------
Lang1:  por.rst.cstn    Lang2:  deu.rst.pcc
Saving run to:  runs/full_shot/FullShot=v4_por.rst.cstn_deu.rst.pcc_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 4148 examples
read 573 examples
read 272 examples
read 2164 examples
read 241 examples
read 260 examples
Total prediction labels:  42
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (pt): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (pt): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=42, bias=True)
    )
  )
)
{'train@por.rst.cstn_loss': 2.508918046951294, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.2774831243972999, 'train@por.rst.cstn_f1@por.rst.cstn': 0.013575674655595393, 'train@por.rst.cstn_precision@por.rst.cstn': 0.008671347637415621, 'train@por.rst.cstn_recall@por.rst.cstn': 0.03125, 'train@por.rst.cstn_loss@por.rst.cstn': 2.508917808532715, 'train@por.rst.cstn_runtime': 51.2696, 'train@por.rst.cstn_samples_per_second': 80.906, 'train@por.rst.cstn_steps_per_second': 2.536, 'epoch': 1.0}
{'loss': 3.0392, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.615081310272217, 'eval_accuracy@por.rst.cstn': 0.28097731239092494, 'eval_f1@por.rst.cstn': 0.019940549913301957, 'eval_precision@por.rst.cstn': 0.012771696017769315, 'eval_recall@por.rst.cstn': 0.045454545454545456, 'eval_loss@por.rst.cstn': 2.615081310272217, 'eval_runtime': 7.3156, 'eval_samples_per_second': 78.325, 'eval_steps_per_second': 2.46, 'epoch': 1.0}
{'train@por.rst.cstn_loss': 2.208914279937744, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.383076181292189, 'train@por.rst.cstn_f1@por.rst.cstn': 0.05749253648909142, 'train@por.rst.cstn_precision@por.rst.cstn': 0.09099757321377741, 'train@por.rst.cstn_recall@por.rst.cstn': 0.06593652930741328, 'train@por.rst.cstn_loss@por.rst.cstn': 2.208914279937744, 'train@por.rst.cstn_runtime': 50.5124, 'train@por.rst.cstn_samples_per_second': 82.118, 'train@por.rst.cstn_steps_per_second': 2.574, 'epoch': 2.0}
{'loss': 2.387, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.333563804626465, 'eval_accuracy@por.rst.cstn': 0.33158813263525305, 'eval_f1@por.rst.cstn': 0.07455247455247456, 'eval_precision@por.rst.cstn': 0.09783390041692624, 'eval_recall@por.rst.cstn': 0.08797033006519173, 'eval_loss@por.rst.cstn': 2.3335633277893066, 'eval_runtime': 7.3053, 'eval_samples_per_second': 78.436, 'eval_steps_per_second': 2.464, 'epoch': 2.0}
{'train@por.rst.cstn_loss': 1.9564721584320068, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.46648987463837993, 'train@por.rst.cstn_f1@por.rst.cstn': 0.07333396766472397, 'train@por.rst.cstn_precision@por.rst.cstn': 0.0762386205525078, 'train@por.rst.cstn_recall@por.rst.cstn': 0.08986527898027472, 'train@por.rst.cstn_loss@por.rst.cstn': 1.9564721584320068, 'train@por.rst.cstn_runtime': 50.4907, 'train@por.rst.cstn_samples_per_second': 82.154, 'train@por.rst.cstn_steps_per_second': 2.575, 'epoch': 3.0}
{'loss': 2.1385, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.093997001647949, 'eval_accuracy@por.rst.cstn': 0.37521815008726006, 'eval_f1@por.rst.cstn': 0.09212165682753919, 'eval_precision@por.rst.cstn': 0.09208034290271132, 'eval_recall@por.rst.cstn': 0.12565862644393952, 'eval_loss@por.rst.cstn': 2.093997001647949, 'eval_runtime': 7.3164, 'eval_samples_per_second': 78.317, 'eval_steps_per_second': 2.46, 'epoch': 3.0}
{'train@por.rst.cstn_loss': 1.8014132976531982, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5385728061716489, 'train@por.rst.cstn_f1@por.rst.cstn': 0.10755028946715356, 'train@por.rst.cstn_precision@por.rst.cstn': 0.11498287035403446, 'train@por.rst.cstn_recall@por.rst.cstn': 0.11922770325458554, 'train@por.rst.cstn_loss@por.rst.cstn': 1.8014131784439087, 'train@por.rst.cstn_runtime': 50.4906, 'train@por.rst.cstn_samples_per_second': 82.154, 'train@por.rst.cstn_steps_per_second': 2.575, 'epoch': 4.0}
{'loss': 1.9411, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.957413673400879, 'eval_accuracy@por.rst.cstn': 0.4432809773123909, 'eval_f1@por.rst.cstn': 0.13457223104936442, 'eval_precision@por.rst.cstn': 0.13033353872647407, 'eval_recall@por.rst.cstn': 0.1637177444749735, 'eval_loss@por.rst.cstn': 1.957413911819458, 'eval_runtime': 7.3535, 'eval_samples_per_second': 77.922, 'eval_steps_per_second': 2.448, 'epoch': 4.0}
{'train@por.rst.cstn_loss': 1.7036638259887695, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5438765670202508, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1149349378864741, 'train@por.rst.cstn_precision@por.rst.cstn': 0.12309861318144255, 'train@por.rst.cstn_recall@por.rst.cstn': 0.12934434534586828, 'train@por.rst.cstn_loss@por.rst.cstn': 1.703663945198059, 'train@por.rst.cstn_runtime': 50.5886, 'train@por.rst.cstn_samples_per_second': 81.995, 'train@por.rst.cstn_steps_per_second': 2.57, 'epoch': 5.0}
{'loss': 1.8127, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.8589659929275513, 'eval_accuracy@por.rst.cstn': 0.44851657940663175, 'eval_f1@por.rst.cstn': 0.14720767123887918, 'eval_precision@por.rst.cstn': 0.1398599746919684, 'eval_recall@por.rst.cstn': 0.17403784644267273, 'eval_loss@por.rst.cstn': 1.8589659929275513, 'eval_runtime': 7.3313, 'eval_samples_per_second': 78.158, 'eval_steps_per_second': 2.455, 'epoch': 5.0}
{'train@por.rst.cstn_loss': 1.639715313911438, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5660559305689489, 'train@por.rst.cstn_f1@por.rst.cstn': 0.12390923446502908, 'train@por.rst.cstn_precision@por.rst.cstn': 0.12874859897523683, 'train@por.rst.cstn_recall@por.rst.cstn': 0.13788378724333933, 'train@por.rst.cstn_loss@por.rst.cstn': 1.639715313911438, 'train@por.rst.cstn_runtime': 50.5391, 'train@por.rst.cstn_samples_per_second': 82.075, 'train@por.rst.cstn_steps_per_second': 2.572, 'epoch': 6.0}
{'loss': 1.7319, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.7926342487335205, 'eval_accuracy@por.rst.cstn': 0.4781849912739965, 'eval_f1@por.rst.cstn': 0.16730376585746745, 'eval_precision@por.rst.cstn': 0.17337732046250431, 'eval_recall@por.rst.cstn': 0.1888319320579566, 'eval_loss@por.rst.cstn': 1.7926344871520996, 'eval_runtime': 7.3724, 'eval_samples_per_second': 77.723, 'eval_steps_per_second': 2.442, 'epoch': 6.0}
{'train@por.rst.cstn_loss': 1.597878336906433, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5797974927675988, 'train@por.rst.cstn_f1@por.rst.cstn': 0.13356080086214978, 'train@por.rst.cstn_precision@por.rst.cstn': 0.14331958702667746, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1460133522328783, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5978784561157227, 'train@por.rst.cstn_runtime': 50.4515, 'train@por.rst.cstn_samples_per_second': 82.218, 'train@por.rst.cstn_steps_per_second': 2.577, 'epoch': 7.0}
{'loss': 1.6711, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.7484475374221802, 'eval_accuracy@por.rst.cstn': 0.4816753926701571, 'eval_f1@por.rst.cstn': 0.17620960487015533, 'eval_precision@por.rst.cstn': 0.17683151813111275, 'eval_recall@por.rst.cstn': 0.19720597081997127, 'eval_loss@por.rst.cstn': 1.7484475374221802, 'eval_runtime': 7.2912, 'eval_samples_per_second': 78.587, 'eval_steps_per_second': 2.469, 'epoch': 7.0}
{'train@por.rst.cstn_loss': 1.570771336555481, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5860655737704918, 'train@por.rst.cstn_f1@por.rst.cstn': 0.13745456142070445, 'train@por.rst.cstn_precision@por.rst.cstn': 0.14193808042180106, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14926322286852728, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5707714557647705, 'train@por.rst.cstn_runtime': 50.4678, 'train@por.rst.cstn_samples_per_second': 82.191, 'train@por.rst.cstn_steps_per_second': 2.576, 'epoch': 8.0}
{'loss': 1.6433, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.7232880592346191, 'eval_accuracy@por.rst.cstn': 0.4886561954624782, 'eval_f1@por.rst.cstn': 0.1857241465992246, 'eval_precision@por.rst.cstn': 0.20361545021525143, 'eval_recall@por.rst.cstn': 0.20309339119147732, 'eval_loss@por.rst.cstn': 1.7232881784439087, 'eval_runtime': 7.3415, 'eval_samples_per_second': 78.05, 'eval_steps_per_second': 2.452, 'epoch': 8.0}
{'train@por.rst.cstn_loss': 1.550735592842102, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5935390549662488, 'train@por.rst.cstn_f1@por.rst.cstn': 0.14257139356243648, 'train@por.rst.cstn_precision@por.rst.cstn': 0.15370935971441682, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1534602707612748, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5507354736328125, 'train@por.rst.cstn_runtime': 50.5772, 'train@por.rst.cstn_samples_per_second': 82.013, 'train@por.rst.cstn_steps_per_second': 2.57, 'epoch': 9.0}
{'loss': 1.6207, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.703953742980957, 'eval_accuracy@por.rst.cstn': 0.493891797556719, 'eval_f1@por.rst.cstn': 0.18869970838294828, 'eval_precision@por.rst.cstn': 0.20043864118143498, 'eval_recall@por.rst.cstn': 0.2042126328817612, 'eval_loss@por.rst.cstn': 1.703953742980957, 'eval_runtime': 7.325, 'eval_samples_per_second': 78.226, 'eval_steps_per_second': 2.457, 'epoch': 9.0}
{'train@por.rst.cstn_loss': 1.5344502925872803, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5981195756991321, 'train@por.rst.cstn_f1@por.rst.cstn': 0.14810424516517023, 'train@por.rst.cstn_precision@por.rst.cstn': 0.15261337379548184, 'train@por.rst.cstn_recall@por.rst.cstn': 0.15931338795871497, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5344502925872803, 'train@por.rst.cstn_runtime': 50.486, 'train@por.rst.cstn_samples_per_second': 82.161, 'train@por.rst.cstn_steps_per_second': 2.575, 'epoch': 10.0}
{'loss': 1.5908, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.6886283159255981, 'eval_accuracy@por.rst.cstn': 0.5008726003490401, 'eval_f1@por.rst.cstn': 0.1964405672994267, 'eval_precision@por.rst.cstn': 0.20284039993139746, 'eval_recall@por.rst.cstn': 0.21288328318700692, 'eval_loss@por.rst.cstn': 1.6886283159255981, 'eval_runtime': 7.3036, 'eval_samples_per_second': 78.454, 'eval_steps_per_second': 2.465, 'epoch': 10.0}
{'train@por.rst.cstn_loss': 1.5265538692474365, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.6012536162005786, 'train@por.rst.cstn_f1@por.rst.cstn': 0.15012786948076767, 'train@por.rst.cstn_precision@por.rst.cstn': 0.15468774319742218, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1609097470260597, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5265538692474365, 'train@por.rst.cstn_runtime': 50.5078, 'train@por.rst.cstn_samples_per_second': 82.126, 'train@por.rst.cstn_steps_per_second': 2.574, 'epoch': 11.0}
{'loss': 1.5861, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.6819164752960205, 'eval_accuracy@por.rst.cstn': 0.5095986038394416, 'eval_f1@por.rst.cstn': 0.19869500871243143, 'eval_precision@por.rst.cstn': 0.20527629387549887, 'eval_recall@por.rst.cstn': 0.21476332014609525, 'eval_loss@por.rst.cstn': 1.6819162368774414, 'eval_runtime': 7.3316, 'eval_samples_per_second': 78.155, 'eval_steps_per_second': 2.455, 'epoch': 11.0}
{'train@por.rst.cstn_loss': 1.5240843296051025, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.6017357762777242, 'train@por.rst.cstn_f1@por.rst.cstn': 0.15009825207682329, 'train@por.rst.cstn_precision@por.rst.cstn': 0.15377865272567165, 'train@por.rst.cstn_recall@por.rst.cstn': 0.16092261718976253, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5240840911865234, 'train@por.rst.cstn_runtime': 50.5768, 'train@por.rst.cstn_samples_per_second': 82.014, 'train@por.rst.cstn_steps_per_second': 2.57, 'epoch': 12.0}
{'loss': 1.5803, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.6783682107925415, 'eval_accuracy@por.rst.cstn': 0.5078534031413613, 'eval_f1@por.rst.cstn': 0.19856719201439665, 'eval_precision@por.rst.cstn': 0.2046531413779878, 'eval_recall@por.rst.cstn': 0.21428485124657373, 'eval_loss@por.rst.cstn': 1.6783682107925415, 'eval_runtime': 7.3435, 'eval_samples_per_second': 78.028, 'eval_steps_per_second': 2.451, 'epoch': 12.0}
{'train_runtime': 1968.3658, 'train_samples_per_second': 25.288, 'train_steps_per_second': 0.793, 'train_loss': 1.8952247326190654, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.8952
  train_runtime            = 0:32:48.36
  train_samples_per_second =     25.288
  train_steps_per_second   =      0.793
{'train@deu.rst.pcc_loss': 3.1137876510620117, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.12754158964879853, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.029637688654067167, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.023041591682191626, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.056775130720864625, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.1137876510620117, 'train@deu.rst.pcc_runtime': 26.581, 'train@deu.rst.pcc_samples_per_second': 81.412, 'train@deu.rst.pcc_steps_per_second': 2.558, 'epoch': 1.0}
{'loss': 3.4149, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.1885223388671875, 'eval_accuracy@deu.rst.pcc': 0.11203319502074689, 'eval_f1@deu.rst.pcc': 0.028912921018184174, 'eval_precision@deu.rst.pcc': 0.02607551216175939, 'eval_recall@deu.rst.pcc': 0.059330715580715586, 'eval_loss@deu.rst.pcc': 3.1885225772857666, 'eval_runtime': 3.309, 'eval_samples_per_second': 72.832, 'eval_steps_per_second': 2.418, 'epoch': 1.0}
{'train@deu.rst.pcc_loss': 2.9101455211639404, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.17375231053604437, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.05834175659944622, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.05973859827702242, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.09099627903092457, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.9101457595825195, 'train@deu.rst.pcc_runtime': 26.5712, 'train@deu.rst.pcc_samples_per_second': 81.442, 'train@deu.rst.pcc_steps_per_second': 2.559, 'epoch': 2.0}
{'loss': 3.0307, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.974194049835205, 'eval_accuracy@deu.rst.pcc': 0.15767634854771784, 'eval_f1@deu.rst.pcc': 0.0410024735202723, 'eval_precision@deu.rst.pcc': 0.030556708182348017, 'eval_recall@deu.rst.pcc': 0.08106303418803419, 'eval_loss@deu.rst.pcc': 2.974194288253784, 'eval_runtime': 3.3094, 'eval_samples_per_second': 72.823, 'eval_steps_per_second': 2.417, 'epoch': 2.0}
{'train@deu.rst.pcc_loss': 2.8168258666992188, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.19500924214417745, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.06491451527537023, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.06357009601323715, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.10989725818274745, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.816826105117798, 'train@deu.rst.pcc_runtime': 26.554, 'train@deu.rst.pcc_samples_per_second': 81.494, 'train@deu.rst.pcc_steps_per_second': 2.561, 'epoch': 3.0}
{'loss': 2.9059, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.9016878604888916, 'eval_accuracy@deu.rst.pcc': 0.16182572614107885, 'eval_f1@deu.rst.pcc': 0.05060761795675589, 'eval_precision@deu.rst.pcc': 0.03485449735449735, 'eval_recall@deu.rst.pcc': 0.1063034188034188, 'eval_loss@deu.rst.pcc': 2.9016878604888916, 'eval_runtime': 3.3128, 'eval_samples_per_second': 72.749, 'eval_steps_per_second': 2.415, 'epoch': 3.0}
{'train@deu.rst.pcc_loss': 2.748668670654297, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.21164510166358594, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.07226663667003848, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.09345148312447665, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.1206133044551008, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.748668909072876, 'train@deu.rst.pcc_runtime': 26.5102, 'train@deu.rst.pcc_samples_per_second': 81.629, 'train@deu.rst.pcc_steps_per_second': 2.565, 'epoch': 4.0}
{'loss': 2.825, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.8452329635620117, 'eval_accuracy@deu.rst.pcc': 0.18672199170124482, 'eval_f1@deu.rst.pcc': 0.06917893332054724, 'eval_precision@deu.rst.pcc': 0.08401501719320868, 'eval_recall@deu.rst.pcc': 0.13354700854700854, 'eval_loss@deu.rst.pcc': 2.8452327251434326, 'eval_runtime': 3.3067, 'eval_samples_per_second': 72.882, 'eval_steps_per_second': 2.419, 'epoch': 4.0}
{'train@deu.rst.pcc_loss': 2.699110984802246, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.21487985212569316, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08158396259072555, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.12099488151840283, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.1266624960253248, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.699110746383667, 'train@deu.rst.pcc_runtime': 26.5173, 'train@deu.rst.pcc_samples_per_second': 81.607, 'train@deu.rst.pcc_steps_per_second': 2.564, 'epoch': 5.0}
{'loss': 2.7634, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.7990639209747314, 'eval_accuracy@deu.rst.pcc': 0.21161825726141079, 'eval_f1@deu.rst.pcc': 0.07794317819229192, 'eval_precision@deu.rst.pcc': 0.08893592649488875, 'eval_recall@deu.rst.pcc': 0.1406949531949532, 'eval_loss@deu.rst.pcc': 2.7990639209747314, 'eval_runtime': 3.2978, 'eval_samples_per_second': 73.079, 'eval_steps_per_second': 2.426, 'epoch': 5.0}
{'train@deu.rst.pcc_loss': 2.6575238704681396, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.23105360443622922, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09080647208319796, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.12410348713649966, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.1369058449455473, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6575238704681396, 'train@deu.rst.pcc_runtime': 26.5702, 'train@deu.rst.pcc_samples_per_second': 81.445, 'train@deu.rst.pcc_steps_per_second': 2.559, 'epoch': 6.0}
{'loss': 2.7215, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.7714784145355225, 'eval_accuracy@deu.rst.pcc': 0.1825726141078838, 'eval_f1@deu.rst.pcc': 0.0724578275224827, 'eval_precision@deu.rst.pcc': 0.08367751957322707, 'eval_recall@deu.rst.pcc': 0.1320589133089133, 'eval_loss@deu.rst.pcc': 2.7714786529541016, 'eval_runtime': 3.3051, 'eval_samples_per_second': 72.919, 'eval_steps_per_second': 2.421, 'epoch': 6.0}
{'train@deu.rst.pcc_loss': 2.626523494720459, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2342883548983364, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09718282790471318, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.18886301612762904, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.14096979321359177, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.626523494720459, 'train@deu.rst.pcc_runtime': 26.5555, 'train@deu.rst.pcc_samples_per_second': 81.49, 'train@deu.rst.pcc_steps_per_second': 2.561, 'epoch': 7.0}
{'loss': 2.6906, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.7458114624023438, 'eval_accuracy@deu.rst.pcc': 0.1991701244813278, 'eval_f1@deu.rst.pcc': 0.07832071261340358, 'eval_precision@deu.rst.pcc': 0.08755397201789478, 'eval_recall@deu.rst.pcc': 0.1396647334147334, 'eval_loss@deu.rst.pcc': 2.7458114624023438, 'eval_runtime': 3.3903, 'eval_samples_per_second': 71.086, 'eval_steps_per_second': 2.36, 'epoch': 7.0}
{'train@deu.rst.pcc_loss': 2.6030044555664062, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.23706099815157117, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.10018935776950107, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.20894247641204006, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.14486617394738047, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.603004217147827, 'train@deu.rst.pcc_runtime': 26.5632, 'train@deu.rst.pcc_samples_per_second': 81.466, 'train@deu.rst.pcc_steps_per_second': 2.56, 'epoch': 8.0}
{'loss': 2.6548, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.7273056507110596, 'eval_accuracy@deu.rst.pcc': 0.2074688796680498, 'eval_f1@deu.rst.pcc': 0.08388469455748315, 'eval_precision@deu.rst.pcc': 0.08250429168532618, 'eval_recall@deu.rst.pcc': 0.14807819495319494, 'eval_loss@deu.rst.pcc': 2.7273056507110596, 'eval_runtime': 3.3122, 'eval_samples_per_second': 72.761, 'eval_steps_per_second': 2.415, 'epoch': 8.0}
{'train@deu.rst.pcc_loss': 2.5830471515655518, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.24260628465804066, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.10514319863232005, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.21180075671488768, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.1478150798571574, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.5830471515655518, 'train@deu.rst.pcc_runtime': 26.5854, 'train@deu.rst.pcc_samples_per_second': 81.398, 'train@deu.rst.pcc_steps_per_second': 2.558, 'epoch': 9.0}
{'loss': 2.6438, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.7175469398498535, 'eval_accuracy@deu.rst.pcc': 0.2074688796680498, 'eval_f1@deu.rst.pcc': 0.08711843711843713, 'eval_precision@deu.rst.pcc': 0.08508865702156711, 'eval_recall@deu.rst.pcc': 0.14955125892625892, 'eval_loss@deu.rst.pcc': 2.7175464630126953, 'eval_runtime': 3.2815, 'eval_samples_per_second': 73.443, 'eval_steps_per_second': 2.438, 'epoch': 9.0}
{'train@deu.rst.pcc_loss': 2.570230007171631, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.24353049907578558, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.10529275791860997, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.1909859668356577, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.14911270789028672, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.570230007171631, 'train@deu.rst.pcc_runtime': 26.5689, 'train@deu.rst.pcc_samples_per_second': 81.449, 'train@deu.rst.pcc_steps_per_second': 2.559, 'epoch': 10.0}
{'loss': 2.6289, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.705695390701294, 'eval_accuracy@deu.rst.pcc': 0.2157676348547718, 'eval_f1@deu.rst.pcc': 0.09391715504073639, 'eval_precision@deu.rst.pcc': 0.09209710683637486, 'eval_recall@deu.rst.pcc': 0.15507120194620194, 'eval_loss@deu.rst.pcc': 2.705695629119873, 'eval_runtime': 3.3429, 'eval_samples_per_second': 72.093, 'eval_steps_per_second': 2.393, 'epoch': 10.0}
{'train@deu.rst.pcc_loss': 2.5624523162841797, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2476894639556377, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.10989484365531903, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.19899620298140908, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.15205345227528422, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.562452793121338, 'train@deu.rst.pcc_runtime': 26.5335, 'train@deu.rst.pcc_samples_per_second': 81.557, 'train@deu.rst.pcc_steps_per_second': 2.563, 'epoch': 11.0}
{'loss': 2.6164, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.699594497680664, 'eval_accuracy@deu.rst.pcc': 0.2157676348547718, 'eval_f1@deu.rst.pcc': 0.09372125235175222, 'eval_precision@deu.rst.pcc': 0.0916566164678178, 'eval_recall@deu.rst.pcc': 0.15507120194620194, 'eval_loss@deu.rst.pcc': 2.699594259262085, 'eval_runtime': 3.3218, 'eval_samples_per_second': 72.55, 'eval_steps_per_second': 2.408, 'epoch': 11.0}
{'train@deu.rst.pcc_loss': 2.559680461883545, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.24722735674676524, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.10935236584374651, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.1985551065099469, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.15180923743528402, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.559680461883545, 'train@deu.rst.pcc_runtime': 26.513, 'train@deu.rst.pcc_samples_per_second': 81.62, 'train@deu.rst.pcc_steps_per_second': 2.565, 'epoch': 12.0}
{'loss': 2.606, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.697549819946289, 'eval_accuracy@deu.rst.pcc': 0.21991701244813278, 'eval_f1@deu.rst.pcc': 0.09718580021311646, 'eval_precision@deu.rst.pcc': 0.09393738977072312, 'eval_recall@deu.rst.pcc': 0.15827633015133014, 'eval_loss@deu.rst.pcc': 2.697549819946289, 'eval_runtime': 3.325, 'eval_samples_per_second': 72.48, 'eval_steps_per_second': 2.406, 'epoch': 12.0}
{'train_runtime': 1022.6346, 'train_samples_per_second': 25.393, 'train_steps_per_second': 0.798, 'train_loss': 2.7918217602898094, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.8952
  train_runtime            = 0:32:48.36
  train_samples_per_second =     25.288
  train_steps_per_second   =      0.793
-------------------------------------------------------------------
Lang1:  rus.rst.rrt    Lang2:  deu.rst.pcc
Saving run to:  runs/full_shot/FullShot=v4_rus.rst.rrt_deu.rst.pcc_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 28822 examples
read 2855 examples
read 2843 examples
read 2164 examples
read 241 examples
read 260 examples
Total prediction labels:  34
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (ru): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=34, bias=True)
    )
  )
)
{'train@rus.rst.rrt_loss': 1.71973717212677, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.49514259940323363, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.179452487891644, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.22064898133739505, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.1950446434894247, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.71973717212677, 'train@rus.rst.rrt_runtime': 349.897, 'train@rus.rst.rrt_samples_per_second': 82.373, 'train@rus.rst.rrt_steps_per_second': 2.575, 'epoch': 1.0}
{'loss': 2.1528, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.7536686658859253, 'eval_accuracy@rus.rst.rrt': 0.47180385288966725, 'eval_f1@rus.rst.rrt': 0.19404180441862404, 'eval_precision@rus.rst.rrt': 0.1916187648935704, 'eval_recall@rus.rst.rrt': 0.21297646835732797, 'eval_loss@rus.rst.rrt': 1.7536686658859253, 'eval_runtime': 34.9186, 'eval_samples_per_second': 81.762, 'eval_steps_per_second': 2.577, 'epoch': 1.0}
{'train@rus.rst.rrt_loss': 1.5026373863220215, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5418430365692873, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.22555544337572214, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.322185670721432, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.2310978927725541, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.5026373863220215, 'train@rus.rst.rrt_runtime': 349.0959, 'train@rus.rst.rrt_samples_per_second': 82.562, 'train@rus.rst.rrt_steps_per_second': 2.581, 'epoch': 2.0}
{'loss': 1.641, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.5648716688156128, 'eval_accuracy@rus.rst.rrt': 0.5183887915936952, 'eval_f1@rus.rst.rrt': 0.2508060342218809, 'eval_precision@rus.rst.rrt': 0.35774307262400695, 'eval_recall@rus.rst.rrt': 0.2566703662657139, 'eval_loss@rus.rst.rrt': 1.5648716688156128, 'eval_runtime': 34.9557, 'eval_samples_per_second': 81.675, 'eval_steps_per_second': 2.575, 'epoch': 2.0}
{'train@rus.rst.rrt_loss': 1.4215364456176758, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5658524738047325, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.2763153158710096, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.3935009410694464, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.27236783048322466, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.4215364456176758, 'train@rus.rst.rrt_runtime': 349.1853, 'train@rus.rst.rrt_samples_per_second': 82.541, 'train@rus.rst.rrt_steps_per_second': 2.58, 'epoch': 3.0}
{'loss': 1.5101, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.4878787994384766, 'eval_accuracy@rus.rst.rrt': 0.5387040280210158, 'eval_f1@rus.rst.rrt': 0.30223913379016176, 'eval_precision@rus.rst.rrt': 0.4427876431390665, 'eval_recall@rus.rst.rrt': 0.29924229093626875, 'eval_loss@rus.rst.rrt': 1.4878790378570557, 'eval_runtime': 34.9067, 'eval_samples_per_second': 81.79, 'eval_steps_per_second': 2.578, 'epoch': 3.0}
{'train@rus.rst.rrt_loss': 1.3686072826385498, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5799736312539032, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3075204639783391, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4443047362388464, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.292196027137112, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3686072826385498, 'train@rus.rst.rrt_runtime': 350.2163, 'train@rus.rst.rrt_samples_per_second': 82.298, 'train@rus.rst.rrt_steps_per_second': 2.573, 'epoch': 4.0}
{'loss': 1.4479, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.4397695064544678, 'eval_accuracy@rus.rst.rrt': 0.553415061295972, 'eval_f1@rus.rst.rrt': 0.3480568481757644, 'eval_precision@rus.rst.rrt': 0.4881065507721241, 'eval_recall@rus.rst.rrt': 0.330326251760507, 'eval_loss@rus.rst.rrt': 1.4397693872451782, 'eval_runtime': 35.0455, 'eval_samples_per_second': 81.466, 'eval_steps_per_second': 2.568, 'epoch': 4.0}
{'train@rus.rst.rrt_loss': 1.3351582288742065, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5896190410103394, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3272521097424566, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.43881858398383927, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.30940382796685423, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.335158109664917, 'train@rus.rst.rrt_runtime': 350.2166, 'train@rus.rst.rrt_samples_per_second': 82.298, 'train@rus.rst.rrt_steps_per_second': 2.573, 'epoch': 5.0}
{'loss': 1.4075, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.4147034883499146, 'eval_accuracy@rus.rst.rrt': 0.5632224168126094, 'eval_f1@rus.rst.rrt': 0.37253832665032666, 'eval_precision@rus.rst.rrt': 0.48082281327877324, 'eval_recall@rus.rst.rrt': 0.3555570875409153, 'eval_loss@rus.rst.rrt': 1.414703607559204, 'eval_runtime': 35.0784, 'eval_samples_per_second': 81.389, 'eval_steps_per_second': 2.566, 'epoch': 5.0}
{'train@rus.rst.rrt_loss': 1.3120958805084229, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5962459232530706, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3409312717678739, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4449023620840325, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3217188505185144, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3120958805084229, 'train@rus.rst.rrt_runtime': 349.3384, 'train@rus.rst.rrt_samples_per_second': 82.505, 'train@rus.rst.rrt_steps_per_second': 2.579, 'epoch': 6.0}
{'loss': 1.3784, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.3947432041168213, 'eval_accuracy@rus.rst.rrt': 0.5667250437828372, 'eval_f1@rus.rst.rrt': 0.3841155037961381, 'eval_precision@rus.rst.rrt': 0.48580277051544757, 'eval_recall@rus.rst.rrt': 0.36803780415604537, 'eval_loss@rus.rst.rrt': 1.3947432041168213, 'eval_runtime': 34.9255, 'eval_samples_per_second': 81.745, 'eval_steps_per_second': 2.577, 'epoch': 6.0}
{'train@rus.rst.rrt_loss': 1.2960195541381836, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6000971480119354, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.35032244375891436, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.43552919012165553, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.33146649093267466, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2960196733474731, 'train@rus.rst.rrt_runtime': 350.0144, 'train@rus.rst.rrt_samples_per_second': 82.345, 'train@rus.rst.rrt_steps_per_second': 2.574, 'epoch': 7.0}
{'loss': 1.3572, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.3867542743682861, 'eval_accuracy@rus.rst.rrt': 0.5688266199649737, 'eval_f1@rus.rst.rrt': 0.395232170675373, 'eval_precision@rus.rst.rrt': 0.47832867185344013, 'eval_recall@rus.rst.rrt': 0.38114029292286555, 'eval_loss@rus.rst.rrt': 1.3867542743682861, 'eval_runtime': 34.9961, 'eval_samples_per_second': 81.581, 'eval_steps_per_second': 2.572, 'epoch': 7.0}
{'train@rus.rst.rrt_loss': 1.2836142778396606, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6028381097772535, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.359390470275889, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4719415381851224, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34299832969780597, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.283614158630371, 'train@rus.rst.rrt_runtime': 350.0948, 'train@rus.rst.rrt_samples_per_second': 82.326, 'train@rus.rst.rrt_steps_per_second': 2.574, 'epoch': 8.0}
{'loss': 1.3428, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.3758538961410522, 'eval_accuracy@rus.rst.rrt': 0.578984238178634, 'eval_f1@rus.rst.rrt': 0.40538740683669056, 'eval_precision@rus.rst.rrt': 0.45994465835896925, 'eval_recall@rus.rst.rrt': 0.3950481202202493, 'eval_loss@rus.rst.rrt': 1.3758540153503418, 'eval_runtime': 35.0213, 'eval_samples_per_second': 81.522, 'eval_steps_per_second': 2.57, 'epoch': 8.0}
{'train@rus.rst.rrt_loss': 1.2705531120300293, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6053708972312817, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3619423265579482, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4344642553442398, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3433797786356236, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2705532312393188, 'train@rus.rst.rrt_runtime': 349.0753, 'train@rus.rst.rrt_samples_per_second': 82.567, 'train@rus.rst.rrt_steps_per_second': 2.581, 'epoch': 9.0}
{'loss': 1.3295, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.3685723543167114, 'eval_accuracy@rus.rst.rrt': 0.5712784588441331, 'eval_f1@rus.rst.rrt': 0.4035539841446724, 'eval_precision@rus.rst.rrt': 0.4697238860360078, 'eval_recall@rus.rst.rrt': 0.38981009980189224, 'eval_loss@rus.rst.rrt': 1.3685723543167114, 'eval_runtime': 34.9083, 'eval_samples_per_second': 81.786, 'eval_steps_per_second': 2.578, 'epoch': 9.0}
{'train@rus.rst.rrt_loss': 1.264760136604309, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6080424675595032, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36459064958869514, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.48556018081512886, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3436195598425763, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.264760136604309, 'train@rus.rst.rrt_runtime': 349.8186, 'train@rus.rst.rrt_samples_per_second': 82.391, 'train@rus.rst.rrt_steps_per_second': 2.576, 'epoch': 10.0}
{'loss': 1.3218, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.3665834665298462, 'eval_accuracy@rus.rst.rrt': 0.5765323992994746, 'eval_f1@rus.rst.rrt': 0.41072787857693305, 'eval_precision@rus.rst.rrt': 0.4872814732815844, 'eval_recall@rus.rst.rrt': 0.39312853598687464, 'eval_loss@rus.rst.rrt': 1.3665833473205566, 'eval_runtime': 34.9651, 'eval_samples_per_second': 81.653, 'eval_steps_per_second': 2.574, 'epoch': 10.0}
{'train@rus.rst.rrt_loss': 1.260617733001709, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6091180348345014, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36820699938560647, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.48408216633271656, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.349079702187498, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.260617733001709, 'train@rus.rst.rrt_runtime': 350.1414, 'train@rus.rst.rrt_samples_per_second': 82.315, 'train@rus.rst.rrt_steps_per_second': 2.573, 'epoch': 11.0}
{'loss': 1.3153, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.365626335144043, 'eval_accuracy@rus.rst.rrt': 0.574430823117338, 'eval_f1@rus.rst.rrt': 0.41029644386020364, 'eval_precision@rus.rst.rrt': 0.47682270275742794, 'eval_recall@rus.rst.rrt': 0.3956615164985305, 'eval_loss@rus.rst.rrt': 1.365626335144043, 'eval_runtime': 35.0383, 'eval_samples_per_second': 81.482, 'eval_steps_per_second': 2.569, 'epoch': 11.0}
{'train@rus.rst.rrt_loss': 1.2591922283172607, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6090486433974047, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36770959021167454, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4807703103364277, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3489099116830627, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2591922283172607, 'train@rus.rst.rrt_runtime': 349.1433, 'train@rus.rst.rrt_samples_per_second': 82.551, 'train@rus.rst.rrt_steps_per_second': 2.581, 'epoch': 12.0}
{'loss': 1.3107, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.363753080368042, 'eval_accuracy@rus.rst.rrt': 0.5761821366024519, 'eval_f1@rus.rst.rrt': 0.41325294316523387, 'eval_precision@rus.rst.rrt': 0.485197453312048, 'eval_recall@rus.rst.rrt': 0.39856597446755887, 'eval_loss@rus.rst.rrt': 1.363753080368042, 'eval_runtime': 35.0019, 'eval_samples_per_second': 81.567, 'eval_steps_per_second': 2.571, 'epoch': 12.0}
{'train_runtime': 13424.6614, 'train_samples_per_second': 25.763, 'train_steps_per_second': 0.805, 'train_loss': 1.459596137845247, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.4596
  train_runtime            = 3:43:44.66
  train_samples_per_second =     25.763
  train_steps_per_second   =      0.805
{'train@deu.rst.pcc_loss': 3.233719825744629, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.08040665434380777, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.037970117091908645, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.058629208244721176, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.04689739238157001, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.23371958732605, 'train@deu.rst.pcc_runtime': 26.5011, 'train@deu.rst.pcc_samples_per_second': 81.657, 'train@deu.rst.pcc_steps_per_second': 2.566, 'epoch': 1.0}
{'loss': 4.0125, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.2406885623931885, 'eval_accuracy@deu.rst.pcc': 0.07883817427385892, 'eval_f1@deu.rst.pcc': 0.04142841672081438, 'eval_precision@deu.rst.pcc': 0.055074814234297, 'eval_recall@deu.rst.pcc': 0.052077205202205205, 'eval_loss@deu.rst.pcc': 3.2406883239746094, 'eval_runtime': 3.2247, 'eval_samples_per_second': 74.736, 'eval_steps_per_second': 2.481, 'epoch': 1.0}
{'train@deu.rst.pcc_loss': 2.921762228012085, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.12199630314232902, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.04273984609283577, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.09667027429615696, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.05901268424354126, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.921762466430664, 'train@deu.rst.pcc_runtime': 26.5191, 'train@deu.rst.pcc_samples_per_second': 81.602, 'train@deu.rst.pcc_steps_per_second': 2.564, 'epoch': 2.0}
{'loss': 3.0831, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.9422500133514404, 'eval_accuracy@deu.rst.pcc': 0.11618257261410789, 'eval_f1@deu.rst.pcc': 0.03433529775697104, 'eval_precision@deu.rst.pcc': 0.06235478490913273, 'eval_recall@deu.rst.pcc': 0.04988067488067488, 'eval_loss@deu.rst.pcc': 2.9422500133514404, 'eval_runtime': 3.2382, 'eval_samples_per_second': 74.425, 'eval_steps_per_second': 2.471, 'epoch': 2.0}
{'train@deu.rst.pcc_loss': 2.7971463203430176, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.16728280961182995, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.07886260940308884, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.13301240427786717, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.08920198656260145, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7971465587615967, 'train@deu.rst.pcc_runtime': 26.4824, 'train@deu.rst.pcc_samples_per_second': 81.715, 'train@deu.rst.pcc_steps_per_second': 2.568, 'epoch': 3.0}
{'loss': 2.901, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.8402793407440186, 'eval_accuracy@deu.rst.pcc': 0.14522821576763487, 'eval_f1@deu.rst.pcc': 0.07819349795894419, 'eval_precision@deu.rst.pcc': 0.11146995651946147, 'eval_recall@deu.rst.pcc': 0.09255871443371444, 'eval_loss@deu.rst.pcc': 2.8402795791625977, 'eval_runtime': 3.2393, 'eval_samples_per_second': 74.399, 'eval_steps_per_second': 2.47, 'epoch': 3.0}
{'train@deu.rst.pcc_loss': 2.6990888118743896, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.20933456561922367, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.11494240684779641, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.13930227864428815, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.1253686323541135, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6990888118743896, 'train@deu.rst.pcc_runtime': 26.517, 'train@deu.rst.pcc_samples_per_second': 81.608, 'train@deu.rst.pcc_steps_per_second': 2.564, 'epoch': 4.0}
{'loss': 2.7979, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.7489778995513916, 'eval_accuracy@deu.rst.pcc': 0.1908713692946058, 'eval_f1@deu.rst.pcc': 0.11400445281796752, 'eval_precision@deu.rst.pcc': 0.13822639223733418, 'eval_recall@deu.rst.pcc': 0.1320931290931291, 'eval_loss@deu.rst.pcc': 2.7489778995513916, 'eval_runtime': 3.2595, 'eval_samples_per_second': 73.938, 'eval_steps_per_second': 2.454, 'epoch': 4.0}
{'train@deu.rst.pcc_loss': 2.621980905532837, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.24260628465804066, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.13805767681450032, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.19502911839031628, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.1519486089309744, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.621980905532837, 'train@deu.rst.pcc_runtime': 26.4914, 'train@deu.rst.pcc_samples_per_second': 81.687, 'train@deu.rst.pcc_steps_per_second': 2.567, 'epoch': 5.0}
{'loss': 2.7225, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.6737663745880127, 'eval_accuracy@deu.rst.pcc': 0.1950207468879668, 'eval_f1@deu.rst.pcc': 0.12091193564940989, 'eval_precision@deu.rst.pcc': 0.14982839382839383, 'eval_recall@deu.rst.pcc': 0.13854778554778555, 'eval_loss@deu.rst.pcc': 2.6737661361694336, 'eval_runtime': 3.2419, 'eval_samples_per_second': 74.339, 'eval_steps_per_second': 2.468, 'epoch': 5.0}
{'train@deu.rst.pcc_loss': 2.5566444396972656, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.261090573012939, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.1524338834192681, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.1899148505549536, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.17084838724872972, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.5566442012786865, 'train@deu.rst.pcc_runtime': 26.4908, 'train@deu.rst.pcc_samples_per_second': 81.689, 'train@deu.rst.pcc_steps_per_second': 2.567, 'epoch': 6.0}
{'loss': 2.6526, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.6263656616210938, 'eval_accuracy@deu.rst.pcc': 0.22821576763485477, 'eval_f1@deu.rst.pcc': 0.14396370212810916, 'eval_precision@deu.rst.pcc': 0.16399832915622387, 'eval_recall@deu.rst.pcc': 0.16888134088134088, 'eval_loss@deu.rst.pcc': 2.6263656616210938, 'eval_runtime': 3.2374, 'eval_samples_per_second': 74.443, 'eval_steps_per_second': 2.471, 'epoch': 6.0}
{'train@deu.rst.pcc_loss': 2.501126766204834, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2846580406654344, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.17346164979113235, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.20347886046676486, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.19223076479991136, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.501126766204834, 'train@deu.rst.pcc_runtime': 26.4662, 'train@deu.rst.pcc_samples_per_second': 81.765, 'train@deu.rst.pcc_steps_per_second': 2.569, 'epoch': 7.0}
{'loss': 2.6134, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.5760371685028076, 'eval_accuracy@deu.rst.pcc': 0.22821576763485477, 'eval_f1@deu.rst.pcc': 0.15141943314960393, 'eval_precision@deu.rst.pcc': 0.17079484623053914, 'eval_recall@deu.rst.pcc': 0.17107770007770007, 'eval_loss@deu.rst.pcc': 2.5760369300842285, 'eval_runtime': 3.2253, 'eval_samples_per_second': 74.721, 'eval_steps_per_second': 2.48, 'epoch': 7.0}
{'train@deu.rst.pcc_loss': 2.462604522705078, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.29944547134935307, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.19026475249415065, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.21898120981983488, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.2096718326881921, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.462604284286499, 'train@deu.rst.pcc_runtime': 26.4842, 'train@deu.rst.pcc_samples_per_second': 81.709, 'train@deu.rst.pcc_steps_per_second': 2.568, 'epoch': 8.0}
{'loss': 2.5488, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.54312801361084, 'eval_accuracy@deu.rst.pcc': 0.2572614107883817, 'eval_f1@deu.rst.pcc': 0.1697212272058216, 'eval_precision@deu.rst.pcc': 0.17805610646517042, 'eval_recall@deu.rst.pcc': 0.19565811965811966, 'eval_loss@deu.rst.pcc': 2.543128490447998, 'eval_runtime': 3.2777, 'eval_samples_per_second': 73.527, 'eval_steps_per_second': 2.441, 'epoch': 8.0}
{'train@deu.rst.pcc_loss': 2.4315128326416016, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.3082255083179298, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.19701664978736722, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.223204482565524, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.21687727624475037, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.4315130710601807, 'train@deu.rst.pcc_runtime': 26.4547, 'train@deu.rst.pcc_samples_per_second': 81.8, 'train@deu.rst.pcc_steps_per_second': 2.57, 'epoch': 9.0}
{'loss': 2.5156, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.526036500930786, 'eval_accuracy@deu.rst.pcc': 0.26556016597510373, 'eval_f1@deu.rst.pcc': 0.17280114759207318, 'eval_precision@deu.rst.pcc': 0.18241190647630934, 'eval_recall@deu.rst.pcc': 0.2018119658119658, 'eval_loss@deu.rst.pcc': 2.526036262512207, 'eval_runtime': 3.2454, 'eval_samples_per_second': 74.259, 'eval_steps_per_second': 2.465, 'epoch': 9.0}
{'train@deu.rst.pcc_loss': 2.411430835723877, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.31284658040665436, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.20479232179651297, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.23278302956570895, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.22464750411390458, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.411430835723877, 'train@deu.rst.pcc_runtime': 26.4442, 'train@deu.rst.pcc_samples_per_second': 81.833, 'train@deu.rst.pcc_steps_per_second': 2.571, 'epoch': 10.0}
{'loss': 2.4923, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.5032265186309814, 'eval_accuracy@deu.rst.pcc': 0.27385892116182575, 'eval_f1@deu.rst.pcc': 0.1837228740618862, 'eval_precision@deu.rst.pcc': 0.19242929292929292, 'eval_recall@deu.rst.pcc': 0.20737529137529137, 'eval_loss@deu.rst.pcc': 2.5032265186309814, 'eval_runtime': 4.2889, 'eval_samples_per_second': 56.191, 'eval_steps_per_second': 1.865, 'epoch': 10.0}
{'train@deu.rst.pcc_loss': 2.400050401687622, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.316543438077634, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.2050591333670208, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.2334666527487195, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.22598939523188288, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.400050401687622, 'train@deu.rst.pcc_runtime': 26.431, 'train@deu.rst.pcc_samples_per_second': 81.874, 'train@deu.rst.pcc_steps_per_second': 2.573, 'epoch': 11.0}
{'loss': 2.4865, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.497933864593506, 'eval_accuracy@deu.rst.pcc': 0.27385892116182575, 'eval_f1@deu.rst.pcc': 0.1883099192155662, 'eval_precision@deu.rst.pcc': 0.19458027512968126, 'eval_recall@deu.rst.pcc': 0.2133872793872794, 'eval_loss@deu.rst.pcc': 2.497934103012085, 'eval_runtime': 3.2556, 'eval_samples_per_second': 74.027, 'eval_steps_per_second': 2.457, 'epoch': 11.0}
{'train@deu.rst.pcc_loss': 2.3963053226470947, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.3183918669131238, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.20611108154201638, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.23461649696430179, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.2273967947228064, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.396305561065674, 'train@deu.rst.pcc_runtime': 26.4808, 'train@deu.rst.pcc_samples_per_second': 81.72, 'train@deu.rst.pcc_steps_per_second': 2.568, 'epoch': 12.0}
{'loss': 2.4655, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.4950761795043945, 'eval_accuracy@deu.rst.pcc': 0.2821576763485477, 'eval_f1@deu.rst.pcc': 0.19270336275022623, 'eval_precision@deu.rst.pcc': 0.19740095549753142, 'eval_recall@deu.rst.pcc': 0.2201005661005661, 'eval_loss@deu.rst.pcc': 2.4950761795043945, 'eval_runtime': 3.2592, 'eval_samples_per_second': 73.945, 'eval_steps_per_second': 2.455, 'epoch': 12.0}
{'train_runtime': 1019.7795, 'train_samples_per_second': 25.464, 'train_steps_per_second': 0.8, 'train_loss': 2.7743084664438284, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.4596
  train_runtime            = 3:43:44.66
  train_samples_per_second =     25.763
  train_steps_per_second   =      0.805
-------------------------------------------------------------------
Lang1:  spa.rst.rststb    Lang2:  deu.rst.pcc
Saving run to:  runs/full_shot/FullShot=v4_spa.rst.rststb_deu.rst.pcc_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2240 examples
read 383 examples
read 426 examples
read 2164 examples
read 241 examples
read 260 examples
Total prediction labels:  33
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=33, bias=True)
    )
  )
)
{'train@spa.rst.rststb_loss': 2.7296829223632812, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.25892857142857145, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.034928162603972204, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.03737354757083874, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.05329036008416541, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.7296831607818604, 'train@spa.rst.rststb_runtime': 27.1854, 'train@spa.rst.rststb_samples_per_second': 82.397, 'train@spa.rst.rststb_steps_per_second': 2.575, 'epoch': 1.0}
{'loss': 3.0918, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.7834043502807617, 'eval_accuracy@spa.rst.rststb': 0.21409921671018275, 'eval_f1@spa.rst.rststb': 0.027422563228189825, 'eval_precision@spa.rst.rststb': 0.025042949649795164, 'eval_recall@spa.rst.rststb': 0.04950717966678341, 'eval_loss@spa.rst.rststb': 2.7834036350250244, 'eval_runtime': 4.9234, 'eval_samples_per_second': 77.791, 'eval_steps_per_second': 2.437, 'epoch': 1.0}
{'train@spa.rst.rststb_loss': 2.5200843811035156, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.2955357142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.044259132462208375, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.043154373952795866, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.06207143820188859, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.5200843811035156, 'train@spa.rst.rststb_runtime': 27.3473, 'train@spa.rst.rststb_samples_per_second': 81.909, 'train@spa.rst.rststb_steps_per_second': 2.56, 'epoch': 2.0}
{'loss': 2.6495, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.6411221027374268, 'eval_accuracy@spa.rst.rststb': 0.25326370757180156, 'eval_f1@spa.rst.rststb': 0.0413221775623672, 'eval_precision@spa.rst.rststb': 0.041405063169432536, 'eval_recall@spa.rst.rststb': 0.0608386122621495, 'eval_loss@spa.rst.rststb': 2.6411221027374268, 'eval_runtime': 4.9409, 'eval_samples_per_second': 77.516, 'eval_steps_per_second': 2.429, 'epoch': 2.0}
{'train@spa.rst.rststb_loss': 2.3996212482452393, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.315625, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.049260935548706475, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.05405322775945208, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.07038117885648748, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.3996212482452393, 'train@spa.rst.rststb_runtime': 27.3629, 'train@spa.rst.rststb_samples_per_second': 81.863, 'train@spa.rst.rststb_steps_per_second': 2.558, 'epoch': 3.0}
{'loss': 2.5059, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.555492639541626, 'eval_accuracy@spa.rst.rststb': 0.26631853785900783, 'eval_f1@spa.rst.rststb': 0.04865545736702664, 'eval_precision@spa.rst.rststb': 0.057817014104713176, 'eval_recall@spa.rst.rststb': 0.07082039359698943, 'eval_loss@spa.rst.rststb': 2.555492639541626, 'eval_runtime': 4.9547, 'eval_samples_per_second': 77.301, 'eval_steps_per_second': 2.422, 'epoch': 3.0}
{'train@spa.rst.rststb_loss': 2.2996952533721924, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.3517857142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.07390817720026074, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.08559759018091705, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.09059395982308867, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.2996954917907715, 'train@spa.rst.rststb_runtime': 27.3964, 'train@spa.rst.rststb_samples_per_second': 81.763, 'train@spa.rst.rststb_steps_per_second': 2.555, 'epoch': 4.0}
{'loss': 2.3977, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.486043930053711, 'eval_accuracy@spa.rst.rststb': 0.32114882506527415, 'eval_f1@spa.rst.rststb': 0.08276654038668521, 'eval_precision@spa.rst.rststb': 0.11222540399559033, 'eval_recall@spa.rst.rststb': 0.09862835416581545, 'eval_loss@spa.rst.rststb': 2.486043930053711, 'eval_runtime': 4.938, 'eval_samples_per_second': 77.561, 'eval_steps_per_second': 2.43, 'epoch': 4.0}
{'train@spa.rst.rststb_loss': 2.211724042892456, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.39017857142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.09071250262510962, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.08249534414939333, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.11403538686532813, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.211724042892456, 'train@spa.rst.rststb_runtime': 27.385, 'train@spa.rst.rststb_samples_per_second': 81.796, 'train@spa.rst.rststb_steps_per_second': 2.556, 'epoch': 5.0}
{'loss': 2.3022, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.418383836746216, 'eval_accuracy@spa.rst.rststb': 0.3394255874673629, 'eval_f1@spa.rst.rststb': 0.09193787883866271, 'eval_precision@spa.rst.rststb': 0.07873893987850726, 'eval_recall@spa.rst.rststb': 0.12015754003428454, 'eval_loss@spa.rst.rststb': 2.418384075164795, 'eval_runtime': 4.954, 'eval_samples_per_second': 77.311, 'eval_steps_per_second': 2.422, 'epoch': 5.0}
{'train@spa.rst.rststb_loss': 2.145616292953491, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4004464285714286, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.09587890453524879, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.1043072163848137, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.12030158063165805, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.145616054534912, 'train@spa.rst.rststb_runtime': 27.4247, 'train@spa.rst.rststb_samples_per_second': 81.678, 'train@spa.rst.rststb_steps_per_second': 2.552, 'epoch': 6.0}
{'loss': 2.2314, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.367664098739624, 'eval_accuracy@spa.rst.rststb': 0.3733681462140992, 'eval_f1@spa.rst.rststb': 0.10375197269607858, 'eval_precision@spa.rst.rststb': 0.09213363875913283, 'eval_recall@spa.rst.rststb': 0.13581055831279673, 'eval_loss@spa.rst.rststb': 2.367663860321045, 'eval_runtime': 4.9641, 'eval_samples_per_second': 77.154, 'eval_steps_per_second': 2.417, 'epoch': 6.0}
{'train@spa.rst.rststb_loss': 2.0906670093536377, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4160714285714286, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10451803498876815, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.1357810646964474, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.1293598192183341, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0906670093536377, 'train@spa.rst.rststb_runtime': 27.4114, 'train@spa.rst.rststb_samples_per_second': 81.718, 'train@spa.rst.rststb_steps_per_second': 2.554, 'epoch': 7.0}
{'loss': 2.1775, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.3279926776885986, 'eval_accuracy@spa.rst.rststb': 0.381201044386423, 'eval_f1@spa.rst.rststb': 0.10753848372256558, 'eval_precision@spa.rst.rststb': 0.0944377700621919, 'eval_recall@spa.rst.rststb': 0.14278384094650862, 'eval_loss@spa.rst.rststb': 2.3279929161071777, 'eval_runtime': 4.9659, 'eval_samples_per_second': 77.125, 'eval_steps_per_second': 2.416, 'epoch': 7.0}
{'train@spa.rst.rststb_loss': 2.0511486530303955, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4232142857142857, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10670862378477027, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.1303543968723749, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.1316699375033419, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0511486530303955, 'train@spa.rst.rststb_runtime': 27.4013, 'train@spa.rst.rststb_samples_per_second': 81.748, 'train@spa.rst.rststb_steps_per_second': 2.555, 'epoch': 8.0}
{'loss': 2.1254, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.2978897094726562, 'eval_accuracy@spa.rst.rststb': 0.38903394255874674, 'eval_f1@spa.rst.rststb': 0.11077662971427331, 'eval_precision@spa.rst.rststb': 0.0984470670346753, 'eval_recall@spa.rst.rststb': 0.14473062874663842, 'eval_loss@spa.rst.rststb': 2.2978897094726562, 'eval_runtime': 4.9579, 'eval_samples_per_second': 77.251, 'eval_steps_per_second': 2.42, 'epoch': 8.0}
{'train@spa.rst.rststb_loss': 2.020991563796997, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4232142857142857, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10622585377176035, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.16308500839048776, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13368136320047191, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.020991802215576, 'train@spa.rst.rststb_runtime': 27.4132, 'train@spa.rst.rststb_samples_per_second': 81.712, 'train@spa.rst.rststb_steps_per_second': 2.554, 'epoch': 9.0}
{'loss': 2.0972, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.2729227542877197, 'eval_accuracy@spa.rst.rststb': 0.3838120104438642, 'eval_f1@spa.rst.rststb': 0.10657606966843129, 'eval_precision@spa.rst.rststb': 0.09152567610547996, 'eval_recall@spa.rst.rststb': 0.14497156360589605, 'eval_loss@spa.rst.rststb': 2.2729227542877197, 'eval_runtime': 4.9921, 'eval_samples_per_second': 76.721, 'eval_steps_per_second': 2.404, 'epoch': 9.0}
{'train@spa.rst.rststb_loss': 2.0008480548858643, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.42723214285714284, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10823602755801376, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.15995614956130288, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13467779305490793, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0008480548858643, 'train@spa.rst.rststb_runtime': 27.4146, 'train@spa.rst.rststb_samples_per_second': 81.708, 'train@spa.rst.rststb_steps_per_second': 2.553, 'epoch': 10.0}
{'loss': 2.0639, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.2596275806427, 'eval_accuracy@spa.rst.rststb': 0.39425587467362927, 'eval_f1@spa.rst.rststb': 0.11176788944376981, 'eval_precision@spa.rst.rststb': 0.09791366822371007, 'eval_recall@spa.rst.rststb': 0.14766585220691955, 'eval_loss@spa.rst.rststb': 2.2596275806427, 'eval_runtime': 5.0287, 'eval_samples_per_second': 76.163, 'eval_steps_per_second': 2.386, 'epoch': 10.0}
{'train@spa.rst.rststb_loss': 1.9890804290771484, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.43080357142857145, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.11178465420903916, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.1721472127864569, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13663419112024044, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.989080548286438, 'train@spa.rst.rststb_runtime': 27.3991, 'train@spa.rst.rststb_samples_per_second': 81.755, 'train@spa.rst.rststb_steps_per_second': 2.555, 'epoch': 11.0}
{'loss': 2.049, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.24973201751709, 'eval_accuracy@spa.rst.rststb': 0.3968668407310705, 'eval_f1@spa.rst.rststb': 0.11271900866848321, 'eval_precision@spa.rst.rststb': 0.09887503060721876, 'eval_recall@spa.rst.rststb': 0.1483147814736295, 'eval_loss@spa.rst.rststb': 2.24973201751709, 'eval_runtime': 4.9419, 'eval_samples_per_second': 77.5, 'eval_steps_per_second': 2.428, 'epoch': 11.0}
{'train@spa.rst.rststb_loss': 1.9855049848556519, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.43125, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.11249712952294667, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.1730595938123328, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13691576100386002, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.985505223274231, 'train@spa.rst.rststb_runtime': 27.4597, 'train@spa.rst.rststb_samples_per_second': 81.574, 'train@spa.rst.rststb_steps_per_second': 2.549, 'epoch': 12.0}
{'loss': 2.0364, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.247471809387207, 'eval_accuracy@spa.rst.rststb': 0.39947780678851175, 'eval_f1@spa.rst.rststb': 0.1137727694857025, 'eval_precision@spa.rst.rststb': 0.10074623669430627, 'eval_recall@spa.rst.rststb': 0.14886513920615566, 'eval_loss@spa.rst.rststb': 2.247472047805786, 'eval_runtime': 4.9503, 'eval_samples_per_second': 77.368, 'eval_steps_per_second': 2.424, 'epoch': 12.0}
{'train_runtime': 1072.4832, 'train_samples_per_second': 25.063, 'train_steps_per_second': 0.783, 'train_loss': 2.3106722695486885, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.3107
  train_runtime            = 0:17:52.48
  train_samples_per_second =     25.063
  train_steps_per_second   =      0.783
{'train@deu.rst.pcc_loss': 3.080176591873169, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.12985212569316082, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.050519169100639645, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.07426536935857353, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.07013302594718353, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.080176591873169, 'train@deu.rst.pcc_runtime': 26.5086, 'train@deu.rst.pcc_samples_per_second': 81.634, 'train@deu.rst.pcc_steps_per_second': 2.565, 'epoch': 1.0}
{'loss': 3.3174, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.081289291381836, 'eval_accuracy@deu.rst.pcc': 0.1037344398340249, 'eval_f1@deu.rst.pcc': 0.03964636296406556, 'eval_precision@deu.rst.pcc': 0.07453357100415924, 'eval_recall@deu.rst.pcc': 0.06379731379731379, 'eval_loss@deu.rst.pcc': 3.081289052963257, 'eval_runtime': 3.2338, 'eval_samples_per_second': 74.525, 'eval_steps_per_second': 2.474, 'epoch': 1.0}
{'train@deu.rst.pcc_loss': 2.8641529083251953, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.18484288354898337, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08047445706057688, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.08750162521887345, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.11194251052125094, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.8641531467437744, 'train@deu.rst.pcc_runtime': 26.5024, 'train@deu.rst.pcc_samples_per_second': 81.653, 'train@deu.rst.pcc_steps_per_second': 2.566, 'epoch': 2.0}
{'loss': 2.984, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.8861584663391113, 'eval_accuracy@deu.rst.pcc': 0.13692946058091288, 'eval_f1@deu.rst.pcc': 0.07928337935797132, 'eval_precision@deu.rst.pcc': 0.09812596367786426, 'eval_recall@deu.rst.pcc': 0.10796321733821734, 'eval_loss@deu.rst.pcc': 2.8861589431762695, 'eval_runtime': 3.2605, 'eval_samples_per_second': 73.915, 'eval_steps_per_second': 2.454, 'epoch': 2.0}
{'train@deu.rst.pcc_loss': 2.763902425765991, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.22273567467652494, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09490889758153818, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.09854054982223942, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.13502332248843543, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7639026641845703, 'train@deu.rst.pcc_runtime': 26.486, 'train@deu.rst.pcc_samples_per_second': 81.704, 'train@deu.rst.pcc_steps_per_second': 2.567, 'epoch': 3.0}
{'loss': 2.8488, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.7971367835998535, 'eval_accuracy@deu.rst.pcc': 0.2074688796680498, 'eval_f1@deu.rst.pcc': 0.09334106590613976, 'eval_precision@deu.rst.pcc': 0.09192527120867637, 'eval_recall@deu.rst.pcc': 0.13828474765974766, 'eval_loss@deu.rst.pcc': 2.7971365451812744, 'eval_runtime': 3.2293, 'eval_samples_per_second': 74.63, 'eval_steps_per_second': 2.477, 'epoch': 3.0}
{'train@deu.rst.pcc_loss': 2.69999361038208, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.23151571164510165, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.10022209023646766, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.1595929702531652, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.14120475947605796, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.699993848800659, 'train@deu.rst.pcc_runtime': 26.4393, 'train@deu.rst.pcc_samples_per_second': 81.848, 'train@deu.rst.pcc_steps_per_second': 2.572, 'epoch': 4.0}
{'loss': 2.7749, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.7398722171783447, 'eval_accuracy@deu.rst.pcc': 0.22821576763485477, 'eval_f1@deu.rst.pcc': 0.10721806040157983, 'eval_precision@deu.rst.pcc': 0.15574251193053634, 'eval_recall@deu.rst.pcc': 0.15346227846227847, 'eval_loss@deu.rst.pcc': 2.7398719787597656, 'eval_runtime': 3.2373, 'eval_samples_per_second': 74.444, 'eval_steps_per_second': 2.471, 'epoch': 4.0}
{'train@deu.rst.pcc_loss': 2.6519253253936768, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2444547134935305, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.10882218954972335, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.14307731439898896, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.1507768364030323, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6519253253936768, 'train@deu.rst.pcc_runtime': 26.4724, 'train@deu.rst.pcc_samples_per_second': 81.745, 'train@deu.rst.pcc_steps_per_second': 2.569, 'epoch': 5.0}
{'loss': 2.7198, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.694052219390869, 'eval_accuracy@deu.rst.pcc': 0.22406639004149378, 'eval_f1@deu.rst.pcc': 0.1046318262489105, 'eval_precision@deu.rst.pcc': 0.11774846221737138, 'eval_recall@deu.rst.pcc': 0.1519741832241832, 'eval_loss@deu.rst.pcc': 2.69405198097229, 'eval_runtime': 3.223, 'eval_samples_per_second': 74.776, 'eval_steps_per_second': 2.482, 'epoch': 5.0}
{'train@deu.rst.pcc_loss': 2.613830804824829, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.24491682070240295, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.11332844128833347, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.1703427393178608, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.15275170426561754, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.613830804824829, 'train@deu.rst.pcc_runtime': 26.4377, 'train@deu.rst.pcc_samples_per_second': 81.853, 'train@deu.rst.pcc_steps_per_second': 2.572, 'epoch': 6.0}
{'loss': 2.6853, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.6596896648406982, 'eval_accuracy@deu.rst.pcc': 0.22406639004149378, 'eval_f1@deu.rst.pcc': 0.10145052317826735, 'eval_precision@deu.rst.pcc': 0.10323094289173645, 'eval_recall@deu.rst.pcc': 0.15397738835238836, 'eval_loss@deu.rst.pcc': 2.6596899032592773, 'eval_runtime': 3.2231, 'eval_samples_per_second': 74.773, 'eval_steps_per_second': 2.482, 'epoch': 6.0}
{'train@deu.rst.pcc_loss': 2.583951234817505, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2546210720887246, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.1211532818013949, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.17650271149291233, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.1598177080550487, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.583951473236084, 'train@deu.rst.pcc_runtime': 26.4441, 'train@deu.rst.pcc_samples_per_second': 81.833, 'train@deu.rst.pcc_steps_per_second': 2.571, 'epoch': 7.0}
{'loss': 2.6536, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.633000373840332, 'eval_accuracy@deu.rst.pcc': 0.23651452282157676, 'eval_f1@deu.rst.pcc': 0.11315363766021662, 'eval_precision@deu.rst.pcc': 0.11749181198263463, 'eval_recall@deu.rst.pcc': 0.16245849058349057, 'eval_loss@deu.rst.pcc': 2.633000373840332, 'eval_runtime': 3.2269, 'eval_samples_per_second': 74.684, 'eval_steps_per_second': 2.479, 'epoch': 7.0}
{'train@deu.rst.pcc_loss': 2.5623044967651367, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.26386321626617376, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.13510789790882663, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.21222565845586255, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.16652720445678013, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.562304735183716, 'train@deu.rst.pcc_runtime': 26.4304, 'train@deu.rst.pcc_samples_per_second': 81.875, 'train@deu.rst.pcc_steps_per_second': 2.573, 'epoch': 8.0}
{'loss': 2.6194, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.6183130741119385, 'eval_accuracy@deu.rst.pcc': 0.23236514522821577, 'eval_f1@deu.rst.pcc': 0.11431669113682497, 'eval_precision@deu.rst.pcc': 0.11616437189353856, 'eval_recall@deu.rst.pcc': 0.16072642635142634, 'eval_loss@deu.rst.pcc': 2.6183133125305176, 'eval_runtime': 3.2273, 'eval_samples_per_second': 74.676, 'eval_steps_per_second': 2.479, 'epoch': 8.0}
{'train@deu.rst.pcc_loss': 2.5431370735168457, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.27402957486136786, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.14582225830848222, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.21429214035775906, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.17423458733004735, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.543137311935425, 'train@deu.rst.pcc_runtime': 26.4324, 'train@deu.rst.pcc_samples_per_second': 81.869, 'train@deu.rst.pcc_steps_per_second': 2.573, 'epoch': 9.0}
{'loss': 2.6038, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.603476047515869, 'eval_accuracy@deu.rst.pcc': 0.24896265560165975, 'eval_f1@deu.rst.pcc': 0.12488265732776675, 'eval_precision@deu.rst.pcc': 0.11883206051850924, 'eval_recall@deu.rst.pcc': 0.17234906297406294, 'eval_loss@deu.rst.pcc': 2.603476047515869, 'eval_runtime': 3.2209, 'eval_samples_per_second': 74.823, 'eval_steps_per_second': 2.484, 'epoch': 9.0}
{'train@deu.rst.pcc_loss': 2.5307581424713135, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.27726432532347506, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.15029341757077158, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.21614300865948582, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.17705873443428508, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.5307583808898926, 'train@deu.rst.pcc_runtime': 26.3844, 'train@deu.rst.pcc_samples_per_second': 82.018, 'train@deu.rst.pcc_steps_per_second': 2.577, 'epoch': 10.0}
{'loss': 2.5845, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.591400384902954, 'eval_accuracy@deu.rst.pcc': 0.25311203319502074, 'eval_f1@deu.rst.pcc': 0.1278122597078513, 'eval_precision@deu.rst.pcc': 0.12321393716321254, 'eval_recall@deu.rst.pcc': 0.17673067673067674, 'eval_loss@deu.rst.pcc': 2.591399908065796, 'eval_runtime': 3.2484, 'eval_samples_per_second': 74.191, 'eval_steps_per_second': 2.463, 'epoch': 10.0}
{'train@deu.rst.pcc_loss': 2.5232412815093994, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.27726432532347506, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.15059871410040493, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.21156760862127827, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.17745158086268958, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.5232417583465576, 'train@deu.rst.pcc_runtime': 26.4258, 'train@deu.rst.pcc_samples_per_second': 81.89, 'train@deu.rst.pcc_steps_per_second': 2.573, 'epoch': 11.0}
{'loss': 2.5716, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.5849609375, 'eval_accuracy@deu.rst.pcc': 0.25311203319502074, 'eval_f1@deu.rst.pcc': 0.1277412828076061, 'eval_precision@deu.rst.pcc': 0.12304924678113083, 'eval_recall@deu.rst.pcc': 0.17673067673067674, 'eval_loss@deu.rst.pcc': 2.5849609375, 'eval_runtime': 3.221, 'eval_samples_per_second': 74.822, 'eval_steps_per_second': 2.484, 'epoch': 11.0}
{'train@deu.rst.pcc_loss': 2.5209572315216064, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2777264325323475, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.15112046604562443, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.2115834034972526, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.177826841770201, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.5209574699401855, 'train@deu.rst.pcc_runtime': 26.4475, 'train@deu.rst.pcc_samples_per_second': 81.822, 'train@deu.rst.pcc_steps_per_second': 2.571, 'epoch': 12.0}
{'loss': 2.5674, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.5828423500061035, 'eval_accuracy@deu.rst.pcc': 0.25311203319502074, 'eval_f1@deu.rst.pcc': 0.1278122597078513, 'eval_precision@deu.rst.pcc': 0.12321393716321254, 'eval_recall@deu.rst.pcc': 0.17673067673067674, 'eval_loss@deu.rst.pcc': 2.5828421115875244, 'eval_runtime': 3.2266, 'eval_samples_per_second': 74.692, 'eval_steps_per_second': 2.479, 'epoch': 12.0}
{'train_runtime': 1018.0491, 'train_samples_per_second': 25.508, 'train_steps_per_second': 0.802, 'train_loss': 2.7442111595004213, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.3107
  train_runtime            = 0:17:52.48
  train_samples_per_second =     25.063
  train_steps_per_second   =      0.783
-------------------------------------------------------------------
Lang1:  spa.rst.sctb    Lang2:  deu.rst.pcc
Saving run to:  runs/full_shot/FullShot=v4_spa.rst.sctb_deu.rst.pcc_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 439 examples
read 94 examples
read 159 examples
read 2164 examples
read 241 examples
read 260 examples
Total prediction labels:  31
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=31, bias=True)
    )
  )
)
{'train@spa.rst.sctb_loss': 3.1359968185424805, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3621867881548975, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.029821950710108607, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.038805346700083544, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04603942652329749, 'train@spa.rst.sctb_loss@spa.rst.sctb': 3.1359970569610596, 'train@spa.rst.sctb_runtime': 5.514, 'train@spa.rst.sctb_samples_per_second': 79.615, 'train@spa.rst.sctb_steps_per_second': 2.539, 'epoch': 1.0}
{'loss': 3.2841, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.109952449798584, 'eval_accuracy@spa.rst.sctb': 0.3829787234042553, 'eval_f1@spa.rst.sctb': 0.04652844744455159, 'eval_precision@spa.rst.sctb': 0.05710508922670192, 'eval_recall@spa.rst.sctb': 0.06811145510835914, 'eval_loss@spa.rst.sctb': 3.109952926635742, 'eval_runtime': 1.423, 'eval_samples_per_second': 66.06, 'eval_steps_per_second': 2.108, 'epoch': 1.0}
{'train@spa.rst.sctb_loss': 2.8989226818084717, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.36446469248291574, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.029968149540517963, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03890455304928989, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04631720430107527, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.8989224433898926, 'train@spa.rst.sctb_runtime': 5.5403, 'train@spa.rst.sctb_samples_per_second': 79.238, 'train@spa.rst.sctb_steps_per_second': 2.527, 'epoch': 2.0}
{'loss': 3.0343, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.870145797729492, 'eval_accuracy@spa.rst.sctb': 0.4148936170212766, 'eval_f1@spa.rst.sctb': 0.05950226244343891, 'eval_precision@spa.rst.sctb': 0.07273254129237902, 'eval_recall@spa.rst.sctb': 0.07739938080495357, 'eval_loss@spa.rst.sctb': 2.870145797729492, 'eval_runtime': 1.4392, 'eval_samples_per_second': 65.313, 'eval_steps_per_second': 2.084, 'epoch': 2.0}
{'train@spa.rst.sctb_loss': 2.675811290740967, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.36674259681093396, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03067146730536935, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.0398170246618934, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04676523297491039, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.675811290740967, 'train@spa.rst.sctb_runtime': 5.5677, 'train@spa.rst.sctb_samples_per_second': 78.848, 'train@spa.rst.sctb_steps_per_second': 2.515, 'epoch': 3.0}
{'loss': 2.819, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.6492912769317627, 'eval_accuracy@spa.rst.sctb': 0.4148936170212766, 'eval_f1@spa.rst.sctb': 0.05950226244343891, 'eval_precision@spa.rst.sctb': 0.07273254129237902, 'eval_recall@spa.rst.sctb': 0.07739938080495357, 'eval_loss@spa.rst.sctb': 2.6492912769317627, 'eval_runtime': 1.4339, 'eval_samples_per_second': 65.557, 'eval_steps_per_second': 2.092, 'epoch': 3.0}
{'train@spa.rst.sctb_loss': 2.494771718978882, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3712984054669704, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03155538620257036, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.041999801113762926, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.047491039426523295, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.4947714805603027, 'train@spa.rst.sctb_runtime': 5.5609, 'train@spa.rst.sctb_samples_per_second': 78.944, 'train@spa.rst.sctb_steps_per_second': 2.518, 'epoch': 4.0}
{'loss': 2.6184, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.47782564163208, 'eval_accuracy@spa.rst.sctb': 0.4148936170212766, 'eval_f1@spa.rst.sctb': 0.06032085561497326, 'eval_precision@spa.rst.sctb': 0.08088235294117647, 'eval_recall@spa.rst.sctb': 0.07739938080495357, 'eval_loss@spa.rst.sctb': 2.4778261184692383, 'eval_runtime': 1.4441, 'eval_samples_per_second': 65.093, 'eval_steps_per_second': 2.077, 'epoch': 4.0}
{'train@spa.rst.sctb_loss': 2.3696324825286865, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3690205011389522, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.030858395989974936, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.04119674185463659, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.047043010752688165, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.3696324825286865, 'train@spa.rst.sctb_runtime': 5.5776, 'train@spa.rst.sctb_samples_per_second': 78.708, 'train@spa.rst.sctb_steps_per_second': 2.51, 'epoch': 5.0}
{'loss': 2.4714, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.3627827167510986, 'eval_accuracy@spa.rst.sctb': 0.39361702127659576, 'eval_f1@spa.rst.sctb': 0.05202420310648119, 'eval_precision@spa.rst.sctb': 0.0803921568627451, 'eval_recall@spa.rst.sctb': 0.07120743034055728, 'eval_loss@spa.rst.sctb': 2.3627827167510986, 'eval_runtime': 1.437, 'eval_samples_per_second': 65.414, 'eval_steps_per_second': 2.088, 'epoch': 5.0}
{'train@spa.rst.sctb_loss': 2.29160213470459, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.37585421412300685, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.032915420596580015, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.04339710050141704, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.048387096774193554, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.29160213470459, 'train@spa.rst.sctb_runtime': 5.604, 'train@spa.rst.sctb_samples_per_second': 78.337, 'train@spa.rst.sctb_steps_per_second': 2.498, 'epoch': 6.0}
{'loss': 2.3748, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.295619010925293, 'eval_accuracy@spa.rst.sctb': 0.39361702127659576, 'eval_f1@spa.rst.sctb': 0.05202420310648119, 'eval_precision@spa.rst.sctb': 0.0803921568627451, 'eval_recall@spa.rst.sctb': 0.07120743034055728, 'eval_loss@spa.rst.sctb': 2.295619010925293, 'eval_runtime': 1.4465, 'eval_samples_per_second': 64.984, 'eval_steps_per_second': 2.074, 'epoch': 6.0}
{'train@spa.rst.sctb_loss': 2.2423102855682373, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3895216400911162, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03616486975155695, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.036550632911392406, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.051245519713261654, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.2423102855682373, 'train@spa.rst.sctb_runtime': 5.588, 'train@spa.rst.sctb_samples_per_second': 78.561, 'train@spa.rst.sctb_steps_per_second': 2.505, 'epoch': 7.0}
{'loss': 2.3073, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.2556653022766113, 'eval_accuracy@spa.rst.sctb': 0.43617021276595747, 'eval_f1@spa.rst.sctb': 0.06651474148981627, 'eval_precision@spa.rst.sctb': 0.07512495194156094, 'eval_recall@spa.rst.sctb': 0.08359133126934984, 'eval_loss@spa.rst.sctb': 2.2556653022766113, 'eval_runtime': 1.4423, 'eval_samples_per_second': 65.172, 'eval_steps_per_second': 2.08, 'epoch': 7.0}
{'train@spa.rst.sctb_loss': 2.2134690284729004, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3940774487471526, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03757971454600668, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03564749053030303, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05248207885304659, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.2134687900543213, 'train@spa.rst.sctb_runtime': 5.6045, 'train@spa.rst.sctb_samples_per_second': 78.33, 'train@spa.rst.sctb_steps_per_second': 2.498, 'epoch': 8.0}
{'loss': 2.2499, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.233769416809082, 'eval_accuracy@spa.rst.sctb': 0.43617021276595747, 'eval_f1@spa.rst.sctb': 0.06484110885733604, 'eval_precision@spa.rst.sctb': 0.0661684169834418, 'eval_recall@spa.rst.sctb': 0.08359133126934984, 'eval_loss@spa.rst.sctb': 2.233769178390503, 'eval_runtime': 1.4393, 'eval_samples_per_second': 65.311, 'eval_steps_per_second': 2.084, 'epoch': 8.0}
{'train@spa.rst.sctb_loss': 2.1932737827301025, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4054669703872437, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.0403994853109179, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.037030831099195714, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05506272401433692, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.1932737827301025, 'train@spa.rst.sctb_runtime': 5.6123, 'train@spa.rst.sctb_samples_per_second': 78.221, 'train@spa.rst.sctb_steps_per_second': 2.495, 'epoch': 9.0}
{'loss': 2.2541, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.217989206314087, 'eval_accuracy@spa.rst.sctb': 0.43617021276595747, 'eval_f1@spa.rst.sctb': 0.06583481877599526, 'eval_precision@spa.rst.sctb': 0.05828427627735586, 'eval_recall@spa.rst.sctb': 0.08621821934515432, 'eval_loss@spa.rst.sctb': 2.2179887294769287, 'eval_runtime': 1.4449, 'eval_samples_per_second': 65.057, 'eval_steps_per_second': 2.076, 'epoch': 9.0}
{'train@spa.rst.sctb_loss': 2.17978572845459, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4009111617312073, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.039861345449065035, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.0336646378313045, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.0550179211469534, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.179785966873169, 'train@spa.rst.sctb_runtime': 5.598, 'train@spa.rst.sctb_samples_per_second': 78.42, 'train@spa.rst.sctb_steps_per_second': 2.501, 'epoch': 10.0}
{'loss': 2.2204, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.208045244216919, 'eval_accuracy@spa.rst.sctb': 0.44680851063829785, 'eval_f1@spa.rst.sctb': 0.06782479073500745, 'eval_precision@spa.rst.sctb': 0.058369453044375644, 'eval_recall@spa.rst.sctb': 0.08931419457735247, 'eval_loss@spa.rst.sctb': 2.208045482635498, 'eval_runtime': 1.4476, 'eval_samples_per_second': 64.934, 'eval_steps_per_second': 2.072, 'epoch': 10.0}
{'train@spa.rst.sctb_loss': 2.1724257469177246, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4054669703872437, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.04068428295571106, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03430701475114082, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05591397849462366, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.1724257469177246, 'train@spa.rst.sctb_runtime': 5.6284, 'train@spa.rst.sctb_samples_per_second': 77.998, 'train@spa.rst.sctb_steps_per_second': 2.487, 'epoch': 11.0}
{'loss': 2.2188, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.2023122310638428, 'eval_accuracy@spa.rst.sctb': 0.44680851063829785, 'eval_f1@spa.rst.sctb': 0.06782479073500745, 'eval_precision@spa.rst.sctb': 0.058369453044375644, 'eval_recall@spa.rst.sctb': 0.08931419457735247, 'eval_loss@spa.rst.sctb': 2.2023119926452637, 'eval_runtime': 1.4402, 'eval_samples_per_second': 65.267, 'eval_steps_per_second': 2.083, 'epoch': 11.0}
{'train@spa.rst.sctb_loss': 2.169996500015259, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.40774487471526194, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.041041745971323436, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03447380236394771, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05636200716845879, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.169996738433838, 'train@spa.rst.sctb_runtime': 5.6237, 'train@spa.rst.sctb_samples_per_second': 78.063, 'train@spa.rst.sctb_steps_per_second': 2.489, 'epoch': 12.0}
{'loss': 2.2006, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.200650453567505, 'eval_accuracy@spa.rst.sctb': 0.44680851063829785, 'eval_f1@spa.rst.sctb': 0.06782479073500745, 'eval_precision@spa.rst.sctb': 0.058369453044375644, 'eval_recall@spa.rst.sctb': 0.08931419457735247, 'eval_loss@spa.rst.sctb': 2.200650691986084, 'eval_runtime': 1.4316, 'eval_samples_per_second': 65.663, 'eval_steps_per_second': 2.096, 'epoch': 12.0}
{'train_runtime': 218.3693, 'train_samples_per_second': 24.124, 'train_steps_per_second': 0.769, 'train_loss': 2.5044165679386685, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.5044
  train_runtime            = 0:03:38.36
  train_samples_per_second =     24.124
  train_steps_per_second   =      0.769
{'train@deu.rst.pcc_loss': 3.0555074214935303, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.10351201478743069, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.0161768584332869, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.032269979632182345, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.04319065861532048, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.0555074214935303, 'train@deu.rst.pcc_runtime': 26.4994, 'train@deu.rst.pcc_samples_per_second': 81.662, 'train@deu.rst.pcc_steps_per_second': 2.566, 'epoch': 1.0}
{'loss': 3.3048, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.0808298587799072, 'eval_accuracy@deu.rst.pcc': 0.058091286307053944, 'eval_f1@deu.rst.pcc': 0.015781246044403938, 'eval_precision@deu.rst.pcc': 0.04812691131498472, 'eval_recall@deu.rst.pcc': 0.04515796703296704, 'eval_loss@deu.rst.pcc': 3.0808303356170654, 'eval_runtime': 3.2149, 'eval_samples_per_second': 74.962, 'eval_steps_per_second': 2.488, 'epoch': 1.0}
{'train@deu.rst.pcc_loss': 2.9192264080047607, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.12014787430683918, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.02454842515190693, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.025030288170123347, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.04893097959362813, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.91922664642334, 'train@deu.rst.pcc_runtime': 26.4937, 'train@deu.rst.pcc_samples_per_second': 81.68, 'train@deu.rst.pcc_steps_per_second': 2.567, 'epoch': 2.0}
{'loss': 2.9928, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.95619797706604, 'eval_accuracy@deu.rst.pcc': 0.1078838174273859, 'eval_f1@deu.rst.pcc': 0.019438623937296545, 'eval_precision@deu.rst.pcc': 0.020097117794486214, 'eval_recall@deu.rst.pcc': 0.0429512617012617, 'eval_loss@deu.rst.pcc': 2.956198215484619, 'eval_runtime': 3.2525, 'eval_samples_per_second': 74.096, 'eval_steps_per_second': 2.46, 'epoch': 2.0}
{'train@deu.rst.pcc_loss': 2.862872838973999, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.15619223659889095, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.04666820582459425, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.07783021027405751, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.07105314309023819, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.86287260055542, 'train@deu.rst.pcc_runtime': 26.5039, 'train@deu.rst.pcc_samples_per_second': 81.648, 'train@deu.rst.pcc_steps_per_second': 2.566, 'epoch': 3.0}
{'loss': 2.9208, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.9178078174591064, 'eval_accuracy@deu.rst.pcc': 0.11203319502074689, 'eval_f1@deu.rst.pcc': 0.029568350762827423, 'eval_precision@deu.rst.pcc': 0.030098969290411354, 'eval_recall@deu.rst.pcc': 0.05063975376475377, 'eval_loss@deu.rst.pcc': 2.9178075790405273, 'eval_runtime': 3.2347, 'eval_samples_per_second': 74.504, 'eval_steps_per_second': 2.473, 'epoch': 3.0}
{'train@deu.rst.pcc_loss': 2.816169261932373, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2088724584103512, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08665188443792188, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.09711151926390135, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.11596175688215352, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.816169261932373, 'train@deu.rst.pcc_runtime': 26.5289, 'train@deu.rst.pcc_samples_per_second': 81.572, 'train@deu.rst.pcc_steps_per_second': 2.563, 'epoch': 4.0}
{'loss': 2.8741, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.8817265033721924, 'eval_accuracy@deu.rst.pcc': 0.14107883817427386, 'eval_f1@deu.rst.pcc': 0.0610454002695382, 'eval_precision@deu.rst.pcc': 0.0885764556740689, 'eval_recall@deu.rst.pcc': 0.08598519536019535, 'eval_loss@deu.rst.pcc': 2.8817267417907715, 'eval_runtime': 3.2177, 'eval_samples_per_second': 74.899, 'eval_steps_per_second': 2.486, 'epoch': 4.0}
{'train@deu.rst.pcc_loss': 2.776402235031128, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.22504621072088724, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09189485455357702, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.1112779831874233, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.13215242133199204, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.776402235031128, 'train@deu.rst.pcc_runtime': 26.5044, 'train@deu.rst.pcc_samples_per_second': 81.647, 'train@deu.rst.pcc_steps_per_second': 2.566, 'epoch': 5.0}
{'loss': 2.8327, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.849620819091797, 'eval_accuracy@deu.rst.pcc': 0.17842323651452283, 'eval_f1@deu.rst.pcc': 0.0815594601988779, 'eval_precision@deu.rst.pcc': 0.08275560224089636, 'eval_recall@deu.rst.pcc': 0.12395706145706148, 'eval_loss@deu.rst.pcc': 2.849620819091797, 'eval_runtime': 3.231, 'eval_samples_per_second': 74.591, 'eval_steps_per_second': 2.476, 'epoch': 5.0}
{'train@deu.rst.pcc_loss': 2.7413830757141113, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.22504621072088724, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09030715459577726, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.08652240261255705, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.13468799896343236, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7413828372955322, 'train@deu.rst.pcc_runtime': 26.4763, 'train@deu.rst.pcc_samples_per_second': 81.734, 'train@deu.rst.pcc_steps_per_second': 2.568, 'epoch': 6.0}
{'loss': 2.7958, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.8179244995117188, 'eval_accuracy@deu.rst.pcc': 0.1908713692946058, 'eval_f1@deu.rst.pcc': 0.08467469285265895, 'eval_precision@deu.rst.pcc': 0.08107604379723254, 'eval_recall@deu.rst.pcc': 0.13474893162393164, 'eval_loss@deu.rst.pcc': 2.8179244995117188, 'eval_runtime': 3.2168, 'eval_samples_per_second': 74.919, 'eval_steps_per_second': 2.487, 'epoch': 6.0}
{'train@deu.rst.pcc_loss': 2.714188575744629, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.22966728280961182, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.092966402570414, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.0872541504125974, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.13833036937930737, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.714188814163208, 'train@deu.rst.pcc_runtime': 26.4904, 'train@deu.rst.pcc_samples_per_second': 81.69, 'train@deu.rst.pcc_steps_per_second': 2.567, 'epoch': 7.0}
{'loss': 2.7653, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.7966768741607666, 'eval_accuracy@deu.rst.pcc': 0.1908713692946058, 'eval_f1@deu.rst.pcc': 0.08769706384284866, 'eval_precision@deu.rst.pcc': 0.0895308485552388, 'eval_recall@deu.rst.pcc': 0.13640237077737077, 'eval_loss@deu.rst.pcc': 2.7966771125793457, 'eval_runtime': 3.2147, 'eval_samples_per_second': 74.969, 'eval_steps_per_second': 2.489, 'epoch': 7.0}
{'train@deu.rst.pcc_loss': 2.692379951477051, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2342883548983364, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09580997009019776, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.10343725644698107, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.1415470576167971, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.692379951477051, 'train@deu.rst.pcc_runtime': 26.4303, 'train@deu.rst.pcc_samples_per_second': 81.876, 'train@deu.rst.pcc_steps_per_second': 2.573, 'epoch': 8.0}
{'loss': 2.7358, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.7798094749450684, 'eval_accuracy@deu.rst.pcc': 0.1950207468879668, 'eval_f1@deu.rst.pcc': 0.0877036041640787, 'eval_precision@deu.rst.pcc': 0.08786659124786556, 'eval_recall@deu.rst.pcc': 0.13885335116952766, 'eval_loss@deu.rst.pcc': 2.7798099517822266, 'eval_runtime': 3.2292, 'eval_samples_per_second': 74.632, 'eval_steps_per_second': 2.477, 'epoch': 8.0}
{'train@deu.rst.pcc_loss': 2.6754961013793945, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.23983364140480593, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09919327861514625, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.13846972009489655, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.14456540453034766, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6754958629608154, 'train@deu.rst.pcc_runtime': 26.4298, 'train@deu.rst.pcc_samples_per_second': 81.877, 'train@deu.rst.pcc_steps_per_second': 2.573, 'epoch': 9.0}
{'loss': 2.7301, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.7696382999420166, 'eval_accuracy@deu.rst.pcc': 0.2074688796680498, 'eval_f1@deu.rst.pcc': 0.09898558280735976, 'eval_precision@deu.rst.pcc': 0.10721230158730159, 'eval_recall@deu.rst.pcc': 0.14757842239459887, 'eval_loss@deu.rst.pcc': 2.7696382999420166, 'eval_runtime': 3.2262, 'eval_samples_per_second': 74.701, 'eval_steps_per_second': 2.48, 'epoch': 9.0}
{'train@deu.rst.pcc_loss': 2.663785934448242, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.23937153419593346, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09923567612230419, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.10233688898791399, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.14507281847519207, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.663785696029663, 'train@deu.rst.pcc_runtime': 26.4708, 'train@deu.rst.pcc_samples_per_second': 81.75, 'train@deu.rst.pcc_steps_per_second': 2.569, 'epoch': 10.0}
{'loss': 2.7065, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.7578463554382324, 'eval_accuracy@deu.rst.pcc': 0.21161825726141079, 'eval_f1@deu.rst.pcc': 0.10049357801914845, 'eval_precision@deu.rst.pcc': 0.10644528554175292, 'eval_recall@deu.rst.pcc': 0.15078355059972706, 'eval_loss@deu.rst.pcc': 2.7578461170196533, 'eval_runtime': 3.241, 'eval_samples_per_second': 74.36, 'eval_steps_per_second': 2.468, 'epoch': 10.0}
{'train@deu.rst.pcc_loss': 2.656656503677368, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.24306839186691312, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.10191076884188381, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.14219099391896664, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.1473531842217268, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.656656503677368, 'train@deu.rst.pcc_runtime': 26.4335, 'train@deu.rst.pcc_samples_per_second': 81.866, 'train@deu.rst.pcc_steps_per_second': 2.572, 'epoch': 11.0}
{'loss': 2.6934, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.752415418624878, 'eval_accuracy@deu.rst.pcc': 0.21161825726141079, 'eval_f1@deu.rst.pcc': 0.10038803761894133, 'eval_precision@deu.rst.pcc': 0.11012111662846957, 'eval_recall@deu.rst.pcc': 0.14831236981972276, 'eval_loss@deu.rst.pcc': 2.752415418624878, 'eval_runtime': 3.2221, 'eval_samples_per_second': 74.797, 'eval_steps_per_second': 2.483, 'epoch': 11.0}
{'train@deu.rst.pcc_loss': 2.6543002128601074, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.24306839186691312, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.10202217147910007, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.14225901535588317, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.14761819671057627, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6542999744415283, 'train@deu.rst.pcc_runtime': 26.4079, 'train@deu.rst.pcc_samples_per_second': 81.945, 'train@deu.rst.pcc_steps_per_second': 2.575, 'epoch': 12.0}
{'loss': 2.6857, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.750011682510376, 'eval_accuracy@deu.rst.pcc': 0.2157676348547718, 'eval_f1@deu.rst.pcc': 0.1025496721618339, 'eval_precision@deu.rst.pcc': 0.11183330540587573, 'eval_recall@deu.rst.pcc': 0.15151749802485096, 'eval_loss@deu.rst.pcc': 2.750011920928955, 'eval_runtime': 3.2327, 'eval_samples_per_second': 74.55, 'eval_steps_per_second': 2.475, 'epoch': 12.0}
{'train_runtime': 1018.5082, 'train_samples_per_second': 25.496, 'train_steps_per_second': 0.801, 'train_loss': 2.836483020408481, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.5044
  train_runtime            = 0:03:38.36
  train_samples_per_second =     24.124
  train_steps_per_second   =      0.769
-------------------------------------------------------------------
Lang1:  tur.pdtb.tdb    Lang2:  deu.rst.pcc
Saving run to:  runs/full_shot/FullShot=v4_tur.pdtb.tdb_deu.rst.pcc_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2451 examples
read 312 examples
read 422 examples
read 2164 examples
read 241 examples
read 260 examples
Total prediction labels:  49
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (tr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (tr): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=49, bias=True)
    )
  )
)
{'train@tur.pdtb.tdb_loss': 2.968057632446289, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01741967515426626, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010891738953044898, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.96805739402771, 'train@tur.pdtb.tdb_runtime': 29.7611, 'train@tur.pdtb.tdb_samples_per_second': 82.356, 'train@tur.pdtb.tdb_steps_per_second': 2.587, 'epoch': 1.0}
{'loss': 3.4463, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.8946704864501953, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.8946707248687744, 'eval_runtime': 4.1993, 'eval_samples_per_second': 74.298, 'eval_steps_per_second': 2.381, 'epoch': 1.0}
{'train@tur.pdtb.tdb_loss': 2.4687938690185547, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01741967515426626, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010891738953044898, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.4687938690185547, 'train@tur.pdtb.tdb_runtime': 30.0792, 'train@tur.pdtb.tdb_samples_per_second': 81.485, 'train@tur.pdtb.tdb_steps_per_second': 2.56, 'epoch': 2.0}
{'loss': 2.6849, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.3507118225097656, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.3507115840911865, 'eval_runtime': 4.3049, 'eval_samples_per_second': 72.475, 'eval_steps_per_second': 2.323, 'epoch': 2.0}
{'train@tur.pdtb.tdb_loss': 2.3786349296569824, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01741967515426626, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010891738953044898, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.3786346912384033, 'train@tur.pdtb.tdb_runtime': 30.0789, 'train@tur.pdtb.tdb_samples_per_second': 81.486, 'train@tur.pdtb.tdb_steps_per_second': 2.56, 'epoch': 3.0}
{'loss': 2.4404, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.295682191848755, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.295682191848755, 'eval_runtime': 4.282, 'eval_samples_per_second': 72.863, 'eval_steps_per_second': 2.335, 'epoch': 3.0}
{'train@tur.pdtb.tdb_loss': 2.3080945014953613, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.27090983272133823, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.03196982368810306, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.05945882178300934, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.05331512851564645, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.3080947399139404, 'train@tur.pdtb.tdb_runtime': 30.1357, 'train@tur.pdtb.tdb_samples_per_second': 81.332, 'train@tur.pdtb.tdb_steps_per_second': 2.555, 'epoch': 4.0}
{'loss': 2.3689, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.253203868865967, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.02147679608819505, 'eval_precision@tur.pdtb.tdb': 0.017983048304830482, 'eval_recall@tur.pdtb.tdb': 0.046243798724309, 'eval_loss@tur.pdtb.tdb': 2.253203868865967, 'eval_runtime': 4.2958, 'eval_samples_per_second': 72.63, 'eval_steps_per_second': 2.328, 'epoch': 4.0}
{'train@tur.pdtb.tdb_loss': 2.2543067932128906, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3182374541003672, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.08223161315838289, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.10235077908123548, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.09733163033162473, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.2543067932128906, 'train@tur.pdtb.tdb_runtime': 30.0962, 'train@tur.pdtb.tdb_samples_per_second': 81.439, 'train@tur.pdtb.tdb_steps_per_second': 2.558, 'epoch': 5.0}
{'loss': 2.318, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.2168548107147217, 'eval_accuracy@tur.pdtb.tdb': 0.2948717948717949, 'eval_f1@tur.pdtb.tdb': 0.0731330975064563, 'eval_precision@tur.pdtb.tdb': 0.07894385026737968, 'eval_recall@tur.pdtb.tdb': 0.09864323604578744, 'eval_loss@tur.pdtb.tdb': 2.2168548107147217, 'eval_runtime': 4.2454, 'eval_samples_per_second': 73.49, 'eval_steps_per_second': 2.355, 'epoch': 5.0}
{'train@tur.pdtb.tdb_loss': 2.2051334381103516, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3268053855569155, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.08543943748144522, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.09058499230081911, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.10622417500298098, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.2051334381103516, 'train@tur.pdtb.tdb_runtime': 30.0614, 'train@tur.pdtb.tdb_samples_per_second': 81.533, 'train@tur.pdtb.tdb_steps_per_second': 2.561, 'epoch': 6.0}
{'loss': 2.2722, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.1888322830200195, 'eval_accuracy@tur.pdtb.tdb': 0.30128205128205127, 'eval_f1@tur.pdtb.tdb': 0.07512722634067984, 'eval_precision@tur.pdtb.tdb': 0.08537498537498538, 'eval_recall@tur.pdtb.tdb': 0.10140877390053121, 'eval_loss@tur.pdtb.tdb': 2.1888322830200195, 'eval_runtime': 4.2608, 'eval_samples_per_second': 73.226, 'eval_steps_per_second': 2.347, 'epoch': 6.0}
{'train@tur.pdtb.tdb_loss': 2.1716015338897705, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3382292941656467, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09069334688470251, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.10943399216732846, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11237764028949673, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1716015338897705, 'train@tur.pdtb.tdb_runtime': 30.0834, 'train@tur.pdtb.tdb_samples_per_second': 81.473, 'train@tur.pdtb.tdb_steps_per_second': 2.56, 'epoch': 7.0}
{'loss': 2.2317, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.1669328212738037, 'eval_accuracy@tur.pdtb.tdb': 0.3076923076923077, 'eval_f1@tur.pdtb.tdb': 0.08140650799677722, 'eval_precision@tur.pdtb.tdb': 0.08068784857953136, 'eval_recall@tur.pdtb.tdb': 0.10795456490769713, 'eval_loss@tur.pdtb.tdb': 2.1669325828552246, 'eval_runtime': 4.2388, 'eval_samples_per_second': 73.606, 'eval_steps_per_second': 2.359, 'epoch': 7.0}
{'train@tur.pdtb.tdb_loss': 2.1534178256988525, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3419012647898817, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09268850338060297, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.10752993694342915, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11362480722992893, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1534178256988525, 'train@tur.pdtb.tdb_runtime': 30.0751, 'train@tur.pdtb.tdb_samples_per_second': 81.496, 'train@tur.pdtb.tdb_steps_per_second': 2.56, 'epoch': 8.0}
{'loss': 2.2111, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.152940511703491, 'eval_accuracy@tur.pdtb.tdb': 0.3076923076923077, 'eval_f1@tur.pdtb.tdb': 0.08106461394863923, 'eval_precision@tur.pdtb.tdb': 0.08163045344987889, 'eval_recall@tur.pdtb.tdb': 0.10716531163793358, 'eval_loss@tur.pdtb.tdb': 2.152940511703491, 'eval_runtime': 4.2441, 'eval_samples_per_second': 73.513, 'eval_steps_per_second': 2.356, 'epoch': 8.0}
{'train@tur.pdtb.tdb_loss': 2.127929210662842, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3427172582619339, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09248056060421216, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.09956257926466311, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11487959324357559, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.127929210662842, 'train@tur.pdtb.tdb_runtime': 30.1303, 'train@tur.pdtb.tdb_samples_per_second': 81.347, 'train@tur.pdtb.tdb_steps_per_second': 2.556, 'epoch': 9.0}
{'loss': 2.1848, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.1352014541625977, 'eval_accuracy@tur.pdtb.tdb': 0.3173076923076923, 'eval_f1@tur.pdtb.tdb': 0.08358870031441391, 'eval_precision@tur.pdtb.tdb': 0.08022825659189295, 'eval_recall@tur.pdtb.tdb': 0.11203953222795858, 'eval_loss@tur.pdtb.tdb': 2.1352014541625977, 'eval_runtime': 4.236, 'eval_samples_per_second': 73.655, 'eval_steps_per_second': 2.361, 'epoch': 9.0}
{'train@tur.pdtb.tdb_loss': 2.114898920059204, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.34720522235822115, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09596894424769721, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.09910374245208192, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11721781377546395, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.114899158477783, 'train@tur.pdtb.tdb_runtime': 30.1187, 'train@tur.pdtb.tdb_samples_per_second': 81.378, 'train@tur.pdtb.tdb_steps_per_second': 2.557, 'epoch': 10.0}
{'loss': 2.1704, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.126384735107422, 'eval_accuracy@tur.pdtb.tdb': 0.32371794871794873, 'eval_f1@tur.pdtb.tdb': 0.08769798641930779, 'eval_precision@tur.pdtb.tdb': 0.0820761967501098, 'eval_recall@tur.pdtb.tdb': 0.11606901915964253, 'eval_loss@tur.pdtb.tdb': 2.126384735107422, 'eval_runtime': 4.2489, 'eval_samples_per_second': 73.431, 'eval_steps_per_second': 2.354, 'epoch': 10.0}
{'train@tur.pdtb.tdb_loss': 2.1085989475250244, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.34802121583027334, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09639281657844641, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.1010049483354624, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11734695427267443, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1085989475250244, 'train@tur.pdtb.tdb_runtime': 30.1411, 'train@tur.pdtb.tdb_samples_per_second': 81.318, 'train@tur.pdtb.tdb_steps_per_second': 2.555, 'epoch': 11.0}
{'loss': 2.1603, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.122783899307251, 'eval_accuracy@tur.pdtb.tdb': 0.32051282051282054, 'eval_f1@tur.pdtb.tdb': 0.08549810844892812, 'eval_precision@tur.pdtb.tdb': 0.08018891919249203, 'eval_recall@tur.pdtb.tdb': 0.1140171396272527, 'eval_loss@tur.pdtb.tdb': 2.12278413772583, 'eval_runtime': 4.2656, 'eval_samples_per_second': 73.143, 'eval_steps_per_second': 2.344, 'epoch': 11.0}
{'train@tur.pdtb.tdb_loss': 2.105435609817505, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.35006119951040393, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09755656000379442, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.10017325325543379, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11835863536345236, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.105435371398926, 'train@tur.pdtb.tdb_runtime': 30.1029, 'train@tur.pdtb.tdb_samples_per_second': 81.421, 'train@tur.pdtb.tdb_steps_per_second': 2.558, 'epoch': 12.0}
{'loss': 2.1498, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.1205084323883057, 'eval_accuracy@tur.pdtb.tdb': 0.3173076923076923, 'eval_f1@tur.pdtb.tdb': 0.08512062971947529, 'eval_precision@tur.pdtb.tdb': 0.07942876936239768, 'eval_recall@tur.pdtb.tdb': 0.11346949450129434, 'eval_loss@tur.pdtb.tdb': 2.1205081939697266, 'eval_runtime': 4.2516, 'eval_samples_per_second': 73.384, 'eval_steps_per_second': 2.352, 'epoch': 12.0}
{'train_runtime': 1164.0168, 'train_samples_per_second': 25.268, 'train_steps_per_second': 0.794, 'train_loss': 2.3865643455868675, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.3866
  train_runtime            = 0:19:24.01
  train_samples_per_second =     25.268
  train_steps_per_second   =      0.794
{'train@deu.rst.pcc_loss': 3.1276917457580566, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.09426987060998152, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.014056312207441076, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.021355599491523155, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.03891732880513236, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.1276919841766357, 'train@deu.rst.pcc_runtime': 26.6156, 'train@deu.rst.pcc_samples_per_second': 81.306, 'train@deu.rst.pcc_steps_per_second': 2.555, 'epoch': 1.0}
{'loss': 3.4185, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.1722686290740967, 'eval_accuracy@deu.rst.pcc': 0.06639004149377593, 'eval_f1@deu.rst.pcc': 0.009827441077441079, 'eval_precision@deu.rst.pcc': 0.005720701454234388, 'eval_recall@deu.rst.pcc': 0.04683048433048433, 'eval_loss@deu.rst.pcc': 3.172268867492676, 'eval_runtime': 3.3649, 'eval_samples_per_second': 71.621, 'eval_steps_per_second': 2.377, 'epoch': 1.0}
{'train@deu.rst.pcc_loss': 2.954946517944336, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.13031423290203328, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.03182854201570082, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.07437669993967248, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.05487701288497762, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.954946279525757, 'train@deu.rst.pcc_runtime': 26.5719, 'train@deu.rst.pcc_samples_per_second': 81.44, 'train@deu.rst.pcc_steps_per_second': 2.559, 'epoch': 2.0}
{'loss': 3.0539, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.9884650707244873, 'eval_accuracy@deu.rst.pcc': 0.0995850622406639, 'eval_f1@deu.rst.pcc': 0.020548508339206014, 'eval_precision@deu.rst.pcc': 0.014893923288156613, 'eval_recall@deu.rst.pcc': 0.043281949531949526, 'eval_loss@deu.rst.pcc': 2.9884653091430664, 'eval_runtime': 3.3689, 'eval_samples_per_second': 71.536, 'eval_steps_per_second': 2.375, 'epoch': 2.0}
{'train@deu.rst.pcc_loss': 2.882343053817749, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.16173752310536044, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.06311696226261275, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.11794002231556423, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.0796090884431692, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.882343053817749, 'train@deu.rst.pcc_runtime': 26.5922, 'train@deu.rst.pcc_samples_per_second': 81.377, 'train@deu.rst.pcc_steps_per_second': 2.557, 'epoch': 3.0}
{'loss': 2.9514, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.930168390274048, 'eval_accuracy@deu.rst.pcc': 0.1037344398340249, 'eval_f1@deu.rst.pcc': 0.03271378664170679, 'eval_precision@deu.rst.pcc': 0.04104319912246741, 'eval_recall@deu.rst.pcc': 0.051383801383801386, 'eval_loss@deu.rst.pcc': 2.9301679134368896, 'eval_runtime': 3.3577, 'eval_samples_per_second': 71.776, 'eval_steps_per_second': 2.383, 'epoch': 3.0}
{'train@deu.rst.pcc_loss': 2.8303589820861816, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.19408502772643252, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.07408579516512963, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.10912233822172292, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.10884331830041186, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.8303592205047607, 'train@deu.rst.pcc_runtime': 26.6181, 'train@deu.rst.pcc_samples_per_second': 81.298, 'train@deu.rst.pcc_steps_per_second': 2.555, 'epoch': 4.0}
{'loss': 2.8966, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.8920087814331055, 'eval_accuracy@deu.rst.pcc': 0.13692946058091288, 'eval_f1@deu.rst.pcc': 0.05026455026455027, 'eval_precision@deu.rst.pcc': 0.03560894472487381, 'eval_recall@deu.rst.pcc': 0.09222374847374848, 'eval_loss@deu.rst.pcc': 2.8920083045959473, 'eval_runtime': 3.3479, 'eval_samples_per_second': 71.986, 'eval_steps_per_second': 2.39, 'epoch': 4.0}
{'train@deu.rst.pcc_loss': 2.788944721221924, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.20471349353049909, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.07365543187408764, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.13126752449306364, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.11903100039662663, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7889442443847656, 'train@deu.rst.pcc_runtime': 26.5776, 'train@deu.rst.pcc_samples_per_second': 81.422, 'train@deu.rst.pcc_steps_per_second': 2.559, 'epoch': 5.0}
{'loss': 2.8474, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.860090970993042, 'eval_accuracy@deu.rst.pcc': 0.14107883817427386, 'eval_f1@deu.rst.pcc': 0.046797915515665776, 'eval_precision@deu.rst.pcc': 0.03227943390545016, 'eval_recall@deu.rst.pcc': 0.10032560032560033, 'eval_loss@deu.rst.pcc': 2.8600916862487793, 'eval_runtime': 3.3772, 'eval_samples_per_second': 71.361, 'eval_steps_per_second': 2.369, 'epoch': 5.0}
{'train@deu.rst.pcc_loss': 2.7526447772979736, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2121072088724584, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08266250141849378, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.14900681781261702, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.12663430365775877, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7526450157165527, 'train@deu.rst.pcc_runtime': 26.5478, 'train@deu.rst.pcc_samples_per_second': 81.513, 'train@deu.rst.pcc_steps_per_second': 2.561, 'epoch': 6.0}
{'loss': 2.8053, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.830505132675171, 'eval_accuracy@deu.rst.pcc': 0.12863070539419086, 'eval_f1@deu.rst.pcc': 0.04419657577327729, 'eval_precision@deu.rst.pcc': 0.06593023069931443, 'eval_recall@deu.rst.pcc': 0.09592490842490842, 'eval_loss@deu.rst.pcc': 2.83050537109375, 'eval_runtime': 3.4198, 'eval_samples_per_second': 70.471, 'eval_steps_per_second': 2.339, 'epoch': 6.0}
{'train@deu.rst.pcc_loss': 2.7250025272369385, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2134935304990758, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08453428384110206, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.13239189171363497, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.12777211163446361, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7250022888183594, 'train@deu.rst.pcc_runtime': 26.5574, 'train@deu.rst.pcc_samples_per_second': 81.484, 'train@deu.rst.pcc_steps_per_second': 2.56, 'epoch': 7.0}
{'loss': 2.7832, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.810340166091919, 'eval_accuracy@deu.rst.pcc': 0.14522821576763487, 'eval_f1@deu.rst.pcc': 0.05334832340646295, 'eval_precision@deu.rst.pcc': 0.0690133447607118, 'eval_recall@deu.rst.pcc': 0.10442104192104192, 'eval_loss@deu.rst.pcc': 2.81033992767334, 'eval_runtime': 3.4017, 'eval_samples_per_second': 70.846, 'eval_steps_per_second': 2.352, 'epoch': 7.0}
{'train@deu.rst.pcc_loss': 2.7029733657836914, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2158040665434381, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08650739358237819, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.14174456998646368, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.12994239846769706, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7029733657836914, 'train@deu.rst.pcc_runtime': 26.5875, 'train@deu.rst.pcc_samples_per_second': 81.392, 'train@deu.rst.pcc_steps_per_second': 2.558, 'epoch': 8.0}
{'loss': 2.7523, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.7942492961883545, 'eval_accuracy@deu.rst.pcc': 0.14937759336099585, 'eval_f1@deu.rst.pcc': 0.05723093890946738, 'eval_precision@deu.rst.pcc': 0.06881684491978611, 'eval_recall@deu.rst.pcc': 0.10762617012617011, 'eval_loss@deu.rst.pcc': 2.7942488193511963, 'eval_runtime': 3.354, 'eval_samples_per_second': 71.854, 'eval_steps_per_second': 2.385, 'epoch': 8.0}
{'train@deu.rst.pcc_loss': 2.685887098312378, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.22134935304990758, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09159784559609004, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.13764864199997476, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.1340557880576295, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.685887098312378, 'train@deu.rst.pcc_runtime': 26.6094, 'train@deu.rst.pcc_samples_per_second': 81.325, 'train@deu.rst.pcc_steps_per_second': 2.555, 'epoch': 9.0}
{'loss': 2.7399, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.784878730773926, 'eval_accuracy@deu.rst.pcc': 0.15352697095435686, 'eval_f1@deu.rst.pcc': 0.061853798417075584, 'eval_precision@deu.rst.pcc': 0.06924200986700987, 'eval_recall@deu.rst.pcc': 0.11083129833129833, 'eval_loss@deu.rst.pcc': 2.7848782539367676, 'eval_runtime': 3.3948, 'eval_samples_per_second': 70.991, 'eval_steps_per_second': 2.357, 'epoch': 9.0}
{'train@deu.rst.pcc_loss': 2.6742091178894043, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2199630314232902, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.0905865010559693, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.13889099495917903, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.13370822929491946, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6742091178894043, 'train@deu.rst.pcc_runtime': 26.6328, 'train@deu.rst.pcc_samples_per_second': 81.253, 'train@deu.rst.pcc_steps_per_second': 2.553, 'epoch': 10.0}
{'loss': 2.7174, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.7736432552337646, 'eval_accuracy@deu.rst.pcc': 0.15767634854771784, 'eval_f1@deu.rst.pcc': 0.061739616854706365, 'eval_precision@deu.rst.pcc': 0.06187695743248752, 'eval_recall@deu.rst.pcc': 0.11231939356939358, 'eval_loss@deu.rst.pcc': 2.7736434936523438, 'eval_runtime': 3.428, 'eval_samples_per_second': 70.304, 'eval_steps_per_second': 2.334, 'epoch': 10.0}
{'train@deu.rst.pcc_loss': 2.6670942306518555, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.22597042513863216, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09503844551819021, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.13288380084049392, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.13740108449406532, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6670942306518555, 'train@deu.rst.pcc_runtime': 26.6083, 'train@deu.rst.pcc_samples_per_second': 81.328, 'train@deu.rst.pcc_steps_per_second': 2.556, 'epoch': 11.0}
{'loss': 2.7141, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.768566131591797, 'eval_accuracy@deu.rst.pcc': 0.16597510373443983, 'eval_f1@deu.rst.pcc': 0.06744278126631069, 'eval_precision@deu.rst.pcc': 0.07165359751566648, 'eval_recall@deu.rst.pcc': 0.11708518877636526, 'eval_loss@deu.rst.pcc': 2.768566131591797, 'eval_runtime': 3.3657, 'eval_samples_per_second': 71.604, 'eval_steps_per_second': 2.377, 'epoch': 11.0}
{'train@deu.rst.pcc_loss': 2.664712429046631, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2268946395563771, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09605098469447113, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.13443476991544798, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.13851935429142845, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.66471266746521, 'train@deu.rst.pcc_runtime': 26.6295, 'train@deu.rst.pcc_samples_per_second': 81.263, 'train@deu.rst.pcc_steps_per_second': 2.554, 'epoch': 12.0}
{'loss': 2.7051, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.766343593597412, 'eval_accuracy@deu.rst.pcc': 0.16182572614107885, 'eval_f1@deu.rst.pcc': 0.06526246612030927, 'eval_precision@deu.rst.pcc': 0.0699752170511646, 'eval_recall@deu.rst.pcc': 0.11477037396155043, 'eval_loss@deu.rst.pcc': 2.766343832015991, 'eval_runtime': 3.3616, 'eval_samples_per_second': 71.691, 'eval_steps_per_second': 2.38, 'epoch': 12.0}
{'train_runtime': 1024.5512, 'train_samples_per_second': 25.346, 'train_steps_per_second': 0.796, 'train_loss': 2.865428756265079, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.3866
  train_runtime            = 0:19:24.01
  train_samples_per_second =     25.268
  train_steps_per_second   =      0.794
-------------------------------------------------------------------
Lang1:  zho.rst.sctb    Lang2:  deu.rst.pcc
Saving run to:  runs/full_shot/FullShot=v4_zho.rst.sctb_deu.rst.pcc_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 439 examples
read 94 examples
read 159 examples
read 2164 examples
read 241 examples
read 260 examples
Total prediction labels:  31
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (zh): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=31, bias=True)
    )
  )
)
{'train@zho.rst.sctb_loss': 3.1967015266418457, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3348519362186788, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.02005677652438983, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03208061960922373, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038866396761133605, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.1967015266418457, 'train@zho.rst.sctb_runtime': 5.4721, 'train@zho.rst.sctb_samples_per_second': 80.225, 'train@zho.rst.sctb_steps_per_second': 2.558, 'epoch': 1.0}
{'loss': 3.3278, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.228278875350952, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026947368421052633, 'eval_precision@zho.rst.sctb': 0.018109790605546124, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 3.2282798290252686, 'eval_runtime': 1.4228, 'eval_samples_per_second': 66.066, 'eval_steps_per_second': 2.108, 'epoch': 1.0}
{'train@zho.rst.sctb_loss': 3.001386880874634, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.33712984054669703, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.020866713061116596, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.038520348153375676, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.039271255060728746, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.001387357711792, 'train@zho.rst.sctb_runtime': 5.4642, 'train@zho.rst.sctb_samples_per_second': 80.341, 'train@zho.rst.sctb_steps_per_second': 2.562, 'epoch': 2.0}
{'loss': 3.1138, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.043928861618042, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 3.0439298152923584, 'eval_runtime': 1.4081, 'eval_samples_per_second': 66.757, 'eval_steps_per_second': 2.131, 'epoch': 2.0}
{'train@zho.rst.sctb_loss': 2.823399543762207, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.33712984054669703, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.020866713061116596, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.038520348153375676, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.039271255060728746, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.823399543762207, 'train@zho.rst.sctb_runtime': 5.4642, 'train@zho.rst.sctb_samples_per_second': 80.341, 'train@zho.rst.sctb_steps_per_second': 2.562, 'epoch': 3.0}
{'loss': 2.9409, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.8766746520996094, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.876673698425293, 'eval_runtime': 1.4403, 'eval_samples_per_second': 65.265, 'eval_steps_per_second': 2.083, 'epoch': 3.0}
{'train@zho.rst.sctb_loss': 2.672743082046509, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.33712984054669703, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.020866713061116596, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.038520348153375676, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.039271255060728746, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.6727426052093506, 'train@zho.rst.sctb_runtime': 5.5, 'train@zho.rst.sctb_samples_per_second': 79.818, 'train@zho.rst.sctb_steps_per_second': 2.545, 'epoch': 4.0}
{'loss': 2.769, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.737492561340332, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.7374937534332275, 'eval_runtime': 1.4118, 'eval_samples_per_second': 66.582, 'eval_steps_per_second': 2.125, 'epoch': 4.0}
{'train@zho.rst.sctb_loss': 2.5525057315826416, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.33712984054669703, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.020866713061116596, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.038520348153375676, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.039271255060728746, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.5525059700012207, 'train@zho.rst.sctb_runtime': 5.5052, 'train@zho.rst.sctb_samples_per_second': 79.743, 'train@zho.rst.sctb_steps_per_second': 2.543, 'epoch': 5.0}
{'loss': 2.6563, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.6298341751098633, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 2.6298346519470215, 'eval_runtime': 1.4375, 'eval_samples_per_second': 65.393, 'eval_steps_per_second': 2.087, 'epoch': 5.0}
{'train@zho.rst.sctb_loss': 2.458120107650757, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.33940774487471526, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.0216710875331565, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03601559730591988, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.03967611336032389, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.4581198692321777, 'train@zho.rst.sctb_runtime': 5.4826, 'train@zho.rst.sctb_samples_per_second': 80.071, 'train@zho.rst.sctb_steps_per_second': 2.554, 'epoch': 6.0}
{'loss': 2.5437, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.550050735473633, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 2.550051212310791, 'eval_runtime': 1.4227, 'eval_samples_per_second': 66.071, 'eval_steps_per_second': 2.109, 'epoch': 6.0}
{'train@zho.rst.sctb_loss': 2.388902425765991, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3530751708428246, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.02604446835216066, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.0398088729616357, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04210526315789474, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.3889029026031494, 'train@zho.rst.sctb_runtime': 5.5023, 'train@zho.rst.sctb_samples_per_second': 79.785, 'train@zho.rst.sctb_steps_per_second': 2.544, 'epoch': 7.0}
{'loss': 2.4635, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.493966817855835, 'eval_accuracy@zho.rst.sctb': 0.35106382978723405, 'eval_f1@zho.rst.sctb': 0.0367722585151403, 'eval_precision@zho.rst.sctb': 0.044444444444444446, 'eval_recall@zho.rst.sctb': 0.057178792569659444, 'eval_loss@zho.rst.sctb': 2.493967056274414, 'eval_runtime': 1.4257, 'eval_samples_per_second': 65.934, 'eval_steps_per_second': 2.104, 'epoch': 7.0}
{'train@zho.rst.sctb_loss': 2.3451168537139893, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3621867881548975, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.029060142596459546, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.038982946877683715, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.0438661194609284, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.3451168537139893, 'train@zho.rst.sctb_runtime': 5.5097, 'train@zho.rst.sctb_samples_per_second': 79.678, 'train@zho.rst.sctb_steps_per_second': 2.541, 'epoch': 8.0}
{'loss': 2.4022, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.460200309753418, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.038810260946483856, 'eval_precision@zho.rst.sctb': 0.040100250626566414, 'eval_recall@zho.rst.sctb': 0.05698529411764706, 'eval_loss@zho.rst.sctb': 2.460200309753418, 'eval_runtime': 1.4233, 'eval_samples_per_second': 66.045, 'eval_steps_per_second': 2.108, 'epoch': 8.0}
{'train@zho.rst.sctb_loss': 2.3150253295898438, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3826879271070615, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.034414578794775476, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.040824534942182, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04765126726193777, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.315025568008423, 'train@zho.rst.sctb_runtime': 5.519, 'train@zho.rst.sctb_samples_per_second': 79.543, 'train@zho.rst.sctb_steps_per_second': 2.537, 'epoch': 9.0}
{'loss': 2.3616, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.43689227104187, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.038810260946483856, 'eval_precision@zho.rst.sctb': 0.040100250626566414, 'eval_recall@zho.rst.sctb': 0.05698529411764706, 'eval_loss@zho.rst.sctb': 2.436892032623291, 'eval_runtime': 1.4438, 'eval_samples_per_second': 65.104, 'eval_steps_per_second': 2.078, 'epoch': 9.0}
{'train@zho.rst.sctb_loss': 2.2955379486083984, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.39635535307517084, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.03676512572086272, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03820332480818414, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.05008041705950862, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.2955381870269775, 'train@zho.rst.sctb_runtime': 5.521, 'train@zho.rst.sctb_samples_per_second': 79.515, 'train@zho.rst.sctb_steps_per_second': 2.536, 'epoch': 10.0}
{'loss': 2.3409, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.4220006465911865, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.038236617183985605, 'eval_precision@zho.rst.sctb': 0.035500515995872034, 'eval_recall@zho.rst.sctb': 0.05698529411764706, 'eval_loss@zho.rst.sctb': 2.421999931335449, 'eval_runtime': 1.4397, 'eval_samples_per_second': 65.29, 'eval_steps_per_second': 2.084, 'epoch': 10.0}
{'train@zho.rst.sctb_loss': 2.2859771251678467, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.39635535307517084, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.037091158692796736, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03697783697783698, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.05036326326881482, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.2859771251678467, 'train@zho.rst.sctb_runtime': 5.508, 'train@zho.rst.sctb_samples_per_second': 79.703, 'train@zho.rst.sctb_steps_per_second': 2.542, 'epoch': 11.0}
{'loss': 2.3274, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.414884090423584, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.038236617183985605, 'eval_precision@zho.rst.sctb': 0.035500515995872034, 'eval_recall@zho.rst.sctb': 0.05698529411764706, 'eval_loss@zho.rst.sctb': 2.414884090423584, 'eval_runtime': 1.4197, 'eval_samples_per_second': 66.209, 'eval_steps_per_second': 2.113, 'epoch': 11.0}
{'train@zho.rst.sctb_loss': 2.282841444015503, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.4009111617312073, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.03789412342043921, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03723919141389519, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.051172979868005106, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.282841444015503, 'train@zho.rst.sctb_runtime': 5.5229, 'train@zho.rst.sctb_samples_per_second': 79.487, 'train@zho.rst.sctb_steps_per_second': 2.535, 'epoch': 12.0}
{'loss': 2.3108, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.4126031398773193, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.038236617183985605, 'eval_precision@zho.rst.sctb': 0.035500515995872034, 'eval_recall@zho.rst.sctb': 0.05698529411764706, 'eval_loss@zho.rst.sctb': 2.412602424621582, 'eval_runtime': 1.4155, 'eval_samples_per_second': 66.407, 'eval_steps_per_second': 2.119, 'epoch': 12.0}
{'train_runtime': 214.8291, 'train_samples_per_second': 24.522, 'train_steps_per_second': 0.782, 'train_loss': 2.6298230716160367, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.6298
  train_runtime            = 0:03:34.82
  train_samples_per_second =     24.522
  train_steps_per_second   =      0.782
{'train@deu.rst.pcc_loss': 3.0334787368774414, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.10859519408502773, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.018993452998665568, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.017002718753805976, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.04388780025072248, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.0334787368774414, 'train@deu.rst.pcc_runtime': 26.5057, 'train@deu.rst.pcc_samples_per_second': 81.643, 'train@deu.rst.pcc_steps_per_second': 2.565, 'epoch': 1.0}
{'loss': 3.2589, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.0534839630126953, 'eval_accuracy@deu.rst.pcc': 0.0912863070539419, 'eval_f1@deu.rst.pcc': 0.015608092213596802, 'eval_precision@deu.rst.pcc': 0.011067708333333334, 'eval_recall@deu.rst.pcc': 0.045584045584045586, 'eval_loss@deu.rst.pcc': 3.0534842014312744, 'eval_runtime': 3.2313, 'eval_samples_per_second': 74.584, 'eval_steps_per_second': 2.476, 'epoch': 1.0}
{'train@deu.rst.pcc_loss': 2.928389549255371, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.12338262476894639, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.023439434961637053, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.02277599230175527, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.05018307969121435, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.928389072418213, 'train@deu.rst.pcc_runtime': 26.4991, 'train@deu.rst.pcc_samples_per_second': 81.663, 'train@deu.rst.pcc_steps_per_second': 2.566, 'epoch': 2.0}
{'loss': 2.9981, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.9603869915008545, 'eval_accuracy@deu.rst.pcc': 0.11203319502074689, 'eval_f1@deu.rst.pcc': 0.019007809102148724, 'eval_precision@deu.rst.pcc': 0.021572331212442564, 'eval_recall@deu.rst.pcc': 0.04615638990638991, 'eval_loss@deu.rst.pcc': 2.9603869915008545, 'eval_runtime': 3.2457, 'eval_samples_per_second': 74.252, 'eval_steps_per_second': 2.465, 'epoch': 2.0}
{'train@deu.rst.pcc_loss': 2.8683834075927734, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.1511090573012939, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.04451060478535948, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.060109504118575324, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.0670555268484602, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.8683829307556152, 'train@deu.rst.pcc_runtime': 26.5154, 'train@deu.rst.pcc_samples_per_second': 81.613, 'train@deu.rst.pcc_steps_per_second': 2.565, 'epoch': 3.0}
{'loss': 2.93, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.9176740646362305, 'eval_accuracy@deu.rst.pcc': 0.11618257261410789, 'eval_f1@deu.rst.pcc': 0.025984574992815496, 'eval_precision@deu.rst.pcc': 0.03146998553519768, 'eval_recall@deu.rst.pcc': 0.05184167684167684, 'eval_loss@deu.rst.pcc': 2.9176735877990723, 'eval_runtime': 3.2316, 'eval_samples_per_second': 74.576, 'eval_steps_per_second': 2.476, 'epoch': 3.0}
{'train@deu.rst.pcc_loss': 2.8210713863372803, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.19454713493530498, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08227873966267982, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.09092736233039543, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.10472839655303029, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.821070909500122, 'train@deu.rst.pcc_runtime': 26.4808, 'train@deu.rst.pcc_samples_per_second': 81.72, 'train@deu.rst.pcc_steps_per_second': 2.568, 'epoch': 4.0}
{'loss': 2.8804, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.882676601409912, 'eval_accuracy@deu.rst.pcc': 0.16597510373443983, 'eval_f1@deu.rst.pcc': 0.0770769275103921, 'eval_precision@deu.rst.pcc': 0.11041699551018976, 'eval_recall@deu.rst.pcc': 0.09888838013838014, 'eval_loss@deu.rst.pcc': 2.882675886154175, 'eval_runtime': 3.2276, 'eval_samples_per_second': 74.668, 'eval_steps_per_second': 2.479, 'epoch': 4.0}
{'train@deu.rst.pcc_loss': 2.780163049697876, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.21765249537892792, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08986559954237981, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.0979926607545644, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.12734503543736178, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.780163049697876, 'train@deu.rst.pcc_runtime': 26.4328, 'train@deu.rst.pcc_samples_per_second': 81.868, 'train@deu.rst.pcc_steps_per_second': 2.573, 'epoch': 5.0}
{'loss': 2.8393, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.8499128818511963, 'eval_accuracy@deu.rst.pcc': 0.1825726141078838, 'eval_f1@deu.rst.pcc': 0.08135153319376465, 'eval_precision@deu.rst.pcc': 0.08241944411423817, 'eval_recall@deu.rst.pcc': 0.11081857956857956, 'eval_loss@deu.rst.pcc': 2.8499128818511963, 'eval_runtime': 3.229, 'eval_samples_per_second': 74.636, 'eval_steps_per_second': 2.478, 'epoch': 5.0}
{'train@deu.rst.pcc_loss': 2.7441086769104004, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.22735674676524953, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09270096634163325, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.09332259081133319, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.13717381370108359, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7441084384918213, 'train@deu.rst.pcc_runtime': 26.4379, 'train@deu.rst.pcc_samples_per_second': 81.852, 'train@deu.rst.pcc_steps_per_second': 2.572, 'epoch': 6.0}
{'loss': 2.8012, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.8171305656433105, 'eval_accuracy@deu.rst.pcc': 0.21161825726141079, 'eval_f1@deu.rst.pcc': 0.09515760721354145, 'eval_precision@deu.rst.pcc': 0.08666028737341508, 'eval_recall@deu.rst.pcc': 0.14816722629222628, 'eval_loss@deu.rst.pcc': 2.8171303272247314, 'eval_runtime': 3.2304, 'eval_samples_per_second': 74.603, 'eval_steps_per_second': 2.476, 'epoch': 6.0}
{'train@deu.rst.pcc_loss': 2.715975522994995, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.23105360443622922, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09346065751857481, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.09037954733501373, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.13983688182659249, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.715975284576416, 'train@deu.rst.pcc_runtime': 26.4405, 'train@deu.rst.pcc_samples_per_second': 81.844, 'train@deu.rst.pcc_steps_per_second': 2.572, 'epoch': 7.0}
{'loss': 2.7697, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.7944931983947754, 'eval_accuracy@deu.rst.pcc': 0.2157676348547718, 'eval_f1@deu.rst.pcc': 0.09599682223936272, 'eval_precision@deu.rst.pcc': 0.08409185090786725, 'eval_recall@deu.rst.pcc': 0.1521990740740741, 'eval_loss@deu.rst.pcc': 2.7944934368133545, 'eval_runtime': 3.2236, 'eval_samples_per_second': 74.76, 'eval_steps_per_second': 2.482, 'epoch': 7.0}
{'train@deu.rst.pcc_loss': 2.693375825881958, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2301293900184843, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09225246114467812, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.08712894988514791, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.14099189144802554, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.693375587463379, 'train@deu.rst.pcc_runtime': 26.4161, 'train@deu.rst.pcc_samples_per_second': 81.92, 'train@deu.rst.pcc_steps_per_second': 2.574, 'epoch': 8.0}
{'loss': 2.7398, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.77705717086792, 'eval_accuracy@deu.rst.pcc': 0.2033195020746888, 'eval_f1@deu.rst.pcc': 0.0855970487839005, 'eval_precision@deu.rst.pcc': 0.07050721421609579, 'eval_recall@deu.rst.pcc': 0.14519103581603582, 'eval_loss@deu.rst.pcc': 2.77705717086792, 'eval_runtime': 3.2255, 'eval_samples_per_second': 74.718, 'eval_steps_per_second': 2.48, 'epoch': 8.0}
{'train@deu.rst.pcc_loss': 2.675898313522339, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.23706099815157117, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.0961789414494465, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.09056590810496344, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.14454636665500778, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.675898313522339, 'train@deu.rst.pcc_runtime': 26.4529, 'train@deu.rst.pcc_samples_per_second': 81.806, 'train@deu.rst.pcc_steps_per_second': 2.571, 'epoch': 9.0}
{'loss': 2.7323, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.766469955444336, 'eval_accuracy@deu.rst.pcc': 0.21161825726141079, 'eval_f1@deu.rst.pcc': 0.09203180294281987, 'eval_precision@deu.rst.pcc': 0.07831375409500409, 'eval_recall@deu.rst.pcc': 0.14995683102300747, 'eval_loss@deu.rst.pcc': 2.766469717025757, 'eval_runtime': 3.2379, 'eval_samples_per_second': 74.43, 'eval_steps_per_second': 2.471, 'epoch': 9.0}
{'train@deu.rst.pcc_loss': 2.663745164871216, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.23798521256931607, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09733970922143126, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.09322510291109562, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.14577913464975017, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.663745164871216, 'train@deu.rst.pcc_runtime': 26.4382, 'train@deu.rst.pcc_samples_per_second': 81.851, 'train@deu.rst.pcc_steps_per_second': 2.572, 'epoch': 10.0}
{'loss': 2.7091, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.754436492919922, 'eval_accuracy@deu.rst.pcc': 0.22406639004149378, 'eval_f1@deu.rst.pcc': 0.09754118608018864, 'eval_precision@deu.rst.pcc': 0.0878577441077441, 'eval_recall@deu.rst.pcc': 0.15613814970432618, 'eval_loss@deu.rst.pcc': 2.754436492919922, 'eval_runtime': 3.2281, 'eval_samples_per_second': 74.656, 'eval_steps_per_second': 2.478, 'epoch': 10.0}
{'train@deu.rst.pcc_loss': 2.656280994415283, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.23798521256931607, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09800555157227922, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.12825561722536613, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.14610444752468157, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.656280994415283, 'train@deu.rst.pcc_runtime': 26.469, 'train@deu.rst.pcc_samples_per_second': 81.756, 'train@deu.rst.pcc_steps_per_second': 2.569, 'epoch': 11.0}
{'loss': 2.6943, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.7487809658050537, 'eval_accuracy@deu.rst.pcc': 0.2157676348547718, 'eval_f1@deu.rst.pcc': 0.093512779781037, 'eval_precision@deu.rst.pcc': 0.08621669915529566, 'eval_recall@deu.rst.pcc': 0.14972789329406977, 'eval_loss@deu.rst.pcc': 2.7487809658050537, 'eval_runtime': 3.2354, 'eval_samples_per_second': 74.489, 'eval_steps_per_second': 2.473, 'epoch': 11.0}
{'train@deu.rst.pcc_loss': 2.653836727142334, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.23752310536044363, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09771699329152489, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.1277358757037041, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.1456568144717094, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.653836488723755, 'train@deu.rst.pcc_runtime': 26.4777, 'train@deu.rst.pcc_samples_per_second': 81.729, 'train@deu.rst.pcc_steps_per_second': 2.568, 'epoch': 12.0}
{'loss': 2.688, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.7462716102600098, 'eval_accuracy@deu.rst.pcc': 0.21991701244813278, 'eval_f1@deu.rst.pcc': 0.09623689594265318, 'eval_precision@deu.rst.pcc': 0.08815317864553858, 'eval_recall@deu.rst.pcc': 0.15293302149919796, 'eval_loss@deu.rst.pcc': 2.746272087097168, 'eval_runtime': 3.2204, 'eval_samples_per_second': 74.836, 'eval_steps_per_second': 2.484, 'epoch': 12.0}
{'train_runtime': 1018.1136, 'train_samples_per_second': 25.506, 'train_steps_per_second': 0.801, 'train_loss': 2.8367593615662816, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.6298
  train_runtime            = 0:03:34.82
  train_samples_per_second =     24.522
  train_steps_per_second   =      0.782

Epoch 00004: reducing learning rate of group 0 to 1.8000e-05.
Epoch 00007: reducing learning rate of group 0 to 1.6200e-05.
Epoch 00010: reducing learning rate of group 0 to 1.4580e-05.
Epoch 00013: reducing learning rate of group 0 to 1.3122e-05.
Epoch 00016: reducing learning rate of group 0 to 1.1810e-05.
Epoch 00019: reducing learning rate of group 0 to 1.0629e-05.
Epoch 00022: reducing learning rate of group 0 to 9.5659e-06.
Epoch 00025: reducing learning rate of group 0 to 8.6093e-06.
Epoch 00028: reducing learning rate of group 0 to 7.7484e-06.
Epoch 00031: reducing learning rate of group 0 to 6.9736e-06.
Epoch 00034: reducing learning rate of group 0 to 6.2762e-06.
Epoch 00037: reducing learning rate of group 0 to 5.6486e-06.
Epoch 00040: reducing learning rate of group 0 to 5.0837e-06.
Epoch 00043: reducing learning rate of group 0 to 4.5754e-06.
Epoch 00046: reducing learning rate of group 0 to 4.1178e-06.
Epoch 00049: reducing learning rate of group 0 to 3.7060e-06.
Epoch 00052: reducing learning rate of group 0 to 3.3354e-06.
Epoch 00055: reducing learning rate of group 0 to 3.0019e-06.
Epoch 00058: reducing learning rate of group 0 to 2.7017e-06.
Epoch 00061: reducing learning rate of group 0 to 2.4315e-06.
Epoch 00064: reducing learning rate of group 0 to 2.1884e-06.
Epoch 00067: reducing learning rate of group 0 to 1.9695e-06.
Epoch 00070: reducing learning rate of group 0 to 1.7726e-06.
Epoch 00073: reducing learning rate of group 0 to 1.5953e-06.
Epoch 00076: reducing learning rate of group 0 to 1.4358e-06.
Epoch 00079: reducing learning rate of group 0 to 1.2922e-06.
Epoch 00082: reducing learning rate of group 0 to 1.1630e-06.
Epoch 00085: reducing learning rate of group 0 to 1.0467e-06.
Epoch 00088: reducing learning rate of group 0 to 9.4203e-07.
Epoch 00091: reducing learning rate of group 0 to 8.4782e-07.
Epoch 00094: reducing learning rate of group 0 to 7.6304e-07.
Epoch 00097: reducing learning rate of group 0 to 6.8674e-07.
Epoch 00100: reducing learning rate of group 0 to 6.1806e-07.
Epoch 00103: reducing learning rate of group 0 to 5.5626e-07.
Epoch 00106: reducing learning rate of group 0 to 5.0063e-07.
Epoch 1: train_loss: 3.0776 train_acc: 0.1437 | val_loss: 3.0420 val_acc: 0.1557
00:00:30.50
train_size: 541
Epoch 2: train_loss: 3.0248 train_acc: 0.1516 | val_loss: 3.0185 val_acc: 0.1434
00:00:29.87
train_size: 541
Epoch 3: train_loss: 2.9993 train_acc: 0.1516 | val_loss: 2.9947 val_acc: 0.1434
00:00:29.95
train_size: 541
Epoch 4: train_loss: 2.9788 train_acc: 0.1516 | val_loss: 2.9770 val_acc: 0.1434
00:00:29.84
train_size: 541
Epoch 5: train_loss: 2.9581 train_acc: 0.1516 | val_loss: 2.9700 val_acc: 0.1434
00:00:30.18
train_size: 541
Epoch 6: train_loss: 2.9398 train_acc: 0.1516 | val_loss: 2.9439 val_acc: 0.1434
00:00:29.71
train_size: 541
Epoch 7: train_loss: 2.9241 train_acc: 0.1516 | val_loss: 2.9294 val_acc: 0.1434
00:00:29.80
train_size: 541
Epoch 8: train_loss: 2.9068 train_acc: 0.1516 | val_loss: 2.9160 val_acc: 0.1434
00:00:29.81
train_size: 541
Epoch 9: train_loss: 2.8907 train_acc: 0.1516 | val_loss: 2.9094 val_acc: 0.1434
00:00:29.90
train_size: 541
Epoch 10: train_loss: 2.8746 train_acc: 0.1516 | val_loss: 2.8837 val_acc: 0.1434
00:00:29.98
train_size: 541
Epoch 11: train_loss: 2.8582 train_acc: 0.1525 | val_loss: 2.8608 val_acc: 0.1557
00:00:29.84
train_size: 541
Epoch 12: train_loss: 2.8432 train_acc: 0.1557 | val_loss: 2.8574 val_acc: 0.1557
00:00:30.07
train_size: 541
Epoch 13: train_loss: 2.8281 train_acc: 0.1691 | val_loss: 2.8386 val_acc: 0.1598
00:00:29.90
train_size: 541
Epoch 14: train_loss: 2.8129 train_acc: 0.1811 | val_loss: 2.8404 val_acc: 0.1598
00:00:29.85
train_size: 541
Epoch 15: train_loss: 2.7979 train_acc: 0.1844 | val_loss: 2.8181 val_acc: 0.1680
00:00:29.88
train_size: 541
Epoch 16: train_loss: 2.7826 train_acc: 0.1922 | val_loss: 2.8066 val_acc: 0.1844
00:00:29.98
train_size: 541
Epoch 17: train_loss: 2.7680 train_acc: 0.1992 | val_loss: 2.7915 val_acc: 0.1844
00:00:29.98
train_size: 541
Epoch 18: train_loss: 2.7536 train_acc: 0.2052 | val_loss: 2.7740 val_acc: 0.2008
00:00:29.91
train_size: 541
Epoch 19: train_loss: 2.7378 train_acc: 0.2089 | val_loss: 2.7634 val_acc: 0.2049
00:00:29.55
train_size: 541
Epoch 20: train_loss: 2.7227 train_acc: 0.2135 | val_loss: 2.7562 val_acc: 0.1926
00:00:29.55
train_size: 541
Epoch 21: train_loss: 2.7076 train_acc: 0.2149 | val_loss: 2.7622 val_acc: 0.1967
00:00:29.67
train_size: 541
Epoch 22: train_loss: 2.6931 train_acc: 0.2181 | val_loss: 2.7273 val_acc: 0.2090
00:00:29.58
train_size: 541
Epoch 23: train_loss: 2.6753 train_acc: 0.2209 | val_loss: 2.7338 val_acc: 0.1967
00:00:29.71
train_size: 541
Epoch 24: train_loss: 2.6580 train_acc: 0.2181 | val_loss: 2.7101 val_acc: 0.2090
00:00:29.38
train_size: 541
Epoch 25: train_loss: 2.6415 train_acc: 0.2255 | val_loss: 2.7045 val_acc: 0.1967
00:00:29.52
train_size: 541
Epoch 26: train_loss: 2.6277 train_acc: 0.2218 | val_loss: 2.6980 val_acc: 0.1967
00:00:29.90
train_size: 541
Epoch 27: train_loss: 2.6088 train_acc: 0.2274 | val_loss: 2.6986 val_acc: 0.1926
00:00:31.12
train_size: 541
Epoch 28: train_loss: 2.5929 train_acc: 0.2324 | val_loss: 2.6802 val_acc: 0.2008
00:00:31.26
train_size: 541
Epoch 29: train_loss: 2.5769 train_acc: 0.2361 | val_loss: 2.6841 val_acc: 0.1967
00:00:31.34
train_size: 541
Epoch 30: train_loss: 2.5588 train_acc: 0.2375 | val_loss: 2.6590 val_acc: 0.2213
00:00:31.19
train_size: 541
Epoch 31: train_loss: 2.5448 train_acc: 0.2412 | val_loss: 2.6631 val_acc: 0.2295
00:00:31.32
train_size: 541
Epoch 32: train_loss: 2.5291 train_acc: 0.2449 | val_loss: 2.6453 val_acc: 0.2131
00:00:31.35
train_size: 541
Epoch 33: train_loss: 2.5126 train_acc: 0.2482 | val_loss: 2.6399 val_acc: 0.2254
00:00:31.36
train_size: 541
Epoch 34: train_loss: 2.5012 train_acc: 0.2435 | val_loss: 2.6273 val_acc: 0.2172
00:00:31.15
train_size: 541
Epoch 35: train_loss: 2.4814 train_acc: 0.2505 | val_loss: 2.6304 val_acc: 0.2295
00:00:31.24
train_size: 541
Epoch 36: train_loss: 2.4666 train_acc: 0.2579 | val_loss: 2.6324 val_acc: 0.2295
00:00:31.11
train_size: 541
Epoch 37: train_loss: 2.4532 train_acc: 0.2579 | val_loss: 2.6213 val_acc: 0.2295
00:00:31.07
train_size: 541
Epoch 38: train_loss: 2.4346 train_acc: 0.2620 | val_loss: 2.6412 val_acc: 0.2377
00:00:31.23
train_size: 541
Epoch 39: train_loss: 2.4229 train_acc: 0.2685 | val_loss: 2.5929 val_acc: 0.2418
00:00:31.17
train_size: 541
Epoch 40: train_loss: 2.4076 train_acc: 0.2726 | val_loss: 2.6005 val_acc: 0.2377
00:00:31.31
train_size: 541
Epoch 41: train_loss: 2.3922 train_acc: 0.2652 | val_loss: 2.5727 val_acc: 0.2295
00:00:31.49
train_size: 541
Epoch 42: train_loss: 2.3725 train_acc: 0.2750 | val_loss: 2.5747 val_acc: 0.2582
00:00:31.15
train_size: 541
Epoch 43: train_loss: 2.3602 train_acc: 0.2800 | val_loss: 2.5706 val_acc: 0.2418
00:00:31.06
train_size: 541
Epoch 44: train_loss: 2.3403 train_acc: 0.2897 | val_loss: 2.5606 val_acc: 0.2336
00:00:31.24
train_size: 541
Epoch 45: train_loss: 2.3333 train_acc: 0.2897 | val_loss: 2.6027 val_acc: 0.2336
00:00:31.70
train_size: 541
Epoch 46: train_loss: 2.3099 train_acc: 0.2962 | val_loss: 2.5912 val_acc: 0.2172
00:00:31.25
train_size: 541
Epoch 47: train_loss: 2.2994 train_acc: 0.3041 | val_loss: 2.5621 val_acc: 0.2377
00:00:31.29
train_size: 541
Epoch 48: train_loss: 2.2839 train_acc: 0.3008 | val_loss: 2.5944 val_acc: 0.2172
00:00:31.66
train_size: 541
Epoch 49: train_loss: 2.2602 train_acc: 0.3142 | val_loss: 2.5610 val_acc: 0.2254
00:00:37.61
train_size: 541
Epoch 50: train_loss: 2.2516 train_acc: 0.3212 | val_loss: 2.5579 val_acc: 0.2295
00:01:02.16
train_size: 541
Epoch 51: train_loss: 2.2359 train_acc: 0.3286 | val_loss: 2.5429 val_acc: 0.2377
00:01:02.13
train_size: 541
Epoch 52: train_loss: 2.2225 train_acc: 0.3295 | val_loss: 2.5320 val_acc: 0.2418
00:01:02.20
train_size: 541
Epoch 53: train_loss: 2.2089 train_acc: 0.3309 | val_loss: 2.5510 val_acc: 0.2459
00:01:02.20
train_size: 541


Test Loss: 2.646 |  Test Acc: 21.92%

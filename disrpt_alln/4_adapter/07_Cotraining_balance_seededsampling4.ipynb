{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seeding for comparing experiment in part 2\n",
    "import torch\n",
    "import json\n",
    "SEED = 2019\n",
    "SAMPLE_SEED = 2023\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('cuda:5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNLI Bert\n",
    "## Second Tutorial\n",
    "https://towardsdatascience.com/fine-tuning-pre-trained-transformer-models-for-sentence-entailment-d87caf9ec9db\n",
    "Check his Github code for complete notebook. I never referred to it. Medium was enough.\n",
    "BERT in keras-tf: https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define macros\n",
    "BERT_MODEL = 'bert-base-german-cased'\n",
    "batch_size = 4\n",
    "batches_per_epoch = None\n",
    "\n",
    "save_path_suffix = 'cotraining_baseline_de_en_allshuffle_de_labelset_balanceen2000_seededsampling_4_'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# custom reader needed to handle quotechars\n",
    "def read_df_custom(file):\n",
    "    header = 'doc     unit1_toks      unit2_toks      unit1_txt       unit2_txt       s1_toks s2_toks unit1_sent      unit2_sent      dir     nuc_children    sat_children    genre   u1_discontinuous        u2_discontinuous       u1_issent        u2_issent       u1_length       u2_length       length_ratio    u1_speaker      u2_speaker      same_speaker    u1_func u1_pos  u1_depdir       u2_func u2_pos  u2_depdir       doclen  u1_position      u2_position     percent_distance        distance        lex_overlap_words       lex_overlap_length      unit1_case      unit2_case      label'\n",
    "    extracted_columns = ['unit1_txt', 'unit1_sent', 'unit2_txt', 'unit2_sent', 'dir', 'label', 'distance', 'u1_depdir', 'u2_depdir', 'u2_func', 'u1_position', 'u2_position', 'sat_children', 'nuc_children', 'genre', 'unit1_case', 'unit2_case',\n",
    "                            'u1_discontinuous', 'u2_discontinuous', 'same_speaker', 'lex_overlap_length', 'u1_func']\n",
    "    header = header.split()\n",
    "    df = pd.DataFrame(columns=extracted_columns)\n",
    "    file = open(file, 'r')\n",
    "\n",
    "    rows = []\n",
    "    count = 0 \n",
    "    for line in file:\n",
    "        line = line[:-1].split('\\t')\n",
    "        count+=1\n",
    "        if count ==1: continue\n",
    "        row = {}\n",
    "        for column in extracted_columns:\n",
    "            index = header.index(column)\n",
    "            try:\n",
    "                row[column] = line[index]\n",
    "            except:\n",
    "                print(count, line)\n",
    "            row[column] = line[index]\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame.from_records(rows)])\n",
    "    return df\n",
    "\n",
    "train_df_en = read_df_custom('../../processed/translated/eng.rst.rstdt_train_enriched_translated.rels')\n",
    "test_df_en = read_df_custom('../../processed/translated/eng.rst.rstdt_test_enriched_translated.rels')\n",
    "val_df_en = read_df_custom('../../processed/translated/eng.rst.rstdt_dev_enriched_translated.rels')\n",
    "train_df_de = read_df_custom('../../processed/deu.rst.pcc_train_enriched.rels')\n",
    "test_df_de = read_df_custom('../../processed/deu.rst.pcc_test_enriched.rels')\n",
    "val_df_de = read_df_custom('../../processed/deu.rst.pcc_dev_enriched.rels')\n",
    "\n",
    "lang='deu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>aux_new_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>joint</th>\n",
       "      <td>201</td>\n",
       "      <td>538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elaboration</th>\n",
       "      <td>166</td>\n",
       "      <td>444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>background</th>\n",
       "      <td>131</td>\n",
       "      <td>351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>condition</th>\n",
       "      <td>99</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cause</th>\n",
       "      <td>92</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contrast</th>\n",
       "      <td>38</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             target  aux_new_count\n",
       "label                             \n",
       "joint           201            538\n",
       "elaboration     166            444\n",
       "background      131            351\n",
       "condition        99            265\n",
       "cause            92            246\n",
       "contrast         38            102\n",
       "summary           6             16"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def dist_scaling(df_target, df_aux, plt=False):\n",
    "    dist1 = df_target.groupby('label').count()['dir'].reset_index().rename(columns={\"dir\": \"target\"})\n",
    "    dist1[\"target\"] = dist1[\"target\"]\n",
    "    dist2 = df_aux.groupby('label').count()['dir'].reset_index().rename(columns={\"dir\": \"aux\"})\n",
    "    j = pd.merge(dist1,dist2,on='label')\n",
    "    j.sort_values(by=[\"target\"], inplace=True, ascending=False)\n",
    "    j.set_index('label', inplace=True)\n",
    "    j['ratios'] = j[\"aux\"]/j[\"target\"]\n",
    "    minimum_ratio = j['ratios'].min()\n",
    "    j['aux_new'] = j['target']*minimum_ratio\n",
    "    j['aux_new_count'] = j['aux_new'].round().astype(int)\n",
    "    j = j[['target', 'aux_new_count']]\n",
    "\n",
    "    if plt:\n",
    "        ax = j.plot.bar(rot=0, title=\"New Label distribution for Aux and Target datasets\")\n",
    "        ax.set_xlabel(\"Labels\")\n",
    "        ax.set_ylabel(\"Percentages\")\n",
    "    return j\n",
    "\n",
    "dist_scaling(train_df_de, train_df_en, plt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "\n",
    "data = pd.DataFrame({'cols1':[4, 5, 5, 4, 321, 32, 5, 40, 50, 60],\n",
    "                     'cols2':[45, 66, 6, 6, 1, 432, 3, 40, 50, 60],\n",
    "                     'label':['A', 'B', 'C', 'C', 'A', 'B', 'B', 'A', 'D', 'F']})\n",
    "\n",
    "freq = pd.DataFrame({'label':['A', 'B', 'C', 'E'],\n",
    "                     'nostoextract':[2, 2, 2, 30], })\n",
    "\n",
    "def bootstrap(data, freq):\n",
    "    data_labels = list(data['label'].unique())\n",
    "    #drop the labels in freq that dont overlap\n",
    "    freq_bak = freq.copy(deep=True)\n",
    "    freq = freq[freq['label'].isin(data_labels)]\n",
    "\n",
    "    #save the labels in data that dont overlap\n",
    "    freq_labels = list(freq['label'].unique())\n",
    "    extra_data = data[~data.label.isin(freq_labels)]\n",
    "\n",
    "    #drop the labels in data that dont overlap\n",
    "    data = data[data['label'].isin(freq_labels)]\n",
    "\n",
    "    #bootstrap!\n",
    "    freq = freq.set_index('label')\n",
    "\n",
    "    def sampleClass(classgroup):\n",
    "        cls = classgroup['label'].iloc[0]\n",
    "        nDesired = freq.nostoextract[cls]\n",
    "        nRows = len(classgroup)\n",
    "\n",
    "        nSamples = min(nRows, nDesired)\n",
    "        return classgroup.sample(nSamples, random_state=SAMPLE_SEED)\n",
    "\n",
    "    samples = data.groupby('label').apply(sampleClass)\n",
    "    samples.index = samples.index.get_level_values(1)\n",
    "\n",
    "    #add back the extra samples\n",
    "    samples = pd.concat([samples, extra_data])\n",
    "\n",
    "    return samples\n",
    "\n",
    "def get_distr(target_df, aux_df):\n",
    "    '''this function takes the target distribution and multiplies it by constant. aux_new_count denotes the target's scaled distribution that needs to be applies on aux_df'''\n",
    "    distr_combined = dist_scaling(target_df, aux_df)\n",
    "    distr_aux_new = distr_combined[['aux_new_count']]\n",
    "    distr_aux_new = distr_aux_new.rename(columns={\"aux_new_count\": \"nostoextract\"}).reset_index()\n",
    "    return distr_aux_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11430\n",
      "before\n",
      "label\n",
      "background      711\n",
      "cause           563\n",
      "condition       265\n",
      "contrast        875\n",
      "elaboration    6454\n",
      "evaluation      459\n",
      "joint          1749\n",
      "means           183\n",
      "summary         171\n",
      "Name: dir, dtype: int64\n",
      "after\n",
      "label\n",
      "background     351\n",
      "cause          246\n",
      "condition      265\n",
      "contrast       102\n",
      "elaboration    444\n",
      "evaluation     228\n",
      "joint          538\n",
      "means           21\n",
      "summary         16\n",
      "Name: dir, dtype: int64\n",
      "2211\n"
     ]
    }
   ],
   "source": [
    "def process_labels_and_merge_evaluation_for_german(df):\n",
    "    for delete_label in ['attribution', 'comparison', 'explanation', 'enablement', 'temporal', 'textual-organization', 'topic-change', 'topic-comment']:\n",
    "        df = df[df.label != delete_label]\n",
    "    df['label'] = df['label'].str.replace(\"manner-means\", \"means\")\n",
    "    df['label'] = df['label'].str.replace(\"evaluation-n\", \"evaluation\")\n",
    "    df['label'] = df['label'].str.replace(\"evaluation-s\", \"evaluation\")\n",
    "    return df\n",
    "\n",
    "def balance_dataset(df, distribution_df):\n",
    "    freq = get_distr(distribution_df, df)\n",
    "    print('before')\n",
    "    print(df.groupby('label').count()['dir'])\n",
    "    df = bootstrap(df, freq)\n",
    "    print('after')\n",
    "    print(df.groupby('label').count()['dir'])\n",
    "    return df\n",
    "\n",
    "train_df_de = process_labels_and_merge_evaluation_for_german(train_df_de)\n",
    "train_df_en = process_labels_and_merge_evaluation_for_german(train_df_en)\n",
    "test_df = process_labels_and_merge_evaluation_for_german(test_df_de)\n",
    "val_df = process_labels_and_merge_evaluation_for_german(val_df_de)\n",
    "\n",
    "print(len(train_df_en))\n",
    "train_df_en = balance_dataset(train_df_en, train_df_de)\n",
    "print(len(train_df_en))\n",
    "\n",
    "train_df = pd.concat([train_df_de, train_df_en])\n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12998"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['nuc_children'].astype(int).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4376 4375 1094\n"
     ]
    }
   ],
   "source": [
    "def get_batches_per_epoch(train_df, batch_size=4):\n",
    "    size = len(train_df)\n",
    "    if size%batch_size!=0:\n",
    "        return int(size/batch_size)+1\n",
    "    else:\n",
    "        return int(size/batch_size)\n",
    "\n",
    "batches_per_epoch = get_batches_per_epoch(train_df, batch_size)\n",
    "print(batches_per_epoch*batch_size, len(train_df), batches_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping any empty values\n",
    "train_df.dropna(inplace=True)\n",
    "val_df.dropna(inplace=True)\n",
    "test_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare a dataset handler class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unit1_txt</th>\n",
       "      <th>unit1_sent</th>\n",
       "      <th>unit2_txt</th>\n",
       "      <th>unit2_sent</th>\n",
       "      <th>dir</th>\n",
       "      <th>label</th>\n",
       "      <th>distance</th>\n",
       "      <th>u1_depdir</th>\n",
       "      <th>u2_depdir</th>\n",
       "      <th>u2_func</th>\n",
       "      <th>...</th>\n",
       "      <th>sat_children</th>\n",
       "      <th>nuc_children</th>\n",
       "      <th>genre</th>\n",
       "      <th>unit1_case</th>\n",
       "      <th>unit2_case</th>\n",
       "      <th>u1_discontinuous</th>\n",
       "      <th>u2_discontinuous</th>\n",
       "      <th>same_speaker</th>\n",
       "      <th>lex_overlap_length</th>\n",
       "      <th>u1_func</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Die Fußball-Kleinmacht vom Dnepr steht kurz da...</td>\n",
       "      <td>Die Fußball-Kleinmacht vom Dnepr steht kurz da...</td>\n",
       "      <td>Dabei haben die deutschen Nationalkicker einen...</td>\n",
       "      <td>Dabei haben die deutschen Nationalkicker einen...</td>\n",
       "      <td>1&gt;2</td>\n",
       "      <td>concession</td>\n",
       "      <td>1</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>root</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>news</td>\n",
       "      <td>cap_initial</td>\n",
       "      <td>cap_initial</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>root</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Die Lehniner Gemeinden machen nach außen hin -...</td>\n",
       "      <td>Punkt zwei : Die Lehniner Gemeinden machen nac...</td>\n",
       "      <td>Die Politik im dortigen Amtsausschuss läuft eb...</td>\n",
       "      <td>Die Politik im dortigen Amtsausschuss läuft eb...</td>\n",
       "      <td>1&lt;2</td>\n",
       "      <td>reason</td>\n",
       "      <td>1</td>\n",
       "      <td>LEFT</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>root</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>news</td>\n",
       "      <td>cap_initial</td>\n",
       "      <td>cap_initial</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>appos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Die Fed führte in der Tat 1,5 Milliarden Dolla...</td>\n",
       "      <td>Die Fed führte in der Tat 1,5 Milliarden Dolla...</td>\n",
       "      <td>Die zusätzliche Liquidität sollte dazu tendier...</td>\n",
       "      <td>Analysten antworteten, dass die zusätzliche Li...</td>\n",
       "      <td>1&lt;2</td>\n",
       "      <td>elaboration</td>\n",
       "      <td>2</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>LEFT</td>\n",
       "      <td>ccomp</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>news</td>\n",
       "      <td>cap_initial</td>\n",
       "      <td>other</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>root</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nein , mit dem Euro hat das alles nichts zu tu...</td>\n",
       "      <td>Nein , mit dem Euro hat das alles nichts zu tu...</td>\n",
       "      <td>Es gab die ersten blanken Münzen gestern eben ...</td>\n",
       "      <td>Es gab die ersten blanken Münzen gestern eben ...</td>\n",
       "      <td>1&gt;2</td>\n",
       "      <td>circumstance</td>\n",
       "      <td>1</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>root</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>news</td>\n",
       "      <td>cap_initial</td>\n",
       "      <td>cap_initial</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>root</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Die finnischen Staatsanleihen werden niedriger...</td>\n",
       "      <td>Japanische Staatsanleihen endeten, nachdem der...</td>\n",
       "      <td>, nach dem der Dollar gegenüber dem Yen leicht...</td>\n",
       "      <td>Japanische Staatsanleihen endeten, nachdem der...</td>\n",
       "      <td>1&lt;2</td>\n",
       "      <td>background</td>\n",
       "      <td>1</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>LEFT</td>\n",
       "      <td>obl</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>news</td>\n",
       "      <td>cap_initial</td>\n",
       "      <td>other</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>root</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           unit1_txt  \\\n",
       "0  Die Fußball-Kleinmacht vom Dnepr steht kurz da...   \n",
       "1  Die Lehniner Gemeinden machen nach außen hin -...   \n",
       "2  Die Fed führte in der Tat 1,5 Milliarden Dolla...   \n",
       "3  Nein , mit dem Euro hat das alles nichts zu tu...   \n",
       "4  Die finnischen Staatsanleihen werden niedriger...   \n",
       "\n",
       "                                          unit1_sent  \\\n",
       "0  Die Fußball-Kleinmacht vom Dnepr steht kurz da...   \n",
       "1  Punkt zwei : Die Lehniner Gemeinden machen nac...   \n",
       "2  Die Fed führte in der Tat 1,5 Milliarden Dolla...   \n",
       "3  Nein , mit dem Euro hat das alles nichts zu tu...   \n",
       "4  Japanische Staatsanleihen endeten, nachdem der...   \n",
       "\n",
       "                                           unit2_txt  \\\n",
       "0  Dabei haben die deutschen Nationalkicker einen...   \n",
       "1  Die Politik im dortigen Amtsausschuss läuft eb...   \n",
       "2  Die zusätzliche Liquidität sollte dazu tendier...   \n",
       "3  Es gab die ersten blanken Münzen gestern eben ...   \n",
       "4  , nach dem der Dollar gegenüber dem Yen leicht...   \n",
       "\n",
       "                                          unit2_sent  dir         label  \\\n",
       "0  Dabei haben die deutschen Nationalkicker einen...  1>2    concession   \n",
       "1  Die Politik im dortigen Amtsausschuss läuft eb...  1<2        reason   \n",
       "2  Analysten antworteten, dass die zusätzliche Li...  1<2   elaboration   \n",
       "3  Es gab die ersten blanken Münzen gestern eben ...  1>2  circumstance   \n",
       "4  Japanische Staatsanleihen endeten, nachdem der...  1<2    background   \n",
       "\n",
       "  distance u1_depdir u2_depdir u2_func  ... sat_children nuc_children genre  \\\n",
       "0        1      ROOT      ROOT    root  ...            0            2  news   \n",
       "1        1      LEFT      ROOT    root  ...            1            2  news   \n",
       "2        2      ROOT      LEFT   ccomp  ...            1            1  news   \n",
       "3        1      ROOT      ROOT    root  ...            1            2  news   \n",
       "4        1      ROOT      LEFT     obl  ...            1            4  news   \n",
       "\n",
       "    unit1_case   unit2_case u1_discontinuous u2_discontinuous same_speaker  \\\n",
       "0  cap_initial  cap_initial            False            False         True   \n",
       "1  cap_initial  cap_initial            False            False         True   \n",
       "2  cap_initial        other            False            False         True   \n",
       "3  cap_initial  cap_initial            False            False         True   \n",
       "4  cap_initial        other            False            False         True   \n",
       "\n",
       "  lex_overlap_length u1_func  \n",
       "0                  0    root  \n",
       "1                  1   appos  \n",
       "2                  0    root  \n",
       "3                  0    root  \n",
       "4                  0    root  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['unit1_txt', 'unit1_sent', 'unit2_txt', 'unit2_sent', 'dir', 'label',\n",
       "       'distance', 'u1_depdir', 'u2_depdir', 'u2_func', 'u1_position',\n",
       "       'u2_position', 'sat_children', 'nuc_children', 'genre', 'unit1_case',\n",
       "       'unit2_case', 'u1_discontinuous', 'u2_discontinuous', 'same_speaker',\n",
       "       'lex_overlap_length', 'u1_func'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-24 10:46:31.623132: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-24 10:46:31.845986: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda-10.1/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-10.2/targets/x86_64-linux/lib/:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/home/VD/kaveri/anaconda3/envs/py310/lib/\n",
      "2023-02-24 10:46:31.846006: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-24 10:46:31.886873: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-02-24 10:46:33.027240: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda-10.1/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-10.2/targets/x86_64-linux/lib/:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/home/VD/kaveri/anaconda3/envs/py310/lib/\n",
      "2023-02-24 10:46:33.027331: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda-10.1/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-10.2/targets/x86_64-linux/lib/:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/home/VD/kaveri/anaconda3/envs/py310/lib/\n",
      "2023-02-24 10:46:33.027338: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing.sharedctypes import Value\n",
    "import torch\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, ConcatDataset\n",
    "from sys import path\n",
    "path.append('/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/allennlp/data/data_loaders/')\n",
    "from allennlp.data import allennlp_collate, Vocabulary\n",
    "from features_custom_original import get_vocab_feature_name\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertTokenizer, BertTokenizer\n",
    "\n",
    "class MNLIDataBert(Dataset):\n",
    "\n",
    "  def __init__(self, train_df, val_df, test_df):\n",
    "    self.lang = lang\n",
    "    self.num_labels = set()\n",
    "    self.train_df = train_df\n",
    "    self.val_df = val_df\n",
    "    self.test_df = test_df\n",
    "\n",
    "    self.tokenizer = BertTokenizer.from_pretrained(BERT_MODEL, do_lower_case=True) # Using a pre-trained BERT tokenizer to encode sentences\n",
    "    self.train_data = None\n",
    "    self.val_data = None\n",
    "    self.test_data = None\n",
    "    self.train_idx = None\n",
    "    self.val_idx = None\n",
    "    self.test_idx = None\n",
    "    self.vocab = Vocabulary(counter=None, max_vocab_size=100000)\n",
    "    self.init_data()\n",
    "\n",
    "  def init_data(self):\n",
    "    self.get_label_mapping()\n",
    "    self.init_feature_list()\n",
    "    self.init_feature_mappings_and_bins()\n",
    "    self.apply_bins()\n",
    "    self.calculate_unique_values()\n",
    "    self.train_data, self.train_idx = self.load_data(self.train_df)\n",
    "    self.val_data, self.val_idx = self.load_data(self.val_df)\n",
    "    self.test_data, self.test_idx = self.load_data(self.test_df)\n",
    "    \n",
    "\n",
    "  def combine_unique_column_values_to_dict(self, column_name):\n",
    "    ini_set = set([*self.train_df[column_name].unique(), *self.val_df[column_name].unique()])\n",
    "    res = dict.fromkeys(ini_set, 0)\n",
    "    return res\n",
    "\n",
    "  def get_label_mapping(self):\n",
    "    labels = {}\n",
    "    labels_list = list(set(list(self.train_df['label'].unique()) + list(self.test_df['label'].unique()) + list(self.val_df['label'].unique())))\n",
    "    for i in range(len(labels_list)):\n",
    "        labels[labels_list[i]] = i\n",
    "    self.label_dict = labels\n",
    "    # needed later for classification report object to generate precision and recall on test dataset\n",
    "    self.rev_label_dict = {self.label_dict[k]:k for k in self.label_dict.keys()} \n",
    "\n",
    "  def init_feature_mappings_and_bins(self):\n",
    "    self.feature_maps = { 'genre': self.combine_unique_column_values_to_dict('genre'),\n",
    "                          'unit1_case': self.combine_unique_column_values_to_dict('unit1_case'),\n",
    "                          'unit2_case': self.combine_unique_column_values_to_dict('unit2_case'),\n",
    "                          'u1_func': self.combine_unique_column_values_to_dict('u1_func'),\n",
    "                          'u2_func': self.combine_unique_column_values_to_dict('u2_func') }\n",
    "\n",
    "    self.bins = {\n",
    "      'distance': [[-1e9, -8], [-8, -2], [-2, 0], [0, 2], [2, 8], [8, 1e9]],\n",
    "      'u1_position': [[0.0, 0.1], [0.1, 0.2], [0.2, 0.3], [0.3, 0.4], [0.4, 0.5], [0.5, 0.6], [0.6, 0.7], [0.7, 0.8], [0.8, 0.9], [0.9, 1.0], [1.0, 1e9]],\n",
    "      'u2_position': [[0.0, 0.1], [0.1, 0.2], [0.2, 0.3], [0.3, 0.4], [0.4, 0.5], [0.5, 0.6], [0.6, 0.7], [0.7, 0.8], [0.8, 0.9], [0.9, 1.0], [1.0, 1e9]],\n",
    "      'lex_overlap_length': [[0, 2], [2, 7], [7, 1e9]]\n",
    "    }   \n",
    "\n",
    "  def add_directionality(self, premise, hypothesis, dir):\n",
    "    if dir == \"1<2\":\n",
    "        hypothesis = '< ' + hypothesis + ' {'\n",
    "    else:\n",
    "        premise = '} ' + premise + ' >'\n",
    "    return premise, hypothesis\n",
    "\n",
    "  def init_feature_list(self):\n",
    "    if self.lang=='nld':\n",
    "      self.feature_list = ['distance', 'u1_depdir', 'sat_children', 'genre', 'u1_position']\n",
    "    elif self.lang=='deu':\n",
    "      self.feature_list = ['distance', 'u1_depdir', 'u2_depdir', 'u2_func', 'u1_position', 'u2_position', 'sat_children', 'nuc_children']\n",
    "    elif self.lang=='eng.rst.gum':\n",
    "      self.feature_list = ['distance', 'same_speaker', 'u2_func', 'u2_depdir', 'unit1_case', 'unit2_case', 'nuc_children',\n",
    "                      'sat_children', 'genre', 'lex_overlap_length', 'u2_discontinuous', 'u1_discontinuous', 'u1_position', 'u2_position']\n",
    "    elif self.lang=='fas':\n",
    "      self.feature_list = ['distance', 'nuc_children', 'sat_children', 'u2_discontinuous', 'genre']\n",
    "    elif self.lang=='spa.rst.sctb':\n",
    "      self.feature_list = ['distance', 'u1_position', 'sat_children']\n",
    "    elif self.lang=='zho.rst.sctb':\n",
    "      self.feature_list = ['sat_children', 'nuc_children', 'genre', 'u2_discontinuous', 'u1_discontinuous', 'u1_depdir', 'u1_func']\n",
    "    else: \n",
    "      raise ValueError()\n",
    "\n",
    "  def get_mapping_from_dictionary(self, column_name, dict_val):\n",
    "    return self.feature_maps[column_name][dict_val]\n",
    "\n",
    "  def get_allen_features_list(self, features, feature_name):\n",
    "    if feature_name in ['distance', 'u1_depdir', 'u2_depdir', 'u1_func', 'u2_func', \n",
    "    'u1_position', 'u2_position', 'genre', 'same_speaker', 'unit1_case', 'unit2_case',\n",
    "    'lex_overlap_length', 'u2_discontinuous', 'u1_discontinuous', 'dir']: feature_value = self.apply_vocab(features[feature_name], feature_name) #for categorical values\n",
    "    elif feature_name in ['sat_children', 'nuc_children']: feature_value = float(features[feature_name]) #for identiy values\n",
    "    else: \n",
    "      print(feature_name)\n",
    "      raise ValueError()\n",
    "    return feature_value\n",
    "\n",
    "  def transform_feature(self, features):\n",
    "    assert len(features)==17\n",
    "    #after applying the vocab. we need to pass them as int\n",
    "    return {feature_name: torch.tensor(int(self.get_allen_features_list(features, feature_name))).to(device) for feature_name in self.feature_list+['dir']}\n",
    "\n",
    "  def calculate_unique_values(self):\n",
    "    for feature_name in self.feature_list+['dir']:\n",
    "      vocab_feature_name = get_vocab_feature_name(feature_name)\n",
    "      self.vocab.add_tokens_to_namespace(train_df[feature_name].apply(lambda x: str(x)), namespace=vocab_feature_name)\n",
    "      self.vocab.add_tokens_to_namespace(val_df[feature_name].apply(lambda x: str(x)), namespace=vocab_feature_name)\n",
    "\n",
    "  def apply_bins(self):\n",
    "    for df in [self.train_df, self.test_df, self.val_df]:\n",
    "      for feature_name in self.bins.keys():\n",
    "        if feature_name=='u2_func':\n",
    "          print(df[feature_name].unique())\n",
    "          raise ValueError()\n",
    "        df[feature_name] = df[feature_name].apply(lambda x: self.get_mapping_from_bin(feature_name, float(x)))\n",
    "\n",
    "  def get_mapping_from_bin(self, column_name, dict_val):\n",
    "    bins = self.bins[column_name]\n",
    "    for b,i in zip(bins, range(len(bins))):\n",
    "      left = b[0]\n",
    "      right = b[1]\n",
    "      if left<=dict_val and right>=dict_val: return i\n",
    "\n",
    "  def apply_vocab(self, feature_value, feature_name):\n",
    "    return self.vocab.get_token_index(str(feature_value), namespace=get_vocab_feature_name(feature_name))\n",
    "\n",
    "  def set_labels(self):\n",
    "    self.num_labels = len(self.num_labels)\n",
    "    \n",
    "  def load_data(self, df):\n",
    "    MAX_LEN = 512 \n",
    "    token_ids = []\n",
    "    mask_ids = []\n",
    "    # seg_ids = []\n",
    "    y = []\n",
    "    feats = []\n",
    "    idx = []\n",
    "    idx_map = {}\n",
    "\n",
    "    self.num_labels.update(df['label'].unique())\n",
    "\n",
    "    count=0\n",
    "    for row in df.iterrows():\n",
    "      row = row[1]\n",
    "      premise = row['unit1_txt']\n",
    "      hypothesis = row['unit2_txt']\n",
    "      label = row['label']\n",
    "      dir = row['dir']\n",
    "\n",
    "      features = {'distance': row['distance'],\n",
    "                'u1_depdir': row['u1_depdir'],\n",
    "                'u2_depdir': row['u2_depdir'],\n",
    "                'u1_func': row['u1_func'],\n",
    "                'u2_func': row['u2_func'],\n",
    "                'u1_position': row['u1_position'],\n",
    "                'u2_position': row['u2_position'],\n",
    "                'sat_children': row['sat_children'],\n",
    "                'nuc_children': row['nuc_children'],\n",
    "                'genre': row['genre'],\n",
    "                'unit1_case': row['unit1_case'],\n",
    "                'unit2_case': row['unit2_case'],\n",
    "                'u1_discontinuous': row['u1_discontinuous'],\n",
    "                'u2_discontinuous': row['u2_discontinuous'],\n",
    "                'same_speaker': row['same_speaker'],\n",
    "                'lex_overlap_length': row['lex_overlap_length'],\n",
    "                'dir': row['dir']}\n",
    "\n",
    "      premise, hypothesis = self.add_directionality(premise, hypothesis, dir)\n",
    "      encoded = self.tokenizer.encode_plus(premise, hypothesis, add_special_tokens = True, max_length=MAX_LEN, truncation=True, padding=False) #padding='max_length'\n",
    "      pair_token_ids = torch.tensor(encoded['input_ids'])\n",
    "\n",
    "      # segment_ids = torch.tensor(encoded['token_type_ids'])\n",
    "      attention_mask_ids = torch.tensor(encoded['attention_mask'])\n",
    "      assert len(pair_token_ids)==len(attention_mask_ids)\n",
    "\n",
    "      features = self.transform_feature(features)\n",
    "\n",
    "      token_ids.append(pair_token_ids)\n",
    "      # seg_ids.append(segment_ids)\n",
    "      mask_ids.append(attention_mask_ids)\n",
    "      y.append(self.label_dict[label])\n",
    "      feats.append(features)\n",
    "      \n",
    "      idx_map[count] = [premise, hypothesis]\n",
    "      idx.append(count)\n",
    "      count+=1\n",
    "      \n",
    "    token_ids = pad_sequence(token_ids, batch_first=True)\n",
    "    mask_ids = pad_sequence(mask_ids, batch_first=True)\n",
    "    # seg_ids = pad_sequence(seg_ids, batch_first=True)\n",
    "    y = torch.tensor(y)\n",
    "    idx = torch.tensor(idx)\n",
    "\n",
    "    class featureDataset(Dataset):\n",
    "      def __init__(self, token_ids, mask_ids, feats, y, idx):\n",
    "          self.token_ids = token_ids\n",
    "          self.mask_ids = mask_ids\n",
    "          # self.seg_ids = seg_ids\n",
    "          self.feats = feats\n",
    "          self.y = y\n",
    "          self.idx = idx\n",
    "\n",
    "      def __len__(self):\n",
    "          return len(self.feats)\n",
    "\n",
    "      def __getitem__(self, idx):\n",
    "          return self.token_ids[idx], self.mask_ids[idx], self.feats[idx], self.y[idx], self.idx[idx]\n",
    "          # return self.token_ids[idx], self.mask_ids[idx], self.seg_ids[idx], self.feats[idx], self.y[idx], self.idx[idx]\n",
    "\n",
    "    # dataset = featureDataset(token_ids, mask_ids, seg_ids, feats, y, idx)\n",
    "    dataset = featureDataset(token_ids, mask_ids, feats, y, idx)\n",
    "    return dataset, idx_map\n",
    "\n",
    "  def get_data_loaders(self, batch_size=4, batches_per_epoch=402, shuffle=True): #1609 samples / 64:25=1600 / 402:4=1608\n",
    "    self.set_labels()\n",
    "    train_loader_torch = DataLoader(\n",
    "      self.train_data,\n",
    "      shuffle=shuffle,\n",
    "      batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    val_loader_torch = DataLoader(\n",
    "      self.val_data,\n",
    "      shuffle=shuffle,\n",
    "      batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    test_loader_torch = DataLoader(\n",
    "      self.test_data,\n",
    "      shuffle=False,\n",
    "      batch_size=batch_size,\n",
    "    )\n",
    "    \n",
    "    train_loader = LoaderWrapper(train_loader_torch, n_step=batches_per_epoch)\n",
    "    val_loader = LoaderWrapper(val_loader_torch, n_step=batches_per_epoch)\n",
    "    test_loader = LoaderWrapper(test_loader_torch, n_step=batches_per_epoch)\n",
    "\n",
    "    return train_loader, val_loader_torch, test_loader_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoaderWrapper:\n",
    "    def __init__(self, loader, n_step):\n",
    "        self.step = n_step\n",
    "        self.idx = 0\n",
    "        self.iter_loader = iter(loader)\n",
    "        self.loader = loader\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.step\n",
    "\n",
    "    def __next__(self):\n",
    "        # if reached number of steps desired, stop\n",
    "        if self.idx == self.step:\n",
    "            self.idx = 0\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            self.idx += 1\n",
    "        # while True\n",
    "        try:\n",
    "            return next(self.iter_loader)\n",
    "        except StopIteration:\n",
    "            # reinstate iter_loader, then continue\n",
    "            self.iter_loader = iter(self.loader)\n",
    "            return next(self.iter_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mnliloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_dataset = MNLIDataBert(train_df, val_df, test_df)\n",
    "mnli_dataset_en = MNLIDataBert(train_df_en, val_df_en, test_df_en) #to apply bins because df passed by reference\n",
    "\n",
    "train_loader, val_loader, test_loader = mnli_dataset.get_data_loaders(batch_size=batch_size, batches_per_epoch=batches_per_epoch) #64X250\n",
    "label_dict = mnli_dataset.label_dict # required by custom func to calculate accuracy, bert model\n",
    "rev_label_dict = mnli_dataset.rev_label_dict # required by custom func to calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '3': 2, '4': 3, '5': 4}\n",
      "distance :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '3': 2, '4': 3, '5': 4}\n",
      "u1_depdir :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, 'ROOT': 2, 'LEFT': 3, 'RIGHT': 4}\n",
      "u1_depdir :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, 'ROOT': 2, 'LEFT': 3, 'RIGHT': 4}\n",
      "u2_depdir :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, 'ROOT': 2, 'LEFT': 3, 'RIGHT': 4}\n",
      "u2_depdir :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, 'ROOT': 2, 'LEFT': 3, 'RIGHT': 4}\n",
      "u2_func :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, 'root': 2, 'ccomp': 3, 'obl': 4, 'acl': 5, 'nmod': 6, 'appos': 7, 'conj': 8, 'advcl': 9, 'xcomp': 10, 'dep': 11, 'acl:relcl': 12, 'punct': 13, 'parataxis': 14, 'cc': 15, 'compound': 16, 'flat': 17, 'aux': 18, 'list': 19, 'nsubj': 20, 'orphan': 21, 'obj': 22, 'amod': 23, 'csubj:pass': 24, 'advmod': 25, 'nsubj:pass': 26, 'det': 27, 'csubj': 28, 'mark': 29, 'iobj': 30, 'nmod:npmod': 31, 'case': 32}\n",
      "u2_func :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, 'root': 2, 'ccomp': 3, 'obl': 4, 'acl': 5, 'nmod': 6, 'appos': 7, 'conj': 8, 'advcl': 9, 'xcomp': 10, 'dep': 11, 'acl:relcl': 12, 'punct': 13, 'parataxis': 14, 'cc': 15, 'compound': 16, 'flat': 17, 'aux': 18, 'list': 19, 'nsubj': 20, 'orphan': 21, 'obj': 22, 'amod': 23, 'csubj:pass': 24, 'advmod': 25, 'nsubj:pass': 26, 'det': 27, 'csubj': 28, 'mark': 29, 'iobj': 30, 'nmod:npmod': 31, 'case': 32, 'nummod': 33, 'nmod:poss': 34}\n",
      "u1_position :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '6': 2, '5': 3, '2': 4, '9': 5, '1': 6, '8': 7, '0': 8, '4': 9, '7': 10, '3': 11}\n",
      "u1_position :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '6': 2, '5': 3, '2': 4, '9': 5, '1': 6, '8': 7, '0': 8, '4': 9, '7': 10, '3': 11}\n",
      "u2_position :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '8': 2, '6': 3, '2': 4, '9': 5, '1': 6, '3': 7, '0': 8, '5': 9, '7': 10, '4': 11}\n",
      "u2_position :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '8': 2, '6': 3, '2': 4, '9': 5, '1': 6, '3': 7, '0': 8, '5': 9, '7': 10, '4': 11}\n",
      "sat_children :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '0': 2, '1': 3, '2': 4, '3': 5, '5': 6, '4': 7, '6': 8, '7': 9, '16': 10}\n",
      "sat_children :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '0': 2, '1': 3, '2': 4, '3': 5, '5': 6, '4': 7, '6': 8, '7': 9, '16': 10, '14': 11}\n",
      "nuc_children :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '2': 2, '1': 3, '4': 4, '3': 5, '5': 6, '6': 7, '9': 8, '8': 9, '7': 10, '10': 11, '16': 12, '11': 13, '12': 14, '15': 15, '21': 16, '13': 17}\n",
      "nuc_children :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '2': 2, '1': 3, '4': 4, '3': 5, '5': 6, '6': 7, '9': 8, '8': 9, '7': 10, '10': 11, '16': 12, '11': 13, '12': 14, '15': 15, '21': 16, '13': 17, '14': 18}\n",
      "dir :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '1>2': 2, '1<2': 3}\n"
     ]
    }
   ],
   "source": [
    "for feature in mnli_dataset.feature_list:\n",
    "    vocab_feature_name = get_vocab_feature_name(feature)\n",
    "    print(feature, ': ', mnli_dataset.vocab.get_token_to_index_vocabulary(vocab_feature_name))\n",
    "    mnli_dataset.vocab.add_tokens_to_namespace(val_df_en[feature].apply(lambda x: str(x)), namespace=vocab_feature_name)\n",
    "    print(feature, ': ', mnli_dataset.vocab.get_token_to_index_vocabulary(vocab_feature_name))\n",
    "print('dir', ': ', mnli_dataset.vocab.get_token_to_index_vocabulary('dir'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (pair_token_ids, mask_ids, feat, y, idx) in enumerate(train_loader):\n",
    "    assert len(feat)==len(mnli_dataset.feature_list)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from torch import optim\n",
    "import os\n",
    "path.append(os.path.join(os.getcwd(), '../utils/'))\n",
    "from CategoricalAccuracy import CategoricalAccuracy as CA\n",
    "import numpy as np\n",
    "\n",
    "ca = CA()\n",
    "\n",
    "x = torch.tensor(np.array([[[1,0,0], [1,0,0], [1,0,0]]]))\n",
    "y1 = torch.tensor(np.array([[0], [1], [1]]))\n",
    "y2 = torch.tensor(np.array([[0], [0], [0]]))\n",
    "\n",
    "ca(x,y1)\n",
    "print(ca.get_metric(reset=True))\n",
    "ca(x,y2)\n",
    "print(ca.get_metric(reset=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define evaulation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to evaluate model for train and test. And also use classification report for testing\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# helper function to calculate the batch accuracy\n",
    "def multi_acc(y_pred, y_test, allennlp=False):\n",
    "  if allennlp==False:\n",
    "    acc = (torch.log_softmax(y_pred, dim=1).argmax(dim=1) == y_test).sum().float() / float(y_test.size(0))\n",
    "    return acc\n",
    "\n",
    "# freeze model weights and measure validation / test \n",
    "def evaluate_accuracy(model, optimizer, data_loader, rev_label_dict, label_dict, is_training=True):\n",
    "  model.eval()\n",
    "  total_val_acc  = 0\n",
    "  total_val_loss = 0\n",
    "  \n",
    "  #for classification report\n",
    "  y_true = []\n",
    "  y_pred = []\n",
    "  idx_list = []\n",
    "  premise_list = []\n",
    "  hypo_list = []\n",
    "  idx_map = mnli_dataset.val_idx if is_training else mnli_dataset.test_idx\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for batch_idx, (pair_token_ids, mask_ids, feat, y, idx) in enumerate(data_loader):      \n",
    "      optimizer.zero_grad()\n",
    "      pair_token_ids = pair_token_ids.to(device)\n",
    "      mask_ids = mask_ids.to(device)\n",
    "      # seg_ids = seg_ids.to(device)\n",
    "      labels = y.to(device)\n",
    "      # feat = feat.to(device)\n",
    "      \n",
    "      outputs = model(pair_token_ids, \n",
    "                            # token_type_ids=seg_ids, \n",
    "                            attention_mask=mask_ids, \n",
    "                            feat=feat)\n",
    "      criterion = nn.CrossEntropyLoss()\n",
    "      loss = criterion(outputs, labels)\n",
    "      acc = multi_acc(outputs, labels)\n",
    "\n",
    "      total_val_loss += loss.item()\n",
    "      total_val_acc  += acc.item()\n",
    "\n",
    "      # log predictions for classification report\n",
    "      argmax_predictions = torch.argmax(outputs,dim=1).tolist()\n",
    "      labels_list = labels.tolist()\n",
    "      assert(len(labels_list)==len(argmax_predictions))\n",
    "      for p in argmax_predictions: y_pred.append(rev_label_dict[int(p)])\n",
    "      for l in labels_list: y_true.append(rev_label_dict[l])\n",
    "      for i in idx.tolist():\n",
    "        idx_list.append(i)\n",
    "        premise_list.append(idx_map[i][0])\n",
    "        hypo_list.append(idx_map[i][1])\n",
    "\n",
    "  val_acc  = total_val_acc/len(data_loader)\n",
    "  val_loss = total_val_loss/len(data_loader)\n",
    "  cr = classification_report(y_true, y_pred)\n",
    "\n",
    "  idx_json = {'idx': idx_list, 'gold_label': y_true, 'pred_label': y_pred, 'premise': premise_list, 'hypothesis': hypo_list}\n",
    "  \n",
    "  return val_acc, val_loss, cr, model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define custom bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASSIGN: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing FeaturefulBert: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing FeaturefulBert from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FeaturefulBert from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleDict(\n",
      "  (distance): Embedding(5, 3, padding_idx=0)\n",
      "  (u1_depdir): Embedding(5, 3, padding_idx=0)\n",
      "  (u2_depdir): Embedding(5, 3, padding_idx=0)\n",
      "  (u2_func): Embedding(35, 6, padding_idx=0)\n",
      "  (u1_position): Embedding(12, 4, padding_idx=0)\n",
      "  (u2_position): Embedding(12, 4, padding_idx=0)\n",
      "  (sat_children): Identity()\n",
      "  (nuc_children): Identity()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from typing import Any, Dict, Optional\n",
    "from transformers import BertModel, BertConfig\n",
    "import torch.nn as nn\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from featurefulbertembedder_custom_allennlp_dims import FeaturefulBertEmbedder\n",
    "from featureful_bert_custom_allennlp_dims import get_combined_feature_tensor_2 as get_combined_feature_tensor_forward\n",
    "from features_custom_allennlp_dims import get_feature_modules\n",
    "\n",
    "class CustomPooler2(nn.Module):\n",
    "    def __init__(self, *,\n",
    "                        requires_grad: bool = True,\n",
    "                        dropout: float = 0.0,\n",
    "                        transformer_kwargs: Optional[Dict[str, Any]] = None, ) -> None:\n",
    "        super().__init__()\n",
    "        bert = BertModel.from_pretrained(BERT_MODEL) #only used to pass config. BertAttentionClass used in FeatureFulBert\n",
    "        self._dropout = torch.nn.Dropout(p=dropout)\n",
    "        self.pooler = copy.deepcopy(bert.pooler)\n",
    "        for param in self.pooler.parameters():\n",
    "            param.requires_grad = requires_grad\n",
    "        self._embedding_dim = bert.config.hidden_size\n",
    "\n",
    "    def get_input_dim(self) -> int:\n",
    "        return self._embedding_dim\n",
    "\n",
    "    def get_output_dim(self) -> int:\n",
    "        return self._embedding_dim\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, mask: torch.BoolTensor = None, num_wrapping_dims: int = 0):\n",
    "        pooler = self.pooler\n",
    "        \n",
    "        for _ in range(num_wrapping_dims):\n",
    "            pooler = TimeDistributed(pooler)\n",
    "        pooled = pooler(tokens)\n",
    "        pooled = self._dropout(pooled)\n",
    "        return pooled\n",
    "\n",
    "class MyModule(nn.Module):    \n",
    "    def __init__(self, feature_list):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.feature_list = feature_list\n",
    "        self.feature_modules = nn.ModuleDict()\n",
    "        self.dims = 0\n",
    "        self.feature_modules, dims = get_feature_modules(feature_list, mnli_dataset.vocab, use_allennlp_dims=True)\n",
    "        self.dims += dims\n",
    "\n",
    "        print(self.feature_modules)\n",
    "        for feature in feature_list:\n",
    "            if feature not in ['distance', 'u1_depdir', 'u2_depdir', 'u2_func', 'u1_position', 'u2_position', \n",
    "                                'nuc_children', 'sat_children']: raise ValueError()\n",
    "            # elif 'genre' in feature_list:               self.modules['genre'] = nn.Embedding(mnli_dataset_en.distance_unique, 3)\n",
    "            # elif 'unit1_case' in feature_list:          self.modules['unit1_case'] = nn.Embedding(mnli_dataset_en.distance_unique, 3)\n",
    "            # elif 'unit2_case' in feature_list:          self.modules['unit2_case'] = nn.Embedding(mnli_dataset_en.distance_unique, 3)\n",
    "            # elif 'u1_discontinuous' in feature_list:    self.modules['u1_discontinuous'] = nn.Embedding(mnli_dataset_en.distance_unique, 3)\n",
    "            # elif 'u2_discontinuous' in feature_list:    self.modules['u2_discontinuous'] = nn.Embedding(mnli_dataset_en.distance_unique, 3)\n",
    "            # elif 'same_speaker' in feature_list:        self.modules['same_speaker'] = nn.Embedding(mnli_dataset_en.distance_unique, 3)\n",
    "            # elif 'lex_overlap_length' in feature_list:  self.modules['lex_overlap_length'] = nn.Embedding(mnli_dataset_en.distance_unique, 3)\n",
    "            # elif 'u1_func' in feature_list:             self.modules['u1_func'] = nn.Embedding(mnli_dataset_en.distance_unique, 3)\n",
    "\n",
    "    def forward(self, features):\n",
    "        feature_list = ['distance', 'u1_depdir', 'u2_depdir', 'u2_func', 'u1_position', 'u2_position', 'sat_children', 'nuc_children']\n",
    "        for i in range(len(feature_list)):\n",
    "            feature_name = feature_list[i]\n",
    "            feature_modules = self.feature_modules[feature_name]\n",
    "            feature = features[...,i]\n",
    "            if feature_name not in ['sat_children', 'nuc_children']:\n",
    "                if torch.max(feature)>feature_modules.num_embeddings:\n",
    "                    print(feature_name, feature)\n",
    "                    raise ValueError()\n",
    "\n",
    "        return get_combined_feature_tensor_forward(features, self.feature_list, self.feature_modules)\n",
    "\n",
    "class CustomBERTModel(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "          super(CustomBERTModel, self).__init__()\n",
    "          self.num_classes = num_labels\n",
    "          self.feature_list = mnli_dataset.feature_list\n",
    "          print('ASSIGN:', self.num_classes)\n",
    "\n",
    "          self.embedder = self.create_featureful_bert() #BERT MODEL\n",
    "          self.encoder = CustomPooler2()\n",
    "          self.module1 = MyModule(self.feature_list)\n",
    "          self.dropout1 = nn.Dropout(p=0.0)\n",
    "        #   self.dropout_decoder = nn.Dropout(p=0.5)\n",
    "          self.out_features = self.encoder.pooler.dense.out_features+self.module1.dims\n",
    "          self.relation_decoder = nn.Linear(self.out_features, self.num_classes)\n",
    "\n",
    "    def forward(self, pair_token_ids, attention_mask, feat):\n",
    "        direction_tensor = feat['dir'].to(device)\n",
    "        embedded_sentence = self.embedder(token_ids=pair_token_ids, #featurefulmebedder\n",
    "                        mask=attention_mask, \n",
    "                        # type_ids=token_type_ids,\n",
    "                        segment_concat_mask = None,\n",
    "                        direction_tensor = direction_tensor,\n",
    "                        feature_list = self.feature_list,\n",
    "                        features = feat)\n",
    "        # mask = token_type_ids\n",
    "        bertpooler_output = self.encoder(tokens=embedded_sentence, mask=None)\n",
    "        feat = self.convert_to_feature_list(feat)\n",
    "        feat = self.dropout1(feat)\n",
    "        feat = self.module1(feat)\n",
    "        try:\n",
    "            feat_concat = torch.concat((bertpooler_output, feat),-1)\n",
    "        except:\n",
    "            print(bertpooler_output.shape, feat.shape)\n",
    "            raise ValueError()\n",
    "        if feat_concat.shape[-1]!=self.module1.dims+bertpooler_output.shape[-1]: print(feat_concat.shape, self.module1.dims)\n",
    "        assert feat_concat.shape[-1] == self.module1.dims+bertpooler_output.shape[-1]\n",
    "        feat_concat = self.dropout1(feat_concat)\n",
    "        # feat_concat = self.dropout_decoder(feat_concat)\n",
    "        linear1_output = self.relation_decoder(feat_concat)\n",
    "        return linear1_output\n",
    "\n",
    "\n",
    "    def create_featureful_bert(self):\n",
    "        featureful_bert = FeaturefulBertEmbedder(model_name = BERT_MODEL,\n",
    "                                hidden_activation_allen = 'gelu',\n",
    "                                feature_list = self.feature_list, \n",
    "                                vocab=mnli_dataset.vocab,\n",
    "                                use_allen_dims=True)\n",
    "        return featureful_bert\n",
    "\n",
    "    def convert_to_feature_list(self, feat):\n",
    "        feature_linear = [feat[feature_name] for feature_name in self.feature_list]\n",
    "        feature_linear = torch.stack(feature_linear, dim=-1)\n",
    "        return feature_linear\n",
    "\n",
    "model = CustomBERTModel(mnli_dataset.num_labels)\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-6, correct_bias=False) # original 2e-5\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.8, mode='max', patience=35, min_lr=5e-7, verbose=True) #original factor=0.6, min_lr=5e-7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define training regime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prinintg model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomBERTModel(\n",
      "  (embedder): FeaturefulBertEmbedder(\n",
      "    (transformer_model): FeaturefulBert(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30000, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "      (feature_modules): ModuleDict(\n",
      "        (distance): Embedding(5, 3, padding_idx=0)\n",
      "        (u1_depdir): Embedding(5, 3, padding_idx=0)\n",
      "        (u2_depdir): Embedding(5, 3, padding_idx=0)\n",
      "        (u2_func): Embedding(35, 6, padding_idx=0)\n",
      "        (u1_position): Embedding(12, 4, padding_idx=0)\n",
      "        (u2_position): Embedding(12, 4, padding_idx=0)\n",
      "        (sat_children): Identity()\n",
      "        (nuc_children): Identity()\n",
      "      )\n",
      "      (feature_projector): Linear(in_features=26, out_features=768, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (encoder): CustomPooler2(\n",
      "    (_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (module1): MyModule(\n",
      "    (feature_modules): ModuleDict(\n",
      "      (distance): Embedding(5, 3, padding_idx=0)\n",
      "      (u1_depdir): Embedding(5, 3, padding_idx=0)\n",
      "      (u2_depdir): Embedding(5, 3, padding_idx=0)\n",
      "      (u2_func): Embedding(35, 6, padding_idx=0)\n",
      "      (u1_position): Embedding(12, 4, padding_idx=0)\n",
      "      (u2_position): Embedding(12, 4, padding_idx=0)\n",
      "      (sat_children): Identity()\n",
      "      (nuc_children): Identity()\n",
      "    )\n",
      "  )\n",
      "  (dropout1): Dropout(p=0.0, inplace=False)\n",
      "  (relation_decoder): Linear(in_features=793, out_features=25, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def writer_init(save_path_suffix):\n",
    "    writer_path = 'run1/'+save_path_suffix[:-1]+'/'\n",
    "    if os.path.isdir(writer_path):\n",
    "        filelist = [ f for f in os.listdir(writer_path) if 'events.out' in f ]\n",
    "        print(filelist)\n",
    "        for f in filelist:\n",
    "            os.remove(os.path.join(writer_path, f))\n",
    "    else:\n",
    "        os.mkdir(writer_path)\n",
    "    writer = SummaryWriter(log_dir=writer_path)\n",
    "    return writer\n",
    "\n",
    "writer = writer_init(save_path_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import traceback\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import Optional, Iterable, Dict, Any\n",
    "from EarlyStopperUtil import MetricTracker\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer, scheduler, rev_label_dict):  \n",
    "  EarlyStopper = MetricTracker(patience=20, metric_name='+accuracy')\n",
    "  best_val_acc = 0\n",
    "\n",
    "\n",
    "  for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_train_acc  = 0\n",
    "    \n",
    "    # logging for scheduler\n",
    "    losses = []\n",
    "    accuracies= []\n",
    "\n",
    "    train_size = 0\n",
    "\n",
    "    for batch_idx, (pair_token_ids, mask_ids, feat, y, idx) in enumerate(train_loader):\n",
    "      train_size+=1\n",
    "      optimizer.zero_grad()\n",
    "      pair_token_ids = pair_token_ids.to(device)\n",
    "      mask_ids = mask_ids.to(device)\n",
    "      # seg_ids = seg_ids.to(device)\n",
    "      labels = y.to(device)\n",
    "      # feat = feat.to(device)\n",
    "      outputs = model(pair_token_ids = pair_token_ids, \n",
    "                            # token_type_ids=seg_ids, \n",
    "                            attention_mask=mask_ids,\n",
    "                            feat=feat)\n",
    "      criterion = nn.CrossEntropyLoss()\n",
    "      loss = criterion(outputs, labels)\n",
    "      loss.backward()\n",
    "      acc = multi_acc(outputs, labels)\n",
    "      optimizer.step()\n",
    "      total_train_loss += loss.item()\n",
    "      total_train_acc  += acc.item()\n",
    "\n",
    "      losses.append(loss)\n",
    "      accuracies.append(acc)\n",
    "      \n",
    "    mean_loss = sum(losses)/len(losses)\n",
    "    scheduler.step(mean_loss)\n",
    "\n",
    "    train_acc  = total_train_acc/len(train_loader)\n",
    "    train_loss = total_train_loss/len(train_loader)\n",
    "\n",
    "    val_acc, val_loss, cr, model, optimizer = evaluate_accuracy(model, optimizer, val_loader, rev_label_dict, label_dict, None)\n",
    "    if val_acc>best_val_acc:\n",
    "      torch.save(model.state_dict(), 'run1/'+save_path_suffix+'_best.pt')\n",
    "      best_val_acc = val_acc\n",
    "      print(f'Epoch {epoch+1}: Best val_acc: {best_val_acc:.4f}')\n",
    "    if val_acc>=best_val_acc:\n",
    "      torch.save(model.state_dict(), 'run1/'+save_path_suffix+'_best_latest.pt')\n",
    "      best_val_acc = val_acc\n",
    "      print(f'Epoch {epoch+1}: Best val_acc: {best_val_acc:.4f}')\n",
    "    EarlyStopper.add_metric(val_acc)\n",
    "    if EarlyStopper.should_stop_early(): break\n",
    "\n",
    "    end = time.time()\n",
    "    hours, rem = divmod(end-start, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "    print(f'Epoch {epoch+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
    "    print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "    print(f'train_size: {train_size}')\n",
    "\n",
    "    writer.add_scalar('train_loss', train_loss, epoch)\n",
    "    writer.add_scalar('train_acc', train_acc, epoch)\n",
    "    writer.add_scalar('val_loss', val_loss, epoch)\n",
    "    writer.add_scalar('val_acc', val_acc, epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Best val_acc: 0.1967\n",
      "Epoch 1: Best val_acc: 0.1967\n",
      "Epoch 1: train_loss: 2.4951 train_acc: 0.2639 | val_loss: 2.6227 val_acc: 0.1967\n",
      "00:01:44.53\n",
      "train_size: 1094\n",
      "Epoch 2: Best val_acc: 0.2336\n",
      "Epoch 2: Best val_acc: 0.2336\n",
      "Epoch 2: train_loss: 2.1484 train_acc: 0.3595 | val_loss: 2.4946 val_acc: 0.2336\n",
      "00:01:43.79\n",
      "train_size: 1094\n",
      "Epoch 3: Best val_acc: 0.2336\n",
      "Epoch 3: train_loss: 1.9710 train_acc: 0.4041 | val_loss: 2.3836 val_acc: 0.2336\n",
      "00:01:43.52\n",
      "train_size: 1094\n",
      "Epoch 4: Best val_acc: 0.2787\n",
      "Epoch 4: Best val_acc: 0.2787\n",
      "Epoch 4: train_loss: 1.8228 train_acc: 0.4480 | val_loss: 2.3422 val_acc: 0.2787\n",
      "00:01:44.20\n",
      "train_size: 1094\n",
      "Epoch 5: Best val_acc: 0.2869\n",
      "Epoch 5: Best val_acc: 0.2869\n",
      "Epoch 5: train_loss: 1.6935 train_acc: 0.4864 | val_loss: 2.2864 val_acc: 0.2869\n",
      "00:01:44.66\n",
      "train_size: 1094\n",
      "Epoch 6: Best val_acc: 0.2951\n",
      "Epoch 6: Best val_acc: 0.2951\n",
      "Epoch 6: train_loss: 1.5798 train_acc: 0.5314 | val_loss: 2.2600 val_acc: 0.2951\n",
      "00:01:45.19\n",
      "train_size: 1094\n",
      "Epoch 7: Best val_acc: 0.3156\n",
      "Epoch 7: Best val_acc: 0.3156\n",
      "Epoch 7: train_loss: 1.4603 train_acc: 0.5695 | val_loss: 2.3084 val_acc: 0.3156\n",
      "00:01:45.18\n",
      "train_size: 1094\n",
      "Epoch 8: train_loss: 1.3384 train_acc: 0.6198 | val_loss: 2.2874 val_acc: 0.3115\n",
      "00:01:42.98\n",
      "train_size: 1094\n",
      "Epoch 9: Best val_acc: 0.3197\n",
      "Epoch 9: Best val_acc: 0.3197\n",
      "Epoch 9: train_loss: 1.2073 train_acc: 0.6545 | val_loss: 2.3152 val_acc: 0.3197\n",
      "00:01:45.48\n",
      "train_size: 1094\n",
      "Epoch 10: Best val_acc: 0.3320\n",
      "Epoch 10: Best val_acc: 0.3320\n",
      "Epoch 10: train_loss: 1.0948 train_acc: 0.6942 | val_loss: 2.3451 val_acc: 0.3320\n",
      "00:01:45.42\n",
      "train_size: 1094\n",
      "Epoch 11: train_loss: 0.9905 train_acc: 0.7304 | val_loss: 2.4558 val_acc: 0.3197\n",
      "00:01:43.00\n",
      "train_size: 1094\n",
      "Epoch 12: train_loss: 0.8858 train_acc: 0.7646 | val_loss: 2.4540 val_acc: 0.3197\n",
      "00:01:43.30\n",
      "train_size: 1094\n",
      "Epoch 13: train_loss: 0.7808 train_acc: 0.7991 | val_loss: 2.5160 val_acc: 0.3238\n",
      "00:01:42.73\n",
      "train_size: 1094\n",
      "Epoch 14: train_loss: 0.6989 train_acc: 0.8204 | val_loss: 2.6626 val_acc: 0.2951\n",
      "00:01:42.34\n",
      "train_size: 1094\n",
      "Epoch 15: train_loss: 0.6088 train_acc: 0.8441 | val_loss: 2.6334 val_acc: 0.3156\n",
      "00:01:42.80\n",
      "train_size: 1094\n",
      "Epoch 16: Best val_acc: 0.3320\n",
      "Epoch 16: train_loss: 0.5410 train_acc: 0.8675 | val_loss: 2.6515 val_acc: 0.3320\n",
      "00:01:43.51\n",
      "train_size: 1094\n",
      "Epoch 17: train_loss: 0.4818 train_acc: 0.8822 | val_loss: 2.8326 val_acc: 0.2992\n",
      "00:01:44.17\n",
      "train_size: 1094\n",
      "Epoch 18: Best val_acc: 0.3402\n",
      "Epoch 18: Best val_acc: 0.3402\n",
      "Epoch 18: train_loss: 0.4362 train_acc: 0.8930 | val_loss: 2.7589 val_acc: 0.3402\n",
      "00:01:46.12\n",
      "train_size: 1094\n",
      "Epoch 19: train_loss: 0.3714 train_acc: 0.9099 | val_loss: 2.8959 val_acc: 0.3074\n",
      "00:01:43.49\n",
      "train_size: 1094\n",
      "Epoch 20: train_loss: 0.3169 train_acc: 0.9253 | val_loss: 3.0359 val_acc: 0.3279\n",
      "00:01:43.05\n",
      "train_size: 1094\n",
      "Epoch 21: train_loss: 0.2913 train_acc: 0.9305 | val_loss: 2.9221 val_acc: 0.3238\n",
      "00:01:43.37\n",
      "train_size: 1094\n",
      "Epoch 22: train_loss: 0.2458 train_acc: 0.9431 | val_loss: 2.9875 val_acc: 0.3197\n",
      "00:01:43.47\n",
      "train_size: 1094\n",
      "Epoch 23: train_loss: 0.2312 train_acc: 0.9452 | val_loss: 3.0216 val_acc: 0.3320\n",
      "00:01:43.63\n",
      "train_size: 1094\n",
      "Epoch 24: train_loss: 0.1867 train_acc: 0.9565 | val_loss: 3.1176 val_acc: 0.3279\n",
      "00:01:43.48\n",
      "train_size: 1094\n",
      "Epoch 25: train_loss: 0.1665 train_acc: 0.9650 | val_loss: 3.2288 val_acc: 0.3156\n",
      "00:01:43.48\n",
      "train_size: 1094\n",
      "Epoch 26: Best val_acc: 0.3402\n",
      "Epoch 26: train_loss: 0.1475 train_acc: 0.9694 | val_loss: 3.1810 val_acc: 0.3402\n",
      "00:01:44.49\n",
      "train_size: 1094\n",
      "Epoch 27: train_loss: 0.1274 train_acc: 0.9767 | val_loss: 3.2845 val_acc: 0.3033\n",
      "00:01:44.04\n",
      "train_size: 1094\n",
      "Epoch 28: train_loss: 0.1169 train_acc: 0.9767 | val_loss: 3.3048 val_acc: 0.2992\n",
      "00:01:44.08\n",
      "train_size: 1094\n",
      "Epoch 29: train_loss: 0.1036 train_acc: 0.9826 | val_loss: 3.3941 val_acc: 0.3156\n",
      "00:01:43.24\n",
      "train_size: 1094\n",
      "Epoch 30: train_loss: 0.0886 train_acc: 0.9849 | val_loss: 3.4208 val_acc: 0.3238\n",
      "00:01:43.31\n",
      "train_size: 1094\n",
      "Epoch 31: train_loss: 0.0742 train_acc: 0.9886 | val_loss: 3.4013 val_acc: 0.2992\n",
      "00:01:43.70\n",
      "train_size: 1094\n",
      "Epoch 32: train_loss: 0.0743 train_acc: 0.9851 | val_loss: 3.3966 val_acc: 0.3361\n",
      "00:01:43.61\n",
      "train_size: 1094\n",
      "Epoch 33: train_loss: 0.0649 train_acc: 0.9895 | val_loss: 3.4895 val_acc: 0.3279\n",
      "00:01:45.72\n",
      "train_size: 1094\n",
      "Epoch 34: train_loss: 0.0521 train_acc: 0.9929 | val_loss: 3.4910 val_acc: 0.3238\n",
      "00:03:33.97\n",
      "train_size: 1094\n",
      "Epoch 35: train_loss: 0.0514 train_acc: 0.9906 | val_loss: 3.5760 val_acc: 0.3361\n",
      "00:04:08.76\n",
      "train_size: 1094\n",
      "Epoch 36: train_loss: 0.0452 train_acc: 0.9931 | val_loss: 3.5965 val_acc: 0.3115\n",
      "00:04:07.87\n",
      "train_size: 1094\n",
      "Epoch 00037: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 37: train_loss: 0.0372 train_acc: 0.9945 | val_loss: 3.6626 val_acc: 0.3238\n",
      "00:04:07.95\n",
      "train_size: 1094\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    train(model, train_loader, val_loader, optimizer, scheduler, rev_label_dict) #6m30s 13m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/modeling_utils.py:768: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_loss: 4.1106 test_acc: 0.2885\n",
      "00:00:03.08\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    antithesis       0.25      0.11      0.15        18\n",
      "    background       0.45      0.59      0.51        17\n",
      "         cause       0.08      0.50      0.14         2\n",
      "  circumstance       0.31      0.27      0.29        15\n",
      "    concession       0.44      0.54      0.48        13\n",
      "     condition       0.46      0.67      0.55         9\n",
      "   conjunction       0.44      0.57      0.50         7\n",
      "      contrast       0.17      0.25      0.20         8\n",
      " e-elaboration       0.67      0.55      0.60        11\n",
      "   elaboration       0.11      0.20      0.14        10\n",
      "    evaluation       0.27      0.15      0.19        20\n",
      "      evidence       0.31      0.40      0.35        10\n",
      "interpretation       0.12      0.17      0.14        12\n",
      "         joint       0.19      0.24      0.22        29\n",
      "          list       0.24      0.15      0.19        26\n",
      "         means       0.25      0.50      0.33         2\n",
      "   preparation       0.40      0.50      0.44         4\n",
      "       purpose       1.00      0.67      0.80         3\n",
      "        reason       0.55      0.18      0.27        34\n",
      "   restatement       0.00      0.00      0.00         1\n",
      "        result       0.00      0.00      0.00         0\n",
      "      sequence       0.00      0.00      0.00         7\n",
      "  solutionhood       0.00      0.00      0.00         1\n",
      "       summary       0.00      0.00      0.00         1\n",
      "\n",
      "      accuracy                           0.29       260\n",
      "     macro avg       0.28      0.30      0.27       260\n",
      "  weighted avg       0.33      0.29      0.29       260\n",
      "\n",
      "Test Loss: 4.111 |  Test Acc: 28.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "def validate(model, test_loader, optimizer, rev_label_dict, label_dict):\n",
    "  start = time.time()\n",
    "  test_acc, test_loss, cr, model, optimizer = evaluate_accuracy(model, optimizer, test_loader, rev_label_dict, label_dict, is_training=False)\n",
    "  end = time.time()\n",
    "  hours, rem = divmod(end-start, 3600)\n",
    "  minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "  print(f'Test_loss: {test_loss:.4f} test_acc: {test_acc:.4f}')\n",
    "  print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "  print(cr)\n",
    "\n",
    "  return test_loss, test_acc\n",
    "\n",
    "\n",
    "test_loss, test_acc = validate(model, test_loader, optimizer, rev_label_dict, label_dict)\n",
    "writer.add_scalar('test_loss_latest', test_loss, 1)\n",
    "writer.add_scalar('test_acc_latest', test_acc, 1)\n",
    "print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### best earliest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/modeling_utils.py:768: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_loss: 2.9331 test_acc: 0.3115\n",
      "00:00:03.12\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    antithesis       0.22      0.11      0.15        18\n",
      "    background       0.53      0.59      0.56        17\n",
      "         cause       0.22      1.00      0.36         2\n",
      "  circumstance       0.29      0.27      0.28        15\n",
      "    concession       0.50      0.54      0.52        13\n",
      "     condition       0.38      0.56      0.45         9\n",
      "   conjunction       0.33      0.43      0.38         7\n",
      "      contrast       0.29      0.25      0.27         8\n",
      " e-elaboration       0.67      0.55      0.60        11\n",
      "   elaboration       0.10      0.20      0.13        10\n",
      "    evaluation       0.24      0.20      0.22        20\n",
      "      evidence       0.18      0.20      0.19        10\n",
      "interpretation       0.16      0.25      0.19        12\n",
      "         joint       0.23      0.34      0.28        29\n",
      "          list       0.24      0.15      0.19        26\n",
      "         means       0.00      0.00      0.00         2\n",
      "   preparation       0.43      0.75      0.55         4\n",
      "       purpose       1.00      0.67      0.80         3\n",
      "        reason       0.50      0.29      0.37        34\n",
      "   restatement       0.00      0.00      0.00         1\n",
      "      sequence       0.00      0.00      0.00         7\n",
      "  solutionhood       0.00      0.00      0.00         1\n",
      "       summary       0.00      0.00      0.00         1\n",
      "\n",
      "      accuracy                           0.31       260\n",
      "     macro avg       0.28      0.32      0.28       260\n",
      "  weighted avg       0.32      0.31      0.30       260\n",
      "\n",
      "Latest Test Loss: 2.933 |  Latest Test Acc: 31.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('run1/'+save_path_suffix+'_best.pt'))\n",
    "test_loss, test_acc = validate(model, test_loader, optimizer, rev_label_dict, label_dict)\n",
    "writer.add_scalar('test_loss_best_earliest', test_loss, 1)\n",
    "writer.add_scalar('test_acc_best_earliest', test_acc, 1)\n",
    "print(f'Latest Test Loss: {test_loss:.3f} |  Latest Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### best lastest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/modeling_utils.py:768: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_loss: 3.4330 test_acc: 0.2923\n",
      "00:00:03.12\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    antithesis       0.18      0.11      0.14        18\n",
      "    background       0.44      0.41      0.42        17\n",
      "         cause       0.14      0.50      0.22         2\n",
      "  circumstance       0.31      0.33      0.32        15\n",
      "    concession       0.44      0.54      0.48        13\n",
      "     condition       0.50      0.56      0.53         9\n",
      "   conjunction       0.44      0.57      0.50         7\n",
      "      contrast       0.40      0.25      0.31         8\n",
      " e-elaboration       0.62      0.45      0.53        11\n",
      "   elaboration       0.10      0.20      0.13        10\n",
      "    evaluation       0.43      0.15      0.22        20\n",
      "      evidence       0.15      0.30      0.20        10\n",
      "interpretation       0.19      0.25      0.21        12\n",
      "         joint       0.21      0.38      0.27        29\n",
      "          list       0.20      0.15      0.17        26\n",
      "         means       0.50      0.50      0.50         2\n",
      "   preparation       0.40      0.50      0.44         4\n",
      "       purpose       1.00      0.67      0.80         3\n",
      "        reason       0.47      0.21      0.29        34\n",
      "   restatement       0.00      0.00      0.00         1\n",
      "        result       0.00      0.00      0.00         0\n",
      "      sequence       0.00      0.00      0.00         7\n",
      "  solutionhood       0.00      0.00      0.00         1\n",
      "       summary       0.00      0.00      0.00         1\n",
      "\n",
      "      accuracy                           0.29       260\n",
      "     macro avg       0.30      0.29      0.28       260\n",
      "  weighted avg       0.33      0.29      0.29       260\n",
      "\n",
      "Best Test Loss: 3.433 |  Best Test Acc: 29.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('run1/'+save_path_suffix+'_best_latest.pt'))\n",
    "test_loss, test_acc = validate(model, test_loader, optimizer, rev_label_dict, label_dict)\n",
    "writer.add_scalar('test_loss_best_latest', test_loss, 1)\n",
    "writer.add_scalar('test_acc_best_latest', test_acc, 1)\n",
    "print(f'Best Test Loss: {test_loss:.3f} |  Best Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### best val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/modeling_utils.py:768: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_loss: 3.2084 test_acc: 0.3402\n",
      "00:00:03.02\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    antithesis       0.23      0.27      0.25        11\n",
      "    background       0.29      0.24      0.26        17\n",
      "         cause       0.20      0.14      0.17         7\n",
      "  circumstance       0.12      0.08      0.10        13\n",
      "    concession       0.62      0.73      0.67        11\n",
      "     condition       0.57      1.00      0.73         8\n",
      "   conjunction       0.71      0.62      0.67         8\n",
      "      contrast       0.00      0.00      0.00         3\n",
      " e-elaboration       0.78      0.54      0.64        13\n",
      "   elaboration       0.30      0.21      0.25        28\n",
      "    evaluation       0.25      0.08      0.12        13\n",
      "      evidence       0.14      0.38      0.20         8\n",
      "interpretation       0.12      0.15      0.13        13\n",
      "         joint       0.18      0.22      0.20        18\n",
      "          list       0.38      0.44      0.41        18\n",
      "         means       0.00      0.00      0.00         1\n",
      "   preparation       0.80      0.73      0.76        11\n",
      "       purpose       0.80      0.80      0.80         5\n",
      "        reason       0.34      0.36      0.35        28\n",
      "   restatement       0.00      0.00      0.00         1\n",
      "        result       0.00      0.00      0.00         3\n",
      "  solutionhood       0.00      0.00      0.00         1\n",
      "       summary       0.00      0.00      0.00         2\n",
      "\n",
      "      accuracy                           0.34       241\n",
      "     macro avg       0.30      0.30      0.29       241\n",
      "  weighted avg       0.35      0.34      0.34       241\n",
      "\n",
      "Val Loss: 3.208 |  Val Acc: 34.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('run1/'+save_path_suffix+'_best_latest.pt'))\n",
    "test_loss, test_acc = validate(model, val_loader, optimizer, rev_label_dict, label_dict)\n",
    "writer.add_scalar('val_loss_best_latest', test_loss, 1)\n",
    "writer.add_scalar('val_acc_best_latest', test_acc, 1)\n",
    "print(f'Val Loss: {test_loss:.3f} |  Val Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unit1_txt</th>\n",
       "      <th>unit1_sent</th>\n",
       "      <th>unit2_txt</th>\n",
       "      <th>unit2_sent</th>\n",
       "      <th>dir</th>\n",
       "      <th>distance</th>\n",
       "      <th>u1_depdir</th>\n",
       "      <th>u2_depdir</th>\n",
       "      <th>u2_func</th>\n",
       "      <th>u1_position</th>\n",
       "      <th>...</th>\n",
       "      <th>sat_children</th>\n",
       "      <th>nuc_children</th>\n",
       "      <th>genre</th>\n",
       "      <th>unit1_case</th>\n",
       "      <th>unit2_case</th>\n",
       "      <th>u1_discontinuous</th>\n",
       "      <th>u2_discontinuous</th>\n",
       "      <th>same_speaker</th>\n",
       "      <th>lex_overlap_length</th>\n",
       "      <th>u1_func</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>background</th>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>...</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cause</th>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "      <td>...</td>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>condition</th>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>...</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contrast</th>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>...</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elaboration</th>\n",
       "      <td>444</td>\n",
       "      <td>444</td>\n",
       "      <td>444</td>\n",
       "      <td>444</td>\n",
       "      <td>444</td>\n",
       "      <td>444</td>\n",
       "      <td>444</td>\n",
       "      <td>444</td>\n",
       "      <td>444</td>\n",
       "      <td>444</td>\n",
       "      <td>...</td>\n",
       "      <td>444</td>\n",
       "      <td>444</td>\n",
       "      <td>444</td>\n",
       "      <td>444</td>\n",
       "      <td>444</td>\n",
       "      <td>444</td>\n",
       "      <td>444</td>\n",
       "      <td>444</td>\n",
       "      <td>444</td>\n",
       "      <td>444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>evaluation</th>\n",
       "      <td>228</td>\n",
       "      <td>228</td>\n",
       "      <td>228</td>\n",
       "      <td>228</td>\n",
       "      <td>228</td>\n",
       "      <td>228</td>\n",
       "      <td>228</td>\n",
       "      <td>228</td>\n",
       "      <td>228</td>\n",
       "      <td>228</td>\n",
       "      <td>...</td>\n",
       "      <td>228</td>\n",
       "      <td>228</td>\n",
       "      <td>228</td>\n",
       "      <td>228</td>\n",
       "      <td>228</td>\n",
       "      <td>228</td>\n",
       "      <td>228</td>\n",
       "      <td>228</td>\n",
       "      <td>228</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joint</th>\n",
       "      <td>538</td>\n",
       "      <td>538</td>\n",
       "      <td>538</td>\n",
       "      <td>538</td>\n",
       "      <td>538</td>\n",
       "      <td>538</td>\n",
       "      <td>538</td>\n",
       "      <td>538</td>\n",
       "      <td>538</td>\n",
       "      <td>538</td>\n",
       "      <td>...</td>\n",
       "      <td>538</td>\n",
       "      <td>538</td>\n",
       "      <td>538</td>\n",
       "      <td>538</td>\n",
       "      <td>538</td>\n",
       "      <td>538</td>\n",
       "      <td>538</td>\n",
       "      <td>538</td>\n",
       "      <td>538</td>\n",
       "      <td>538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>means</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             unit1_txt  unit1_sent  unit2_txt  unit2_sent  dir  distance  \\\n",
       "label                                                                      \n",
       "background         351         351        351         351  351       351   \n",
       "cause              246         246        246         246  246       246   \n",
       "condition          265         265        265         265  265       265   \n",
       "contrast           102         102        102         102  102       102   \n",
       "elaboration        444         444        444         444  444       444   \n",
       "evaluation         228         228        228         228  228       228   \n",
       "joint              538         538        538         538  538       538   \n",
       "means               21          21         21          21   21        21   \n",
       "summary             16          16         16          16   16        16   \n",
       "\n",
       "             u1_depdir  u2_depdir  u2_func  u1_position  ...  sat_children  \\\n",
       "label                                                    ...                 \n",
       "background         351        351      351          351  ...           351   \n",
       "cause              246        246      246          246  ...           246   \n",
       "condition          265        265      265          265  ...           265   \n",
       "contrast           102        102      102          102  ...           102   \n",
       "elaboration        444        444      444          444  ...           444   \n",
       "evaluation         228        228      228          228  ...           228   \n",
       "joint              538        538      538          538  ...           538   \n",
       "means               21         21       21           21  ...            21   \n",
       "summary             16         16       16           16  ...            16   \n",
       "\n",
       "             nuc_children  genre  unit1_case  unit2_case  u1_discontinuous  \\\n",
       "label                                                                        \n",
       "background            351    351         351         351               351   \n",
       "cause                 246    246         246         246               246   \n",
       "condition             265    265         265         265               265   \n",
       "contrast              102    102         102         102               102   \n",
       "elaboration           444    444         444         444               444   \n",
       "evaluation            228    228         228         228               228   \n",
       "joint                 538    538         538         538               538   \n",
       "means                  21     21          21          21                21   \n",
       "summary                16     16          16          16                16   \n",
       "\n",
       "             u2_discontinuous  same_speaker  lex_overlap_length  u1_func  \n",
       "label                                                                     \n",
       "background                351           351                 351      351  \n",
       "cause                     246           246                 246      246  \n",
       "condition                 265           265                 265      265  \n",
       "contrast                  102           102                 102      102  \n",
       "elaboration               444           444                 444      444  \n",
       "evaluation                228           228                 228      228  \n",
       "joint                     538           538                 538      538  \n",
       "means                      21            21                  21       21  \n",
       "summary                    16            16                  16       16  \n",
       "\n",
       "[9 rows x 21 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_en.groupby(['label']).count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3409ea685db85227fbd9509d1b1ace14d085473eb2d57f3ba9dd0302d25f838"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

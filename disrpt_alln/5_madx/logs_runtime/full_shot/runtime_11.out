-------------------------------------------------------------------
Lang1:  deu.rst.pcc    Lang2:  spa.rst.sctb
Saving run to:  runs/full_shot/FullShot=v4_deu.rst.pcc_spa.rst.sctb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2164 examples
read 241 examples
read 260 examples
read 439 examples
read 94 examples
read 159 examples
Total prediction labels:  31
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=31, bias=True)
    )
  )
)
{'train@deu.rst.pcc_loss': 3.1372485160827637, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.09842883548983364, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.014554204805381079, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.011526161226035742, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.03989331345969209, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.1372487545013428, 'train@deu.rst.pcc_runtime': 26.2579, 'train@deu.rst.pcc_samples_per_second': 82.413, 'train@deu.rst.pcc_steps_per_second': 2.59, 'epoch': 1.0}
{'loss': 3.2929, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.1643266677856445, 'eval_accuracy@deu.rst.pcc': 0.0912863070539419, 'eval_f1@deu.rst.pcc': 0.013937793427230049, 'eval_precision@deu.rst.pcc': 0.011733490566037737, 'eval_recall@deu.rst.pcc': 0.036044973544973546, 'eval_loss@deu.rst.pcc': 3.1643264293670654, 'eval_runtime': 3.1566, 'eval_samples_per_second': 76.348, 'eval_steps_per_second': 2.534, 'epoch': 1.0}
{'train@deu.rst.pcc_loss': 2.957669258117676, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.11829944547134935, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.023992835419884277, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.02337875028429008, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.04837991207573501, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.957669496536255, 'train@deu.rst.pcc_runtime': 26.1159, 'train@deu.rst.pcc_samples_per_second': 82.861, 'train@deu.rst.pcc_steps_per_second': 2.604, 'epoch': 2.0}
{'loss': 3.0487, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.00746488571167, 'eval_accuracy@deu.rst.pcc': 0.11203319502074689, 'eval_f1@deu.rst.pcc': 0.02014906360559914, 'eval_precision@deu.rst.pcc': 0.01633676152906922, 'eval_recall@deu.rst.pcc': 0.046092796092796096, 'eval_loss@deu.rst.pcc': 3.007465124130249, 'eval_runtime': 3.1707, 'eval_samples_per_second': 76.01, 'eval_steps_per_second': 2.523, 'epoch': 2.0}
{'train@deu.rst.pcc_loss': 2.884415864944458, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.12939001848428835, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.03132447106108742, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.05882602117970838, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.055206626644296285, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.884416103363037, 'train@deu.rst.pcc_runtime': 26.1416, 'train@deu.rst.pcc_samples_per_second': 82.78, 'train@deu.rst.pcc_steps_per_second': 2.601, 'epoch': 3.0}
{'loss': 2.9442, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.944774866104126, 'eval_accuracy@deu.rst.pcc': 0.0954356846473029, 'eval_f1@deu.rst.pcc': 0.020230760074449204, 'eval_precision@deu.rst.pcc': 0.01404187428971064, 'eval_recall@deu.rst.pcc': 0.04351088726088726, 'eval_loss@deu.rst.pcc': 2.944774866104126, 'eval_runtime': 3.1876, 'eval_samples_per_second': 75.606, 'eval_steps_per_second': 2.51, 'epoch': 3.0}
{'train@deu.rst.pcc_loss': 2.832432270050049, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.16081330868761554, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.054741413393054904, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.04815327704497441, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.08303870977399284, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.832432270050049, 'train@deu.rst.pcc_runtime': 26.1496, 'train@deu.rst.pcc_samples_per_second': 82.755, 'train@deu.rst.pcc_steps_per_second': 2.6, 'epoch': 4.0}
{'loss': 2.8907, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.9009039402008057, 'eval_accuracy@deu.rst.pcc': 0.14522821576763487, 'eval_f1@deu.rst.pcc': 0.05089309370047076, 'eval_precision@deu.rst.pcc': 0.04099020402811387, 'eval_recall@deu.rst.pcc': 0.08368945868945869, 'eval_loss@deu.rst.pcc': 2.9009037017822266, 'eval_runtime': 3.1909, 'eval_samples_per_second': 75.526, 'eval_steps_per_second': 2.507, 'epoch': 4.0}
{'train@deu.rst.pcc_loss': 2.788729190826416, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.18114602587800369, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.06390927242935071, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.0765454654024679, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.10073764456369685, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.788729190826416, 'train@deu.rst.pcc_runtime': 26.1353, 'train@deu.rst.pcc_samples_per_second': 82.8, 'train@deu.rst.pcc_steps_per_second': 2.602, 'epoch': 5.0}
{'loss': 2.8428, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.864025354385376, 'eval_accuracy@deu.rst.pcc': 0.14522821576763487, 'eval_f1@deu.rst.pcc': 0.05525320471333759, 'eval_precision@deu.rst.pcc': 0.07819907761084231, 'eval_recall@deu.rst.pcc': 0.0940234533984534, 'eval_loss@deu.rst.pcc': 2.864025354385376, 'eval_runtime': 3.1886, 'eval_samples_per_second': 75.581, 'eval_steps_per_second': 2.509, 'epoch': 5.0}
{'train@deu.rst.pcc_loss': 2.748976707458496, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.19824399260628467, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.07395302243786464, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.09160186840252, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.11531941727642457, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.748976469039917, 'train@deu.rst.pcc_runtime': 26.2118, 'train@deu.rst.pcc_samples_per_second': 82.558, 'train@deu.rst.pcc_steps_per_second': 2.594, 'epoch': 6.0}
{'loss': 2.8027, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.8277013301849365, 'eval_accuracy@deu.rst.pcc': 0.13692946058091288, 'eval_f1@deu.rst.pcc': 0.053493810413465424, 'eval_precision@deu.rst.pcc': 0.07058938247663739, 'eval_recall@deu.rst.pcc': 0.09104726292226291, 'eval_loss@deu.rst.pcc': 2.8277010917663574, 'eval_runtime': 3.2054, 'eval_samples_per_second': 75.185, 'eval_steps_per_second': 2.496, 'epoch': 6.0}
{'train@deu.rst.pcc_loss': 2.719205617904663, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.205637707948244, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.07980720976591081, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.094816396773194, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.12160991478472452, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.719205617904663, 'train@deu.rst.pcc_runtime': 26.2525, 'train@deu.rst.pcc_samples_per_second': 82.43, 'train@deu.rst.pcc_steps_per_second': 2.59, 'epoch': 7.0}
{'loss': 2.7745, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.803635358810425, 'eval_accuracy@deu.rst.pcc': 0.14522821576763487, 'eval_f1@deu.rst.pcc': 0.0573678569231929, 'eval_precision@deu.rst.pcc': 0.07313699424725566, 'eval_recall@deu.rst.pcc': 0.10235424297924299, 'eval_loss@deu.rst.pcc': 2.8036351203918457, 'eval_runtime': 3.198, 'eval_samples_per_second': 75.359, 'eval_steps_per_second': 2.502, 'epoch': 7.0}
{'train@deu.rst.pcc_loss': 2.6954636573791504, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.20933456561922367, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08269738534302179, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.0938608738532006, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.12446420986974882, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6954636573791504, 'train@deu.rst.pcc_runtime': 26.1834, 'train@deu.rst.pcc_samples_per_second': 82.648, 'train@deu.rst.pcc_steps_per_second': 2.597, 'epoch': 8.0}
{'loss': 2.7408, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.785222053527832, 'eval_accuracy@deu.rst.pcc': 0.15352697095435686, 'eval_f1@deu.rst.pcc': 0.06324286985374446, 'eval_precision@deu.rst.pcc': 0.07505065200917736, 'eval_recall@deu.rst.pcc': 0.10704746642246642, 'eval_loss@deu.rst.pcc': 2.785221576690674, 'eval_runtime': 3.388, 'eval_samples_per_second': 71.133, 'eval_steps_per_second': 2.361, 'epoch': 8.0}
{'train@deu.rst.pcc_loss': 2.6765966415405273, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2144177449168207, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08754650987589366, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.1321708201515117, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.1288955545573252, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6765966415405273, 'train@deu.rst.pcc_runtime': 26.1753, 'train@deu.rst.pcc_samples_per_second': 82.673, 'train@deu.rst.pcc_steps_per_second': 2.598, 'epoch': 9.0}
{'loss': 2.7233, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.7724947929382324, 'eval_accuracy@deu.rst.pcc': 0.15352697095435686, 'eval_f1@deu.rst.pcc': 0.06725735678810664, 'eval_precision@deu.rst.pcc': 0.07462352462352462, 'eval_recall@deu.rst.pcc': 0.10876449938949939, 'eval_loss@deu.rst.pcc': 2.7724945545196533, 'eval_runtime': 3.1961, 'eval_samples_per_second': 75.405, 'eval_steps_per_second': 2.503, 'epoch': 9.0}
{'train@deu.rst.pcc_loss': 2.6640524864196777, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.22042513863216265, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09124295336895943, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.1366370199863701, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.13315441874684447, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6640522480010986, 'train@deu.rst.pcc_runtime': 26.1294, 'train@deu.rst.pcc_samples_per_second': 82.818, 'train@deu.rst.pcc_steps_per_second': 2.602, 'epoch': 10.0}
{'loss': 2.7072, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.759979009628296, 'eval_accuracy@deu.rst.pcc': 0.16597510373443983, 'eval_f1@deu.rst.pcc': 0.07323339897124873, 'eval_precision@deu.rst.pcc': 0.08900315467266366, 'eval_recall@deu.rst.pcc': 0.11955636955636954, 'eval_loss@deu.rst.pcc': 2.759979486465454, 'eval_runtime': 3.1997, 'eval_samples_per_second': 75.319, 'eval_steps_per_second': 2.5, 'epoch': 10.0}
{'train@deu.rst.pcc_loss': 2.6564793586730957, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.22412199630314233, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09274020236013683, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.13908847262357635, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.1352222841952553, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6564793586730957, 'train@deu.rst.pcc_runtime': 26.1702, 'train@deu.rst.pcc_samples_per_second': 82.689, 'train@deu.rst.pcc_steps_per_second': 2.598, 'epoch': 11.0}
{'loss': 2.6957, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.7539031505584717, 'eval_accuracy@deu.rst.pcc': 0.16597510373443983, 'eval_f1@deu.rst.pcc': 0.07221749107149648, 'eval_precision@deu.rst.pcc': 0.08836705135335558, 'eval_recall@deu.rst.pcc': 0.1178393365893366, 'eval_loss@deu.rst.pcc': 2.75390362739563, 'eval_runtime': 3.1913, 'eval_samples_per_second': 75.518, 'eval_steps_per_second': 2.507, 'epoch': 11.0}
{'train@deu.rst.pcc_loss': 2.653992176055908, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2231977818853974, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09208443868694284, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.11629241112237605, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.1344850675102503, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.653992176055908, 'train@deu.rst.pcc_runtime': 26.1727, 'train@deu.rst.pcc_samples_per_second': 82.682, 'train@deu.rst.pcc_steps_per_second': 2.598, 'epoch': 12.0}
{'loss': 2.687, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.7513296604156494, 'eval_accuracy@deu.rst.pcc': 0.17012448132780084, 'eval_f1@deu.rst.pcc': 0.07584699206941854, 'eval_precision@deu.rst.pcc': 0.08980725080581402, 'eval_recall@deu.rst.pcc': 0.12104446479446479, 'eval_loss@deu.rst.pcc': 2.7513296604156494, 'eval_runtime': 3.1843, 'eval_samples_per_second': 75.684, 'eval_steps_per_second': 2.512, 'epoch': 12.0}
{'train_runtime': 1012.0768, 'train_samples_per_second': 25.658, 'train_steps_per_second': 0.806, 'train_loss': 2.845872261944939, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.8459
  train_runtime            = 0:16:52.07
  train_samples_per_second =     25.658
  train_steps_per_second   =      0.806
{'train@spa.rst.sctb_loss': 3.119088649749756, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.12300683371298406, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02119664755534321, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.017215193277691554, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05478632478632478, 'train@spa.rst.sctb_loss@spa.rst.sctb': 3.1190884113311768, 'train@spa.rst.sctb_runtime': 5.5516, 'train@spa.rst.sctb_samples_per_second': 79.076, 'train@spa.rst.sctb_steps_per_second': 2.522, 'epoch': 1.0}
{'loss': 3.3654, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.1534667015075684, 'eval_accuracy@spa.rst.sctb': 0.07446808510638298, 'eval_f1@spa.rst.sctb': 0.013682003751517157, 'eval_precision@spa.rst.sctb': 0.009975520195838434, 'eval_recall@spa.rst.sctb': 0.0303030303030303, 'eval_loss@spa.rst.sctb': 3.153465509414673, 'eval_runtime': 1.4342, 'eval_samples_per_second': 65.541, 'eval_steps_per_second': 2.092, 'epoch': 1.0}
{'train@spa.rst.sctb_loss': 2.7719109058380127, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.30751708428246016, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.027634079552147278, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.021768716189696072, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04273504273504274, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.7719109058380127, 'train@spa.rst.sctb_runtime': 5.5409, 'train@spa.rst.sctb_samples_per_second': 79.229, 'train@spa.rst.sctb_steps_per_second': 2.527, 'epoch': 2.0}
{'loss': 2.9594, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.803638219833374, 'eval_accuracy@spa.rst.sctb': 0.3191489361702128, 'eval_f1@spa.rst.sctb': 0.034759358288770054, 'eval_precision@spa.rst.sctb': 0.025900393015913927, 'eval_recall@spa.rst.sctb': 0.05704099821746881, 'eval_loss@spa.rst.sctb': 2.803637981414795, 'eval_runtime': 1.4426, 'eval_samples_per_second': 65.161, 'eval_steps_per_second': 2.08, 'epoch': 2.0}
{'train@spa.rst.sctb_loss': 2.5268876552581787, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.33712984054669703, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02101078932424759, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.014111365369946607, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04111111111111111, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.5268874168395996, 'train@spa.rst.sctb_runtime': 5.5491, 'train@spa.rst.sctb_samples_per_second': 79.112, 'train@spa.rst.sctb_steps_per_second': 2.523, 'epoch': 3.0}
{'loss': 2.6859, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.561826229095459, 'eval_accuracy@spa.rst.sctb': 0.35106382978723405, 'eval_f1@spa.rst.sctb': 0.03056970819823993, 'eval_precision@spa.rst.sctb': 0.02065081351689612, 'eval_recall@spa.rst.sctb': 0.058823529411764705, 'eval_loss@spa.rst.sctb': 2.561826229095459, 'eval_runtime': 1.4279, 'eval_samples_per_second': 65.831, 'eval_steps_per_second': 2.101, 'epoch': 3.0}
{'train@spa.rst.sctb_loss': 2.386592149734497, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3416856492027335, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.021222410865874366, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.014236902050113895, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.041666666666666664, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.3865928649902344, 'train@spa.rst.sctb_runtime': 5.5328, 'train@spa.rst.sctb_samples_per_second': 79.345, 'train@spa.rst.sctb_steps_per_second': 2.53, 'epoch': 4.0}
{'loss': 2.48, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.4252352714538574, 'eval_accuracy@spa.rst.sctb': 0.35106382978723405, 'eval_f1@spa.rst.sctb': 0.03056970819823993, 'eval_precision@spa.rst.sctb': 0.02065081351689612, 'eval_recall@spa.rst.sctb': 0.058823529411764705, 'eval_loss@spa.rst.sctb': 2.4252352714538574, 'eval_runtime': 1.4323, 'eval_samples_per_second': 65.629, 'eval_steps_per_second': 2.095, 'epoch': 4.0}
{'train@spa.rst.sctb_loss': 2.3033688068389893, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3416856492027335, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.021222410865874366, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.014236902050113895, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.041666666666666664, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.30336856842041, 'train@spa.rst.sctb_runtime': 5.5473, 'train@spa.rst.sctb_samples_per_second': 79.138, 'train@spa.rst.sctb_steps_per_second': 2.524, 'epoch': 5.0}
{'loss': 2.3627, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.3475239276885986, 'eval_accuracy@spa.rst.sctb': 0.35106382978723405, 'eval_f1@spa.rst.sctb': 0.03056970819823993, 'eval_precision@spa.rst.sctb': 0.02065081351689612, 'eval_recall@spa.rst.sctb': 0.058823529411764705, 'eval_loss@spa.rst.sctb': 2.3475239276885986, 'eval_runtime': 1.4326, 'eval_samples_per_second': 65.616, 'eval_steps_per_second': 2.094, 'epoch': 5.0}
{'train@spa.rst.sctb_loss': 2.248558759689331, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3416856492027335, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.021222410865874366, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.014236902050113895, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.041666666666666664, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.248558759689331, 'train@spa.rst.sctb_runtime': 5.5364, 'train@spa.rst.sctb_samples_per_second': 79.294, 'train@spa.rst.sctb_steps_per_second': 2.529, 'epoch': 6.0}
{'loss': 2.2977, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.3010380268096924, 'eval_accuracy@spa.rst.sctb': 0.35106382978723405, 'eval_f1@spa.rst.sctb': 0.03056970819823993, 'eval_precision@spa.rst.sctb': 0.02065081351689612, 'eval_recall@spa.rst.sctb': 0.058823529411764705, 'eval_loss@spa.rst.sctb': 2.3010377883911133, 'eval_runtime': 1.4154, 'eval_samples_per_second': 66.414, 'eval_steps_per_second': 2.12, 'epoch': 6.0}
{'train@spa.rst.sctb_loss': 2.208336591720581, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.357630979498861, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.027868559324365216, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.04231720010408535, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.044973118279569894, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.208336114883423, 'train@spa.rst.sctb_runtime': 6.005, 'train@spa.rst.sctb_samples_per_second': 73.106, 'train@spa.rst.sctb_steps_per_second': 2.331, 'epoch': 7.0}
{'loss': 2.2583, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.267648696899414, 'eval_accuracy@spa.rst.sctb': 0.35106382978723405, 'eval_f1@spa.rst.sctb': 0.03081232492997199, 'eval_precision@spa.rst.sctb': 0.020872865275142316, 'eval_recall@spa.rst.sctb': 0.058823529411764705, 'eval_loss@spa.rst.sctb': 2.2676491737365723, 'eval_runtime': 1.4304, 'eval_samples_per_second': 65.715, 'eval_steps_per_second': 2.097, 'epoch': 7.0}
{'train@spa.rst.sctb_loss': 2.1820013523101807, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3712984054669704, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03209424676715262, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.04263024986709197, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.047661290322580645, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.1820013523101807, 'train@spa.rst.sctb_runtime': 5.5424, 'train@spa.rst.sctb_samples_per_second': 79.208, 'train@spa.rst.sctb_steps_per_second': 2.526, 'epoch': 8.0}
{'loss': 2.2162, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.246643543243408, 'eval_accuracy@spa.rst.sctb': 0.35106382978723405, 'eval_f1@spa.rst.sctb': 0.03570812489218561, 'eval_precision@spa.rst.sctb': 0.040293040293040296, 'eval_recall@spa.rst.sctb': 0.060136973449666946, 'eval_loss@spa.rst.sctb': 2.2466437816619873, 'eval_runtime': 1.4259, 'eval_samples_per_second': 65.923, 'eval_steps_per_second': 2.104, 'epoch': 8.0}
{'train@spa.rst.sctb_loss': 2.1622743606567383, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3735763097949886, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03343174755540041, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.0385866612333605, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04844982078853047, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.162274122238159, 'train@spa.rst.sctb_runtime': 5.5431, 'train@spa.rst.sctb_samples_per_second': 79.197, 'train@spa.rst.sctb_steps_per_second': 2.526, 'epoch': 9.0}
{'loss': 2.2007, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.2299492359161377, 'eval_accuracy@spa.rst.sctb': 0.3723404255319149, 'eval_f1@spa.rst.sctb': 0.04523091881380651, 'eval_precision@spa.rst.sctb': 0.05080213903743316, 'eval_recall@spa.rst.sctb': 0.06632892391406324, 'eval_loss@spa.rst.sctb': 2.2299487590789795, 'eval_runtime': 1.4113, 'eval_samples_per_second': 66.606, 'eval_steps_per_second': 2.126, 'epoch': 9.0}
{'train@spa.rst.sctb_loss': 2.148792028427124, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.38496583143507973, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03644006590380753, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03814071808404302, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.050860215053763445, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.148791551589966, 'train@spa.rst.sctb_runtime': 5.5624, 'train@spa.rst.sctb_samples_per_second': 78.922, 'train@spa.rst.sctb_steps_per_second': 2.517, 'epoch': 10.0}
{'loss': 2.1737, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.2193548679351807, 'eval_accuracy@spa.rst.sctb': 0.3829787234042553, 'eval_f1@spa.rst.sctb': 0.049065377785100964, 'eval_precision@spa.rst.sctb': 0.05129958960328317, 'eval_recall@spa.rst.sctb': 0.06942489914626138, 'eval_loss@spa.rst.sctb': 2.2193548679351807, 'eval_runtime': 1.426, 'eval_samples_per_second': 65.919, 'eval_steps_per_second': 2.104, 'epoch': 10.0}
{'train@spa.rst.sctb_loss': 2.1411502361297607, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.38724373576309795, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03692269916739213, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.038128116609129264, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05130824372759857, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.1411502361297607, 'train@spa.rst.sctb_runtime': 5.5627, 'train@spa.rst.sctb_samples_per_second': 78.919, 'train@spa.rst.sctb_steps_per_second': 2.517, 'epoch': 11.0}
{'loss': 2.175, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.2130706310272217, 'eval_accuracy@spa.rst.sctb': 0.39361702127659576, 'eval_f1@spa.rst.sctb': 0.052912690499928784, 'eval_precision@spa.rst.sctb': 0.05482506728181469, 'eval_recall@spa.rst.sctb': 0.07252087437845951, 'eval_loss@spa.rst.sctb': 2.213070869445801, 'eval_runtime': 1.4406, 'eval_samples_per_second': 65.253, 'eval_steps_per_second': 2.083, 'epoch': 11.0}
{'train@spa.rst.sctb_loss': 2.138509750366211, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3940774487471526, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03851915304867335, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03945487769575916, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05265232974910394, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.138509750366211, 'train@spa.rst.sctb_runtime': 5.5498, 'train@spa.rst.sctb_samples_per_second': 79.101, 'train@spa.rst.sctb_steps_per_second': 2.523, 'epoch': 12.0}
{'loss': 2.1768, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.2111656665802, 'eval_accuracy@spa.rst.sctb': 0.39361702127659576, 'eval_f1@spa.rst.sctb': 0.052912690499928784, 'eval_precision@spa.rst.sctb': 0.05482506728181469, 'eval_recall@spa.rst.sctb': 0.07252087437845951, 'eval_loss@spa.rst.sctb': 2.2111663818359375, 'eval_runtime': 1.4229, 'eval_samples_per_second': 66.06, 'eval_steps_per_second': 2.108, 'epoch': 12.0}
{'train_runtime': 218.036, 'train_samples_per_second': 24.161, 'train_steps_per_second': 0.771, 'train_loss': 2.445974724633353, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.8459
  train_runtime            = 0:16:52.07
  train_samples_per_second =     25.658
  train_steps_per_second   =      0.806
-------------------------------------------------------------------
Lang1:  eng.pdtb.pdtb    Lang2:  spa.rst.sctb
Saving run to:  runs/full_shot/FullShot=v4_eng.pdtb.pdtb_spa.rst.sctb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 43920 examples
read 1674 examples
read 2257 examples
read 439 examples
read 94 examples
read 159 examples
Total prediction labels:  48
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=48, bias=True)
    )
  )
)
{'train@eng.pdtb.pdtb_loss': 1.2753797769546509, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.5922586520947176, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.2611225419802789, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.3562787261395188, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.25326534016731395, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.2753797769546509, 'train@eng.pdtb.pdtb_runtime': 519.2578, 'train@eng.pdtb.pdtb_samples_per_second': 84.582, 'train@eng.pdtb.pdtb_steps_per_second': 2.644, 'epoch': 1.0}
{'loss': 1.89, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.2030673027038574, 'eval_accuracy@eng.pdtb.pdtb': 0.6218637992831542, 'eval_f1@eng.pdtb.pdtb': 0.30461166351257557, 'eval_precision@eng.pdtb.pdtb': 0.3803166196854464, 'eval_recall@eng.pdtb.pdtb': 0.298931098219682, 'eval_loss@eng.pdtb.pdtb': 1.2030673027038574, 'eval_runtime': 20.1927, 'eval_samples_per_second': 82.901, 'eval_steps_per_second': 2.625, 'epoch': 1.0}
{'train@eng.pdtb.pdtb_loss': 1.1027547121047974, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6370446265938069, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.3684297792212341, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.4453590664756409, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.3571905845869782, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.1027545928955078, 'train@eng.pdtb.pdtb_runtime': 519.3923, 'train@eng.pdtb.pdtb_samples_per_second': 84.56, 'train@eng.pdtb.pdtb_steps_per_second': 2.643, 'epoch': 2.0}
{'loss': 1.2271, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.0362406969070435, 'eval_accuracy@eng.pdtb.pdtb': 0.6636798088410991, 'eval_f1@eng.pdtb.pdtb': 0.4140437899099464, 'eval_precision@eng.pdtb.pdtb': 0.4889337844414765, 'eval_recall@eng.pdtb.pdtb': 0.39989428783077174, 'eval_loss@eng.pdtb.pdtb': 1.036240816116333, 'eval_runtime': 20.2235, 'eval_samples_per_second': 82.775, 'eval_steps_per_second': 2.621, 'epoch': 2.0}
{'train@eng.pdtb.pdtb_loss': 1.0519769191741943, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6505692167577414, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.43099464586677244, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.4671040349785453, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.41614677329456873, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.0519769191741943, 'train@eng.pdtb.pdtb_runtime': 519.3581, 'train@eng.pdtb.pdtb_samples_per_second': 84.566, 'train@eng.pdtb.pdtb_steps_per_second': 2.644, 'epoch': 3.0}
{'loss': 1.1235, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 0.9957900047302246, 'eval_accuracy@eng.pdtb.pdtb': 0.6839904420549582, 'eval_f1@eng.pdtb.pdtb': 0.4845857214194484, 'eval_precision@eng.pdtb.pdtb': 0.5660738434231362, 'eval_recall@eng.pdtb.pdtb': 0.46193493207748276, 'eval_loss@eng.pdtb.pdtb': 0.9957901239395142, 'eval_runtime': 20.258, 'eval_samples_per_second': 82.634, 'eval_steps_per_second': 2.616, 'epoch': 3.0}
{'train@eng.pdtb.pdtb_loss': 1.0051496028900146, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.664139344262295, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.44897982276263176, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5139366456006214, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.44546759656844714, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.0051496028900146, 'train@eng.pdtb.pdtb_runtime': 524.6725, 'train@eng.pdtb.pdtb_samples_per_second': 83.709, 'train@eng.pdtb.pdtb_steps_per_second': 2.617, 'epoch': 4.0}
{'loss': 1.0765, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 0.9579839110374451, 'eval_accuracy@eng.pdtb.pdtb': 0.6833930704898447, 'eval_f1@eng.pdtb.pdtb': 0.5136349785440147, 'eval_precision@eng.pdtb.pdtb': 0.5385138336950313, 'eval_recall@eng.pdtb.pdtb': 0.5150602485911235, 'eval_loss@eng.pdtb.pdtb': 0.9579839110374451, 'eval_runtime': 20.1475, 'eval_samples_per_second': 83.087, 'eval_steps_per_second': 2.631, 'epoch': 4.0}
{'train@eng.pdtb.pdtb_loss': 0.9816091656684875, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6716757741347905, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.45758712617430214, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5193666444125781, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4553046160639243, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9816091656684875, 'train@eng.pdtb.pdtb_runtime': 518.2021, 'train@eng.pdtb.pdtb_samples_per_second': 84.755, 'train@eng.pdtb.pdtb_steps_per_second': 2.65, 'epoch': 5.0}
{'loss': 1.047, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 0.9388433694839478, 'eval_accuracy@eng.pdtb.pdtb': 0.6821983273596177, 'eval_f1@eng.pdtb.pdtb': 0.5115577962126527, 'eval_precision@eng.pdtb.pdtb': 0.5310249042204548, 'eval_recall@eng.pdtb.pdtb': 0.5109525407375671, 'eval_loss@eng.pdtb.pdtb': 0.9388433694839478, 'eval_runtime': 20.167, 'eval_samples_per_second': 83.007, 'eval_steps_per_second': 2.628, 'epoch': 5.0}
{'train@eng.pdtb.pdtb_loss': 0.9623204469680786, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6770491803278689, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4647811178202013, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.525126913657013, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.46130225559593824, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9623205065727234, 'train@eng.pdtb.pdtb_runtime': 520.1858, 'train@eng.pdtb.pdtb_samples_per_second': 84.431, 'train@eng.pdtb.pdtb_steps_per_second': 2.639, 'epoch': 6.0}
{'loss': 1.0257, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 0.9324176907539368, 'eval_accuracy@eng.pdtb.pdtb': 0.6821983273596177, 'eval_f1@eng.pdtb.pdtb': 0.5186270160270359, 'eval_precision@eng.pdtb.pdtb': 0.5423011487333109, 'eval_recall@eng.pdtb.pdtb': 0.5136837931382969, 'eval_loss@eng.pdtb.pdtb': 0.9324177503585815, 'eval_runtime': 20.2342, 'eval_samples_per_second': 82.731, 'eval_steps_per_second': 2.619, 'epoch': 6.0}
{'train@eng.pdtb.pdtb_loss': 0.9503054022789001, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6790983606557377, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4683516798029037, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5332430360081309, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.46132514530546836, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9503053426742554, 'train@eng.pdtb.pdtb_runtime': 519.0972, 'train@eng.pdtb.pdtb_samples_per_second': 84.608, 'train@eng.pdtb.pdtb_steps_per_second': 2.645, 'epoch': 7.0}
{'loss': 1.0092, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 0.9175544381141663, 'eval_accuracy@eng.pdtb.pdtb': 0.6911589008363201, 'eval_f1@eng.pdtb.pdtb': 0.5307635867983325, 'eval_precision@eng.pdtb.pdtb': 0.5709078644185858, 'eval_recall@eng.pdtb.pdtb': 0.5159671805050496, 'eval_loss@eng.pdtb.pdtb': 0.9175543189048767, 'eval_runtime': 20.173, 'eval_samples_per_second': 82.982, 'eval_steps_per_second': 2.627, 'epoch': 7.0}
{'train@eng.pdtb.pdtb_loss': 0.9390528202056885, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6828096539162113, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.49073366576359684, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5699175826553979, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4835025975801589, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9390529990196228, 'train@eng.pdtb.pdtb_runtime': 518.7015, 'train@eng.pdtb.pdtb_samples_per_second': 84.673, 'train@eng.pdtb.pdtb_steps_per_second': 2.647, 'epoch': 8.0}
{'loss': 0.9973, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 0.9142282009124756, 'eval_accuracy@eng.pdtb.pdtb': 0.6881720430107527, 'eval_f1@eng.pdtb.pdtb': 0.5400968565003311, 'eval_precision@eng.pdtb.pdtb': 0.5583083486824171, 'eval_recall@eng.pdtb.pdtb': 0.5391954513119279, 'eval_loss@eng.pdtb.pdtb': 0.9142282009124756, 'eval_runtime': 20.2118, 'eval_samples_per_second': 82.823, 'eval_steps_per_second': 2.622, 'epoch': 8.0}
{'train@eng.pdtb.pdtb_loss': 0.9305735230445862, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6856329690346084, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4949878849309198, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5774121465308599, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4843730562231688, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9305735230445862, 'train@eng.pdtb.pdtb_runtime': 518.8948, 'train@eng.pdtb.pdtb_samples_per_second': 84.641, 'train@eng.pdtb.pdtb_steps_per_second': 2.646, 'epoch': 9.0}
{'loss': 0.9864, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 0.9128890633583069, 'eval_accuracy@eng.pdtb.pdtb': 0.6905615292712067, 'eval_f1@eng.pdtb.pdtb': 0.5613702809862637, 'eval_precision@eng.pdtb.pdtb': 0.6237665567887868, 'eval_recall@eng.pdtb.pdtb': 0.5489862943919352, 'eval_loss@eng.pdtb.pdtb': 0.9128891229629517, 'eval_runtime': 20.2005, 'eval_samples_per_second': 82.869, 'eval_steps_per_second': 2.624, 'epoch': 9.0}
{'train@eng.pdtb.pdtb_loss': 0.9268226623535156, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6859972677595628, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4947553635488992, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5782052716619127, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4835713038014805, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9268225431442261, 'train@eng.pdtb.pdtb_runtime': 519.2596, 'train@eng.pdtb.pdtb_samples_per_second': 84.582, 'train@eng.pdtb.pdtb_steps_per_second': 2.644, 'epoch': 10.0}
{'loss': 0.9799, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 0.9064435362815857, 'eval_accuracy@eng.pdtb.pdtb': 0.6905615292712067, 'eval_f1@eng.pdtb.pdtb': 0.5563829798703276, 'eval_precision@eng.pdtb.pdtb': 0.6202136334062712, 'eval_recall@eng.pdtb.pdtb': 0.5430740414624351, 'eval_loss@eng.pdtb.pdtb': 0.9064435958862305, 'eval_runtime': 20.2313, 'eval_samples_per_second': 82.743, 'eval_steps_per_second': 2.62, 'epoch': 10.0}
{'train@eng.pdtb.pdtb_loss': 0.9225994348526001, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6875227686703097, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4970409085199999, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5703388895608466, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.48753560099767596, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9225993156433105, 'train@eng.pdtb.pdtb_runtime': 520.0222, 'train@eng.pdtb.pdtb_samples_per_second': 84.458, 'train@eng.pdtb.pdtb_steps_per_second': 2.64, 'epoch': 11.0}
{'loss': 0.9732, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.90701824426651, 'eval_accuracy@eng.pdtb.pdtb': 0.6935483870967742, 'eval_f1@eng.pdtb.pdtb': 0.5646353801883889, 'eval_precision@eng.pdtb.pdtb': 0.6240871405560731, 'eval_recall@eng.pdtb.pdtb': 0.5513902941096939, 'eval_loss@eng.pdtb.pdtb': 0.90701824426651, 'eval_runtime': 20.1609, 'eval_samples_per_second': 83.032, 'eval_steps_per_second': 2.629, 'epoch': 11.0}
{'train@eng.pdtb.pdtb_loss': 0.9211416840553284, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6880009107468124, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4974482343206886, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5788483668237595, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.48599983217674386, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9211418032646179, 'train@eng.pdtb.pdtb_runtime': 517.8304, 'train@eng.pdtb.pdtb_samples_per_second': 84.815, 'train@eng.pdtb.pdtb_steps_per_second': 2.651, 'epoch': 12.0}
{'loss': 0.9704, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.9050084352493286, 'eval_accuracy@eng.pdtb.pdtb': 0.6917562724014337, 'eval_f1@eng.pdtb.pdtb': 0.5611938956875309, 'eval_precision@eng.pdtb.pdtb': 0.6230529940593883, 'eval_recall@eng.pdtb.pdtb': 0.5481665428620495, 'eval_loss@eng.pdtb.pdtb': 0.9050084948539734, 'eval_runtime': 20.1943, 'eval_samples_per_second': 82.894, 'eval_steps_per_second': 2.624, 'epoch': 12.0}
{'train_runtime': 19594.0972, 'train_samples_per_second': 26.898, 'train_steps_per_second': 0.841, 'train_loss': 1.1088565419848377, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.1089
  train_runtime            = 5:26:34.09
  train_samples_per_second =     26.898
  train_steps_per_second   =      0.841
{'train@spa.rst.sctb_loss': 4.115069389343262, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.018223234624145785, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.015228506289809624, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.019387802189220628, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.06606412553993199, 'train@spa.rst.sctb_loss@spa.rst.sctb': 4.115070343017578, 'train@spa.rst.sctb_runtime': 5.7271, 'train@spa.rst.sctb_samples_per_second': 76.653, 'train@spa.rst.sctb_steps_per_second': 2.445, 'epoch': 1.0}
{'loss': 4.7964, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 4.259124279022217, 'eval_accuracy@spa.rst.sctb': 0.0, 'eval_f1@spa.rst.sctb': 0.0, 'eval_precision@spa.rst.sctb': 0.0, 'eval_recall@spa.rst.sctb': 0.0, 'eval_loss@spa.rst.sctb': 4.259124279022217, 'eval_runtime': 1.5578, 'eval_samples_per_second': 60.34, 'eval_steps_per_second': 1.926, 'epoch': 1.0}
{'train@spa.rst.sctb_loss': 3.2986996173858643, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.05011389521640091, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02573858889648364, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.023392634645831578, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.039663174340593695, 'train@spa.rst.sctb_loss@spa.rst.sctb': 3.298699140548706, 'train@spa.rst.sctb_runtime': 5.7021, 'train@spa.rst.sctb_samples_per_second': 76.989, 'train@spa.rst.sctb_steps_per_second': 2.455, 'epoch': 2.0}
{'loss': 3.6613, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.3089983463287354, 'eval_accuracy@spa.rst.sctb': 0.031914893617021274, 'eval_f1@spa.rst.sctb': 0.008522727272727272, 'eval_precision@spa.rst.sctb': 0.006493506493506493, 'eval_recall@spa.rst.sctb': 0.012396694214876032, 'eval_loss@spa.rst.sctb': 3.3089988231658936, 'eval_runtime': 1.5655, 'eval_samples_per_second': 60.045, 'eval_steps_per_second': 1.916, 'epoch': 2.0}
{'train@spa.rst.sctb_loss': 2.826700448989868, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.16856492027334852, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.04794270744301155, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.051578987242414005, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05474508317250252, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.82669997215271, 'train@spa.rst.sctb_runtime': 5.7042, 'train@spa.rst.sctb_samples_per_second': 76.96, 'train@spa.rst.sctb_steps_per_second': 2.454, 'epoch': 3.0}
{'loss': 3.0847, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.8063838481903076, 'eval_accuracy@spa.rst.sctb': 0.10638297872340426, 'eval_f1@spa.rst.sctb': 0.024694564694564696, 'eval_precision@spa.rst.sctb': 0.04004329004329005, 'eval_recall@spa.rst.sctb': 0.025214551530341002, 'eval_loss@spa.rst.sctb': 2.8063838481903076, 'eval_runtime': 1.573, 'eval_samples_per_second': 59.76, 'eval_steps_per_second': 1.907, 'epoch': 3.0}
{'train@spa.rst.sctb_loss': 2.5153820514678955, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.30068337129840544, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.06356518933553118, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.06805102234298517, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.06797284256961676, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.5153815746307373, 'train@spa.rst.sctb_runtime': 5.6861, 'train@spa.rst.sctb_samples_per_second': 77.206, 'train@spa.rst.sctb_steps_per_second': 2.462, 'epoch': 4.0}
{'loss': 2.724, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.509838104248047, 'eval_accuracy@spa.rst.sctb': 0.2765957446808511, 'eval_f1@spa.rst.sctb': 0.0539939373663027, 'eval_precision@spa.rst.sctb': 0.04755411255411256, 'eval_recall@spa.rst.sctb': 0.06682615629984051, 'eval_loss@spa.rst.sctb': 2.509838104248047, 'eval_runtime': 1.5571, 'eval_samples_per_second': 60.368, 'eval_steps_per_second': 1.927, 'epoch': 4.0}
{'train@spa.rst.sctb_loss': 2.3147594928741455, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4054669703872437, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.0635123937808558, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.07124247551474135, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.07147240602885764, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.3147597312927246, 'train@spa.rst.sctb_runtime': 5.6974, 'train@spa.rst.sctb_samples_per_second': 77.053, 'train@spa.rst.sctb_steps_per_second': 2.457, 'epoch': 5.0}
{'loss': 2.4521, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.3265221118927, 'eval_accuracy@spa.rst.sctb': 0.3829787234042553, 'eval_f1@spa.rst.sctb': 0.06927015976277254, 'eval_precision@spa.rst.sctb': 0.06180873379099923, 'eval_recall@spa.rst.sctb': 0.08226307395282464, 'eval_loss@spa.rst.sctb': 2.3265225887298584, 'eval_runtime': 1.5577, 'eval_samples_per_second': 60.345, 'eval_steps_per_second': 1.926, 'epoch': 5.0}
{'train@spa.rst.sctb_loss': 2.187091588973999, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4168564920273349, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.06038508404114253, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.09014802585534971, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.06869876390037681, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.187091588973999, 'train@spa.rst.sctb_runtime': 5.692, 'train@spa.rst.sctb_samples_per_second': 77.126, 'train@spa.rst.sctb_steps_per_second': 2.46, 'epoch': 6.0}
{'loss': 2.3056, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.217891216278076, 'eval_accuracy@spa.rst.sctb': 0.4574468085106383, 'eval_f1@spa.rst.sctb': 0.07733619763694952, 'eval_precision@spa.rst.sctb': 0.08499697519661223, 'eval_recall@spa.rst.sctb': 0.0925879291530261, 'eval_loss@spa.rst.sctb': 2.217890739440918, 'eval_runtime': 1.5638, 'eval_samples_per_second': 60.111, 'eval_steps_per_second': 1.918, 'epoch': 6.0}
{'train@spa.rst.sctb_loss': 2.105172634124756, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4419134396355353, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.06635599208414743, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.13796904780744312, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.07363091955833892, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.105172634124756, 'train@spa.rst.sctb_runtime': 5.6923, 'train@spa.rst.sctb_samples_per_second': 77.122, 'train@spa.rst.sctb_steps_per_second': 2.459, 'epoch': 7.0}
{'loss': 2.1977, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.1532809734344482, 'eval_accuracy@spa.rst.sctb': 0.4787234042553192, 'eval_f1@spa.rst.sctb': 0.08832658569500675, 'eval_precision@spa.rst.sctb': 0.11473532683373684, 'eval_recall@spa.rst.sctb': 0.1057322450511305, 'eval_loss@spa.rst.sctb': 2.15328049659729, 'eval_runtime': 1.5355, 'eval_samples_per_second': 61.219, 'eval_steps_per_second': 1.954, 'epoch': 7.0}
{'train@spa.rst.sctb_loss': 2.0549728870391846, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.45558086560364464, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.07105760289804236, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.14579641403891094, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.0770039354313548, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.0549728870391846, 'train@spa.rst.sctb_runtime': 5.6836, 'train@spa.rst.sctb_samples_per_second': 77.24, 'train@spa.rst.sctb_steps_per_second': 2.463, 'epoch': 8.0}
{'loss': 2.1379, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.117902994155884, 'eval_accuracy@spa.rst.sctb': 0.46808510638297873, 'eval_f1@spa.rst.sctb': 0.07965686274509805, 'eval_precision@spa.rst.sctb': 0.11429859300041856, 'eval_recall@spa.rst.sctb': 0.1003846514682428, 'eval_loss@spa.rst.sctb': 2.117902994155884, 'eval_runtime': 1.5576, 'eval_samples_per_second': 60.349, 'eval_steps_per_second': 1.926, 'epoch': 8.0}
{'train@spa.rst.sctb_loss': 2.0204830169677734, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.46697038724373574, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.08467993892412497, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.14925595238095238, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.08632762548085128, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.0204832553863525, 'train@spa.rst.sctb_runtime': 5.6864, 'train@spa.rst.sctb_samples_per_second': 77.202, 'train@spa.rst.sctb_steps_per_second': 2.462, 'epoch': 9.0}
{'loss': 2.1224, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.0931031703948975, 'eval_accuracy@spa.rst.sctb': 0.46808510638297873, 'eval_f1@spa.rst.sctb': 0.07965686274509805, 'eval_precision@spa.rst.sctb': 0.11429859300041856, 'eval_recall@spa.rst.sctb': 0.1003846514682428, 'eval_loss@spa.rst.sctb': 2.0931031703948975, 'eval_runtime': 1.5585, 'eval_samples_per_second': 60.315, 'eval_steps_per_second': 1.925, 'epoch': 9.0}
{'train@spa.rst.sctb_loss': 1.9972608089447021, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.47380410022779046, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.08792222577337445, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.15338924914896646, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.08886730802053383, 'train@spa.rst.sctb_loss@spa.rst.sctb': 1.9972608089447021, 'train@spa.rst.sctb_runtime': 5.6763, 'train@spa.rst.sctb_samples_per_second': 77.339, 'train@spa.rst.sctb_steps_per_second': 2.466, 'epoch': 10.0}
{'loss': 2.0804, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.0783145427703857, 'eval_accuracy@spa.rst.sctb': 0.46808510638297873, 'eval_f1@spa.rst.sctb': 0.07965686274509805, 'eval_precision@spa.rst.sctb': 0.11429859300041856, 'eval_recall@spa.rst.sctb': 0.1003846514682428, 'eval_loss@spa.rst.sctb': 2.078315019607544, 'eval_runtime': 1.5644, 'eval_samples_per_second': 60.088, 'eval_steps_per_second': 1.918, 'epoch': 10.0}
{'train@spa.rst.sctb_loss': 1.9842453002929688, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4760820045558087, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.0905262009109819, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.15355323245248464, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.09085143500466081, 'train@spa.rst.sctb_loss@spa.rst.sctb': 1.9842453002929688, 'train@spa.rst.sctb_runtime': 5.6948, 'train@spa.rst.sctb_samples_per_second': 77.088, 'train@spa.rst.sctb_steps_per_second': 2.458, 'epoch': 11.0}
{'loss': 2.0694, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.0691730976104736, 'eval_accuracy@spa.rst.sctb': 0.46808510638297873, 'eval_f1@spa.rst.sctb': 0.07965686274509805, 'eval_precision@spa.rst.sctb': 0.11429859300041856, 'eval_recall@spa.rst.sctb': 0.1003846514682428, 'eval_loss@spa.rst.sctb': 2.0691726207733154, 'eval_runtime': 1.5367, 'eval_samples_per_second': 61.17, 'eval_steps_per_second': 1.952, 'epoch': 11.0}
{'train@spa.rst.sctb_loss': 1.9797972440719604, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4760820045558087, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.0905262009109819, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.15355323245248464, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.09085143500466081, 'train@spa.rst.sctb_loss@spa.rst.sctb': 1.9797970056533813, 'train@spa.rst.sctb_runtime': 5.672, 'train@spa.rst.sctb_samples_per_second': 77.397, 'train@spa.rst.sctb_steps_per_second': 2.468, 'epoch': 12.0}
{'loss': 2.0511, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.0664660930633545, 'eval_accuracy@spa.rst.sctb': 0.46808510638297873, 'eval_f1@spa.rst.sctb': 0.07965686274509805, 'eval_precision@spa.rst.sctb': 0.11429859300041856, 'eval_recall@spa.rst.sctb': 0.1003846514682428, 'eval_loss@spa.rst.sctb': 2.0664656162261963, 'eval_runtime': 2.3737, 'eval_samples_per_second': 39.6, 'eval_steps_per_second': 1.264, 'epoch': 12.0}
{'train_runtime': 221.8669, 'train_samples_per_second': 23.744, 'train_steps_per_second': 0.757, 'train_loss': 2.6402469135466076, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.1089
  train_runtime            = 5:26:34.09
  train_samples_per_second =     26.898
  train_steps_per_second   =      0.841
-------------------------------------------------------------------
Lang1:  eng.rst.gum    Lang2:  spa.rst.sctb
Saving run to:  runs/full_shot/FullShot=v4_eng.rst.gum_spa.rst.sctb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 13897 examples
read 2149 examples
read 2091 examples
read 439 examples
read 94 examples
read 159 examples
Total prediction labels:  29
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=29, bias=True)
    )
  )
)
{'train@eng.rst.gum_loss': 2.4933974742889404, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.262574656400662, 'train@eng.rst.gum_f1@eng.rst.gum': 0.050863463608374654, 'train@eng.rst.gum_precision@eng.rst.gum': 0.05775417050295596, 'train@eng.rst.gum_recall@eng.rst.gum': 0.06906324568481832, 'train@eng.rst.gum_loss@eng.rst.gum': 2.4933974742889404, 'train@eng.rst.gum_runtime': 163.4544, 'train@eng.rst.gum_samples_per_second': 85.021, 'train@eng.rst.gum_steps_per_second': 2.661, 'epoch': 1.0}
{'loss': 2.7496, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.573429822921753, 'eval_accuracy@eng.rst.gum': 0.2568636575151233, 'eval_f1@eng.rst.gum': 0.052184806083505626, 'eval_precision@eng.rst.gum': 0.05119016478927692, 'eval_recall@eng.rst.gum': 0.07369720503506551, 'eval_loss@eng.rst.gum': 2.573430061340332, 'eval_runtime': 25.6139, 'eval_samples_per_second': 83.9, 'eval_steps_per_second': 2.655, 'epoch': 1.0}
{'train@eng.rst.gum_loss': 2.0539448261260986, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.40354033244585163, 'train@eng.rst.gum_f1@eng.rst.gum': 0.16873063058877918, 'train@eng.rst.gum_precision@eng.rst.gum': 0.21480960662184045, 'train@eng.rst.gum_recall@eng.rst.gum': 0.18219695073182954, 'train@eng.rst.gum_loss@eng.rst.gum': 2.0539450645446777, 'train@eng.rst.gum_runtime': 164.1049, 'train@eng.rst.gum_samples_per_second': 84.684, 'train@eng.rst.gum_steps_per_second': 2.651, 'epoch': 2.0}
{'loss': 2.3282, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.158597230911255, 'eval_accuracy@eng.rst.gum': 0.37924616100511865, 'eval_f1@eng.rst.gum': 0.1647527153086698, 'eval_precision@eng.rst.gum': 0.23367161510879794, 'eval_recall@eng.rst.gum': 0.1812622594027438, 'eval_loss@eng.rst.gum': 2.158597230911255, 'eval_runtime': 25.7154, 'eval_samples_per_second': 83.568, 'eval_steps_per_second': 2.644, 'epoch': 2.0}
{'train@eng.rst.gum_loss': 1.7986170053482056, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.4849967618910556, 'train@eng.rst.gum_f1@eng.rst.gum': 0.2752995160281422, 'train@eng.rst.gum_precision@eng.rst.gum': 0.4170834421446827, 'train@eng.rst.gum_recall@eng.rst.gum': 0.2858388844363768, 'train@eng.rst.gum_loss@eng.rst.gum': 1.7986171245574951, 'train@eng.rst.gum_runtime': 163.4958, 'train@eng.rst.gum_samples_per_second': 84.999, 'train@eng.rst.gum_steps_per_second': 2.661, 'epoch': 3.0}
{'loss': 1.9969, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.9264297485351562, 'eval_accuracy@eng.rst.gum': 0.44299674267100975, 'eval_f1@eng.rst.gum': 0.2504476473818715, 'eval_precision@eng.rst.gum': 0.28386090063270975, 'eval_recall@eng.rst.gum': 0.26771083995914163, 'eval_loss@eng.rst.gum': 1.9264295101165771, 'eval_runtime': 25.5745, 'eval_samples_per_second': 84.029, 'eval_steps_per_second': 2.659, 'epoch': 3.0}
{'train@eng.rst.gum_loss': 1.6695653200149536, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5082391883140246, 'train@eng.rst.gum_f1@eng.rst.gum': 0.30761611073860146, 'train@eng.rst.gum_precision@eng.rst.gum': 0.4029318042940828, 'train@eng.rst.gum_recall@eng.rst.gum': 0.31481541560382414, 'train@eng.rst.gum_loss@eng.rst.gum': 1.6695653200149536, 'train@eng.rst.gum_runtime': 163.7996, 'train@eng.rst.gum_samples_per_second': 84.841, 'train@eng.rst.gum_steps_per_second': 2.656, 'epoch': 4.0}
{'loss': 1.8105, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.8239715099334717, 'eval_accuracy@eng.rst.gum': 0.4676593764541647, 'eval_f1@eng.rst.gum': 0.28973970037857444, 'eval_precision@eng.rst.gum': 0.3526560712031933, 'eval_recall@eng.rst.gum': 0.30174288107585967, 'eval_loss@eng.rst.gum': 1.8239713907241821, 'eval_runtime': 25.6591, 'eval_samples_per_second': 83.752, 'eval_steps_per_second': 2.65, 'epoch': 4.0}
{'train@eng.rst.gum_loss': 1.5943880081176758, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.526588472332158, 'train@eng.rst.gum_f1@eng.rst.gum': 0.34854847221615515, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5140875749154482, 'train@eng.rst.gum_recall@eng.rst.gum': 0.35476507441966343, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5943882465362549, 'train@eng.rst.gum_runtime': 163.5891, 'train@eng.rst.gum_samples_per_second': 84.951, 'train@eng.rst.gum_steps_per_second': 2.659, 'epoch': 5.0}
{'loss': 1.7094, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.7645736932754517, 'eval_accuracy@eng.rst.gum': 0.4881340158213122, 'eval_f1@eng.rst.gum': 0.33266791645665394, 'eval_precision@eng.rst.gum': 0.42218301290745647, 'eval_recall@eng.rst.gum': 0.34255188209924203, 'eval_loss@eng.rst.gum': 1.7645738124847412, 'eval_runtime': 25.6188, 'eval_samples_per_second': 83.884, 'eval_steps_per_second': 2.654, 'epoch': 5.0}
{'train@eng.rst.gum_loss': 1.5429890155792236, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5429229330071238, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3882084950602107, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5464846977734463, 'train@eng.rst.gum_recall@eng.rst.gum': 0.38932553844313195, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5429891347885132, 'train@eng.rst.gum_runtime': 163.8673, 'train@eng.rst.gum_samples_per_second': 84.806, 'train@eng.rst.gum_steps_per_second': 2.655, 'epoch': 6.0}
{'loss': 1.6477, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.7276785373687744, 'eval_accuracy@eng.rst.gum': 0.4904606793857608, 'eval_f1@eng.rst.gum': 0.3528951173284672, 'eval_precision@eng.rst.gum': 0.40725154703321087, 'eval_recall@eng.rst.gum': 0.36432991552796246, 'eval_loss@eng.rst.gum': 1.7276785373687744, 'eval_runtime': 25.6513, 'eval_samples_per_second': 83.777, 'eval_steps_per_second': 2.651, 'epoch': 6.0}
{'train@eng.rst.gum_loss': 1.5078858137130737, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5509822263797942, 'train@eng.rst.gum_f1@eng.rst.gum': 0.39966301956636685, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5480074551496794, 'train@eng.rst.gum_recall@eng.rst.gum': 0.40140324902255886, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5078858137130737, 'train@eng.rst.gum_runtime': 164.0695, 'train@eng.rst.gum_samples_per_second': 84.702, 'train@eng.rst.gum_steps_per_second': 2.651, 'epoch': 7.0}
{'loss': 1.6024, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.7072404623031616, 'eval_accuracy@eng.rst.gum': 0.49976733364355513, 'eval_f1@eng.rst.gum': 0.3675896224852902, 'eval_precision@eng.rst.gum': 0.43943265822631306, 'eval_recall@eng.rst.gum': 0.37718133303404916, 'eval_loss@eng.rst.gum': 1.707240343093872, 'eval_runtime': 25.7211, 'eval_samples_per_second': 83.55, 'eval_steps_per_second': 2.644, 'epoch': 7.0}
{'train@eng.rst.gum_loss': 1.483758568763733, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5570266964092969, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4117328214004598, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5498127704999465, 'train@eng.rst.gum_recall@eng.rst.gum': 0.41105620560930134, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4837584495544434, 'train@eng.rst.gum_runtime': 163.6138, 'train@eng.rst.gum_samples_per_second': 84.938, 'train@eng.rst.gum_steps_per_second': 2.659, 'epoch': 8.0}
{'loss': 1.5735, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.6922881603240967, 'eval_accuracy@eng.rst.gum': 0.5039553280595626, 'eval_f1@eng.rst.gum': 0.37838400898008356, 'eval_precision@eng.rst.gum': 0.5026767201348391, 'eval_recall@eng.rst.gum': 0.38636238660125893, 'eval_loss@eng.rst.gum': 1.6922881603240967, 'eval_runtime': 25.6168, 'eval_samples_per_second': 83.89, 'eval_steps_per_second': 2.655, 'epoch': 8.0}
{'train@eng.rst.gum_loss': 1.4626784324645996, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5645103259696337, 'train@eng.rst.gum_f1@eng.rst.gum': 0.42505315013324424, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5399698235493633, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4252892170982748, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4626784324645996, 'train@eng.rst.gum_runtime': 163.9089, 'train@eng.rst.gum_samples_per_second': 84.785, 'train@eng.rst.gum_steps_per_second': 2.654, 'epoch': 9.0}
{'loss': 1.5459, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.6745939254760742, 'eval_accuracy@eng.rst.gum': 0.5090739879013495, 'eval_f1@eng.rst.gum': 0.38805207160968164, 'eval_precision@eng.rst.gum': 0.49820508301196936, 'eval_recall@eng.rst.gum': 0.39890253769849776, 'eval_loss@eng.rst.gum': 1.6745940446853638, 'eval_runtime': 25.6071, 'eval_samples_per_second': 83.922, 'eval_steps_per_second': 2.656, 'epoch': 9.0}
{'train@eng.rst.gum_loss': 1.4483753442764282, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5683960567028855, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4285577499761957, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5381902891907926, 'train@eng.rst.gum_recall@eng.rst.gum': 0.425891440503633, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4483753442764282, 'train@eng.rst.gum_runtime': 164.0464, 'train@eng.rst.gum_samples_per_second': 84.714, 'train@eng.rst.gum_steps_per_second': 2.652, 'epoch': 10.0}
{'loss': 1.5306, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.6665886640548706, 'eval_accuracy@eng.rst.gum': 0.5086086551884598, 'eval_f1@eng.rst.gum': 0.38575983300489236, 'eval_precision@eng.rst.gum': 0.503115674206459, 'eval_recall@eng.rst.gum': 0.39396269757979085, 'eval_loss@eng.rst.gum': 1.666588544845581, 'eval_runtime': 25.7249, 'eval_samples_per_second': 83.538, 'eval_steps_per_second': 2.643, 'epoch': 10.0}
{'train@eng.rst.gum_loss': 1.441202163696289, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5696193423040944, 'train@eng.rst.gum_f1@eng.rst.gum': 0.43410294518222375, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5358809958176353, 'train@eng.rst.gum_recall@eng.rst.gum': 0.43319955615329653, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4412025213241577, 'train@eng.rst.gum_runtime': 164.0028, 'train@eng.rst.gum_samples_per_second': 84.736, 'train@eng.rst.gum_steps_per_second': 2.652, 'epoch': 11.0}
{'loss': 1.5245, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.661154866218567, 'eval_accuracy@eng.rst.gum': 0.5132619823173569, 'eval_f1@eng.rst.gum': 0.3945013702978944, 'eval_precision@eng.rst.gum': 0.4921166692747772, 'eval_recall@eng.rst.gum': 0.403600338744287, 'eval_loss@eng.rst.gum': 1.661154866218567, 'eval_runtime': 25.6535, 'eval_samples_per_second': 83.77, 'eval_steps_per_second': 2.651, 'epoch': 11.0}
{'train@eng.rst.gum_loss': 1.4383628368377686, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5698352162337195, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4344213608528796, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5389752717976733, 'train@eng.rst.gum_recall@eng.rst.gum': 0.43271518339773307, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4383628368377686, 'train@eng.rst.gum_runtime': 164.0864, 'train@eng.rst.gum_samples_per_second': 84.693, 'train@eng.rst.gum_steps_per_second': 2.651, 'epoch': 12.0}
{'loss': 1.5106, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.6597808599472046, 'eval_accuracy@eng.rst.gum': 0.5146579804560261, 'eval_f1@eng.rst.gum': 0.3951331837172368, 'eval_precision@eng.rst.gum': 0.49301665271171247, 'eval_recall@eng.rst.gum': 0.4034755082374038, 'eval_loss@eng.rst.gum': 1.659780740737915, 'eval_runtime': 25.7344, 'eval_samples_per_second': 83.507, 'eval_steps_per_second': 2.642, 'epoch': 12.0}
{'train_runtime': 6421.6809, 'train_samples_per_second': 25.969, 'train_steps_per_second': 0.813, 'train_loss': 1.7941637294959292, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.7942
  train_runtime            = 1:47:01.68
  train_samples_per_second =     25.969
  train_steps_per_second   =      0.813
{'train@spa.rst.sctb_loss': 2.7378835678100586, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.37813211845102507, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.11827270920357101, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.14823579648005444, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.1278365548728452, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.7378838062286377, 'train@spa.rst.sctb_runtime': 5.5686, 'train@spa.rst.sctb_samples_per_second': 78.835, 'train@spa.rst.sctb_steps_per_second': 2.514, 'epoch': 1.0}
{'loss': 3.0735, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.883134126663208, 'eval_accuracy@spa.rst.sctb': 0.30851063829787234, 'eval_f1@spa.rst.sctb': 0.042008486562942, 'eval_precision@spa.rst.sctb': 0.045588235294117645, 'eval_recall@spa.rst.sctb': 0.05242424242424243, 'eval_loss@spa.rst.sctb': 2.8831348419189453, 'eval_runtime': 1.4126, 'eval_samples_per_second': 66.546, 'eval_steps_per_second': 2.124, 'epoch': 1.0}
{'train@spa.rst.sctb_loss': 2.4121475219726562, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4123006833712984, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.11892053747681519, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.1556534718402696, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.12765136968766003, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.4121475219726562, 'train@spa.rst.sctb_runtime': 5.5576, 'train@spa.rst.sctb_samples_per_second': 78.991, 'train@spa.rst.sctb_steps_per_second': 2.519, 'epoch': 2.0}
{'loss': 2.5773, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.584550142288208, 'eval_accuracy@spa.rst.sctb': 0.3617021276595745, 'eval_f1@spa.rst.sctb': 0.04943153732081068, 'eval_precision@spa.rst.sctb': 0.05198358413132695, 'eval_recall@spa.rst.sctb': 0.07058823529411765, 'eval_loss@spa.rst.sctb': 2.584549903869629, 'eval_runtime': 1.4257, 'eval_samples_per_second': 65.934, 'eval_steps_per_second': 2.104, 'epoch': 2.0}
{'train@spa.rst.sctb_loss': 2.2117838859558105, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.41002277904328016, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.11343804941151488, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.15998078498078497, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.11611687147977472, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.2117834091186523, 'train@spa.rst.sctb_runtime': 5.5509, 'train@spa.rst.sctb_samples_per_second': 79.086, 'train@spa.rst.sctb_steps_per_second': 2.522, 'epoch': 3.0}
{'loss': 2.3642, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.4067482948303223, 'eval_accuracy@spa.rst.sctb': 0.3617021276595745, 'eval_f1@spa.rst.sctb': 0.05117168818747011, 'eval_precision@spa.rst.sctb': 0.0803921568627451, 'eval_recall@spa.rst.sctb': 0.07058823529411765, 'eval_loss@spa.rst.sctb': 2.4067487716674805, 'eval_runtime': 1.4253, 'eval_samples_per_second': 65.951, 'eval_steps_per_second': 2.105, 'epoch': 3.0}
{'train@spa.rst.sctb_loss': 2.094960927963257, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.42596810933940776, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.1321203297843457, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.16557365439093485, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.13269713261648744, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.0949606895446777, 'train@spa.rst.sctb_runtime': 6.4223, 'train@spa.rst.sctb_samples_per_second': 68.356, 'train@spa.rst.sctb_steps_per_second': 2.18, 'epoch': 4.0}
{'loss': 2.2078, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.3022894859313965, 'eval_accuracy@spa.rst.sctb': 0.3829787234042553, 'eval_f1@spa.rst.sctb': 0.06283574437732653, 'eval_precision@spa.rst.sctb': 0.07730079983800749, 'eval_recall@spa.rst.sctb': 0.07809362979641618, 'eval_loss@spa.rst.sctb': 2.3022894859313965, 'eval_runtime': 1.4253, 'eval_samples_per_second': 65.951, 'eval_steps_per_second': 2.105, 'epoch': 4.0}
{'train@spa.rst.sctb_loss': 2.0163822174072266, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.44419134396355353, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.13737774926584337, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.1666466461082425, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.13832821300563236, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.0163824558258057, 'train@spa.rst.sctb_runtime': 5.5601, 'train@spa.rst.sctb_samples_per_second': 78.956, 'train@spa.rst.sctb_steps_per_second': 2.518, 'epoch': 5.0}
{'loss': 2.0998, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.2344484329223633, 'eval_accuracy@spa.rst.sctb': 0.3829787234042553, 'eval_f1@spa.rst.sctb': 0.06530876006185202, 'eval_precision@spa.rst.sctb': 0.07573529411764707, 'eval_recall@spa.rst.sctb': 0.07940707383431841, 'eval_loss@spa.rst.sctb': 2.234447956085205, 'eval_runtime': 1.4202, 'eval_samples_per_second': 66.189, 'eval_steps_per_second': 2.112, 'epoch': 5.0}
{'train@spa.rst.sctb_loss': 1.9587277173995972, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4601366742596811, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.1416480301913951, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.19029252200074875, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.14215816889203986, 'train@spa.rst.sctb_loss@spa.rst.sctb': 1.9587277173995972, 'train@spa.rst.sctb_runtime': 5.5472, 'train@spa.rst.sctb_samples_per_second': 79.139, 'train@spa.rst.sctb_steps_per_second': 2.524, 'epoch': 6.0}
{'loss': 2.0337, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.1890628337860107, 'eval_accuracy@spa.rst.sctb': 0.3829787234042553, 'eval_f1@spa.rst.sctb': 0.06530876006185202, 'eval_precision@spa.rst.sctb': 0.07573529411764707, 'eval_recall@spa.rst.sctb': 0.07940707383431841, 'eval_loss@spa.rst.sctb': 2.1890628337860107, 'eval_runtime': 1.4161, 'eval_samples_per_second': 66.379, 'eval_steps_per_second': 2.118, 'epoch': 6.0}
{'train@spa.rst.sctb_loss': 1.9141064882278442, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4874715261958998, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.13744082562779858, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.18735107485107486, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.14435282011894915, 'train@spa.rst.sctb_loss@spa.rst.sctb': 1.9141064882278442, 'train@spa.rst.sctb_runtime': 5.5434, 'train@spa.rst.sctb_samples_per_second': 79.193, 'train@spa.rst.sctb_steps_per_second': 2.526, 'epoch': 7.0}
{'loss': 1.9999, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.15255069732666, 'eval_accuracy@spa.rst.sctb': 0.3829787234042553, 'eval_f1@spa.rst.sctb': 0.06530876006185202, 'eval_precision@spa.rst.sctb': 0.07573529411764707, 'eval_recall@spa.rst.sctb': 0.07940707383431841, 'eval_loss@spa.rst.sctb': 2.152550458908081, 'eval_runtime': 1.4186, 'eval_samples_per_second': 66.264, 'eval_steps_per_second': 2.115, 'epoch': 7.0}
{'train@spa.rst.sctb_loss': 1.8831255435943604, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.489749430523918, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.14298893502623336, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.18740990424069204, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.14782504234117136, 'train@spa.rst.sctb_loss@spa.rst.sctb': 1.8831255435943604, 'train@spa.rst.sctb_runtime': 5.5618, 'train@spa.rst.sctb_samples_per_second': 78.931, 'train@spa.rst.sctb_steps_per_second': 2.517, 'epoch': 8.0}
{'loss': 1.947, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.128786563873291, 'eval_accuracy@spa.rst.sctb': 0.39361702127659576, 'eval_f1@spa.rst.sctb': 0.06897759103641457, 'eval_precision@spa.rst.sctb': 0.07923238340215256, 'eval_recall@spa.rst.sctb': 0.08250304906651655, 'eval_loss@spa.rst.sctb': 2.1287872791290283, 'eval_runtime': 1.4025, 'eval_samples_per_second': 67.022, 'eval_steps_per_second': 2.139, 'epoch': 8.0}
{'train@spa.rst.sctb_loss': 1.8597331047058105, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.5056947608200456, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.14866628968834594, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.18857415230138166, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.15572154771751545, 'train@spa.rst.sctb_loss@spa.rst.sctb': 1.8597328662872314, 'train@spa.rst.sctb_runtime': 5.5635, 'train@spa.rst.sctb_samples_per_second': 78.907, 'train@spa.rst.sctb_steps_per_second': 2.516, 'epoch': 9.0}
{'loss': 1.951, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.1098527908325195, 'eval_accuracy@spa.rst.sctb': 0.4148936170212766, 'eval_f1@spa.rst.sctb': 0.09197860962566845, 'eval_precision@spa.rst.sctb': 0.1484593837535014, 'eval_recall@spa.rst.sctb': 0.09961534853175719, 'eval_loss@spa.rst.sctb': 2.1098527908325195, 'eval_runtime': 1.4098, 'eval_samples_per_second': 66.676, 'eval_steps_per_second': 2.128, 'epoch': 9.0}
{'train@spa.rst.sctb_loss': 1.8435570001602173, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.510250569476082, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.1537673136341485, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.20008101851851853, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.15867720764898183, 'train@spa.rst.sctb_loss@spa.rst.sctb': 1.8435571193695068, 'train@spa.rst.sctb_runtime': 5.5266, 'train@spa.rst.sctb_samples_per_second': 79.434, 'train@spa.rst.sctb_steps_per_second': 2.533, 'epoch': 10.0}
{'loss': 1.9258, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.0964627265930176, 'eval_accuracy@spa.rst.sctb': 0.425531914893617, 'eval_f1@spa.rst.sctb': 0.09248717408110199, 'eval_precision@spa.rst.sctb': 0.14196078431372547, 'eval_recall@spa.rst.sctb': 0.10271132376395534, 'eval_loss@spa.rst.sctb': 2.0964624881744385, 'eval_runtime': 1.427, 'eval_samples_per_second': 65.872, 'eval_steps_per_second': 2.102, 'epoch': 10.0}
{'train@spa.rst.sctb_loss': 1.8340966701507568, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.5148063781321185, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.15450384367665862, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.20464745169714116, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.1595732649966521, 'train@spa.rst.sctb_loss@spa.rst.sctb': 1.8340970277786255, 'train@spa.rst.sctb_runtime': 5.5734, 'train@spa.rst.sctb_samples_per_second': 78.766, 'train@spa.rst.sctb_steps_per_second': 2.512, 'epoch': 11.0}
{'loss': 1.9195, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.0887670516967773, 'eval_accuracy@spa.rst.sctb': 0.43617021276595747, 'eval_f1@spa.rst.sctb': 0.09576766843809176, 'eval_precision@spa.rst.sctb': 0.14455179161061513, 'eval_recall@spa.rst.sctb': 0.10580729899615349, 'eval_loss@spa.rst.sctb': 2.0887672901153564, 'eval_runtime': 1.4106, 'eval_samples_per_second': 66.638, 'eval_steps_per_second': 2.127, 'epoch': 11.0}
{'train@spa.rst.sctb_loss': 1.830996036529541, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.5170842824601367, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.15414017055469945, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.2033509700176367, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.1600212936704872, 'train@spa.rst.sctb_loss@spa.rst.sctb': 1.8309956789016724, 'train@spa.rst.sctb_runtime': 5.559, 'train@spa.rst.sctb_samples_per_second': 78.972, 'train@spa.rst.sctb_steps_per_second': 2.518, 'epoch': 12.0}
{'loss': 1.9074, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.0866193771362305, 'eval_accuracy@spa.rst.sctb': 0.43617021276595747, 'eval_f1@spa.rst.sctb': 0.09576766843809176, 'eval_precision@spa.rst.sctb': 0.14455179161061513, 'eval_recall@spa.rst.sctb': 0.10580729899615349, 'eval_loss@spa.rst.sctb': 2.0866191387176514, 'eval_runtime': 1.4172, 'eval_samples_per_second': 66.327, 'eval_steps_per_second': 2.117, 'epoch': 12.0}
{'train_runtime': 218.521, 'train_samples_per_second': 24.108, 'train_steps_per_second': 0.769, 'train_loss': 2.1672455923897878, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.7942
  train_runtime            = 1:47:01.68
  train_samples_per_second =     25.969
  train_steps_per_second   =      0.813
-------------------------------------------------------------------
Lang1:  eng.rst.rstdt    Lang2:  spa.rst.sctb
Saving run to:  runs/full_shot/FullShot=v4_eng.rst.rstdt_spa.rst.sctb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 16002 examples
read 1621 examples
read 2155 examples
read 439 examples
read 94 examples
read 159 examples
Total prediction labels:  33
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=33, bias=True)
    )
  )
)
{'train@eng.rst.rstdt_loss': 1.7662534713745117, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.5075615548056492, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.08314337057396537, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.14884900614210914, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.11026724777689804, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.7662534713745117, 'train@eng.rst.rstdt_runtime': 188.7782, 'train@eng.rst.rstdt_samples_per_second': 84.766, 'train@eng.rst.rstdt_steps_per_second': 2.654, 'epoch': 1.0}
{'loss': 2.1781, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.7486642599105835, 'eval_accuracy@eng.rst.rstdt': 0.5157310302282542, 'eval_f1@eng.rst.rstdt': 0.08269062982890432, 'eval_precision@eng.rst.rstdt': 0.07841911871781718, 'eval_recall@eng.rst.rstdt': 0.10877336723289514, 'eval_loss@eng.rst.rstdt': 1.748664379119873, 'eval_runtime': 19.4213, 'eval_samples_per_second': 83.465, 'eval_steps_per_second': 2.626, 'epoch': 1.0}
{'train@eng.rst.rstdt_loss': 1.4245023727416992, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6057992750906137, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.20966022480907506, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.3048010787930479, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.21752814852142913, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.4245023727416992, 'train@eng.rst.rstdt_runtime': 188.854, 'train@eng.rst.rstdt_samples_per_second': 84.732, 'train@eng.rst.rstdt_steps_per_second': 2.653, 'epoch': 2.0}
{'loss': 1.6207, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.4325542449951172, 'eval_accuracy@eng.rst.rstdt': 0.6070326958667489, 'eval_f1@eng.rst.rstdt': 0.2075285348817588, 'eval_precision@eng.rst.rstdt': 0.24763284828988816, 'eval_recall@eng.rst.rstdt': 0.21582624927255425, 'eval_loss@eng.rst.rstdt': 1.4325542449951172, 'eval_runtime': 19.5117, 'eval_samples_per_second': 83.079, 'eval_steps_per_second': 2.614, 'epoch': 2.0}
{'train@eng.rst.rstdt_loss': 1.3117557764053345, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6330458692663417, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.291763971237805, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.4502097001311199, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.27605111583409686, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.311755895614624, 'train@eng.rst.rstdt_runtime': 188.9379, 'train@eng.rst.rstdt_samples_per_second': 84.694, 'train@eng.rst.rstdt_steps_per_second': 2.652, 'epoch': 3.0}
{'loss': 1.4229, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.3427231311798096, 'eval_accuracy@eng.rst.rstdt': 0.6354102405922271, 'eval_f1@eng.rst.rstdt': 0.29423668632460337, 'eval_precision@eng.rst.rstdt': 0.4279097496390572, 'eval_recall@eng.rst.rstdt': 0.28033042203801034, 'eval_loss@eng.rst.rstdt': 1.3427231311798096, 'eval_runtime': 19.5379, 'eval_samples_per_second': 82.967, 'eval_steps_per_second': 2.61, 'epoch': 3.0}
{'train@eng.rst.rstdt_loss': 1.245259165763855, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6452943382077241, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3386712576150922, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.4325469409133242, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.32215982312125985, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.2452592849731445, 'train@eng.rst.rstdt_runtime': 189.5943, 'train@eng.rst.rstdt_samples_per_second': 84.401, 'train@eng.rst.rstdt_steps_per_second': 2.642, 'epoch': 4.0}
{'loss': 1.3301, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.3060288429260254, 'eval_accuracy@eng.rst.rstdt': 0.6366440468846392, 'eval_f1@eng.rst.rstdt': 0.32305845064900024, 'eval_precision@eng.rst.rstdt': 0.4389421778945516, 'eval_recall@eng.rst.rstdt': 0.3195683719903754, 'eval_loss@eng.rst.rstdt': 1.3060288429260254, 'eval_runtime': 19.5042, 'eval_samples_per_second': 83.11, 'eval_steps_per_second': 2.615, 'epoch': 4.0}
{'train@eng.rst.rstdt_loss': 1.1946079730987549, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6561054868141483, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.35338326273167586, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.4419549899770413, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.33089530652311094, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1946079730987549, 'train@eng.rst.rstdt_runtime': 189.0499, 'train@eng.rst.rstdt_samples_per_second': 84.644, 'train@eng.rst.rstdt_steps_per_second': 2.65, 'epoch': 5.0}
{'loss': 1.271, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.2608155012130737, 'eval_accuracy@eng.rst.rstdt': 0.6421961752004935, 'eval_f1@eng.rst.rstdt': 0.32883691923210506, 'eval_precision@eng.rst.rstdt': 0.42417722172063677, 'eval_recall@eng.rst.rstdt': 0.32062254236321036, 'eval_loss@eng.rst.rstdt': 1.2608155012130737, 'eval_runtime': 19.4909, 'eval_samples_per_second': 83.167, 'eval_steps_per_second': 2.617, 'epoch': 5.0}
{'train@eng.rst.rstdt_loss': 1.1620076894760132, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.662604674415698, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.372802805737294, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5041285209838664, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3441590556194483, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1620078086853027, 'train@eng.rst.rstdt_runtime': 189.0577, 'train@eng.rst.rstdt_samples_per_second': 84.641, 'train@eng.rst.rstdt_steps_per_second': 2.65, 'epoch': 6.0}
{'loss': 1.2309, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.2356619834899902, 'eval_accuracy@eng.rst.rstdt': 0.6458975940777298, 'eval_f1@eng.rst.rstdt': 0.340742530798543, 'eval_precision@eng.rst.rstdt': 0.4337067407363285, 'eval_recall@eng.rst.rstdt': 0.3320526376187265, 'eval_loss@eng.rst.rstdt': 1.2356619834899902, 'eval_runtime': 19.5029, 'eval_samples_per_second': 83.116, 'eval_steps_per_second': 2.615, 'epoch': 6.0}
{'train@eng.rst.rstdt_loss': 1.1411943435668945, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6641044869391326, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3838659469842307, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5594869942992536, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3520780574728194, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1411943435668945, 'train@eng.rst.rstdt_runtime': 189.1787, 'train@eng.rst.rstdt_samples_per_second': 84.587, 'train@eng.rst.rstdt_steps_per_second': 2.648, 'epoch': 7.0}
{'loss': 1.2059, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.2196472883224487, 'eval_accuracy@eng.rst.rstdt': 0.6446637877853177, 'eval_f1@eng.rst.rstdt': 0.3437380583882618, 'eval_precision@eng.rst.rstdt': 0.4172600076265436, 'eval_recall@eng.rst.rstdt': 0.33323431002092463, 'eval_loss@eng.rst.rstdt': 1.2196471691131592, 'eval_runtime': 19.4838, 'eval_samples_per_second': 83.197, 'eval_steps_per_second': 2.618, 'epoch': 7.0}
{'train@eng.rst.rstdt_loss': 1.125441551208496, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6682914635670542, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.39891243063288984, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5873720526179026, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3661972916872585, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.125441551208496, 'train@eng.rst.rstdt_runtime': 189.1421, 'train@eng.rst.rstdt_samples_per_second': 84.603, 'train@eng.rst.rstdt_steps_per_second': 2.649, 'epoch': 8.0}
{'loss': 1.1848, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.214261531829834, 'eval_accuracy@eng.rst.rstdt': 0.6440468846391116, 'eval_f1@eng.rst.rstdt': 0.35437371048807365, 'eval_precision@eng.rst.rstdt': 0.5287365624534153, 'eval_recall@eng.rst.rstdt': 0.340318066670295, 'eval_loss@eng.rst.rstdt': 1.214261531829834, 'eval_runtime': 19.5042, 'eval_samples_per_second': 83.11, 'eval_steps_per_second': 2.615, 'epoch': 8.0}
{'train@eng.rst.rstdt_loss': 1.1134194135665894, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6703537057867767, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.40461567044226104, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5852592708262792, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3690626281325223, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1134194135665894, 'train@eng.rst.rstdt_runtime': 188.8259, 'train@eng.rst.rstdt_samples_per_second': 84.745, 'train@eng.rst.rstdt_steps_per_second': 2.653, 'epoch': 9.0}
{'loss': 1.1711, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.2040603160858154, 'eval_accuracy@eng.rst.rstdt': 0.6434299814929056, 'eval_f1@eng.rst.rstdt': 0.353804763545684, 'eval_precision@eng.rst.rstdt': 0.523013558160617, 'eval_recall@eng.rst.rstdt': 0.33853589111988025, 'eval_loss@eng.rst.rstdt': 1.204060435295105, 'eval_runtime': 19.4669, 'eval_samples_per_second': 83.27, 'eval_steps_per_second': 2.62, 'epoch': 9.0}
{'train@eng.rst.rstdt_loss': 1.1087901592254639, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6717285339332584, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4157882119384598, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6319369557829546, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.38089080351938515, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1087902784347534, 'train@eng.rst.rstdt_runtime': 188.9117, 'train@eng.rst.rstdt_samples_per_second': 84.706, 'train@eng.rst.rstdt_steps_per_second': 2.652, 'epoch': 10.0}
{'loss': 1.16, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.2092018127441406, 'eval_accuracy@eng.rst.rstdt': 0.6354102405922271, 'eval_f1@eng.rst.rstdt': 0.3514044552265862, 'eval_precision@eng.rst.rstdt': 0.505666231698093, 'eval_recall@eng.rst.rstdt': 0.341664769092203, 'eval_loss@eng.rst.rstdt': 1.2092018127441406, 'eval_runtime': 19.501, 'eval_samples_per_second': 83.124, 'eval_steps_per_second': 2.615, 'epoch': 10.0}
{'train@eng.rst.rstdt_loss': 1.102051854133606, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6717910261217348, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4138618525730792, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6351359445711178, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3769431247487454, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.102051854133606, 'train@eng.rst.rstdt_runtime': 188.8363, 'train@eng.rst.rstdt_samples_per_second': 84.74, 'train@eng.rst.rstdt_steps_per_second': 2.653, 'epoch': 11.0}
{'loss': 1.1521, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.2004495859146118, 'eval_accuracy@eng.rst.rstdt': 0.6421961752004935, 'eval_f1@eng.rst.rstdt': 0.35739532534173113, 'eval_precision@eng.rst.rstdt': 0.5230255672357487, 'eval_recall@eng.rst.rstdt': 0.3432850154229743, 'eval_loss@eng.rst.rstdt': 1.2004495859146118, 'eval_runtime': 19.4539, 'eval_samples_per_second': 83.325, 'eval_steps_per_second': 2.622, 'epoch': 11.0}
{'train@eng.rst.rstdt_loss': 1.1006948947906494, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6719785026871641, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.414337708163527, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6343181876067785, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.37718816787844484, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1006948947906494, 'train@eng.rst.rstdt_runtime': 188.5904, 'train@eng.rst.rstdt_samples_per_second': 84.851, 'train@eng.rst.rstdt_steps_per_second': 2.657, 'epoch': 12.0}
{'loss': 1.1491, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.1989250183105469, 'eval_accuracy@eng.rst.rstdt': 0.6421961752004935, 'eval_f1@eng.rst.rstdt': 0.3572649902584129, 'eval_precision@eng.rst.rstdt': 0.522225389584853, 'eval_recall@eng.rst.rstdt': 0.34342354202729, 'eval_loss@eng.rst.rstdt': 1.1989250183105469, 'eval_runtime': 19.4854, 'eval_samples_per_second': 83.19, 'eval_steps_per_second': 2.617, 'epoch': 12.0}
{'train_runtime': 7280.3283, 'train_samples_per_second': 26.376, 'train_steps_per_second': 0.826, 'train_loss': 1.3397237202840413, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.3397
  train_runtime            = 2:01:20.32
  train_samples_per_second =     26.376
  train_steps_per_second   =      0.826
{'train@spa.rst.sctb_loss': 3.43001127243042, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.04783599088838269, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.017306579729180347, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.02578323993886908, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.023015873015873014, 'train@spa.rst.sctb_loss@spa.rst.sctb': 3.430011510848999, 'train@spa.rst.sctb_runtime': 5.5765, 'train@spa.rst.sctb_samples_per_second': 78.723, 'train@spa.rst.sctb_steps_per_second': 2.511, 'epoch': 1.0}
{'loss': 3.8791, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.3999805450439453, 'eval_accuracy@spa.rst.sctb': 0.06382978723404255, 'eval_f1@spa.rst.sctb': 0.015234306696974996, 'eval_precision@spa.rst.sctb': 0.0162225779116844, 'eval_recall@spa.rst.sctb': 0.06108452950558214, 'eval_loss@spa.rst.sctb': 3.39997935295105, 'eval_runtime': 1.4299, 'eval_samples_per_second': 65.739, 'eval_steps_per_second': 2.098, 'epoch': 1.0}
{'train@spa.rst.sctb_loss': 2.8348593711853027, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.29157175398633256, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02997978402170019, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.05831681737478839, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.0441468253968254, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.8348593711853027, 'train@spa.rst.sctb_runtime': 5.5658, 'train@spa.rst.sctb_samples_per_second': 78.874, 'train@spa.rst.sctb_steps_per_second': 2.515, 'epoch': 2.0}
{'loss': 3.0886, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.8100297451019287, 'eval_accuracy@spa.rst.sctb': 0.30851063829787234, 'eval_f1@spa.rst.sctb': 0.03576248313090418, 'eval_precision@spa.rst.sctb': 0.02587748714509278, 'eval_recall@spa.rst.sctb': 0.05824915824915825, 'eval_loss@spa.rst.sctb': 2.8100297451019287, 'eval_runtime': 1.4412, 'eval_samples_per_second': 65.224, 'eval_steps_per_second': 2.082, 'epoch': 2.0}
{'train@spa.rst.sctb_loss': 2.557751178741455, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3325740318906606, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.021270396270396275, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.014415481832543445, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04055555555555556, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.557751178741455, 'train@spa.rst.sctb_runtime': 5.5554, 'train@spa.rst.sctb_samples_per_second': 79.023, 'train@spa.rst.sctb_steps_per_second': 2.52, 'epoch': 3.0}
{'loss': 2.7159, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.5579867362976074, 'eval_accuracy@spa.rst.sctb': 0.35106382978723405, 'eval_f1@spa.rst.sctb': 0.04556412729026036, 'eval_precision@spa.rst.sctb': 0.04075787618418153, 'eval_recall@spa.rst.sctb': 0.06880570409982174, 'eval_loss@spa.rst.sctb': 2.5579872131347656, 'eval_runtime': 1.4598, 'eval_samples_per_second': 64.393, 'eval_steps_per_second': 2.055, 'epoch': 3.0}
{'train@spa.rst.sctb_loss': 2.4052834510803223, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.35990888382687924, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02901304531413205, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.04377003205128205, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04542114695340502, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.405282974243164, 'train@spa.rst.sctb_runtime': 5.5655, 'train@spa.rst.sctb_samples_per_second': 78.878, 'train@spa.rst.sctb_steps_per_second': 2.515, 'epoch': 4.0}
{'loss': 2.5126, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.43229603767395, 'eval_accuracy@spa.rst.sctb': 0.32978723404255317, 'eval_f1@spa.rst.sctb': 0.03436480474542758, 'eval_precision@spa.rst.sctb': 0.030323757409940724, 'eval_recall@spa.rst.sctb': 0.05657191106107515, 'eval_loss@spa.rst.sctb': 2.43229603767395, 'eval_runtime': 1.4464, 'eval_samples_per_second': 64.989, 'eval_steps_per_second': 2.074, 'epoch': 4.0}
{'train@spa.rst.sctb_loss': 2.3009586334228516, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4031890660592255, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.04074074074074074, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.04458180708180708, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.054103942652329755, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.3009586334228516, 'train@spa.rst.sctb_runtime': 5.5725, 'train@spa.rst.sctb_samples_per_second': 78.779, 'train@spa.rst.sctb_steps_per_second': 2.512, 'epoch': 5.0}
{'loss': 2.3994, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.3482654094696045, 'eval_accuracy@spa.rst.sctb': 0.3723404255319149, 'eval_f1@spa.rst.sctb': 0.04881364310430054, 'eval_precision@spa.rst.sctb': 0.041945892280963015, 'eval_recall@spa.rst.sctb': 0.06895581198986772, 'eval_loss@spa.rst.sctb': 2.3482656478881836, 'eval_runtime': 1.4501, 'eval_samples_per_second': 64.822, 'eval_steps_per_second': 2.069, 'epoch': 5.0}
{'train@spa.rst.sctb_loss': 2.226597785949707, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4419134396355353, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.04834672184355728, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.046682259182259184, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.061890681003584226, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.226597547531128, 'train@spa.rst.sctb_runtime': 5.5698, 'train@spa.rst.sctb_samples_per_second': 78.818, 'train@spa.rst.sctb_steps_per_second': 2.514, 'epoch': 6.0}
{'loss': 2.2905, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.2880373001098633, 'eval_accuracy@spa.rst.sctb': 0.425531914893617, 'eval_f1@spa.rst.sctb': 0.06270810210876802, 'eval_precision@spa.rst.sctb': 0.05218525766470972, 'eval_recall@spa.rst.sctb': 0.08443568815085843, 'eval_loss@spa.rst.sctb': 2.288036346435547, 'eval_runtime': 1.4443, 'eval_samples_per_second': 65.082, 'eval_steps_per_second': 2.077, 'epoch': 6.0}
{'train@spa.rst.sctb_loss': 2.1709377765655518, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4601366742596811, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.05105902148979838, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.04577828358890106, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.06581541218637993, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.1709375381469727, 'train@spa.rst.sctb_runtime': 5.5548, 'train@spa.rst.sctb_samples_per_second': 79.031, 'train@spa.rst.sctb_steps_per_second': 2.52, 'epoch': 7.0}
{'loss': 2.238, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.2444188594818115, 'eval_accuracy@spa.rst.sctb': 0.425531914893617, 'eval_f1@spa.rst.sctb': 0.06194785606550311, 'eval_precision@spa.rst.sctb': 0.0504304599978387, 'eval_recall@spa.rst.sctb': 0.08443568815085843, 'eval_loss@spa.rst.sctb': 2.244419574737549, 'eval_runtime': 1.4349, 'eval_samples_per_second': 65.51, 'eval_steps_per_second': 2.091, 'epoch': 7.0}
{'train@spa.rst.sctb_loss': 2.1325485706329346, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.46697038724373574, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.05199287463816523, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.04604699799097586, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.0671594982078853, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.1325485706329346, 'train@spa.rst.sctb_runtime': 5.5557, 'train@spa.rst.sctb_samples_per_second': 79.018, 'train@spa.rst.sctb_steps_per_second': 2.52, 'epoch': 8.0}
{'loss': 2.1905, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.2146060466766357, 'eval_accuracy@spa.rst.sctb': 0.43617021276595747, 'eval_f1@spa.rst.sctb': 0.06436189287184732, 'eval_precision@spa.rst.sctb': 0.05217086834733893, 'eval_recall@spa.rst.sctb': 0.08753166338305658, 'eval_loss@spa.rst.sctb': 2.2146058082580566, 'eval_runtime': 1.4481, 'eval_samples_per_second': 64.915, 'eval_steps_per_second': 2.072, 'epoch': 8.0}
{'train@spa.rst.sctb_loss': 2.1029741764068604, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.46697038724373574, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.05197132616487455, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.04558178120316849, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.06732974910394264, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.1029744148254395, 'train@spa.rst.sctb_runtime': 5.5698, 'train@spa.rst.sctb_samples_per_second': 78.818, 'train@spa.rst.sctb_steps_per_second': 2.514, 'epoch': 9.0}
{'loss': 2.1902, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.19075083732605, 'eval_accuracy@spa.rst.sctb': 0.44680851063829785, 'eval_f1@spa.rst.sctb': 0.06668763762189368, 'eval_precision@spa.rst.sctb': 0.05381074168797954, 'eval_recall@spa.rst.sctb': 0.0906276386152547, 'eval_loss@spa.rst.sctb': 2.190751075744629, 'eval_runtime': 1.4367, 'eval_samples_per_second': 65.429, 'eval_steps_per_second': 2.088, 'epoch': 9.0}
{'train@spa.rst.sctb_loss': 2.0825088024139404, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.46697038724373574, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.05205073494547179, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.04540044657903606, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.0675, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.0825085639953613, 'train@spa.rst.sctb_runtime': 5.5852, 'train@spa.rst.sctb_samples_per_second': 78.6, 'train@spa.rst.sctb_steps_per_second': 2.507, 'epoch': 10.0}
{'loss': 2.152, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.1754150390625, 'eval_accuracy@spa.rst.sctb': 0.44680851063829785, 'eval_f1@spa.rst.sctb': 0.06668763762189368, 'eval_precision@spa.rst.sctb': 0.05381074168797954, 'eval_recall@spa.rst.sctb': 0.0906276386152547, 'eval_loss@spa.rst.sctb': 2.1754143238067627, 'eval_runtime': 1.4467, 'eval_samples_per_second': 64.977, 'eval_steps_per_second': 2.074, 'epoch': 10.0}
{'train@spa.rst.sctb_loss': 2.070716381072998, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.46924829157175396, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.052397544112194376, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.045597040659581804, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.06794802867383513, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.070716619491577, 'train@spa.rst.sctb_runtime': 5.5617, 'train@spa.rst.sctb_samples_per_second': 78.932, 'train@spa.rst.sctb_steps_per_second': 2.517, 'epoch': 11.0}
{'loss': 2.1257, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.1661288738250732, 'eval_accuracy@spa.rst.sctb': 0.44680851063829785, 'eval_f1@spa.rst.sctb': 0.06668763762189368, 'eval_precision@spa.rst.sctb': 0.05381074168797954, 'eval_recall@spa.rst.sctb': 0.0906276386152547, 'eval_loss@spa.rst.sctb': 2.1661288738250732, 'eval_runtime': 1.4789, 'eval_samples_per_second': 63.561, 'eval_steps_per_second': 2.029, 'epoch': 11.0}
{'train@spa.rst.sctb_loss': 2.066865921020508, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.46924829157175396, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.052397544112194376, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.045597040659581804, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.06794802867383513, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.0668656826019287, 'train@spa.rst.sctb_runtime': 5.5507, 'train@spa.rst.sctb_samples_per_second': 79.089, 'train@spa.rst.sctb_steps_per_second': 2.522, 'epoch': 12.0}
{'loss': 2.1411, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.163257122039795, 'eval_accuracy@spa.rst.sctb': 0.44680851063829785, 'eval_f1@spa.rst.sctb': 0.06668763762189368, 'eval_precision@spa.rst.sctb': 0.05381074168797954, 'eval_recall@spa.rst.sctb': 0.0906276386152547, 'eval_loss@spa.rst.sctb': 2.1632580757141113, 'eval_runtime': 1.4462, 'eval_samples_per_second': 64.996, 'eval_steps_per_second': 2.074, 'epoch': 12.0}
{'train_runtime': 218.0955, 'train_samples_per_second': 24.155, 'train_steps_per_second': 0.77, 'train_loss': 2.493636733009702, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.3397
  train_runtime            = 2:01:20.32
  train_samples_per_second =     26.376
  train_steps_per_second   =      0.826
-------------------------------------------------------------------
Lang1:  eng.sdrt.stac    Lang2:  spa.rst.sctb
Saving run to:  runs/full_shot/FullShot=v4_eng.sdrt.stac_spa.rst.sctb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 9580 examples
read 1145 examples
read 1510 examples
read 439 examples
read 94 examples
read 159 examples
Total prediction labels:  41
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=41, bias=True)
    )
  )
)
{'train@eng.sdrt.stac_loss': 2.100619316101074, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.3446764091858038, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.06363814475315069, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.08565952935186173, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.10785815316709652, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 2.100619316101074, 'train@eng.sdrt.stac_runtime': 113.6861, 'train@eng.sdrt.stac_samples_per_second': 84.267, 'train@eng.sdrt.stac_steps_per_second': 2.639, 'epoch': 1.0}
{'loss': 2.551, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.0618896484375, 'eval_accuracy@eng.sdrt.stac': 0.3502183406113537, 'eval_f1@eng.sdrt.stac': 0.06374787860636917, 'eval_precision@eng.sdrt.stac': 0.04794400225116578, 'eval_recall@eng.sdrt.stac': 0.10938737231768153, 'eval_loss@eng.sdrt.stac': 2.061889886856079, 'eval_runtime': 13.9162, 'eval_samples_per_second': 82.278, 'eval_steps_per_second': 2.587, 'epoch': 1.0}
{'train@eng.sdrt.stac_loss': 1.8983275890350342, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.410125260960334, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.11805499655199146, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.14632885516093536, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.16731518337023385, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.8983275890350342, 'train@eng.sdrt.stac_runtime': 113.6565, 'train@eng.sdrt.stac_samples_per_second': 84.289, 'train@eng.sdrt.stac_steps_per_second': 2.64, 'epoch': 2.0}
{'loss': 2.0287, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.8580785989761353, 'eval_accuracy@eng.sdrt.stac': 0.3947598253275109, 'eval_f1@eng.sdrt.stac': 0.10749173381319421, 'eval_precision@eng.sdrt.stac': 0.11509720368879464, 'eval_recall@eng.sdrt.stac': 0.1592811839620546, 'eval_loss@eng.sdrt.stac': 1.8580785989761353, 'eval_runtime': 13.9234, 'eval_samples_per_second': 82.236, 'eval_steps_per_second': 2.586, 'epoch': 2.0}
{'train@eng.sdrt.stac_loss': 1.8005826473236084, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.43987473903966595, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.1504291977154183, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.13809794106861803, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.1886425545953916, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.8005825281143188, 'train@eng.sdrt.stac_runtime': 113.5596, 'train@eng.sdrt.stac_samples_per_second': 84.361, 'train@eng.sdrt.stac_steps_per_second': 2.642, 'epoch': 3.0}
{'loss': 1.885, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.7609484195709229, 'eval_accuracy@eng.sdrt.stac': 0.4331877729257642, 'eval_f1@eng.sdrt.stac': 0.13983641312271036, 'eval_precision@eng.sdrt.stac': 0.1284833645434743, 'eval_recall@eng.sdrt.stac': 0.1814931552218923, 'eval_loss@eng.sdrt.stac': 1.7609483003616333, 'eval_runtime': 13.9388, 'eval_samples_per_second': 82.145, 'eval_steps_per_second': 2.583, 'epoch': 3.0}
{'train@eng.sdrt.stac_loss': 1.7300238609313965, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.45187891440501043, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.16142830558420612, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.18139594627255523, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.19637198484783427, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.7300240993499756, 'train@eng.sdrt.stac_runtime': 113.6398, 'train@eng.sdrt.stac_samples_per_second': 84.301, 'train@eng.sdrt.stac_steps_per_second': 2.64, 'epoch': 4.0}
{'loss': 1.8023, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.680840015411377, 'eval_accuracy@eng.sdrt.stac': 0.4480349344978166, 'eval_f1@eng.sdrt.stac': 0.14816880339130983, 'eval_precision@eng.sdrt.stac': 0.13904693673974758, 'eval_recall@eng.sdrt.stac': 0.18901264778840549, 'eval_loss@eng.sdrt.stac': 1.6808401346206665, 'eval_runtime': 13.9226, 'eval_samples_per_second': 82.24, 'eval_steps_per_second': 2.586, 'epoch': 4.0}
{'train@eng.sdrt.stac_loss': 1.67576003074646, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.47275574112734864, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.18834017300337993, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.21075272604399578, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2153205405931664, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.6757599115371704, 'train@eng.sdrt.stac_runtime': 113.5716, 'train@eng.sdrt.stac_samples_per_second': 84.352, 'train@eng.sdrt.stac_steps_per_second': 2.642, 'epoch': 5.0}
{'loss': 1.7441, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.628255009651184, 'eval_accuracy@eng.sdrt.stac': 0.462882096069869, 'eval_f1@eng.sdrt.stac': 0.16940907127231575, 'eval_precision@eng.sdrt.stac': 0.2120890459694977, 'eval_recall@eng.sdrt.stac': 0.20045504738663167, 'eval_loss@eng.sdrt.stac': 1.6282551288604736, 'eval_runtime': 13.9347, 'eval_samples_per_second': 82.169, 'eval_steps_per_second': 2.583, 'epoch': 5.0}
{'train@eng.sdrt.stac_loss': 1.6331387758255005, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.47661795407098123, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.18565511707131077, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.2133511893805365, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.21450105844977702, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.6331390142440796, 'train@eng.sdrt.stac_runtime': 113.3069, 'train@eng.sdrt.stac_samples_per_second': 84.549, 'train@eng.sdrt.stac_steps_per_second': 2.648, 'epoch': 6.0}
{'loss': 1.6922, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.6014976501464844, 'eval_accuracy@eng.sdrt.stac': 0.46899563318777293, 'eval_f1@eng.sdrt.stac': 0.1709813157970913, 'eval_precision@eng.sdrt.stac': 0.22975755085140775, 'eval_recall@eng.sdrt.stac': 0.20232822159820163, 'eval_loss@eng.sdrt.stac': 1.601497769355774, 'eval_runtime': 13.866, 'eval_samples_per_second': 82.576, 'eval_steps_per_second': 2.596, 'epoch': 6.0}
{'train@eng.sdrt.stac_loss': 1.6066758632659912, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4907098121085595, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.21960018735880002, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.3102771157659843, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.24017508602743465, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.6066759824752808, 'train@eng.sdrt.stac_runtime': 113.0458, 'train@eng.sdrt.stac_samples_per_second': 84.744, 'train@eng.sdrt.stac_steps_per_second': 2.654, 'epoch': 7.0}
{'loss': 1.6619, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.5786551237106323, 'eval_accuracy@eng.sdrt.stac': 0.485589519650655, 'eval_f1@eng.sdrt.stac': 0.19885976694642027, 'eval_precision@eng.sdrt.stac': 0.21033332779029362, 'eval_recall@eng.sdrt.stac': 0.2216899952788461, 'eval_loss@eng.sdrt.stac': 1.5786551237106323, 'eval_runtime': 13.8766, 'eval_samples_per_second': 82.513, 'eval_steps_per_second': 2.594, 'epoch': 7.0}
{'train@eng.sdrt.stac_loss': 1.5721813440322876, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5051148225469728, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2480646079114477, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.3433174234505495, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.26416676578229525, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5721813440322876, 'train@eng.sdrt.stac_runtime': 113.1013, 'train@eng.sdrt.stac_samples_per_second': 84.703, 'train@eng.sdrt.stac_steps_per_second': 2.652, 'epoch': 8.0}
{'loss': 1.6328, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.5453370809555054, 'eval_accuracy@eng.sdrt.stac': 0.503056768558952, 'eval_f1@eng.sdrt.stac': 0.23570908031236942, 'eval_precision@eng.sdrt.stac': 0.2773123974274333, 'eval_recall@eng.sdrt.stac': 0.24818063002946988, 'eval_loss@eng.sdrt.stac': 1.5453370809555054, 'eval_runtime': 13.88, 'eval_samples_per_second': 82.493, 'eval_steps_per_second': 2.594, 'epoch': 8.0}
{'train@eng.sdrt.stac_loss': 1.554550290107727, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5107515657620042, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.26494528982885157, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.38336601152119437, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.28074489176906386, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.554550290107727, 'train@eng.sdrt.stac_runtime': 113.1324, 'train@eng.sdrt.stac_samples_per_second': 84.68, 'train@eng.sdrt.stac_steps_per_second': 2.652, 'epoch': 9.0}
{'loss': 1.6088, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.5347124338150024, 'eval_accuracy@eng.sdrt.stac': 0.5109170305676856, 'eval_f1@eng.sdrt.stac': 0.24261607904558422, 'eval_precision@eng.sdrt.stac': 0.25923286806886786, 'eval_recall@eng.sdrt.stac': 0.257843658442312, 'eval_loss@eng.sdrt.stac': 1.5347124338150024, 'eval_runtime': 13.8986, 'eval_samples_per_second': 82.382, 'eval_steps_per_second': 2.59, 'epoch': 9.0}
{'train@eng.sdrt.stac_loss': 1.5379568338394165, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5162839248434238, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.27569495947672706, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.4045982621841833, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2917353441362667, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5379568338394165, 'train@eng.sdrt.stac_runtime': 113.0513, 'train@eng.sdrt.stac_samples_per_second': 84.74, 'train@eng.sdrt.stac_steps_per_second': 2.654, 'epoch': 10.0}
{'loss': 1.5946, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.522060751914978, 'eval_accuracy@eng.sdrt.stac': 0.5213973799126638, 'eval_f1@eng.sdrt.stac': 0.25076710642436895, 'eval_precision@eng.sdrt.stac': 0.2589140288157763, 'eval_recall@eng.sdrt.stac': 0.26824212104697354, 'eval_loss@eng.sdrt.stac': 1.5220608711242676, 'eval_runtime': 13.9105, 'eval_samples_per_second': 82.312, 'eval_steps_per_second': 2.588, 'epoch': 10.0}
{'train@eng.sdrt.stac_loss': 1.529370903968811, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5176409185803758, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2818356845056669, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.40541730238425194, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2978884468256008, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.529370903968811, 'train@eng.sdrt.stac_runtime': 113.2566, 'train@eng.sdrt.stac_samples_per_second': 84.587, 'train@eng.sdrt.stac_steps_per_second': 2.649, 'epoch': 11.0}
{'loss': 1.5785, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.516444444656372, 'eval_accuracy@eng.sdrt.stac': 0.5205240174672489, 'eval_f1@eng.sdrt.stac': 0.2547592863168332, 'eval_precision@eng.sdrt.stac': 0.32340750955913466, 'eval_recall@eng.sdrt.stac': 0.27035705069371685, 'eval_loss@eng.sdrt.stac': 1.516444444656372, 'eval_runtime': 13.9232, 'eval_samples_per_second': 82.237, 'eval_steps_per_second': 2.586, 'epoch': 11.0}
{'train@eng.sdrt.stac_loss': 1.5273290872573853, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5186847599164927, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.28390370645893687, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.40732066337529177, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.30054614243995137, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5273293256759644, 'train@eng.sdrt.stac_runtime': 113.1154, 'train@eng.sdrt.stac_samples_per_second': 84.692, 'train@eng.sdrt.stac_steps_per_second': 2.652, 'epoch': 12.0}
{'loss': 1.5744, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.51459538936615, 'eval_accuracy@eng.sdrt.stac': 0.5231441048034935, 'eval_f1@eng.sdrt.stac': 0.2567959845138068, 'eval_precision@eng.sdrt.stac': 0.3239123025492818, 'eval_recall@eng.sdrt.stac': 0.27194106862751316, 'eval_loss@eng.sdrt.stac': 1.51459538936615, 'eval_runtime': 13.9278, 'eval_samples_per_second': 82.21, 'eval_steps_per_second': 2.585, 'epoch': 12.0}
{'train_runtime': 4391.9525, 'train_samples_per_second': 26.175, 'train_steps_per_second': 0.82, 'train_loss': 1.7795261806911893, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.7795
  train_runtime            = 1:13:11.95
  train_samples_per_second =     26.175
  train_steps_per_second   =       0.82
{'train@spa.rst.sctb_loss': 3.162682294845581, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.10250569476082004, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.012808248299319728, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.011760752688172041, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.051666666666666666, 'train@spa.rst.sctb_loss@spa.rst.sctb': 3.162681818008423, 'train@spa.rst.sctb_runtime': 5.6717, 'train@spa.rst.sctb_samples_per_second': 77.401, 'train@spa.rst.sctb_steps_per_second': 2.468, 'epoch': 1.0}
{'loss': 3.3496, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.2361767292022705, 'eval_accuracy@spa.rst.sctb': 0.09574468085106383, 'eval_f1@spa.rst.sctb': 0.014139827179890024, 'eval_precision@spa.rst.sctb': 0.01393188854489164, 'eval_recall@spa.rst.sctb': 0.014354066985645932, 'eval_loss@spa.rst.sctb': 3.236177921295166, 'eval_runtime': 1.5133, 'eval_samples_per_second': 62.115, 'eval_steps_per_second': 1.982, 'epoch': 1.0}
{'train@spa.rst.sctb_loss': 2.925748109817505, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3416856492027335, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02144082332761578, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.014434180138568129, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.041666666666666664, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.925748348236084, 'train@spa.rst.sctb_runtime': 5.6285, 'train@spa.rst.sctb_samples_per_second': 77.996, 'train@spa.rst.sctb_steps_per_second': 2.487, 'epoch': 2.0}
{'loss': 3.0286, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.9880292415618896, 'eval_accuracy@spa.rst.sctb': 0.35106382978723405, 'eval_f1@spa.rst.sctb': 0.03056970819823993, 'eval_precision@spa.rst.sctb': 0.02065081351689612, 'eval_recall@spa.rst.sctb': 0.058823529411764705, 'eval_loss@spa.rst.sctb': 2.9880287647247314, 'eval_runtime': 1.4957, 'eval_samples_per_second': 62.848, 'eval_steps_per_second': 2.006, 'epoch': 2.0}
{'train@spa.rst.sctb_loss': 2.7245328426361084, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3416856492027335, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.021222410865874366, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.014236902050113895, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.041666666666666664, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.7245330810546875, 'train@spa.rst.sctb_runtime': 5.6424, 'train@spa.rst.sctb_samples_per_second': 77.804, 'train@spa.rst.sctb_steps_per_second': 2.481, 'epoch': 3.0}
{'loss': 2.8585, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.7881243228912354, 'eval_accuracy@spa.rst.sctb': 0.35106382978723405, 'eval_f1@spa.rst.sctb': 0.03056970819823993, 'eval_precision@spa.rst.sctb': 0.02065081351689612, 'eval_recall@spa.rst.sctb': 0.058823529411764705, 'eval_loss@spa.rst.sctb': 2.7881245613098145, 'eval_runtime': 1.4814, 'eval_samples_per_second': 63.455, 'eval_steps_per_second': 2.025, 'epoch': 3.0}
{'train@spa.rst.sctb_loss': 2.5583009719848633, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3416856492027335, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.021222410865874366, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.014236902050113895, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.041666666666666664, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.5583009719848633, 'train@spa.rst.sctb_runtime': 5.6445, 'train@spa.rst.sctb_samples_per_second': 77.775, 'train@spa.rst.sctb_steps_per_second': 2.48, 'epoch': 4.0}
{'loss': 2.6609, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.6347901821136475, 'eval_accuracy@spa.rst.sctb': 0.35106382978723405, 'eval_f1@spa.rst.sctb': 0.03056970819823993, 'eval_precision@spa.rst.sctb': 0.02065081351689612, 'eval_recall@spa.rst.sctb': 0.058823529411764705, 'eval_loss@spa.rst.sctb': 2.6347908973693848, 'eval_runtime': 1.4989, 'eval_samples_per_second': 62.714, 'eval_steps_per_second': 2.001, 'epoch': 4.0}
{'train@spa.rst.sctb_loss': 2.433520793914795, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3439635535307517, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02214502822405558, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.055936073059360734, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.042114695340501794, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.433520555496216, 'train@spa.rst.sctb_runtime': 5.6121, 'train@spa.rst.sctb_samples_per_second': 78.224, 'train@spa.rst.sctb_steps_per_second': 2.495, 'epoch': 5.0}
{'loss': 2.5106, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.521676778793335, 'eval_accuracy@spa.rst.sctb': 0.35106382978723405, 'eval_f1@spa.rst.sctb': 0.03056970819823993, 'eval_precision@spa.rst.sctb': 0.02065081351689612, 'eval_recall@spa.rst.sctb': 0.058823529411764705, 'eval_loss@spa.rst.sctb': 2.5216758251190186, 'eval_runtime': 1.4988, 'eval_samples_per_second': 62.719, 'eval_steps_per_second': 2.002, 'epoch': 5.0}
{'train@spa.rst.sctb_loss': 2.3460135459899902, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3530751708428246, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.025649577536369986, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.04915640236079035, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04390681003584229, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.3460137844085693, 'train@spa.rst.sctb_runtime': 5.6308, 'train@spa.rst.sctb_samples_per_second': 77.965, 'train@spa.rst.sctb_steps_per_second': 2.486, 'epoch': 6.0}
{'loss': 2.4208, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.444087266921997, 'eval_accuracy@spa.rst.sctb': 0.35106382978723405, 'eval_f1@spa.rst.sctb': 0.03056970819823993, 'eval_precision@spa.rst.sctb': 0.02065081351689612, 'eval_recall@spa.rst.sctb': 0.058823529411764705, 'eval_loss@spa.rst.sctb': 2.444087505340576, 'eval_runtime': 1.4948, 'eval_samples_per_second': 62.883, 'eval_steps_per_second': 2.007, 'epoch': 6.0}
{'train@spa.rst.sctb_loss': 2.2879507541656494, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3621867881548975, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.028806635305768755, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.04588700234192037, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04569892473118279, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.2879509925842285, 'train@spa.rst.sctb_runtime': 5.6143, 'train@spa.rst.sctb_samples_per_second': 78.193, 'train@spa.rst.sctb_steps_per_second': 2.494, 'epoch': 7.0}
{'loss': 2.3587, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.389894962310791, 'eval_accuracy@spa.rst.sctb': 0.35106382978723405, 'eval_f1@spa.rst.sctb': 0.03105882352941176, 'eval_precision@spa.rst.sctb': 0.021099744245524295, 'eval_recall@spa.rst.sctb': 0.058823529411764705, 'eval_loss@spa.rst.sctb': 2.389894485473633, 'eval_runtime': 1.5044, 'eval_samples_per_second': 62.483, 'eval_steps_per_second': 1.994, 'epoch': 7.0}
{'train@spa.rst.sctb_loss': 2.254757881164551, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.36446469248291574, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02939892607143045, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.037993753848860744, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04614695340501792, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.254757881164551, 'train@spa.rst.sctb_runtime': 5.6279, 'train@spa.rst.sctb_samples_per_second': 78.004, 'train@spa.rst.sctb_steps_per_second': 2.488, 'epoch': 8.0}
{'loss': 2.3036, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.36076021194458, 'eval_accuracy@spa.rst.sctb': 0.3617021276595745, 'eval_f1@spa.rst.sctb': 0.04052503646086533, 'eval_precision@spa.rst.sctb': 0.040998217468805706, 'eval_recall@spa.rst.sctb': 0.06323294868186509, 'eval_loss@spa.rst.sctb': 2.360759735107422, 'eval_runtime': 1.4922, 'eval_samples_per_second': 62.993, 'eval_steps_per_second': 2.01, 'epoch': 8.0}
{'train@spa.rst.sctb_loss': 2.2268035411834717, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3804100227790433, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.033677378165567144, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.036265432098765434, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.0492831541218638, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.226804256439209, 'train@spa.rst.sctb_runtime': 5.6162, 'train@spa.rst.sctb_samples_per_second': 78.167, 'train@spa.rst.sctb_steps_per_second': 2.493, 'epoch': 9.0}
{'loss': 2.2891, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.334944486618042, 'eval_accuracy@spa.rst.sctb': 0.3723404255319149, 'eval_f1@spa.rst.sctb': 0.044947209653092006, 'eval_precision@spa.rst.sctb': 0.04684632473679127, 'eval_recall@spa.rst.sctb': 0.06632892391406324, 'eval_loss@spa.rst.sctb': 2.334944725036621, 'eval_runtime': 1.4777, 'eval_samples_per_second': 63.614, 'eval_steps_per_second': 2.03, 'epoch': 9.0}
{'train@spa.rst.sctb_loss': 2.2080891132354736, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3940774487471526, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03691538774374451, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.035272974226462604, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.0521415770609319, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.2080893516540527, 'train@spa.rst.sctb_runtime': 5.6181, 'train@spa.rst.sctb_samples_per_second': 78.14, 'train@spa.rst.sctb_steps_per_second': 2.492, 'epoch': 10.0}
{'loss': 2.2558, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.319249153137207, 'eval_accuracy@spa.rst.sctb': 0.3723404255319149, 'eval_f1@spa.rst.sctb': 0.044947209653092006, 'eval_precision@spa.rst.sctb': 0.04684632473679127, 'eval_recall@spa.rst.sctb': 0.06632892391406324, 'eval_loss@spa.rst.sctb': 2.319249153137207, 'eval_runtime': 1.4929, 'eval_samples_per_second': 62.965, 'eval_steps_per_second': 2.01, 'epoch': 10.0}
{'train@spa.rst.sctb_loss': 2.1981189250946045, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3940774487471526, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03712156507212982, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03470214783041154, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05231182795698924, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.1981191635131836, 'train@spa.rst.sctb_runtime': 5.6143, 'train@spa.rst.sctb_samples_per_second': 78.194, 'train@spa.rst.sctb_steps_per_second': 2.494, 'epoch': 11.0}
{'loss': 2.2497, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.3103749752044678, 'eval_accuracy@spa.rst.sctb': 0.3829787234042553, 'eval_f1@spa.rst.sctb': 0.049065377785100964, 'eval_precision@spa.rst.sctb': 0.05129958960328317, 'eval_recall@spa.rst.sctb': 0.06942489914626138, 'eval_loss@spa.rst.sctb': 2.3103747367858887, 'eval_runtime': 1.4993, 'eval_samples_per_second': 62.695, 'eval_steps_per_second': 2.001, 'epoch': 11.0}
{'train@spa.rst.sctb_loss': 2.1948654651641846, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3940774487471526, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.037071846282372596, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03441796025841217, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05231182795698924, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.1948654651641846, 'train@spa.rst.sctb_runtime': 5.6105, 'train@spa.rst.sctb_samples_per_second': 78.247, 'train@spa.rst.sctb_steps_per_second': 2.495, 'epoch': 12.0}
{'loss': 2.2402, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.3077545166015625, 'eval_accuracy@spa.rst.sctb': 0.3829787234042553, 'eval_f1@spa.rst.sctb': 0.049065377785100964, 'eval_precision@spa.rst.sctb': 0.05129958960328317, 'eval_recall@spa.rst.sctb': 0.06942489914626138, 'eval_loss@spa.rst.sctb': 2.3077549934387207, 'eval_runtime': 1.5014, 'eval_samples_per_second': 62.607, 'eval_steps_per_second': 1.998, 'epoch': 12.0}
{'train_runtime': 219.5278, 'train_samples_per_second': 23.997, 'train_steps_per_second': 0.765, 'train_loss': 2.5438486621493386, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.7795
  train_runtime            = 1:13:11.95
  train_samples_per_second =     26.175
  train_steps_per_second   =       0.82
-------------------------------------------------------------------
Lang1:  fas.rst.prstc    Lang2:  spa.rst.sctb
Saving run to:  runs/full_shot/FullShot=v4_fas.rst.prstc_spa.rst.sctb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 4100 examples
read 499 examples
read 592 examples
read 439 examples
read 94 examples
read 159 examples
Total prediction labels:  33
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (ar): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=33, bias=True)
    )
  )
)
{'train@fas.rst.prstc_loss': 2.4145357608795166, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.23682926829268292, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.022798790460891488, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.022353434765976874, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.05860060117882396, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.4145359992980957, 'train@fas.rst.prstc_runtime': 48.3835, 'train@fas.rst.prstc_samples_per_second': 84.74, 'train@fas.rst.prstc_steps_per_second': 2.666, 'epoch': 1.0}
{'loss': 2.778, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.3380703926086426, 'eval_accuracy@fas.rst.prstc': 0.24649298597194388, 'eval_f1@fas.rst.prstc': 0.027418545673796077, 'eval_precision@fas.rst.prstc': 0.049698189134808855, 'eval_recall@fas.rst.prstc': 0.0672463768115942, 'eval_loss@fas.rst.prstc': 2.3380706310272217, 'eval_runtime': 6.1967, 'eval_samples_per_second': 80.527, 'eval_steps_per_second': 2.582, 'epoch': 1.0}
{'train@fas.rst.prstc_loss': 2.359025716781616, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.27731707317073173, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04469618573171422, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.032268162253815055, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07273707519014153, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.359025716781616, 'train@fas.rst.prstc_runtime': 48.4945, 'train@fas.rst.prstc_samples_per_second': 84.546, 'train@fas.rst.prstc_steps_per_second': 2.66, 'epoch': 2.0}
{'loss': 2.4038, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.2728586196899414, 'eval_accuracy@fas.rst.prstc': 0.31462925851703405, 'eval_f1@fas.rst.prstc': 0.05629804789441906, 'eval_precision@fas.rst.prstc': 0.04194580587009183, 'eval_recall@fas.rst.prstc': 0.08782133523402233, 'eval_loss@fas.rst.prstc': 2.2728588581085205, 'eval_runtime': 6.2038, 'eval_samples_per_second': 80.434, 'eval_steps_per_second': 2.579, 'epoch': 2.0}
{'train@fas.rst.prstc_loss': 2.3403780460357666, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.24780487804878049, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.03026023774952485, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03046531511710687, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.06190430345624337, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3403778076171875, 'train@fas.rst.prstc_runtime': 48.4377, 'train@fas.rst.prstc_samples_per_second': 84.645, 'train@fas.rst.prstc_steps_per_second': 2.663, 'epoch': 3.0}
{'loss': 2.3607, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.254021406173706, 'eval_accuracy@fas.rst.prstc': 0.2605210420841683, 'eval_f1@fas.rst.prstc': 0.03634702107793472, 'eval_precision@fas.rst.prstc': 0.04255434487992627, 'eval_recall@fas.rst.prstc': 0.07137087194107863, 'eval_loss@fas.rst.prstc': 2.254021406173706, 'eval_runtime': 6.1758, 'eval_samples_per_second': 80.799, 'eval_steps_per_second': 2.591, 'epoch': 3.0}
{'train@fas.rst.prstc_loss': 2.3246917724609375, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.23853658536585365, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.024044095378443764, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03197049817302249, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.05910571975653328, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3246920108795166, 'train@fas.rst.prstc_runtime': 48.4275, 'train@fas.rst.prstc_samples_per_second': 84.663, 'train@fas.rst.prstc_steps_per_second': 2.664, 'epoch': 4.0}
{'loss': 2.3489, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.2432281970977783, 'eval_accuracy@fas.rst.prstc': 0.250501002004008, 'eval_f1@fas.rst.prstc': 0.029755649622611926, 'eval_precision@fas.rst.prstc': 0.04983096686950642, 'eval_recall@fas.rst.prstc': 0.06840579710144928, 'eval_loss@fas.rst.prstc': 2.2432284355163574, 'eval_runtime': 6.163, 'eval_samples_per_second': 80.966, 'eval_steps_per_second': 2.596, 'epoch': 4.0}
{'train@fas.rst.prstc_loss': 2.3083364963531494, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.27902439024390246, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04221364050170919, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.032503222219039105, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07141148658044777, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3083364963531494, 'train@fas.rst.prstc_runtime': 48.4592, 'train@fas.rst.prstc_samples_per_second': 84.607, 'train@fas.rst.prstc_steps_per_second': 2.662, 'epoch': 5.0}
{'loss': 2.331, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.2235162258148193, 'eval_accuracy@fas.rst.prstc': 0.2965931863727455, 'eval_f1@fas.rst.prstc': 0.05005942977506959, 'eval_precision@fas.rst.prstc': 0.04194065343258891, 'eval_recall@fas.rst.prstc': 0.0819719648372535, 'eval_loss@fas.rst.prstc': 2.2235164642333984, 'eval_runtime': 6.1937, 'eval_samples_per_second': 80.565, 'eval_steps_per_second': 2.583, 'epoch': 5.0}
{'train@fas.rst.prstc_loss': 2.2819695472717285, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.27, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.039458251358842816, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03212229154665654, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.0686088379705401, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.2819700241088867, 'train@fas.rst.prstc_runtime': 48.5412, 'train@fas.rst.prstc_samples_per_second': 84.464, 'train@fas.rst.prstc_steps_per_second': 2.658, 'epoch': 6.0}
{'loss': 2.3113, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.2011919021606445, 'eval_accuracy@fas.rst.prstc': 0.28857715430861725, 'eval_f1@fas.rst.prstc': 0.0479254527114655, 'eval_precision@fas.rst.prstc': 0.042843515407905104, 'eval_recall@fas.rst.prstc': 0.0796198622000475, 'eval_loss@fas.rst.prstc': 2.2011916637420654, 'eval_runtime': 6.2105, 'eval_samples_per_second': 80.347, 'eval_steps_per_second': 2.576, 'epoch': 6.0}
{'train@fas.rst.prstc_loss': 2.245227098464966, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2975609756097561, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04878822509526612, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.035662690493819156, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07844378122225432, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.2452268600463867, 'train@fas.rst.prstc_runtime': 48.5772, 'train@fas.rst.prstc_samples_per_second': 84.402, 'train@fas.rst.prstc_steps_per_second': 2.656, 'epoch': 7.0}
{'loss': 2.2834, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.1521859169006348, 'eval_accuracy@fas.rst.prstc': 0.32064128256513025, 'eval_f1@fas.rst.prstc': 0.05883400950360417, 'eval_precision@fas.rst.prstc': 0.04416647515239065, 'eval_recall@fas.rst.prstc': 0.08989308624376337, 'eval_loss@fas.rst.prstc': 2.152186155319214, 'eval_runtime': 6.4855, 'eval_samples_per_second': 76.941, 'eval_steps_per_second': 2.467, 'epoch': 7.0}
{'train@fas.rst.prstc_loss': 2.2107856273651123, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3026829268292683, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.05008487138130957, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.037057136055944004, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08015232716108811, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.210785388946533, 'train@fas.rst.prstc_runtime': 48.5349, 'train@fas.rst.prstc_samples_per_second': 84.475, 'train@fas.rst.prstc_steps_per_second': 2.658, 'epoch': 8.0}
{'loss': 2.2499, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.1147637367248535, 'eval_accuracy@fas.rst.prstc': 0.32665330661322645, 'eval_f1@fas.rst.prstc': 0.060462692316407225, 'eval_precision@fas.rst.prstc': 0.04615375898098411, 'eval_recall@fas.rst.prstc': 0.09176526490852935, 'eval_loss@fas.rst.prstc': 2.1147637367248535, 'eval_runtime': 6.1805, 'eval_samples_per_second': 80.738, 'eval_steps_per_second': 2.589, 'epoch': 8.0}
{'train@fas.rst.prstc_loss': 2.1924915313720703, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3131707317073171, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.052296737468384126, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03935260631766938, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.0832617695195918, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1924917697906494, 'train@fas.rst.prstc_runtime': 48.5404, 'train@fas.rst.prstc_samples_per_second': 84.466, 'train@fas.rst.prstc_steps_per_second': 2.658, 'epoch': 9.0}
{'loss': 2.2246, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.0923285484313965, 'eval_accuracy@fas.rst.prstc': 0.33266533066132264, 'eval_f1@fas.rst.prstc': 0.06205480255823506, 'eval_precision@fas.rst.prstc': 0.04810681826157139, 'eval_recall@fas.rst.prstc': 0.09353765740080779, 'eval_loss@fas.rst.prstc': 2.0923283100128174, 'eval_runtime': 6.2125, 'eval_samples_per_second': 80.322, 'eval_steps_per_second': 2.575, 'epoch': 9.0}
{'train@fas.rst.prstc_loss': 2.1865453720092773, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.31853658536585366, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.05346315712066597, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.04067158198429236, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.0848466566113625, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1865451335906982, 'train@fas.rst.prstc_runtime': 48.6362, 'train@fas.rst.prstc_samples_per_second': 84.299, 'train@fas.rst.prstc_steps_per_second': 2.652, 'epoch': 10.0}
{'loss': 2.2104, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.0841662883758545, 'eval_accuracy@fas.rst.prstc': 0.33867735470941884, 'eval_f1@fas.rst.prstc': 0.06328069689414227, 'eval_precision@fas.rst.prstc': 0.049336225493682294, 'eval_recall@fas.rst.prstc': 0.0952767878355904, 'eval_loss@fas.rst.prstc': 2.0841662883758545, 'eval_runtime': 6.2224, 'eval_samples_per_second': 80.194, 'eval_steps_per_second': 2.571, 'epoch': 10.0}
{'train@fas.rst.prstc_loss': 2.1785385608673096, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.32195121951219513, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.054396421526208924, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.04197969966421639, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08590289143480632, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1785385608673096, 'train@fas.rst.prstc_runtime': 48.7197, 'train@fas.rst.prstc_samples_per_second': 84.155, 'train@fas.rst.prstc_steps_per_second': 2.648, 'epoch': 11.0}
{'loss': 2.2016, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.0740039348602295, 'eval_accuracy@fas.rst.prstc': 0.342685370741483, 'eval_f1@fas.rst.prstc': 0.06425900025900026, 'eval_precision@fas.rst.prstc': 0.05062977793957044, 'eval_recall@fas.rst.prstc': 0.09646947018294132, 'eval_loss@fas.rst.prstc': 2.0740041732788086, 'eval_runtime': 6.2485, 'eval_samples_per_second': 79.859, 'eval_steps_per_second': 2.561, 'epoch': 11.0}
{'train@fas.rst.prstc_loss': 2.1802148818969727, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3221951219512195, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.05425279769730338, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.041618040192001236, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08594482419263395, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1802148818969727, 'train@fas.rst.prstc_runtime': 48.9871, 'train@fas.rst.prstc_samples_per_second': 83.696, 'train@fas.rst.prstc_steps_per_second': 2.633, 'epoch': 12.0}
{'loss': 2.1972, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.07578182220459, 'eval_accuracy@fas.rst.prstc': 0.342685370741483, 'eval_f1@fas.rst.prstc': 0.0642857142857143, 'eval_precision@fas.rst.prstc': 0.05054813849994573, 'eval_recall@fas.rst.prstc': 0.09643620812544548, 'eval_loss@fas.rst.prstc': 2.0757815837860107, 'eval_runtime': 6.2139, 'eval_samples_per_second': 80.304, 'eval_steps_per_second': 2.575, 'epoch': 12.0}
{'train_runtime': 1884.3163, 'train_samples_per_second': 26.11, 'train_steps_per_second': 0.822, 'train_loss': 2.3250729661887313, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.3251
  train_runtime            = 0:31:24.31
  train_samples_per_second =      26.11
  train_steps_per_second   =      0.822
{'train@spa.rst.sctb_loss': 3.261564254760742, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.20956719817767655, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.032988040718798545, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03203793132009589, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.048934331797235026, 'train@spa.rst.sctb_loss@spa.rst.sctb': 3.261563777923584, 'train@spa.rst.sctb_runtime': 5.5798, 'train@spa.rst.sctb_samples_per_second': 78.677, 'train@spa.rst.sctb_steps_per_second': 2.509, 'epoch': 1.0}
{'loss': 3.4234, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.2523975372314453, 'eval_accuracy@spa.rst.sctb': 0.1702127659574468, 'eval_f1@spa.rst.sctb': 0.056666338032797664, 'eval_precision@spa.rst.sctb': 0.07398860398860399, 'eval_recall@spa.rst.sctb': 0.07903597377281588, 'eval_loss@spa.rst.sctb': 3.252397298812866, 'eval_runtime': 1.4517, 'eval_samples_per_second': 64.752, 'eval_steps_per_second': 2.067, 'epoch': 1.0}
{'train@spa.rst.sctb_loss': 3.0167222023010254, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.37585421412300685, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03927515562092377, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.030491616936629728, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.055367383512544804, 'train@spa.rst.sctb_loss@spa.rst.sctb': 3.0167222023010254, 'train@spa.rst.sctb_runtime': 5.5708, 'train@spa.rst.sctb_samples_per_second': 78.804, 'train@spa.rst.sctb_steps_per_second': 2.513, 'epoch': 2.0}
{'loss': 3.1592, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.0058019161224365, 'eval_accuracy@spa.rst.sctb': 0.35106382978723405, 'eval_f1@spa.rst.sctb': 0.05318291700241741, 'eval_precision@spa.rst.sctb': 0.0431917211328976, 'eval_recall@spa.rst.sctb': 0.0772117459423961, 'eval_loss@spa.rst.sctb': 3.0058016777038574, 'eval_runtime': 1.4342, 'eval_samples_per_second': 65.541, 'eval_steps_per_second': 2.092, 'epoch': 2.0}
{'train@spa.rst.sctb_loss': 2.7777788639068604, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3917995444191344, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.04083721903395504, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03169764360406818, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.0574820788530466, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.777778387069702, 'train@spa.rst.sctb_runtime': 5.55, 'train@spa.rst.sctb_samples_per_second': 79.099, 'train@spa.rst.sctb_steps_per_second': 2.523, 'epoch': 3.0}
{'loss': 2.9242, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.7727015018463135, 'eval_accuracy@spa.rst.sctb': 0.3617021276595745, 'eval_f1@spa.rst.sctb': 0.0544891640866873, 'eval_precision@spa.rst.sctb': 0.04350741664654918, 'eval_recall@spa.rst.sctb': 0.07899427713669199, 'eval_loss@spa.rst.sctb': 2.7727015018463135, 'eval_runtime': 1.4461, 'eval_samples_per_second': 65.001, 'eval_steps_per_second': 2.074, 'epoch': 3.0}
{'train@spa.rst.sctb_loss': 2.5705149173736572, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.41002277904328016, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.043184375740345886, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.033497616218204455, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.06106630824372761, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.5705149173736572, 'train@spa.rst.sctb_runtime': 5.5827, 'train@spa.rst.sctb_samples_per_second': 78.636, 'train@spa.rst.sctb_steps_per_second': 2.508, 'epoch': 4.0}
{'loss': 2.7088, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.5816779136657715, 'eval_accuracy@spa.rst.sctb': 0.3723404255319149, 'eval_f1@spa.rst.sctb': 0.056037151702786385, 'eval_precision@spa.rst.sctb': 0.044875405702636734, 'eval_recall@spa.rst.sctb': 0.0807768083309879, 'eval_loss@spa.rst.sctb': 2.5816779136657715, 'eval_runtime': 1.473, 'eval_samples_per_second': 63.817, 'eval_steps_per_second': 2.037, 'epoch': 4.0}
{'train@spa.rst.sctb_loss': 2.4180896282196045, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4191343963553531, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.04338458121122694, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03408300532713678, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.06030465949820788, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.4180896282196045, 'train@spa.rst.sctb_runtime': 5.5533, 'train@spa.rst.sctb_samples_per_second': 79.053, 'train@spa.rst.sctb_steps_per_second': 2.521, 'epoch': 5.0}
{'loss': 2.5459, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.4372901916503906, 'eval_accuracy@spa.rst.sctb': 0.39361702127659576, 'eval_f1@spa.rst.sctb': 0.057763230684123174, 'eval_precision@spa.rst.sctb': 0.04483751542575072, 'eval_recall@spa.rst.sctb': 0.0817149826437752, 'eval_loss@spa.rst.sctb': 2.4372901916503906, 'eval_runtime': 1.4402, 'eval_samples_per_second': 65.269, 'eval_steps_per_second': 2.083, 'epoch': 5.0}
{'train@spa.rst.sctb_loss': 2.319956064224243, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4191343963553531, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.04297983402461014, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.035118514788706125, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.0589426523297491, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.319956064224243, 'train@spa.rst.sctb_runtime': 5.575, 'train@spa.rst.sctb_samples_per_second': 78.744, 'train@spa.rst.sctb_steps_per_second': 2.511, 'epoch': 6.0}
{'loss': 2.416, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.3485679626464844, 'eval_accuracy@spa.rst.sctb': 0.40425531914893614, 'eval_f1@spa.rst.sctb': 0.05774773219289974, 'eval_precision@spa.rst.sctb': 0.04642857142857143, 'eval_recall@spa.rst.sctb': 0.07955718172436439, 'eval_loss@spa.rst.sctb': 2.3485682010650635, 'eval_runtime': 1.4208, 'eval_samples_per_second': 66.161, 'eval_steps_per_second': 2.112, 'epoch': 6.0}
{'train@spa.rst.sctb_loss': 2.2607078552246094, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.428246013667426, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.04410753575357535, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03602539152997869, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.06039426523297491, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.2607076168060303, 'train@spa.rst.sctb_runtime': 5.5313, 'train@spa.rst.sctb_samples_per_second': 79.366, 'train@spa.rst.sctb_steps_per_second': 2.531, 'epoch': 7.0}
{'loss': 2.3224, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.300037384033203, 'eval_accuracy@spa.rst.sctb': 0.40425531914893614, 'eval_f1@spa.rst.sctb': 0.05774773219289974, 'eval_precision@spa.rst.sctb': 0.04642857142857143, 'eval_recall@spa.rst.sctb': 0.07955718172436439, 'eval_loss@spa.rst.sctb': 2.3000376224517822, 'eval_runtime': 1.452, 'eval_samples_per_second': 64.737, 'eval_steps_per_second': 2.066, 'epoch': 7.0}
{'train@spa.rst.sctb_loss': 2.227468729019165, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.42596810933940776, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.04359010372774847, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03572440364893195, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05977598566308243, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.227468252182007, 'train@spa.rst.sctb_runtime': 5.5678, 'train@spa.rst.sctb_samples_per_second': 78.846, 'train@spa.rst.sctb_steps_per_second': 2.514, 'epoch': 8.0}
{'loss': 2.2607, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.272855758666992, 'eval_accuracy@spa.rst.sctb': 0.40425531914893614, 'eval_f1@spa.rst.sctb': 0.05801551389786684, 'eval_precision@spa.rst.sctb': 0.04704441482655525, 'eval_recall@spa.rst.sctb': 0.07955718172436439, 'eval_loss@spa.rst.sctb': 2.272855758666992, 'eval_runtime': 1.4431, 'eval_samples_per_second': 65.137, 'eval_steps_per_second': 2.079, 'epoch': 8.0}
{'train@spa.rst.sctb_loss': 2.2041172981262207, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4328018223234624, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.04455878396556362, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.035922209835253315, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.06129032258064516, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.2041172981262207, 'train@spa.rst.sctb_runtime': 5.5602, 'train@spa.rst.sctb_samples_per_second': 78.954, 'train@spa.rst.sctb_steps_per_second': 2.518, 'epoch': 9.0}
{'loss': 2.2565, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.254016876220703, 'eval_accuracy@spa.rst.sctb': 0.4148936170212766, 'eval_f1@spa.rst.sctb': 0.060186641501520394, 'eval_precision@spa.rst.sctb': 0.04825234441602728, 'eval_recall@spa.rst.sctb': 0.08265315695656253, 'eval_loss@spa.rst.sctb': 2.2540171146392822, 'eval_runtime': 1.4228, 'eval_samples_per_second': 66.068, 'eval_steps_per_second': 2.109, 'epoch': 9.0}
{'train@spa.rst.sctb_loss': 2.1883046627044678, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4328018223234624, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.04478030642564281, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.035898364103359705, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.06163082437275985, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.1883046627044678, 'train@spa.rst.sctb_runtime': 5.5578, 'train@spa.rst.sctb_samples_per_second': 78.988, 'train@spa.rst.sctb_steps_per_second': 2.519, 'epoch': 10.0}
{'loss': 2.2332, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.2427284717559814, 'eval_accuracy@spa.rst.sctb': 0.425531914893617, 'eval_f1@spa.rst.sctb': 0.06225063938618926, 'eval_precision@spa.rst.sctb': 0.04942607225311352, 'eval_recall@spa.rst.sctb': 0.08574913218876068, 'eval_loss@spa.rst.sctb': 2.2427284717559814, 'eval_runtime': 1.4533, 'eval_samples_per_second': 64.682, 'eval_steps_per_second': 2.064, 'epoch': 10.0}
{'train@spa.rst.sctb_loss': 2.17984938621521, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4328018223234624, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.04478030642564281, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.035898364103359705, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.06163082437275985, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.179849147796631, 'train@spa.rst.sctb_runtime': 5.5464, 'train@spa.rst.sctb_samples_per_second': 79.151, 'train@spa.rst.sctb_steps_per_second': 2.524, 'epoch': 11.0}
{'loss': 2.2303, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.235302686691284, 'eval_accuracy@spa.rst.sctb': 0.425531914893617, 'eval_f1@spa.rst.sctb': 0.06225063938618926, 'eval_precision@spa.rst.sctb': 0.04942607225311352, 'eval_recall@spa.rst.sctb': 0.08574913218876068, 'eval_loss@spa.rst.sctb': 2.2353031635284424, 'eval_runtime': 1.4292, 'eval_samples_per_second': 65.772, 'eval_steps_per_second': 2.099, 'epoch': 11.0}
{'train@spa.rst.sctb_loss': 2.1770401000976562, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4328018223234624, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.04473983998304456, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.035811378683407086, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.06163082437275985, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.177039861679077, 'train@spa.rst.sctb_runtime': 5.553, 'train@spa.rst.sctb_samples_per_second': 79.057, 'train@spa.rst.sctb_steps_per_second': 2.521, 'epoch': 12.0}
{'loss': 2.2158, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.2332863807678223, 'eval_accuracy@spa.rst.sctb': 0.425531914893617, 'eval_f1@spa.rst.sctb': 0.06225063938618926, 'eval_precision@spa.rst.sctb': 0.04942607225311352, 'eval_recall@spa.rst.sctb': 0.08574913218876068, 'eval_loss@spa.rst.sctb': 2.2332863807678223, 'eval_runtime': 1.4583, 'eval_samples_per_second': 64.46, 'eval_steps_per_second': 2.057, 'epoch': 12.0}
{'train_runtime': 217.9868, 'train_samples_per_second': 24.167, 'train_steps_per_second': 0.771, 'train_loss': 2.5580416861034574, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.3251
  train_runtime            = 0:31:24.31
  train_samples_per_second =      26.11
  train_steps_per_second   =      0.822
-------------------------------------------------------------------
Lang1:  fra.sdrt.annodis    Lang2:  spa.rst.sctb
Saving run to:  runs/full_shot/FullShot=v4_fra.sdrt.annodis_spa.rst.sctb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2185 examples
read 528 examples
read 625 examples
read 439 examples
read 94 examples
read 159 examples
Total prediction labels:  38
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (fr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=38, bias=True)
    )
  )
)
{'train@fra.sdrt.annodis_loss': 2.6639463901519775, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.22517162471395882, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.044520981339395665, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.04461939982135821, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.06441867220107977, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.6639463901519775, 'train@fra.sdrt.annodis_runtime': 26.317, 'train@fra.sdrt.annodis_samples_per_second': 83.026, 'train@fra.sdrt.annodis_steps_per_second': 2.622, 'epoch': 1.0}
{'loss': 3.1699, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.6595120429992676, 'eval_accuracy@fra.sdrt.annodis': 0.2215909090909091, 'eval_f1@fra.sdrt.annodis': 0.04481515066521071, 'eval_precision@fra.sdrt.annodis': 0.04301415855073983, 'eval_recall@fra.sdrt.annodis': 0.06224067612091149, 'eval_loss@fra.sdrt.annodis': 2.6595122814178467, 'eval_runtime': 6.6801, 'eval_samples_per_second': 79.041, 'eval_steps_per_second': 2.545, 'epoch': 1.0}
{'train@fra.sdrt.annodis_loss': 2.3943288326263428, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.26270022883295197, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.05644604609763274, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.04458724122176267, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.07845201939694366, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.3943288326263428, 'train@fra.sdrt.annodis_runtime': 26.5003, 'train@fra.sdrt.annodis_samples_per_second': 82.452, 'train@fra.sdrt.annodis_steps_per_second': 2.604, 'epoch': 2.0}
{'loss': 2.5282, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.397082567214966, 'eval_accuracy@fra.sdrt.annodis': 0.2746212121212121, 'eval_f1@fra.sdrt.annodis': 0.05847851037223564, 'eval_precision@fra.sdrt.annodis': 0.046295868175023534, 'eval_recall@fra.sdrt.annodis': 0.07993992608045913, 'eval_loss@fra.sdrt.annodis': 2.397082567214966, 'eval_runtime': 6.6666, 'eval_samples_per_second': 79.201, 'eval_steps_per_second': 2.55, 'epoch': 2.0}
{'train@fra.sdrt.annodis_loss': 2.3163108825683594, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.26864988558352404, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.05767575337963052, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.08097790251148279, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.07772588108590955, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.3163108825683594, 'train@fra.sdrt.annodis_runtime': 26.4486, 'train@fra.sdrt.annodis_samples_per_second': 82.613, 'train@fra.sdrt.annodis_steps_per_second': 2.609, 'epoch': 3.0}
{'loss': 2.3809, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.326589584350586, 'eval_accuracy@fra.sdrt.annodis': 0.2537878787878788, 'eval_f1@fra.sdrt.annodis': 0.053400024240073576, 'eval_precision@fra.sdrt.annodis': 0.05242063492063492, 'eval_recall@fra.sdrt.annodis': 0.07171345800629289, 'eval_loss@fra.sdrt.annodis': 2.326589584350586, 'eval_runtime': 6.6649, 'eval_samples_per_second': 79.221, 'eval_steps_per_second': 2.551, 'epoch': 3.0}
{'train@fra.sdrt.annodis_loss': 2.262108087539673, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.2965675057208238, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.0714707064730673, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.08507113193468809, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.08888549605122846, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.262108325958252, 'train@fra.sdrt.annodis_runtime': 26.553, 'train@fra.sdrt.annodis_samples_per_second': 82.288, 'train@fra.sdrt.annodis_steps_per_second': 2.599, 'epoch': 4.0}
{'loss': 2.3264, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.2786505222320557, 'eval_accuracy@fra.sdrt.annodis': 0.25946969696969696, 'eval_f1@fra.sdrt.annodis': 0.05618884916985049, 'eval_precision@fra.sdrt.annodis': 0.05050584680068322, 'eval_recall@fra.sdrt.annodis': 0.07415879015048277, 'eval_loss@fra.sdrt.annodis': 2.2786505222320557, 'eval_runtime': 6.7108, 'eval_samples_per_second': 78.679, 'eval_steps_per_second': 2.533, 'epoch': 4.0}
{'train@fra.sdrt.annodis_loss': 2.217153549194336, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.3308924485125858, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.08006308393910859, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.08723776819918676, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.10219578144836897, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.217153310775757, 'train@fra.sdrt.annodis_runtime': 26.5654, 'train@fra.sdrt.annodis_samples_per_second': 82.25, 'train@fra.sdrt.annodis_steps_per_second': 2.597, 'epoch': 5.0}
{'loss': 2.2723, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.241450548171997, 'eval_accuracy@fra.sdrt.annodis': 0.2916666666666667, 'eval_f1@fra.sdrt.annodis': 0.06331022804208424, 'eval_precision@fra.sdrt.annodis': 0.050361659133588955, 'eval_recall@fra.sdrt.annodis': 0.08706006109952111, 'eval_loss@fra.sdrt.annodis': 2.241450309753418, 'eval_runtime': 6.7102, 'eval_samples_per_second': 78.686, 'eval_steps_per_second': 2.533, 'epoch': 5.0}
{'train@fra.sdrt.annodis_loss': 2.1768906116485596, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.35881006864988557, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.09462021929748299, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.13605280936712305, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.11629593878260087, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.1768906116485596, 'train@fra.sdrt.annodis_runtime': 26.5676, 'train@fra.sdrt.annodis_samples_per_second': 82.243, 'train@fra.sdrt.annodis_steps_per_second': 2.597, 'epoch': 6.0}
{'loss': 2.2313, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.208984851837158, 'eval_accuracy@fra.sdrt.annodis': 0.2916666666666667, 'eval_f1@fra.sdrt.annodis': 0.06593143428071843, 'eval_precision@fra.sdrt.annodis': 0.06301146384479718, 'eval_recall@fra.sdrt.annodis': 0.08999317164471349, 'eval_loss@fra.sdrt.annodis': 2.2089850902557373, 'eval_runtime': 6.7035, 'eval_samples_per_second': 78.764, 'eval_steps_per_second': 2.536, 'epoch': 6.0}
{'train@fra.sdrt.annodis_loss': 2.137556791305542, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.3716247139588101, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.10967806617909634, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12972160100519345, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.12598470998762176, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.137557029724121, 'train@fra.sdrt.annodis_runtime': 26.5736, 'train@fra.sdrt.annodis_samples_per_second': 82.224, 'train@fra.sdrt.annodis_steps_per_second': 2.597, 'epoch': 7.0}
{'loss': 2.1924, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.177053451538086, 'eval_accuracy@fra.sdrt.annodis': 0.3106060606060606, 'eval_f1@fra.sdrt.annodis': 0.08077994599678116, 'eval_precision@fra.sdrt.annodis': 0.11915230738760149, 'eval_recall@fra.sdrt.annodis': 0.09879064013717108, 'eval_loss@fra.sdrt.annodis': 2.177053213119507, 'eval_runtime': 6.6728, 'eval_samples_per_second': 79.127, 'eval_steps_per_second': 2.548, 'epoch': 7.0}
{'train@fra.sdrt.annodis_loss': 2.1026766300201416, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.39862700228832953, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.1254616320397481, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.1289161777498413, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.14183976790448866, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.1026766300201416, 'train@fra.sdrt.annodis_runtime': 26.5833, 'train@fra.sdrt.annodis_samples_per_second': 82.194, 'train@fra.sdrt.annodis_steps_per_second': 2.596, 'epoch': 8.0}
{'loss': 2.1676, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.149709463119507, 'eval_accuracy@fra.sdrt.annodis': 0.32007575757575757, 'eval_f1@fra.sdrt.annodis': 0.08666842784912673, 'eval_precision@fra.sdrt.annodis': 0.10423663509077091, 'eval_recall@fra.sdrt.annodis': 0.10424719925569342, 'eval_loss@fra.sdrt.annodis': 2.1497089862823486, 'eval_runtime': 6.71, 'eval_samples_per_second': 78.688, 'eval_steps_per_second': 2.534, 'epoch': 8.0}
{'train@fra.sdrt.annodis_loss': 2.075813055038452, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.41235697940503435, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.1332580127074293, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.1305995371021626, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1518755000908355, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.075812816619873, 'train@fra.sdrt.annodis_runtime': 26.5632, 'train@fra.sdrt.annodis_samples_per_second': 82.257, 'train@fra.sdrt.annodis_steps_per_second': 2.598, 'epoch': 9.0}
{'loss': 2.1339, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.1281604766845703, 'eval_accuracy@fra.sdrt.annodis': 0.3314393939393939, 'eval_f1@fra.sdrt.annodis': 0.0946918134735412, 'eval_precision@fra.sdrt.annodis': 0.10481999386433308, 'eval_recall@fra.sdrt.annodis': 0.11133979475975174, 'eval_loss@fra.sdrt.annodis': 2.1281607151031494, 'eval_runtime': 7.7581, 'eval_samples_per_second': 68.058, 'eval_steps_per_second': 2.191, 'epoch': 9.0}
{'train@fra.sdrt.annodis_loss': 2.0557453632354736, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.41693363844393594, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.13518088422898644, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.1291532273063546, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.15525085457964882, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.0557453632354736, 'train@fra.sdrt.annodis_runtime': 26.5317, 'train@fra.sdrt.annodis_samples_per_second': 82.354, 'train@fra.sdrt.annodis_steps_per_second': 2.601, 'epoch': 10.0}
{'loss': 2.1126, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.113024950027466, 'eval_accuracy@fra.sdrt.annodis': 0.3333333333333333, 'eval_f1@fra.sdrt.annodis': 0.09608917591848903, 'eval_precision@fra.sdrt.annodis': 0.10396560650536019, 'eval_recall@fra.sdrt.annodis': 0.11258494482651904, 'eval_loss@fra.sdrt.annodis': 2.1130247116088867, 'eval_runtime': 6.7388, 'eval_samples_per_second': 78.352, 'eval_steps_per_second': 2.523, 'epoch': 10.0}
{'train@fra.sdrt.annodis_loss': 2.0434505939483643, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4215102974828375, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.1370029289680373, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12903430004482586, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.15847362223539987, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.0434508323669434, 'train@fra.sdrt.annodis_runtime': 26.5537, 'train@fra.sdrt.annodis_samples_per_second': 82.286, 'train@fra.sdrt.annodis_steps_per_second': 2.599, 'epoch': 11.0}
{'loss': 2.0917, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.1026978492736816, 'eval_accuracy@fra.sdrt.annodis': 0.3390151515151515, 'eval_f1@fra.sdrt.annodis': 0.10006063345950561, 'eval_precision@fra.sdrt.annodis': 0.10173746742967424, 'eval_recall@fra.sdrt.annodis': 0.11665310099090583, 'eval_loss@fra.sdrt.annodis': 2.1026980876922607, 'eval_runtime': 6.728, 'eval_samples_per_second': 78.478, 'eval_steps_per_second': 2.527, 'epoch': 11.0}
{'train@fra.sdrt.annodis_loss': 2.0390419960021973, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4233409610983982, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.13801851521057984, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12880851924884484, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.15998280555237723, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.0390419960021973, 'train@fra.sdrt.annodis_runtime': 26.5746, 'train@fra.sdrt.annodis_samples_per_second': 82.221, 'train@fra.sdrt.annodis_steps_per_second': 2.596, 'epoch': 12.0}
{'loss': 2.086, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.099494218826294, 'eval_accuracy@fra.sdrt.annodis': 0.3390151515151515, 'eval_f1@fra.sdrt.annodis': 0.09979387091173815, 'eval_precision@fra.sdrt.annodis': 0.10018406096615862, 'eval_recall@fra.sdrt.annodis': 0.11663439069182169, 'eval_loss@fra.sdrt.annodis': 2.099494218826294, 'eval_runtime': 6.7073, 'eval_samples_per_second': 78.72, 'eval_steps_per_second': 2.535, 'epoch': 12.0}
{'train_runtime': 1066.3187, 'train_samples_per_second': 24.589, 'train_steps_per_second': 0.777, 'train_loss': 2.3077659606933594, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.3078
  train_runtime            = 0:17:46.31
  train_samples_per_second =     24.589
  train_steps_per_second   =      0.777
{'train@spa.rst.sctb_loss': 3.6211533546447754, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.03189066059225513, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.009093007411557956, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.008678243369331396, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.056563620071684584, 'train@spa.rst.sctb_loss@spa.rst.sctb': 3.6211533546447754, 'train@spa.rst.sctb_runtime': 5.6141, 'train@spa.rst.sctb_samples_per_second': 78.197, 'train@spa.rst.sctb_steps_per_second': 2.494, 'epoch': 1.0}
{'loss': 3.8906, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.672440528869629, 'eval_accuracy@spa.rst.sctb': 0.02127659574468085, 'eval_f1@spa.rst.sctb': 0.010292397660818714, 'eval_precision@spa.rst.sctb': 0.012527233115468408, 'eval_recall@spa.rst.sctb': 0.03070175438596491, 'eval_loss@spa.rst.sctb': 3.672440767288208, 'eval_runtime': 1.4892, 'eval_samples_per_second': 63.121, 'eval_steps_per_second': 2.015, 'epoch': 1.0}
{'train@spa.rst.sctb_loss': 3.1816813945770264, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.1275626423690205, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.021209633278598792, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.016170634920634922, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.031137992831541218, 'train@spa.rst.sctb_loss@spa.rst.sctb': 3.1816813945770264, 'train@spa.rst.sctb_runtime': 5.6081, 'train@spa.rst.sctb_samples_per_second': 78.28, 'train@spa.rst.sctb_steps_per_second': 2.496, 'epoch': 2.0}
{'loss': 3.3912, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.205000877380371, 'eval_accuracy@spa.rst.sctb': 0.11702127659574468, 'eval_f1@spa.rst.sctb': 0.023504273504273504, 'eval_precision@spa.rst.sctb': 0.018518518518518517, 'eval_recall@spa.rst.sctb': 0.03216374269005848, 'eval_loss@spa.rst.sctb': 3.205000877380371, 'eval_runtime': 1.4769, 'eval_samples_per_second': 63.648, 'eval_steps_per_second': 2.031, 'epoch': 2.0}
{'train@spa.rst.sctb_loss': 2.826303720474243, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.30751708428246016, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.035146959727287146, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03154116051421158, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05043906810035842, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.826303482055664, 'train@spa.rst.sctb_runtime': 5.6386, 'train@spa.rst.sctb_samples_per_second': 77.856, 'train@spa.rst.sctb_steps_per_second': 2.483, 'epoch': 3.0}
{'loss': 3.0274, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.8383312225341797, 'eval_accuracy@spa.rst.sctb': 0.30851063829787234, 'eval_f1@spa.rst.sctb': 0.04601846181760451, 'eval_precision@spa.rst.sctb': 0.037485380116959066, 'eval_recall@spa.rst.sctb': 0.06618819776714513, 'eval_loss@spa.rst.sctb': 2.838331460952759, 'eval_runtime': 1.5068, 'eval_samples_per_second': 62.384, 'eval_steps_per_second': 1.991, 'epoch': 3.0}
{'train@spa.rst.sctb_loss': 2.573185443878174, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4054669703872437, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.04280790136550444, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.033216303981106615, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.0606810035842294, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.573185443878174, 'train@spa.rst.sctb_runtime': 5.6459, 'train@spa.rst.sctb_samples_per_second': 77.755, 'train@spa.rst.sctb_steps_per_second': 2.48, 'epoch': 4.0}
{'loss': 2.7259, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.5938870906829834, 'eval_accuracy@spa.rst.sctb': 0.40425531914893614, 'eval_f1@spa.rst.sctb': 0.05784313725490196, 'eval_precision@spa.rst.sctb': 0.045119122917984394, 'eval_recall@spa.rst.sctb': 0.08087062576226663, 'eval_loss@spa.rst.sctb': 2.5938870906829834, 'eval_runtime': 1.4776, 'eval_samples_per_second': 63.616, 'eval_steps_per_second': 2.03, 'epoch': 4.0}
{'train@spa.rst.sctb_loss': 2.420126438140869, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3804100227790433, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.038800526396982664, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.030334907351130643, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05422043010752688, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.420125961303711, 'train@spa.rst.sctb_runtime': 5.6306, 'train@spa.rst.sctb_samples_per_second': 77.966, 'train@spa.rst.sctb_steps_per_second': 2.486, 'epoch': 5.0}
{'loss': 2.532, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.4487054347991943, 'eval_accuracy@spa.rst.sctb': 0.43617021276595747, 'eval_f1@spa.rst.sctb': 0.06307907778496014, 'eval_precision@spa.rst.sctb': 0.051258960412088904, 'eval_recall@spa.rst.sctb': 0.08621821934515432, 'eval_loss@spa.rst.sctb': 2.4487051963806152, 'eval_runtime': 1.4723, 'eval_samples_per_second': 63.844, 'eval_steps_per_second': 2.038, 'epoch': 5.0}
{'train@spa.rst.sctb_loss': 2.33268404006958, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3895216400911162, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.039028596862134196, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03126668090103555, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05431003584229391, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.332683801651001, 'train@spa.rst.sctb_runtime': 5.63, 'train@spa.rst.sctb_samples_per_second': 77.975, 'train@spa.rst.sctb_steps_per_second': 2.487, 'epoch': 6.0}
{'loss': 2.3972, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.3703041076660156, 'eval_accuracy@spa.rst.sctb': 0.4148936170212766, 'eval_f1@spa.rst.sctb': 0.056530214424951264, 'eval_precision@spa.rst.sctb': 0.046769865841073274, 'eval_recall@spa.rst.sctb': 0.0787128248428558, 'eval_loss@spa.rst.sctb': 2.3703038692474365, 'eval_runtime': 1.4683, 'eval_samples_per_second': 64.02, 'eval_steps_per_second': 2.043, 'epoch': 6.0}
{'train@spa.rst.sctb_loss': 2.2772698402404785, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3940774487471526, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.039795197740112996, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.031908610712958536, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05520609318996416, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.2772700786590576, 'train@spa.rst.sctb_runtime': 5.6152, 'train@spa.rst.sctb_samples_per_second': 78.181, 'train@spa.rst.sctb_steps_per_second': 2.493, 'epoch': 7.0}
{'loss': 2.3268, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.322265863418579, 'eval_accuracy@spa.rst.sctb': 0.425531914893617, 'eval_f1@spa.rst.sctb': 0.0593168971398768, 'eval_precision@spa.rst.sctb': 0.04896661367249603, 'eval_recall@spa.rst.sctb': 0.08180880007505395, 'eval_loss@spa.rst.sctb': 2.322265863418579, 'eval_runtime': 1.4628, 'eval_samples_per_second': 64.259, 'eval_steps_per_second': 2.051, 'epoch': 7.0}
{'train@spa.rst.sctb_loss': 2.2422308921813965, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.39635535307517084, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.040251979483093475, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.032114598955370534, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05582437275985663, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.2422311305999756, 'train@spa.rst.sctb_runtime': 5.6215, 'train@spa.rst.sctb_samples_per_second': 78.093, 'train@spa.rst.sctb_steps_per_second': 2.49, 'epoch': 8.0}
{'loss': 2.2835, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.2936973571777344, 'eval_accuracy@spa.rst.sctb': 0.425531914893617, 'eval_f1@spa.rst.sctb': 0.0593168971398768, 'eval_precision@spa.rst.sctb': 0.04896661367249603, 'eval_recall@spa.rst.sctb': 0.08180880007505395, 'eval_loss@spa.rst.sctb': 2.293696880340576, 'eval_runtime': 1.4848, 'eval_samples_per_second': 63.31, 'eval_steps_per_second': 2.021, 'epoch': 8.0}
{'train@spa.rst.sctb_loss': 2.2172586917877197, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.39863325740318906, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.04052141582098089, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.0321865966834018, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.056272401433691756, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.217258930206299, 'train@spa.rst.sctb_runtime': 5.6451, 'train@spa.rst.sctb_samples_per_second': 77.767, 'train@spa.rst.sctb_steps_per_second': 2.48, 'epoch': 9.0}
{'loss': 2.2613, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.2730865478515625, 'eval_accuracy@spa.rst.sctb': 0.43617021276595747, 'eval_f1@spa.rst.sctb': 0.06198668146503885, 'eval_precision@spa.rst.sctb': 0.05099574076205825, 'eval_recall@spa.rst.sctb': 0.08490477530725209, 'eval_loss@spa.rst.sctb': 2.2730865478515625, 'eval_runtime': 1.4657, 'eval_samples_per_second': 64.135, 'eval_steps_per_second': 2.047, 'epoch': 9.0}
{'train@spa.rst.sctb_loss': 2.2003543376922607, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.40774487471526194, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.041797776379274176, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03299588124949025, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05806451612903226, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.2003543376922607, 'train@spa.rst.sctb_runtime': 5.6235, 'train@spa.rst.sctb_samples_per_second': 78.066, 'train@spa.rst.sctb_steps_per_second': 2.49, 'epoch': 10.0}
{'loss': 2.2317, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.2599740028381348, 'eval_accuracy@spa.rst.sctb': 0.43617021276595747, 'eval_f1@spa.rst.sctb': 0.06198668146503885, 'eval_precision@spa.rst.sctb': 0.05099574076205825, 'eval_recall@spa.rst.sctb': 0.08490477530725209, 'eval_loss@spa.rst.sctb': 2.259974241256714, 'eval_runtime': 1.4928, 'eval_samples_per_second': 62.968, 'eval_steps_per_second': 2.01, 'epoch': 10.0}
{'train@spa.rst.sctb_loss': 2.1908223628997803, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4145785876993166, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.0428081313325638, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.0337510674981067, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.059408602150537636, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.1908228397369385, 'train@spa.rst.sctb_runtime': 5.6387, 'train@spa.rst.sctb_samples_per_second': 77.855, 'train@spa.rst.sctb_steps_per_second': 2.483, 'epoch': 11.0}
{'loss': 2.2358, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.252265691757202, 'eval_accuracy@spa.rst.sctb': 0.43617021276595747, 'eval_f1@spa.rst.sctb': 0.06198668146503885, 'eval_precision@spa.rst.sctb': 0.05099574076205825, 'eval_recall@spa.rst.sctb': 0.08490477530725209, 'eval_loss@spa.rst.sctb': 2.252265453338623, 'eval_runtime': 1.4828, 'eval_samples_per_second': 63.393, 'eval_steps_per_second': 2.023, 'epoch': 11.0}
{'train@spa.rst.sctb_loss': 2.1877429485321045, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4123006833712984, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.04259493893640235, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.033557625948930296, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05913082437275985, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.1877427101135254, 'train@spa.rst.sctb_runtime': 5.6421, 'train@spa.rst.sctb_samples_per_second': 77.808, 'train@spa.rst.sctb_steps_per_second': 2.481, 'epoch': 12.0}
{'loss': 2.2336, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.2500312328338623, 'eval_accuracy@spa.rst.sctb': 0.43617021276595747, 'eval_f1@spa.rst.sctb': 0.06198668146503885, 'eval_precision@spa.rst.sctb': 0.05099574076205825, 'eval_recall@spa.rst.sctb': 0.08490477530725209, 'eval_loss@spa.rst.sctb': 2.2500317096710205, 'eval_runtime': 1.4963, 'eval_samples_per_second': 62.82, 'eval_steps_per_second': 2.005, 'epoch': 12.0}
{'train_runtime': 219.6452, 'train_samples_per_second': 23.984, 'train_steps_per_second': 0.765, 'train_loss': 2.628077665964762, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.3078
  train_runtime            = 0:17:46.31
  train_samples_per_second =     24.589
  train_steps_per_second   =      0.777
-------------------------------------------------------------------
Lang1:  nld.rst.nldt    Lang2:  spa.rst.sctb
Saving run to:  runs/full_shot/FullShot=v4_nld.rst.nldt_spa.rst.sctb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 1608 examples
read 331 examples
read 326 examples
read 439 examples
read 94 examples
read 159 examples
Total prediction labels:  36
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=36, bias=True)
    )
  )
)
{'train@nld.rst.nldt_loss': 3.1588525772094727, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26430348258706465, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01306566650270536, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.00825948383084577, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03125, 'train@nld.rst.nldt_loss@nld.rst.nldt': 3.158853054046631, 'train@nld.rst.nldt_runtime': 19.5083, 'train@nld.rst.nldt_samples_per_second': 82.427, 'train@nld.rst.nldt_steps_per_second': 2.614, 'epoch': 1.0}
{'loss': 3.4235, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.1248786449432373, 'eval_accuracy@nld.rst.nldt': 0.27794561933534745, 'eval_f1@nld.rst.nldt': 0.016110673321075214, 'eval_precision@nld.rst.nldt': 0.010294282197605462, 'eval_recall@nld.rst.nldt': 0.037037037037037035, 'eval_loss@nld.rst.nldt': 3.1248786449432373, 'eval_runtime': 4.2901, 'eval_samples_per_second': 77.155, 'eval_steps_per_second': 2.564, 'epoch': 1.0}
{'train@nld.rst.nldt_loss': 2.866328239440918, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26430348258706465, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01306566650270536, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.00825948383084577, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03125, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.866328239440918, 'train@nld.rst.nldt_runtime': 19.5333, 'train@nld.rst.nldt_samples_per_second': 82.321, 'train@nld.rst.nldt_steps_per_second': 2.611, 'epoch': 2.0}
{'loss': 3.0047, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.7873058319091797, 'eval_accuracy@nld.rst.nldt': 0.27794561933534745, 'eval_f1@nld.rst.nldt': 0.016110673321075214, 'eval_precision@nld.rst.nldt': 0.010294282197605462, 'eval_recall@nld.rst.nldt': 0.037037037037037035, 'eval_loss@nld.rst.nldt': 2.787306547164917, 'eval_runtime': 4.2984, 'eval_samples_per_second': 77.005, 'eval_steps_per_second': 2.559, 'epoch': 2.0}
{'train@nld.rst.nldt_loss': 2.7750918865203857, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.27052238805970147, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01842816882875508, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.02118890224358974, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03442985527544351, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.7750914096832275, 'train@nld.rst.nldt_runtime': 19.5739, 'train@nld.rst.nldt_samples_per_second': 82.15, 'train@nld.rst.nldt_steps_per_second': 2.606, 'epoch': 3.0}
{'loss': 2.8418, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.703861713409424, 'eval_accuracy@nld.rst.nldt': 0.2930513595166163, 'eval_f1@nld.rst.nldt': 0.029898775661487523, 'eval_precision@nld.rst.nldt': 0.04765201338410061, 'eval_recall@nld.rst.nldt': 0.046325690770135215, 'eval_loss@nld.rst.nldt': 2.703861713409424, 'eval_runtime': 4.3052, 'eval_samples_per_second': 76.885, 'eval_steps_per_second': 2.555, 'epoch': 3.0}
{'train@nld.rst.nldt_loss': 2.724726915359497, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.28233830845771146, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.026533136732961093, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.02438315751832648, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.04251984126984127, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.724727153778076, 'train@nld.rst.nldt_runtime': 19.5763, 'train@nld.rst.nldt_samples_per_second': 82.14, 'train@nld.rst.nldt_steps_per_second': 2.605, 'epoch': 4.0}
{'loss': 2.7532, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.6629672050476074, 'eval_accuracy@nld.rst.nldt': 0.31419939577039274, 'eval_f1@nld.rst.nldt': 0.040044389099683615, 'eval_precision@nld.rst.nldt': 0.04059312936124531, 'eval_recall@nld.rst.nldt': 0.06038519540935, 'eval_loss@nld.rst.nldt': 2.6629672050476074, 'eval_runtime': 4.3012, 'eval_samples_per_second': 76.954, 'eval_steps_per_second': 2.557, 'epoch': 4.0}
{'train@nld.rst.nldt_loss': 2.6891136169433594, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2904228855721393, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.030548062437759614, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.02600515943612064, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.047315592903828196, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6891138553619385, 'train@nld.rst.nldt_runtime': 19.5659, 'train@nld.rst.nldt_samples_per_second': 82.184, 'train@nld.rst.nldt_steps_per_second': 2.607, 'epoch': 5.0}
{'loss': 2.7253, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.6330394744873047, 'eval_accuracy@nld.rst.nldt': 0.31722054380664655, 'eval_f1@nld.rst.nldt': 0.040862879928629414, 'eval_precision@nld.rst.nldt': 0.03757297938332421, 'eval_recall@nld.rst.nldt': 0.06209902103621911, 'eval_loss@nld.rst.nldt': 2.6330392360687256, 'eval_runtime': 4.3003, 'eval_samples_per_second': 76.971, 'eval_steps_per_second': 2.558, 'epoch': 5.0}
{'train@nld.rst.nldt_loss': 2.657291889190674, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3009950248756219, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03357369885466216, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.02711984653150247, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05240662931839402, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.657291889190674, 'train@nld.rst.nldt_runtime': 19.5985, 'train@nld.rst.nldt_samples_per_second': 82.047, 'train@nld.rst.nldt_steps_per_second': 2.602, 'epoch': 6.0}
{'loss': 2.6928, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.608034610748291, 'eval_accuracy@nld.rst.nldt': 0.32628398791540786, 'eval_f1@nld.rst.nldt': 0.04242508088920796, 'eval_precision@nld.rst.nldt': 0.03736219497089062, 'eval_recall@nld.rst.nldt': 0.06727244843186872, 'eval_loss@nld.rst.nldt': 2.608034372329712, 'eval_runtime': 4.2778, 'eval_samples_per_second': 77.376, 'eval_steps_per_second': 2.571, 'epoch': 6.0}
{'train@nld.rst.nldt_loss': 2.6309194564819336, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30286069651741293, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03426466575736407, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.026616498908850737, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05408146591970121, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6309196949005127, 'train@nld.rst.nldt_runtime': 19.5707, 'train@nld.rst.nldt_samples_per_second': 82.164, 'train@nld.rst.nldt_steps_per_second': 2.606, 'epoch': 7.0}
{'loss': 2.6707, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.5879435539245605, 'eval_accuracy@nld.rst.nldt': 0.32628398791540786, 'eval_f1@nld.rst.nldt': 0.04292245517735714, 'eval_precision@nld.rst.nldt': 0.036775742107981645, 'eval_recall@nld.rst.nldt': 0.06858369756920481, 'eval_loss@nld.rst.nldt': 2.5879440307617188, 'eval_runtime': 4.2836, 'eval_samples_per_second': 77.272, 'eval_steps_per_second': 2.568, 'epoch': 7.0}
{'train@nld.rst.nldt_loss': 2.6099507808685303, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30783582089552236, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03541266514514695, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.026330385015944882, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.057870564892623724, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6099507808685303, 'train@nld.rst.nldt_runtime': 19.5969, 'train@nld.rst.nldt_samples_per_second': 82.054, 'train@nld.rst.nldt_steps_per_second': 2.602, 'epoch': 8.0}
{'loss': 2.6532, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.572371482849121, 'eval_accuracy@nld.rst.nldt': 0.33534743202416917, 'eval_f1@nld.rst.nldt': 0.04774221424785033, 'eval_precision@nld.rst.nldt': 0.07460289421073735, 'eval_recall@nld.rst.nldt': 0.07446259233698847, 'eval_loss@nld.rst.nldt': 2.5723717212677, 'eval_runtime': 4.3098, 'eval_samples_per_second': 76.802, 'eval_steps_per_second': 2.552, 'epoch': 8.0}
{'train@nld.rst.nldt_loss': 2.597177743911743, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31032338308457713, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.036353770896656534, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.02696190848315372, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05909547152194211, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.597177743911743, 'train@nld.rst.nldt_runtime': 19.5922, 'train@nld.rst.nldt_samples_per_second': 82.074, 'train@nld.rst.nldt_steps_per_second': 2.603, 'epoch': 9.0}
{'loss': 2.6286, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.5624358654022217, 'eval_accuracy@nld.rst.nldt': 0.3323262839879154, 'eval_f1@nld.rst.nldt': 0.046821152703505654, 'eval_precision@nld.rst.nldt': 0.07401709401709403, 'eval_recall@nld.rst.nldt': 0.07240497916826419, 'eval_loss@nld.rst.nldt': 2.5624358654022217, 'eval_runtime': 4.2929, 'eval_samples_per_second': 77.104, 'eval_steps_per_second': 2.562, 'epoch': 9.0}
{'train@nld.rst.nldt_loss': 2.58807373046875, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31094527363184077, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03659981251333489, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.027247114323655213, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05916900093370682, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.588073492050171, 'train@nld.rst.nldt_runtime': 19.5759, 'train@nld.rst.nldt_samples_per_second': 82.142, 'train@nld.rst.nldt_steps_per_second': 2.605, 'epoch': 10.0}
{'loss': 2.6207, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.555402994155884, 'eval_accuracy@nld.rst.nldt': 0.3293051359516616, 'eval_f1@nld.rst.nldt': 0.04578915448480666, 'eval_precision@nld.rst.nldt': 0.05442273775607109, 'eval_recall@nld.rst.nldt': 0.07034736599953992, 'eval_loss@nld.rst.nldt': 2.555402994155884, 'eval_runtime': 4.3079, 'eval_samples_per_second': 76.835, 'eval_steps_per_second': 2.553, 'epoch': 10.0}
{'train@nld.rst.nldt_loss': 2.581725835800171, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31281094527363185, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03701753867389642, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.02740067078563283, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.060145891690009334, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.5817253589630127, 'train@nld.rst.nldt_runtime': 19.5667, 'train@nld.rst.nldt_samples_per_second': 82.18, 'train@nld.rst.nldt_steps_per_second': 2.606, 'epoch': 11.0}
{'loss': 2.6152, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.550745725631714, 'eval_accuracy@nld.rst.nldt': 0.3323262839879154, 'eval_f1@nld.rst.nldt': 0.04645589672484841, 'eval_precision@nld.rst.nldt': 0.05429810425919376, 'eval_recall@nld.rst.nldt': 0.07240497916826419, 'eval_loss@nld.rst.nldt': 2.550745964050293, 'eval_runtime': 4.3282, 'eval_samples_per_second': 76.475, 'eval_steps_per_second': 2.541, 'epoch': 11.0}
{'train@nld.rst.nldt_loss': 2.5790770053863525, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31405472636815923, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.037234052251787636, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.02746520262764393, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.06070028011204482, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.5790772438049316, 'train@nld.rst.nldt_runtime': 19.6058, 'train@nld.rst.nldt_samples_per_second': 82.016, 'train@nld.rst.nldt_steps_per_second': 2.601, 'epoch': 12.0}
{'loss': 2.6095, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.5492289066314697, 'eval_accuracy@nld.rst.nldt': 0.33534743202416917, 'eval_f1@nld.rst.nldt': 0.04730955450858657, 'eval_precision@nld.rst.nldt': 0.07320804195804197, 'eval_recall@nld.rst.nldt': 0.07446259233698847, 'eval_loss@nld.rst.nldt': 2.5492289066314697, 'eval_runtime': 4.296, 'eval_samples_per_second': 77.049, 'eval_steps_per_second': 2.561, 'epoch': 12.0}
{'train_runtime': 776.5453, 'train_samples_per_second': 24.849, 'train_steps_per_second': 0.788, 'train_loss': 2.769933762893178, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.7699
  train_runtime            = 0:12:56.54
  train_samples_per_second =     24.849
  train_steps_per_second   =      0.788
{'train@spa.rst.sctb_loss': 3.511280059814453, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.011389521640091117, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.003305785123966942, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.0017391304347826085, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.03333333333333333, 'train@spa.rst.sctb_loss@spa.rst.sctb': 3.511279582977295, 'train@spa.rst.sctb_runtime': 5.6026, 'train@spa.rst.sctb_samples_per_second': 78.356, 'train@spa.rst.sctb_steps_per_second': 2.499, 'epoch': 1.0}
{'loss': 3.7788, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.5546557903289795, 'eval_accuracy@spa.rst.sctb': 0.010638297872340425, 'eval_f1@spa.rst.sctb': 0.0030959752321981426, 'eval_precision@spa.rst.sctb': 0.05263157894736842, 'eval_recall@spa.rst.sctb': 0.001594896331738437, 'eval_loss@spa.rst.sctb': 3.554654598236084, 'eval_runtime': 1.4682, 'eval_samples_per_second': 64.023, 'eval_steps_per_second': 2.043, 'epoch': 1.0}
{'train@spa.rst.sctb_loss': 3.1199533939361572, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.2870159453302961, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.023708879687140555, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.01677814177814178, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04833333333333333, 'train@spa.rst.sctb_loss@spa.rst.sctb': 3.119953155517578, 'train@spa.rst.sctb_runtime': 5.5916, 'train@spa.rst.sctb_samples_per_second': 78.511, 'train@spa.rst.sctb_steps_per_second': 2.504, 'epoch': 2.0}
{'loss': 3.3209, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.1648154258728027, 'eval_accuracy@spa.rst.sctb': 0.30851063829787234, 'eval_f1@spa.rst.sctb': 0.029561671763506627, 'eval_precision@spa.rst.sctb': 0.021198830409356727, 'eval_recall@spa.rst.sctb': 0.04882154882154882, 'eval_loss@spa.rst.sctb': 3.1648144721984863, 'eval_runtime': 1.4708, 'eval_samples_per_second': 63.911, 'eval_steps_per_second': 2.04, 'epoch': 2.0}
{'train@spa.rst.sctb_loss': 2.805185556411743, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3416856492027335, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02136752136752137, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.014367816091954025, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.041666666666666664, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.805185556411743, 'train@spa.rst.sctb_runtime': 5.5954, 'train@spa.rst.sctb_samples_per_second': 78.457, 'train@spa.rst.sctb_steps_per_second': 2.502, 'epoch': 3.0}
{'loss': 3.014, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.8522186279296875, 'eval_accuracy@spa.rst.sctb': 0.35106382978723405, 'eval_f1@spa.rst.sctb': 0.03056970819823993, 'eval_precision@spa.rst.sctb': 0.02065081351689612, 'eval_recall@spa.rst.sctb': 0.058823529411764705, 'eval_loss@spa.rst.sctb': 2.8522188663482666, 'eval_runtime': 1.4442, 'eval_samples_per_second': 65.089, 'eval_steps_per_second': 2.077, 'epoch': 3.0}
{'train@spa.rst.sctb_loss': 2.5843706130981445, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3416856492027335, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.021222410865874366, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.014236902050113895, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.041666666666666664, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.5843706130981445, 'train@spa.rst.sctb_runtime': 5.605, 'train@spa.rst.sctb_samples_per_second': 78.323, 'train@spa.rst.sctb_steps_per_second': 2.498, 'epoch': 4.0}
{'loss': 2.733, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.634408473968506, 'eval_accuracy@spa.rst.sctb': 0.35106382978723405, 'eval_f1@spa.rst.sctb': 0.03056970819823993, 'eval_precision@spa.rst.sctb': 0.02065081351689612, 'eval_recall@spa.rst.sctb': 0.058823529411764705, 'eval_loss@spa.rst.sctb': 2.634409189224243, 'eval_runtime': 1.4679, 'eval_samples_per_second': 64.036, 'eval_steps_per_second': 2.044, 'epoch': 4.0}
{'train@spa.rst.sctb_loss': 2.4409351348876953, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3416856492027335, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.021222410865874366, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.014236902050113895, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.041666666666666664, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.440934896469116, 'train@spa.rst.sctb_runtime': 6.3822, 'train@spa.rst.sctb_samples_per_second': 68.785, 'train@spa.rst.sctb_steps_per_second': 2.194, 'epoch': 5.0}
{'loss': 2.5458, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.492018938064575, 'eval_accuracy@spa.rst.sctb': 0.35106382978723405, 'eval_f1@spa.rst.sctb': 0.03056970819823993, 'eval_precision@spa.rst.sctb': 0.02065081351689612, 'eval_recall@spa.rst.sctb': 0.058823529411764705, 'eval_loss@spa.rst.sctb': 2.4920194149017334, 'eval_runtime': 1.4563, 'eval_samples_per_second': 64.546, 'eval_steps_per_second': 2.06, 'epoch': 5.0}
{'train@spa.rst.sctb_loss': 2.349782943725586, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3439635535307517, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02219911357603337, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.02822375127420999, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.042114695340501794, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.349783182144165, 'train@spa.rst.sctb_runtime': 5.5692, 'train@spa.rst.sctb_samples_per_second': 78.826, 'train@spa.rst.sctb_steps_per_second': 2.514, 'epoch': 6.0}
{'loss': 2.4346, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.4048912525177, 'eval_accuracy@spa.rst.sctb': 0.35106382978723405, 'eval_f1@spa.rst.sctb': 0.03056970819823993, 'eval_precision@spa.rst.sctb': 0.02065081351689612, 'eval_recall@spa.rst.sctb': 0.058823529411764705, 'eval_loss@spa.rst.sctb': 2.4048914909362793, 'eval_runtime': 1.4638, 'eval_samples_per_second': 64.216, 'eval_steps_per_second': 2.049, 'epoch': 6.0}
{'train@spa.rst.sctb_loss': 2.290149211883545, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3621867881548975, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.029635973875065114, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03322866344605475, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04603942652329749, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.290149211883545, 'train@spa.rst.sctb_runtime': 5.5985, 'train@spa.rst.sctb_samples_per_second': 78.414, 'train@spa.rst.sctb_steps_per_second': 2.501, 'epoch': 7.0}
{'loss': 2.3587, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.3487744331359863, 'eval_accuracy@spa.rst.sctb': 0.3829787234042553, 'eval_f1@spa.rst.sctb': 0.04652844744455159, 'eval_precision@spa.rst.sctb': 0.05710508922670192, 'eval_recall@spa.rst.sctb': 0.06811145510835914, 'eval_loss@spa.rst.sctb': 2.3487744331359863, 'eval_runtime': 1.4668, 'eval_samples_per_second': 64.084, 'eval_steps_per_second': 2.045, 'epoch': 7.0}
{'train@spa.rst.sctb_loss': 2.2541403770446777, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.37813211845102507, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03387256765555425, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.032158017192384196, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04934587813620072, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.254140615463257, 'train@spa.rst.sctb_runtime': 5.5769, 'train@spa.rst.sctb_samples_per_second': 78.717, 'train@spa.rst.sctb_steps_per_second': 2.51, 'epoch': 8.0}
{'loss': 2.2982, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.3163180351257324, 'eval_accuracy@spa.rst.sctb': 0.40425531914893614, 'eval_f1@spa.rst.sctb': 0.053909699473009554, 'eval_precision@spa.rst.sctb': 0.055517108804306034, 'eval_recall@spa.rst.sctb': 0.07430340557275542, 'eval_loss@spa.rst.sctb': 2.3163180351257324, 'eval_runtime': 1.4635, 'eval_samples_per_second': 64.231, 'eval_steps_per_second': 2.05, 'epoch': 8.0}
{'train@spa.rst.sctb_loss': 2.2295000553131104, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4054669703872437, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03985834124723014, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.035765112939025984, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05489247311827957, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.2295000553131104, 'train@spa.rst.sctb_runtime': 5.609, 'train@spa.rst.sctb_samples_per_second': 78.267, 'train@spa.rst.sctb_steps_per_second': 2.496, 'epoch': 9.0}
{'loss': 2.2893, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.2936959266662598, 'eval_accuracy@spa.rst.sctb': 0.43617021276595747, 'eval_f1@spa.rst.sctb': 0.06346749226006192, 'eval_precision@spa.rst.sctb': 0.060164236634824876, 'eval_recall@spa.rst.sctb': 0.08359133126934984, 'eval_loss@spa.rst.sctb': 2.2936959266662598, 'eval_runtime': 1.4835, 'eval_samples_per_second': 63.362, 'eval_steps_per_second': 2.022, 'epoch': 9.0}
{'train@spa.rst.sctb_loss': 2.2134180068969727, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4168564920273349, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.042181324110671936, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03643224583728171, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05747311827956989, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.2134180068969727, 'train@spa.rst.sctb_runtime': 5.6083, 'train@spa.rst.sctb_samples_per_second': 78.277, 'train@spa.rst.sctb_steps_per_second': 2.496, 'epoch': 10.0}
{'loss': 2.2594, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.2798657417297363, 'eval_accuracy@spa.rst.sctb': 0.43617021276595747, 'eval_f1@spa.rst.sctb': 0.06346749226006192, 'eval_precision@spa.rst.sctb': 0.060164236634824876, 'eval_recall@spa.rst.sctb': 0.08359133126934984, 'eval_loss@spa.rst.sctb': 2.2798659801483154, 'eval_runtime': 1.4498, 'eval_samples_per_second': 64.836, 'eval_steps_per_second': 2.069, 'epoch': 10.0}
{'train@spa.rst.sctb_loss': 2.2043981552124023, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4145785876993166, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.041902606860472036, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03594909715298549, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05719534050179212, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.2043983936309814, 'train@spa.rst.sctb_runtime': 5.6054, 'train@spa.rst.sctb_samples_per_second': 78.317, 'train@spa.rst.sctb_steps_per_second': 2.498, 'epoch': 11.0}
{'loss': 2.244, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.2717318534851074, 'eval_accuracy@spa.rst.sctb': 0.43617021276595747, 'eval_f1@spa.rst.sctb': 0.06346749226006192, 'eval_precision@spa.rst.sctb': 0.060164236634824876, 'eval_recall@spa.rst.sctb': 0.08359133126934984, 'eval_loss@spa.rst.sctb': 2.2717318534851074, 'eval_runtime': 1.4472, 'eval_samples_per_second': 64.955, 'eval_steps_per_second': 2.073, 'epoch': 11.0}
{'train@spa.rst.sctb_loss': 2.2014241218566895, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4168564920273349, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.042313160145273615, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.0362589762171421, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05764336917562724, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.2014243602752686, 'train@spa.rst.sctb_runtime': 5.5814, 'train@spa.rst.sctb_samples_per_second': 78.655, 'train@spa.rst.sctb_steps_per_second': 2.508, 'epoch': 12.0}
{'loss': 2.2438, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.269402265548706, 'eval_accuracy@spa.rst.sctb': 0.43617021276595747, 'eval_f1@spa.rst.sctb': 0.06346749226006192, 'eval_precision@spa.rst.sctb': 0.060164236634824876, 'eval_recall@spa.rst.sctb': 0.08359133126934984, 'eval_loss@spa.rst.sctb': 2.269402027130127, 'eval_runtime': 1.4452, 'eval_samples_per_second': 65.043, 'eval_steps_per_second': 2.076, 'epoch': 12.0}
{'train_runtime': 219.4253, 'train_samples_per_second': 24.008, 'train_steps_per_second': 0.766, 'train_loss': 2.626708257765997, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.7699
  train_runtime            = 0:12:56.54
  train_samples_per_second =     24.849
  train_steps_per_second   =      0.788
-------------------------------------------------------------------
Lang1:  por.rst.cstn    Lang2:  spa.rst.sctb
Saving run to:  runs/full_shot/FullShot=v4_por.rst.cstn_spa.rst.sctb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 4148 examples
read 573 examples
read 272 examples
read 439 examples
read 94 examples
read 159 examples
Total prediction labels:  38
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (pt): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (pt): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=38, bias=True)
    )
  )
)
{'train@por.rst.cstn_loss': 2.475050210952759, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.2774831243972999, 'train@por.rst.cstn_f1@por.rst.cstn': 0.013575674655595393, 'train@por.rst.cstn_precision@por.rst.cstn': 0.008671347637415621, 'train@por.rst.cstn_recall@por.rst.cstn': 0.03125, 'train@por.rst.cstn_loss@por.rst.cstn': 2.475050449371338, 'train@por.rst.cstn_runtime': 49.9089, 'train@por.rst.cstn_samples_per_second': 83.111, 'train@por.rst.cstn_steps_per_second': 2.605, 'epoch': 1.0}
{'loss': 2.9585, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.567805528640747, 'eval_accuracy@por.rst.cstn': 0.28097731239092494, 'eval_f1@por.rst.cstn': 0.019940549913301957, 'eval_precision@por.rst.cstn': 0.012771696017769315, 'eval_recall@por.rst.cstn': 0.045454545454545456, 'eval_loss@por.rst.cstn': 2.567805767059326, 'eval_runtime': 7.2155, 'eval_samples_per_second': 79.412, 'eval_steps_per_second': 2.495, 'epoch': 1.0}
{'train@por.rst.cstn_loss': 2.199965000152588, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.37246865959498554, 'train@por.rst.cstn_f1@por.rst.cstn': 0.05335791948956896, 'train@por.rst.cstn_precision@por.rst.cstn': 0.088525092807756, 'train@por.rst.cstn_recall@por.rst.cstn': 0.06295895926949449, 'train@por.rst.cstn_loss@por.rst.cstn': 2.199964761734009, 'train@por.rst.cstn_runtime': 50.2173, 'train@por.rst.cstn_samples_per_second': 82.601, 'train@por.rst.cstn_steps_per_second': 2.589, 'epoch': 2.0}
{'loss': 2.3641, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.303581953048706, 'eval_accuracy@por.rst.cstn': 0.3298429319371728, 'eval_f1@por.rst.cstn': 0.06826218350015523, 'eval_precision@por.rst.cstn': 0.09031986531986533, 'eval_recall@por.rst.cstn': 0.08500590318772137, 'eval_loss@por.rst.cstn': 2.303581953048706, 'eval_runtime': 7.2612, 'eval_samples_per_second': 78.912, 'eval_steps_per_second': 2.479, 'epoch': 2.0}
{'train@por.rst.cstn_loss': 1.9679770469665527, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.46962391513982643, 'train@por.rst.cstn_f1@por.rst.cstn': 0.07409531226958653, 'train@por.rst.cstn_precision@por.rst.cstn': 0.07146668295097794, 'train@por.rst.cstn_recall@por.rst.cstn': 0.09023457923003413, 'train@por.rst.cstn_loss@por.rst.cstn': 1.9679769277572632, 'train@por.rst.cstn_runtime': 50.2602, 'train@por.rst.cstn_samples_per_second': 82.531, 'train@por.rst.cstn_steps_per_second': 2.587, 'epoch': 3.0}
{'loss': 2.1299, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.0846166610717773, 'eval_accuracy@por.rst.cstn': 0.387434554973822, 'eval_f1@por.rst.cstn': 0.0957554873321262, 'eval_precision@por.rst.cstn': 0.09021924529961732, 'eval_recall@por.rst.cstn': 0.12800931992215536, 'eval_loss@por.rst.cstn': 2.0846166610717773, 'eval_runtime': 7.2662, 'eval_samples_per_second': 78.858, 'eval_steps_per_second': 2.477, 'epoch': 3.0}
{'train@por.rst.cstn_loss': 1.8095275163650513, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5351976856316297, 'train@por.rst.cstn_f1@por.rst.cstn': 0.10683402980001674, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13777230719128272, 'train@por.rst.cstn_recall@por.rst.cstn': 0.11718789340574709, 'train@por.rst.cstn_loss@por.rst.cstn': 1.8095273971557617, 'train@por.rst.cstn_runtime': 50.087, 'train@por.rst.cstn_samples_per_second': 82.816, 'train@por.rst.cstn_steps_per_second': 2.595, 'epoch': 4.0}
{'loss': 1.9477, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.9460619688034058, 'eval_accuracy@por.rst.cstn': 0.44502617801047123, 'eval_f1@por.rst.cstn': 0.13436820448750783, 'eval_precision@por.rst.cstn': 0.1725195924331251, 'eval_recall@por.rst.cstn': 0.16201342228490137, 'eval_loss@por.rst.cstn': 1.9460619688034058, 'eval_runtime': 7.2203, 'eval_samples_per_second': 79.359, 'eval_steps_per_second': 2.493, 'epoch': 4.0}
{'train@por.rst.cstn_loss': 1.7061116695404053, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.54725168756027, 'train@por.rst.cstn_f1@por.rst.cstn': 0.12049975962433943, 'train@por.rst.cstn_precision@por.rst.cstn': 0.1290473444131208, 'train@por.rst.cstn_recall@por.rst.cstn': 0.13221787577571265, 'train@por.rst.cstn_loss@por.rst.cstn': 1.7061115503311157, 'train@por.rst.cstn_runtime': 50.0143, 'train@por.rst.cstn_samples_per_second': 82.936, 'train@por.rst.cstn_steps_per_second': 2.599, 'epoch': 5.0}
{'loss': 1.8227, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.8441603183746338, 'eval_accuracy@por.rst.cstn': 0.4712041884816754, 'eval_f1@por.rst.cstn': 0.16482986197243069, 'eval_precision@por.rst.cstn': 0.17619008654112164, 'eval_recall@por.rst.cstn': 0.1865140078006731, 'eval_loss@por.rst.cstn': 1.8441603183746338, 'eval_runtime': 7.2353, 'eval_samples_per_second': 79.195, 'eval_steps_per_second': 2.488, 'epoch': 5.0}
{'train@por.rst.cstn_loss': 1.6433199644088745, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5706364513018322, 'train@por.rst.cstn_f1@por.rst.cstn': 0.12878221073468335, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13183648372072237, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1410980411721806, 'train@por.rst.cstn_loss@por.rst.cstn': 1.643319845199585, 'train@por.rst.cstn_runtime': 50.006, 'train@por.rst.cstn_samples_per_second': 82.95, 'train@por.rst.cstn_steps_per_second': 2.6, 'epoch': 6.0}
{'loss': 1.739, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.781415343284607, 'eval_accuracy@por.rst.cstn': 0.4816753926701571, 'eval_f1@por.rst.cstn': 0.1706121551251406, 'eval_precision@por.rst.cstn': 0.17278260373514384, 'eval_recall@por.rst.cstn': 0.19402563031125544, 'eval_loss@por.rst.cstn': 1.7814152240753174, 'eval_runtime': 7.2564, 'eval_samples_per_second': 78.964, 'eval_steps_per_second': 2.481, 'epoch': 6.0}
{'train@por.rst.cstn_loss': 1.5990763902664185, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.579556412729026, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1347357701287114, 'train@por.rst.cstn_precision@por.rst.cstn': 0.1316362437457658, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1471530366466582, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5990763902664185, 'train@por.rst.cstn_runtime': 50.0105, 'train@por.rst.cstn_samples_per_second': 82.943, 'train@por.rst.cstn_steps_per_second': 2.599, 'epoch': 7.0}
{'loss': 1.6777, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.735901951789856, 'eval_accuracy@por.rst.cstn': 0.49040139616055844, 'eval_f1@por.rst.cstn': 0.1780308996169451, 'eval_precision@por.rst.cstn': 0.17472627237310936, 'eval_recall@por.rst.cstn': 0.19942956489919253, 'eval_loss@por.rst.cstn': 1.7359018325805664, 'eval_runtime': 7.2114, 'eval_samples_per_second': 79.457, 'eval_steps_per_second': 2.496, 'epoch': 7.0}
{'train@por.rst.cstn_loss': 1.573235034942627, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5829315332690453, 'train@por.rst.cstn_f1@por.rst.cstn': 0.13741174942008164, 'train@por.rst.cstn_precision@por.rst.cstn': 0.16496515890235303, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14880382412628662, 'train@por.rst.cstn_loss@por.rst.cstn': 1.573235273361206, 'train@por.rst.cstn_runtime': 49.8791, 'train@por.rst.cstn_samples_per_second': 83.161, 'train@por.rst.cstn_steps_per_second': 2.606, 'epoch': 8.0}
{'loss': 1.6461, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.717288613319397, 'eval_accuracy@por.rst.cstn': 0.49214659685863876, 'eval_f1@por.rst.cstn': 0.17807630951807563, 'eval_precision@por.rst.cstn': 0.17634355627151835, 'eval_recall@por.rst.cstn': 0.19909064192400994, 'eval_loss@por.rst.cstn': 1.717288613319397, 'eval_runtime': 7.2081, 'eval_samples_per_second': 79.494, 'eval_steps_per_second': 2.497, 'epoch': 8.0}
{'train@por.rst.cstn_loss': 1.5521467924118042, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5853423336547734, 'train@por.rst.cstn_f1@por.rst.cstn': 0.13816056468202575, 'train@por.rst.cstn_precision@por.rst.cstn': 0.164634321484788, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14976206596081368, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5521466732025146, 'train@por.rst.cstn_runtime': 49.946, 'train@por.rst.cstn_samples_per_second': 83.05, 'train@por.rst.cstn_steps_per_second': 2.603, 'epoch': 9.0}
{'loss': 1.6172, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.6969820261001587, 'eval_accuracy@por.rst.cstn': 0.5113438045375218, 'eval_f1@por.rst.cstn': 0.18747041049227767, 'eval_precision@por.rst.cstn': 0.1845448935023053, 'eval_recall@por.rst.cstn': 0.2077574535812522, 'eval_loss@por.rst.cstn': 1.6969821453094482, 'eval_runtime': 7.2216, 'eval_samples_per_second': 79.345, 'eval_steps_per_second': 2.493, 'epoch': 9.0}
{'train@por.rst.cstn_loss': 1.536405324935913, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.590887174541948, 'train@por.rst.cstn_f1@por.rst.cstn': 0.14077421304904902, 'train@por.rst.cstn_precision@por.rst.cstn': 0.14820594115328123, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1535795196308242, 'train@por.rst.cstn_loss@por.rst.cstn': 1.536405324935913, 'train@por.rst.cstn_runtime': 49.8913, 'train@por.rst.cstn_samples_per_second': 83.141, 'train@por.rst.cstn_steps_per_second': 2.606, 'epoch': 10.0}
{'loss': 1.5973, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.6805336475372314, 'eval_accuracy@por.rst.cstn': 0.5078534031413613, 'eval_f1@por.rst.cstn': 0.19552559494042834, 'eval_precision@por.rst.cstn': 0.22735433456087126, 'eval_recall@por.rst.cstn': 0.21257740335387518, 'eval_loss@por.rst.cstn': 1.680533528327942, 'eval_runtime': 7.2374, 'eval_samples_per_second': 79.173, 'eval_steps_per_second': 2.487, 'epoch': 10.0}
{'train@por.rst.cstn_loss': 1.5284279584884644, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5925747348119575, 'train@por.rst.cstn_f1@por.rst.cstn': 0.14251115658335864, 'train@por.rst.cstn_precision@por.rst.cstn': 0.152514372463546, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1548158321636848, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5284278392791748, 'train@por.rst.cstn_runtime': 50.0513, 'train@por.rst.cstn_samples_per_second': 82.875, 'train@por.rst.cstn_steps_per_second': 2.597, 'epoch': 11.0}
{'loss': 1.587, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.6749337911605835, 'eval_accuracy@por.rst.cstn': 0.5130890052356021, 'eval_f1@por.rst.cstn': 0.19621768988543922, 'eval_precision@por.rst.cstn': 0.20650346289877206, 'eval_recall@por.rst.cstn': 0.21342438246172388, 'eval_loss@por.rst.cstn': 1.674933671951294, 'eval_runtime': 7.223, 'eval_samples_per_second': 79.33, 'eval_steps_per_second': 2.492, 'epoch': 11.0}
{'train@por.rst.cstn_loss': 1.5255753993988037, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5925747348119575, 'train@por.rst.cstn_f1@por.rst.cstn': 0.14320535488992542, 'train@por.rst.cstn_precision@por.rst.cstn': 0.15465431847382122, 'train@por.rst.cstn_recall@por.rst.cstn': 0.15519020793188248, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5255753993988037, 'train@por.rst.cstn_runtime': 49.9465, 'train@por.rst.cstn_samples_per_second': 83.049, 'train@por.rst.cstn_steps_per_second': 2.603, 'epoch': 12.0}
{'loss': 1.5845, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.6711905002593994, 'eval_accuracy@por.rst.cstn': 0.5148342059336823, 'eval_f1@por.rst.cstn': 0.19702046920388072, 'eval_precision@por.rst.cstn': 0.20668771571071326, 'eval_recall@por.rst.cstn': 0.21417511394368063, 'eval_loss@por.rst.cstn': 1.6711902618408203, 'eval_runtime': 7.202, 'eval_samples_per_second': 79.562, 'eval_steps_per_second': 2.499, 'epoch': 12.0}
{'train_runtime': 1957.8406, 'train_samples_per_second': 25.424, 'train_steps_per_second': 0.797, 'train_loss': 1.8893160208677635, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.8893
  train_runtime            = 0:32:37.84
  train_samples_per_second =     25.424
  train_steps_per_second   =      0.797
{'train@spa.rst.sctb_loss': 3.5970003604888916, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.04328018223234624, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02909347442680776, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.033605442176870746, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04228571428571429, 'train@spa.rst.sctb_loss@spa.rst.sctb': 3.5970003604888916, 'train@spa.rst.sctb_runtime': 5.6397, 'train@spa.rst.sctb_samples_per_second': 77.841, 'train@spa.rst.sctb_steps_per_second': 2.482, 'epoch': 1.0}
{'loss': 3.9271, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.5969796180725098, 'eval_accuracy@spa.rst.sctb': 0.02127659574468085, 'eval_f1@spa.rst.sctb': 0.015188834154351396, 'eval_precision@spa.rst.sctb': 0.017857142857142856, 'eval_recall@spa.rst.sctb': 0.01904761904761905, 'eval_loss@spa.rst.sctb': 3.596980333328247, 'eval_runtime': 1.4902, 'eval_samples_per_second': 63.08, 'eval_steps_per_second': 2.013, 'epoch': 1.0}
{'train@spa.rst.sctb_loss': 3.153472661972046, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.22779043280182232, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.04219091068136246, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.036606606606606605, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05036190476190477, 'train@spa.rst.sctb_loss@spa.rst.sctb': 3.153472900390625, 'train@spa.rst.sctb_runtime': 5.617, 'train@spa.rst.sctb_samples_per_second': 78.155, 'train@spa.rst.sctb_steps_per_second': 2.492, 'epoch': 2.0}
{'loss': 3.3848, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.176201343536377, 'eval_accuracy@spa.rst.sctb': 0.24468085106382978, 'eval_f1@spa.rst.sctb': 0.035493827160493825, 'eval_precision@spa.rst.sctb': 0.030059523809523807, 'eval_recall@spa.rst.sctb': 0.043333333333333335, 'eval_loss@spa.rst.sctb': 3.1762020587921143, 'eval_runtime': 1.4616, 'eval_samples_per_second': 64.312, 'eval_steps_per_second': 2.053, 'epoch': 2.0}
{'train@spa.rst.sctb_loss': 2.817758798599243, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.357630979498861, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.04544065905775337, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03833024118738405, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.06316190476190477, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.817758798599243, 'train@spa.rst.sctb_runtime': 5.6263, 'train@spa.rst.sctb_samples_per_second': 78.026, 'train@spa.rst.sctb_steps_per_second': 2.488, 'epoch': 3.0}
{'loss': 3.0351, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.8501362800598145, 'eval_accuracy@spa.rst.sctb': 0.3404255319148936, 'eval_f1@spa.rst.sctb': 0.04129059031019815, 'eval_precision@spa.rst.sctb': 0.03391472868217054, 'eval_recall@spa.rst.sctb': 0.0632996632996633, 'eval_loss@spa.rst.sctb': 2.8501365184783936, 'eval_runtime': 1.4786, 'eval_samples_per_second': 63.574, 'eval_steps_per_second': 2.029, 'epoch': 3.0}
{'train@spa.rst.sctb_loss': 2.5823116302490234, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.36446469248291574, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.047475102519039254, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.04612887828162291, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.06232380952380952, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.5823118686676025, 'train@spa.rst.sctb_runtime': 5.7998, 'train@spa.rst.sctb_samples_per_second': 75.692, 'train@spa.rst.sctb_steps_per_second': 2.414, 'epoch': 4.0}
{'loss': 2.7412, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.6242668628692627, 'eval_accuracy@spa.rst.sctb': 0.3617021276595745, 'eval_f1@spa.rst.sctb': 0.050666666666666665, 'eval_precision@spa.rst.sctb': 0.079923273657289, 'eval_recall@spa.rst.sctb': 0.07058823529411765, 'eval_loss@spa.rst.sctb': 2.6242668628692627, 'eval_runtime': 1.4951, 'eval_samples_per_second': 62.87, 'eval_steps_per_second': 2.007, 'epoch': 4.0}
{'train@spa.rst.sctb_loss': 2.4244320392608643, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.36674259681093396, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.05189723320158104, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.05627450980392157, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.0651984126984127, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.4244322776794434, 'train@spa.rst.sctb_runtime': 5.6135, 'train@spa.rst.sctb_samples_per_second': 78.205, 'train@spa.rst.sctb_steps_per_second': 2.494, 'epoch': 5.0}
{'loss': 2.5449, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.475496292114258, 'eval_accuracy@spa.rst.sctb': 0.3617021276595745, 'eval_f1@spa.rst.sctb': 0.05042016806722689, 'eval_precision@spa.rst.sctb': 0.07969639468690703, 'eval_recall@spa.rst.sctb': 0.07058823529411765, 'eval_loss@spa.rst.sctb': 2.4754955768585205, 'eval_runtime': 1.4883, 'eval_samples_per_second': 63.157, 'eval_steps_per_second': 2.016, 'epoch': 5.0}
{'train@spa.rst.sctb_loss': 2.3213937282562256, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3712984054669704, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.05118727598566308, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.09800469483568075, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.06285202252944189, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.3213937282562256, 'train@spa.rst.sctb_runtime': 5.6145, 'train@spa.rst.sctb_samples_per_second': 78.191, 'train@spa.rst.sctb_steps_per_second': 2.494, 'epoch': 6.0}
{'loss': 2.4358, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.383080005645752, 'eval_accuracy@spa.rst.sctb': 0.3829787234042553, 'eval_f1@spa.rst.sctb': 0.062121622842685464, 'eval_precision@spa.rst.sctb': 0.1389786683904331, 'eval_recall@spa.rst.sctb': 0.07678018575851393, 'eval_loss@spa.rst.sctb': 2.383079767227173, 'eval_runtime': 1.4813, 'eval_samples_per_second': 63.458, 'eval_steps_per_second': 2.025, 'epoch': 6.0}
{'train@spa.rst.sctb_loss': 2.2510197162628174, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3940774487471526, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.05927132598704129, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.08232044691939745, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.06750256016385048, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.2510197162628174, 'train@spa.rst.sctb_runtime': 5.6113, 'train@spa.rst.sctb_samples_per_second': 78.234, 'train@spa.rst.sctb_steps_per_second': 2.495, 'epoch': 7.0}
{'loss': 2.3356, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.3213765621185303, 'eval_accuracy@spa.rst.sctb': 0.40425531914893614, 'eval_f1@spa.rst.sctb': 0.0718907662851313, 'eval_precision@spa.rst.sctb': 0.1394580304031725, 'eval_recall@spa.rst.sctb': 0.08297213622291022, 'eval_loss@spa.rst.sctb': 2.3213770389556885, 'eval_runtime': 1.4691, 'eval_samples_per_second': 63.985, 'eval_steps_per_second': 2.042, 'epoch': 7.0}
{'train@spa.rst.sctb_loss': 2.2066853046417236, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4214123006833713, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.06763016538391058, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.08287778113359508, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.0745852534562212, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.2066848278045654, 'train@spa.rst.sctb_runtime': 5.6188, 'train@spa.rst.sctb_samples_per_second': 78.131, 'train@spa.rst.sctb_steps_per_second': 2.492, 'epoch': 8.0}
{'loss': 2.2716, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.2836222648620605, 'eval_accuracy@spa.rst.sctb': 0.43617021276595747, 'eval_f1@spa.rst.sctb': 0.08220211161387632, 'eval_precision@spa.rst.sctb': 0.1276844070961718, 'eval_recall@spa.rst.sctb': 0.09226006191950464, 'eval_loss@spa.rst.sctb': 2.2836225032806396, 'eval_runtime': 1.4756, 'eval_samples_per_second': 63.704, 'eval_steps_per_second': 2.033, 'epoch': 8.0}
{'train@spa.rst.sctb_loss': 2.1763105392456055, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.42596810933940776, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.06882151248702043, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.08248748048031877, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.0756515616999488, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.1763107776641846, 'train@spa.rst.sctb_runtime': 5.6025, 'train@spa.rst.sctb_samples_per_second': 78.357, 'train@spa.rst.sctb_steps_per_second': 2.499, 'epoch': 9.0}
{'loss': 2.2479, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.258228063583374, 'eval_accuracy@spa.rst.sctb': 0.44680851063829785, 'eval_f1@spa.rst.sctb': 0.08279220779220779, 'eval_precision@spa.rst.sctb': 0.11700882884799489, 'eval_recall@spa.rst.sctb': 0.09535603715170278, 'eval_loss@spa.rst.sctb': 2.258228063583374, 'eval_runtime': 1.484, 'eval_samples_per_second': 63.343, 'eval_steps_per_second': 2.022, 'epoch': 9.0}
{'train@spa.rst.sctb_loss': 2.156430959701538, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.43507972665148065, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.06923804226918799, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.08298552159969817, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.07590757808499744, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.156430721282959, 'train@spa.rst.sctb_runtime': 5.6188, 'train@spa.rst.sctb_samples_per_second': 78.13, 'train@spa.rst.sctb_steps_per_second': 2.492, 'epoch': 10.0}
{'loss': 2.2078, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.2420849800109863, 'eval_accuracy@spa.rst.sctb': 0.44680851063829785, 'eval_f1@spa.rst.sctb': 0.08279220779220779, 'eval_precision@spa.rst.sctb': 0.11700882884799489, 'eval_recall@spa.rst.sctb': 0.09535603715170278, 'eval_loss@spa.rst.sctb': 2.242084503173828, 'eval_runtime': 1.4583, 'eval_samples_per_second': 64.457, 'eval_steps_per_second': 2.057, 'epoch': 10.0}
{'train@spa.rst.sctb_loss': 2.14530611038208, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4328018223234624, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.06715282688487052, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.08218740006395907, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.07392345110087045, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.14530611038208, 'train@spa.rst.sctb_runtime': 5.6379, 'train@spa.rst.sctb_samples_per_second': 77.865, 'train@spa.rst.sctb_steps_per_second': 2.483, 'epoch': 11.0}
{'loss': 2.2105, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.2326080799102783, 'eval_accuracy@spa.rst.sctb': 0.46808510638297873, 'eval_f1@spa.rst.sctb': 0.08851540616246499, 'eval_precision@spa.rst.sctb': 0.12079831932773111, 'eval_recall@spa.rst.sctb': 0.10154798761609907, 'eval_loss@spa.rst.sctb': 2.232607841491699, 'eval_runtime': 1.4987, 'eval_samples_per_second': 62.721, 'eval_steps_per_second': 2.002, 'epoch': 11.0}
{'train@spa.rst.sctb_loss': 2.141631841659546, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.43735763097949887, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.06808360233215725, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.08285574659025279, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.0748195084485407, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.141631841659546, 'train@spa.rst.sctb_runtime': 5.6003, 'train@spa.rst.sctb_samples_per_second': 78.389, 'train@spa.rst.sctb_steps_per_second': 2.5, 'epoch': 12.0}
{'loss': 2.1955, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.2297778129577637, 'eval_accuracy@spa.rst.sctb': 0.46808510638297873, 'eval_f1@spa.rst.sctb': 0.08851540616246499, 'eval_precision@spa.rst.sctb': 0.12079831932773111, 'eval_recall@spa.rst.sctb': 0.10154798761609907, 'eval_loss@spa.rst.sctb': 2.2297778129577637, 'eval_runtime': 1.4646, 'eval_samples_per_second': 64.182, 'eval_steps_per_second': 2.048, 'epoch': 12.0}
{'train_runtime': 219.4797, 'train_samples_per_second': 24.002, 'train_steps_per_second': 0.765, 'train_loss': 2.628155242829096, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.8893
  train_runtime            = 0:32:37.84
  train_samples_per_second =     25.424
  train_steps_per_second   =      0.797
-------------------------------------------------------------------
Lang1:  rus.rst.rrt    Lang2:  spa.rst.sctb
Saving run to:  runs/full_shot/FullShot=v4_rus.rst.rrt_spa.rst.sctb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 28822 examples
read 2855 examples
read 2843 examples
read 439 examples
read 94 examples
read 159 examples
Total prediction labels:  32
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (ru): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=32, bias=True)
    )
  )
)
{'train@rus.rst.rrt_loss': 1.711413860321045, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.49666921101936023, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.18094362573048958, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.22392556024874033, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.19699117579760736, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.711413860321045, 'train@rus.rst.rrt_runtime': 345.7643, 'train@rus.rst.rrt_samples_per_second': 83.357, 'train@rus.rst.rrt_steps_per_second': 2.606, 'epoch': 1.0}
{'loss': 2.1422, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.7478041648864746, 'eval_accuracy@rus.rst.rrt': 0.47530647985989494, 'eval_f1@rus.rst.rrt': 0.19733580355673044, 'eval_precision@rus.rst.rrt': 0.2038652359461545, 'eval_recall@rus.rst.rrt': 0.2160389814429128, 'eval_loss@rus.rst.rrt': 1.747804045677185, 'eval_runtime': 34.6086, 'eval_samples_per_second': 82.494, 'eval_steps_per_second': 2.601, 'epoch': 1.0}
{'train@rus.rst.rrt_loss': 1.4997586011886597, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5430226909999306, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.22587302665984382, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.3626133245507695, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.23214102354525423, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.4997586011886597, 'train@rus.rst.rrt_runtime': 345.6731, 'train@rus.rst.rrt_samples_per_second': 83.379, 'train@rus.rst.rrt_steps_per_second': 2.607, 'epoch': 2.0}
{'loss': 1.6353, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.5623779296875, 'eval_accuracy@rus.rst.rrt': 0.5211908931698774, 'eval_f1@rus.rst.rrt': 0.2469158944886257, 'eval_precision@rus.rst.rrt': 0.29883734488595404, 'eval_recall@rus.rst.rrt': 0.2554877949326341, 'eval_loss@rus.rst.rrt': 1.562377691268921, 'eval_runtime': 34.5562, 'eval_samples_per_second': 82.619, 'eval_steps_per_second': 2.604, 'epoch': 2.0}
{'train@rus.rst.rrt_loss': 1.4210158586502075, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5668586496426341, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.2843510973144346, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.45602537840685176, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.2765879959245754, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.421015977859497, 'train@rus.rst.rrt_runtime': 347.3996, 'train@rus.rst.rrt_samples_per_second': 82.965, 'train@rus.rst.rrt_steps_per_second': 2.594, 'epoch': 3.0}
{'loss': 1.506, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.4872851371765137, 'eval_accuracy@rus.rst.rrt': 0.5422066549912434, 'eval_f1@rus.rst.rrt': 0.3169071031666526, 'eval_precision@rus.rst.rrt': 0.4286068159423952, 'eval_recall@rus.rst.rrt': 0.30950526752834046, 'eval_loss@rus.rst.rrt': 1.4872851371765137, 'eval_runtime': 34.7697, 'eval_samples_per_second': 82.112, 'eval_steps_per_second': 2.588, 'epoch': 3.0}
{'train@rus.rst.rrt_loss': 1.3680448532104492, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5807369370619666, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3155200623534612, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.443807599889239, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.2976072677849015, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3680449724197388, 'train@rus.rst.rrt_runtime': 345.7188, 'train@rus.rst.rrt_samples_per_second': 83.368, 'train@rus.rst.rrt_steps_per_second': 2.606, 'epoch': 4.0}
{'loss': 1.4461, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.4379736185073853, 'eval_accuracy@rus.rst.rrt': 0.5628721541155867, 'eval_f1@rus.rst.rrt': 0.3672243140657876, 'eval_precision@rus.rst.rrt': 0.5225018396286057, 'eval_recall@rus.rst.rrt': 0.3475176740045669, 'eval_loss@rus.rst.rrt': 1.4379733800888062, 'eval_runtime': 34.5658, 'eval_samples_per_second': 82.596, 'eval_steps_per_second': 2.604, 'epoch': 4.0}
{'train@rus.rst.rrt_loss': 1.3348543643951416, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.589584345291791, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.32958977867507117, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4386431886577574, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.31112353960917477, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.334854245185852, 'train@rus.rst.rrt_runtime': 345.9469, 'train@rus.rst.rrt_samples_per_second': 83.313, 'train@rus.rst.rrt_steps_per_second': 2.604, 'epoch': 5.0}
{'loss': 1.4049, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.4142345190048218, 'eval_accuracy@rus.rst.rrt': 0.5663747810858144, 'eval_f1@rus.rst.rrt': 0.3814145894683395, 'eval_precision@rus.rst.rrt': 0.5082321976529297, 'eval_recall@rus.rst.rrt': 0.3629344671274594, 'eval_loss@rus.rst.rrt': 1.4142346382141113, 'eval_runtime': 34.6022, 'eval_samples_per_second': 82.509, 'eval_steps_per_second': 2.601, 'epoch': 5.0}
{'train@rus.rst.rrt_loss': 1.311716079711914, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5962112275345223, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.34123426588663813, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.43660768480604184, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.32246272862297815, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.311716079711914, 'train@rus.rst.rrt_runtime': 347.9649, 'train@rus.rst.rrt_samples_per_second': 82.83, 'train@rus.rst.rrt_steps_per_second': 2.589, 'epoch': 6.0}
{'loss': 1.3779, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.3930855989456177, 'eval_accuracy@rus.rst.rrt': 0.5695271453590193, 'eval_f1@rus.rst.rrt': 0.3871890633479793, 'eval_precision@rus.rst.rrt': 0.4922344263293047, 'eval_recall@rus.rst.rrt': 0.37078690701524464, 'eval_loss@rus.rst.rrt': 1.3930855989456177, 'eval_runtime': 34.7731, 'eval_samples_per_second': 82.104, 'eval_steps_per_second': 2.588, 'epoch': 6.0}
{'train@rus.rst.rrt_loss': 1.2960906028747559, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5995073207966137, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.35240627289906884, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4772785679416201, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.33399104260484963, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2960906028747559, 'train@rus.rst.rrt_runtime': 346.2464, 'train@rus.rst.rrt_samples_per_second': 83.241, 'train@rus.rst.rrt_steps_per_second': 2.602, 'epoch': 7.0}
{'loss': 1.355, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.3851224184036255, 'eval_accuracy@rus.rst.rrt': 0.5733800350262697, 'eval_f1@rus.rst.rrt': 0.3957864830534155, 'eval_precision@rus.rst.rrt': 0.4738094214099626, 'eval_recall@rus.rst.rrt': 0.38245597203653137, 'eval_loss@rus.rst.rrt': 1.3851224184036255, 'eval_runtime': 34.6118, 'eval_samples_per_second': 82.486, 'eval_steps_per_second': 2.6, 'epoch': 7.0}
{'train@rus.rst.rrt_loss': 1.282800316810608, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6037055027409618, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3600844706017348, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.47245282871139965, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34362207473525563, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2828004360198975, 'train@rus.rst.rrt_runtime': 346.0195, 'train@rus.rst.rrt_samples_per_second': 83.296, 'train@rus.rst.rrt_steps_per_second': 2.604, 'epoch': 8.0}
{'loss': 1.3407, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.3730902671813965, 'eval_accuracy@rus.rst.rrt': 0.5768826619964974, 'eval_f1@rus.rst.rrt': 0.40512544718189003, 'eval_precision@rus.rst.rrt': 0.47033330636915266, 'eval_recall@rus.rst.rrt': 0.3931367268543227, 'eval_loss@rus.rst.rrt': 1.3730902671813965, 'eval_runtime': 34.5855, 'eval_samples_per_second': 82.549, 'eval_steps_per_second': 2.602, 'epoch': 8.0}
{'train@rus.rst.rrt_loss': 1.2706327438354492, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.60623829019499, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3634840179406345, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.48642116278925945, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3448281247997741, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2706327438354492, 'train@rus.rst.rrt_runtime': 347.2953, 'train@rus.rst.rrt_samples_per_second': 82.99, 'train@rus.rst.rrt_steps_per_second': 2.594, 'epoch': 9.0}
{'loss': 1.3297, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.3672832250595093, 'eval_accuracy@rus.rst.rrt': 0.5775831873905429, 'eval_f1@rus.rst.rrt': 0.4067679525645422, 'eval_precision@rus.rst.rrt': 0.476632395073005, 'eval_recall@rus.rst.rrt': 0.3925923807815413, 'eval_loss@rus.rst.rrt': 1.3672832250595093, 'eval_runtime': 34.6926, 'eval_samples_per_second': 82.294, 'eval_steps_per_second': 2.594, 'epoch': 9.0}
{'train@rus.rst.rrt_loss': 1.2645055055618286, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6084588161820831, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36514996011272366, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4631398367618196, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34455728085501086, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2645055055618286, 'train@rus.rst.rrt_runtime': 346.7377, 'train@rus.rst.rrt_samples_per_second': 83.123, 'train@rus.rst.rrt_steps_per_second': 2.599, 'epoch': 10.0}
{'loss': 1.3205, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.364148736000061, 'eval_accuracy@rus.rst.rrt': 0.5796847635726795, 'eval_f1@rus.rst.rrt': 0.4112518687711985, 'eval_precision@rus.rst.rrt': 0.4923517860681286, 'eval_recall@rus.rst.rrt': 0.39553977690039743, 'eval_loss@rus.rst.rrt': 1.364148736000061, 'eval_runtime': 34.7044, 'eval_samples_per_second': 82.266, 'eval_steps_per_second': 2.593, 'epoch': 10.0}
{'train@rus.rst.rrt_loss': 1.2605994939804077, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6094649920199847, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3672542426067961, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4621290496905493, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3486172658361962, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2605994939804077, 'train@rus.rst.rrt_runtime': 346.0221, 'train@rus.rst.rrt_samples_per_second': 83.295, 'train@rus.rst.rrt_steps_per_second': 2.604, 'epoch': 11.0}
{'loss': 1.315, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.3637969493865967, 'eval_accuracy@rus.rst.rrt': 0.5796847635726795, 'eval_f1@rus.rst.rrt': 0.4131557671910036, 'eval_precision@rus.rst.rrt': 0.49285896135942325, 'eval_recall@rus.rst.rrt': 0.399068553794554, 'eval_loss@rus.rst.rrt': 1.3637967109680176, 'eval_runtime': 34.6691, 'eval_samples_per_second': 82.35, 'eval_steps_per_second': 2.596, 'epoch': 11.0}
{'train@rus.rst.rrt_loss': 1.2591458559036255, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6093262091457914, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3670140582245146, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4624792817075692, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34879342093267307, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.259145975112915, 'train@rus.rst.rrt_runtime': 346.8192, 'train@rus.rst.rrt_samples_per_second': 83.104, 'train@rus.rst.rrt_steps_per_second': 2.598, 'epoch': 12.0}
{'loss': 1.3094, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.361930251121521, 'eval_accuracy@rus.rst.rrt': 0.580385288966725, 'eval_f1@rus.rst.rrt': 0.4113180914443508, 'eval_precision@rus.rst.rrt': 0.4887273576599625, 'eval_recall@rus.rst.rrt': 0.39771245156773893, 'eval_loss@rus.rst.rrt': 1.361930251121521, 'eval_runtime': 34.7169, 'eval_samples_per_second': 82.236, 'eval_steps_per_second': 2.592, 'epoch': 12.0}
{'train_runtime': 13350.5863, 'train_samples_per_second': 25.906, 'train_steps_per_second': 0.81, 'train_loss': 1.4568900088226624, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.4569
  train_runtime            = 3:42:30.58
  train_samples_per_second =     25.906
  train_steps_per_second   =       0.81
{'train@spa.rst.sctb_loss': 3.4463305473327637, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3211845102505695, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.05265531203031204, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.043475784105647924, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.07212962962962963, 'train@spa.rst.sctb_loss@spa.rst.sctb': 3.4463298320770264, 'train@spa.rst.sctb_runtime': 5.5866, 'train@spa.rst.sctb_samples_per_second': 78.581, 'train@spa.rst.sctb_steps_per_second': 2.506, 'epoch': 1.0}
{'loss': 3.7518, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.2861850261688232, 'eval_accuracy@spa.rst.sctb': 0.3191489361702128, 'eval_f1@spa.rst.sctb': 0.04356060606060606, 'eval_precision@spa.rst.sctb': 0.03434343434343434, 'eval_recall@spa.rst.sctb': 0.059932659932659935, 'eval_loss@spa.rst.sctb': 3.2861833572387695, 'eval_runtime': 1.4491, 'eval_samples_per_second': 64.866, 'eval_steps_per_second': 2.07, 'epoch': 1.0}
{'train@spa.rst.sctb_loss': 2.9475955963134766, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.33029612756264237, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.04722692120072943, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03732962143498555, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.06600859788359788, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.9475953578948975, 'train@spa.rst.sctb_runtime': 5.6067, 'train@spa.rst.sctb_samples_per_second': 78.299, 'train@spa.rst.sctb_steps_per_second': 2.497, 'epoch': 2.0}
{'loss': 3.1653, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.800523042678833, 'eval_accuracy@spa.rst.sctb': 0.32978723404255317, 'eval_f1@spa.rst.sctb': 0.03664302600472814, 'eval_precision@spa.rst.sctb': 0.028233151183970857, 'eval_recall@spa.rst.sctb': 0.052188552188552194, 'eval_loss@spa.rst.sctb': 2.800523042678833, 'eval_runtime': 1.4372, 'eval_samples_per_second': 65.405, 'eval_steps_per_second': 2.087, 'epoch': 2.0}
{'train@spa.rst.sctb_loss': 2.6311469078063965, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.33940774487471526, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.043402484491609127, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.035036494315647676, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.06139550264550264, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.6311464309692383, 'train@spa.rst.sctb_runtime': 5.6109, 'train@spa.rst.sctb_samples_per_second': 78.241, 'train@spa.rst.sctb_steps_per_second': 2.495, 'epoch': 3.0}
{'loss': 2.7893, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.523556709289551, 'eval_accuracy@spa.rst.sctb': 0.32978723404255317, 'eval_f1@spa.rst.sctb': 0.03137651821862348, 'eval_precision@spa.rst.sctb': 0.02297998517420311, 'eval_recall@spa.rst.sctb': 0.04944178628389155, 'eval_loss@spa.rst.sctb': 2.523556709289551, 'eval_runtime': 1.4331, 'eval_samples_per_second': 65.59, 'eval_steps_per_second': 2.093, 'epoch': 3.0}
{'train@spa.rst.sctb_loss': 2.437732458114624, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3553530751708428, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.04810251752216039, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.059599496364202253, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.06385069977811912, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.437732458114624, 'train@spa.rst.sctb_runtime': 5.5958, 'train@spa.rst.sctb_samples_per_second': 78.451, 'train@spa.rst.sctb_steps_per_second': 2.502, 'epoch': 4.0}
{'loss': 2.5461, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.3806090354919434, 'eval_accuracy@spa.rst.sctb': 0.35106382978723405, 'eval_f1@spa.rst.sctb': 0.03466386554621849, 'eval_precision@spa.rst.sctb': 0.024571854058078928, 'eval_recall@spa.rst.sctb': 0.058823529411764705, 'eval_loss@spa.rst.sctb': 2.380608558654785, 'eval_runtime': 1.4434, 'eval_samples_per_second': 65.126, 'eval_steps_per_second': 2.078, 'epoch': 4.0}
{'train@spa.rst.sctb_loss': 2.298204183578491, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3712984054669704, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.053731043529350986, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.07578952946163629, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.06614455735423477, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.2982044219970703, 'train@spa.rst.sctb_runtime': 5.5836, 'train@spa.rst.sctb_samples_per_second': 78.624, 'train@spa.rst.sctb_steps_per_second': 2.507, 'epoch': 5.0}
{'loss': 2.3998, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.277087688446045, 'eval_accuracy@spa.rst.sctb': 0.3617021276595745, 'eval_f1@spa.rst.sctb': 0.043253183026523555, 'eval_precision@spa.rst.sctb': 0.039473684210526314, 'eval_recall@spa.rst.sctb': 0.06323294868186509, 'eval_loss@spa.rst.sctb': 2.277087926864624, 'eval_runtime': 1.4535, 'eval_samples_per_second': 64.673, 'eval_steps_per_second': 2.064, 'epoch': 5.0}
{'train@spa.rst.sctb_loss': 2.20174503326416, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3917995444191344, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.0585645537108303, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.0842231183762528, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.06834240878595717, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.201744794845581, 'train@spa.rst.sctb_runtime': 5.5978, 'train@spa.rst.sctb_samples_per_second': 78.423, 'train@spa.rst.sctb_steps_per_second': 2.501, 'epoch': 6.0}
{'loss': 2.2656, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.2123489379882812, 'eval_accuracy@spa.rst.sctb': 0.3829787234042553, 'eval_f1@spa.rst.sctb': 0.049404864585130236, 'eval_precision@spa.rst.sctb': 0.044053985230455825, 'eval_recall@spa.rst.sctb': 0.06942489914626138, 'eval_loss@spa.rst.sctb': 2.212348461151123, 'eval_runtime': 1.4404, 'eval_samples_per_second': 65.259, 'eval_steps_per_second': 2.083, 'epoch': 6.0}
{'train@spa.rst.sctb_loss': 2.1322035789489746, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.42596810933940776, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.06814598923917947, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.09743265993265993, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.07541609227093098, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.1322035789489746, 'train@spa.rst.sctb_runtime': 5.6027, 'train@spa.rst.sctb_samples_per_second': 78.355, 'train@spa.rst.sctb_steps_per_second': 2.499, 'epoch': 7.0}
{'loss': 2.2277, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.164346933364868, 'eval_accuracy@spa.rst.sctb': 0.3829787234042553, 'eval_f1@spa.rst.sctb': 0.05107010124311162, 'eval_precision@spa.rst.sctb': 0.043921568627450974, 'eval_recall@spa.rst.sctb': 0.07073834318416361, 'eval_loss@spa.rst.sctb': 2.1643476486206055, 'eval_runtime': 1.4398, 'eval_samples_per_second': 65.287, 'eval_steps_per_second': 2.084, 'epoch': 7.0}
{'train@spa.rst.sctb_loss': 2.0846962928771973, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4533029612756264, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.07633048074139183, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.10059786188377161, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.08219269828140796, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.0846962928771973, 'train@spa.rst.sctb_runtime': 5.6023, 'train@spa.rst.sctb_samples_per_second': 78.36, 'train@spa.rst.sctb_steps_per_second': 2.499, 'epoch': 8.0}
{'loss': 2.1473, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.1325149536132812, 'eval_accuracy@spa.rst.sctb': 0.40425531914893614, 'eval_f1@spa.rst.sctb': 0.06381798002219756, 'eval_precision@spa.rst.sctb': 0.1045646300421861, 'eval_recall@spa.rst.sctb': 0.07918191199924947, 'eval_loss@spa.rst.sctb': 2.1325149536132812, 'eval_runtime': 1.4488, 'eval_samples_per_second': 64.881, 'eval_steps_per_second': 2.071, 'epoch': 8.0}
{'train@spa.rst.sctb_loss': 2.0510058403015137, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4601366742596811, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.07966258588191136, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.10859565283041518, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.0840602885764176, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.0510058403015137, 'train@spa.rst.sctb_runtime': 5.6061, 'train@spa.rst.sctb_samples_per_second': 78.307, 'train@spa.rst.sctb_steps_per_second': 2.497, 'epoch': 9.0}
{'loss': 2.1111, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.109306573867798, 'eval_accuracy@spa.rst.sctb': 0.39361702127659576, 'eval_f1@spa.rst.sctb': 0.06231642454552671, 'eval_precision@spa.rst.sctb': 0.10225439323245977, 'eval_recall@spa.rst.sctb': 0.07739938080495355, 'eval_loss@spa.rst.sctb': 2.109307050704956, 'eval_runtime': 1.4561, 'eval_samples_per_second': 64.556, 'eval_steps_per_second': 2.06, 'epoch': 9.0}
{'train@spa.rst.sctb_loss': 2.0278162956237793, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4715261958997722, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.08398867613572242, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.11144996143061159, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.08736019207793401, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.0278165340423584, 'train@spa.rst.sctb_runtime': 5.5897, 'train@spa.rst.sctb_samples_per_second': 78.537, 'train@spa.rst.sctb_steps_per_second': 2.505, 'epoch': 10.0}
{'loss': 2.0878, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.094067096710205, 'eval_accuracy@spa.rst.sctb': 0.39361702127659576, 'eval_f1@spa.rst.sctb': 0.06216960271786086, 'eval_precision@spa.rst.sctb': 0.10168067226890756, 'eval_recall@spa.rst.sctb': 0.07739938080495355, 'eval_loss@spa.rst.sctb': 2.0940675735473633, 'eval_runtime': 1.4457, 'eval_samples_per_second': 65.019, 'eval_steps_per_second': 2.075, 'epoch': 10.0}
{'train@spa.rst.sctb_loss': 2.014298915863037, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4760820045558087, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.08623749886836518, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.11257613437881842, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.08896275618049813, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.014298915863037, 'train@spa.rst.sctb_runtime': 5.5954, 'train@spa.rst.sctb_samples_per_second': 78.457, 'train@spa.rst.sctb_steps_per_second': 2.502, 'epoch': 11.0}
{'loss': 2.0724, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.0847699642181396, 'eval_accuracy@spa.rst.sctb': 0.40425531914893614, 'eval_f1@spa.rst.sctb': 0.07094153254242226, 'eval_precision@spa.rst.sctb': 0.102609464838567, 'eval_recall@spa.rst.sctb': 0.08274697438784126, 'eval_loss@spa.rst.sctb': 2.0847699642181396, 'eval_runtime': 1.4206, 'eval_samples_per_second': 66.167, 'eval_steps_per_second': 2.112, 'epoch': 11.0}
{'train@spa.rst.sctb_loss': 2.0099523067474365, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4783599088838269, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.08731460752764725, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.1130406987356346, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.08976403823178018, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.0099525451660156, 'train@spa.rst.sctb_runtime': 5.5814, 'train@spa.rst.sctb_samples_per_second': 78.655, 'train@spa.rst.sctb_steps_per_second': 2.508, 'epoch': 12.0}
{'loss': 2.057, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.0820624828338623, 'eval_accuracy@spa.rst.sctb': 0.4148936170212766, 'eval_f1@spa.rst.sctb': 0.07838801155136978, 'eval_precision@spa.rst.sctb': 0.10297482837528604, 'eval_recall@spa.rst.sctb': 0.08809456797072895, 'eval_loss@spa.rst.sctb': 2.0820624828338623, 'eval_runtime': 1.4347, 'eval_samples_per_second': 65.517, 'eval_steps_per_second': 2.091, 'epoch': 12.0}
{'train_runtime': 218.7335, 'train_samples_per_second': 24.084, 'train_steps_per_second': 0.768, 'train_loss': 2.468458448137556, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.4569
  train_runtime            = 3:42:30.58
  train_samples_per_second =     25.906
  train_steps_per_second   =       0.81
-------------------------------------------------------------------
Lang1:  spa.rst.rststb    Lang2:  spa.rst.sctb
Saving run to:  runs/full_shot/FullShot=v4_spa.rst.rststb_spa.rst.sctb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2240 examples
read 383 examples
read 426 examples
read 439 examples
read 94 examples
read 159 examples
Total prediction labels:  30
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=30, bias=True)
    )
  )
)
{'train@spa.rst.rststb_loss': 2.707928419113159, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.2330357142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.0285558666795363, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.04764712789700653, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.044744014938020396, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.70792818069458, 'train@spa.rst.rststb_runtime': 26.9974, 'train@spa.rst.rststb_samples_per_second': 82.971, 'train@spa.rst.rststb_steps_per_second': 2.593, 'epoch': 1.0}
{'loss': 3.0635, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.7608091831207275, 'eval_accuracy@spa.rst.rststb': 0.20887728459530025, 'eval_f1@spa.rst.rststb': 0.021263297906988256, 'eval_precision@spa.rst.rststb': 0.04770253122286783, 'eval_recall@spa.rst.rststb': 0.04544054381616708, 'eval_loss@spa.rst.rststb': 2.7608094215393066, 'eval_runtime': 4.9229, 'eval_samples_per_second': 77.8, 'eval_steps_per_second': 2.438, 'epoch': 1.0}
{'train@spa.rst.rststb_loss': 2.5013396739959717, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.2888392857142857, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.045173113300156, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.049745036162479384, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.05985885741438296, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.5013396739959717, 'train@spa.rst.rststb_runtime': 27.1915, 'train@spa.rst.rststb_samples_per_second': 82.379, 'train@spa.rst.rststb_steps_per_second': 2.574, 'epoch': 2.0}
{'loss': 2.6224, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.61538028717041, 'eval_accuracy@spa.rst.rststb': 0.2349869451697128, 'eval_f1@spa.rst.rststb': 0.03893675282174581, 'eval_precision@spa.rst.rststb': 0.053912602124383334, 'eval_recall@spa.rst.rststb': 0.05605546590358359, 'eval_loss@spa.rst.rststb': 2.61538028717041, 'eval_runtime': 4.8807, 'eval_samples_per_second': 78.472, 'eval_steps_per_second': 2.459, 'epoch': 2.0}
{'train@spa.rst.rststb_loss': 2.3817408084869385, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.33883928571428573, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.057038225631183125, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.0625316597000155, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.07646583399519571, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.3817408084869385, 'train@spa.rst.rststb_runtime': 27.0784, 'train@spa.rst.rststb_samples_per_second': 82.723, 'train@spa.rst.rststb_steps_per_second': 2.585, 'epoch': 3.0}
{'loss': 2.4813, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.5307681560516357, 'eval_accuracy@spa.rst.rststb': 0.2819843342036554, 'eval_f1@spa.rst.rststb': 0.055044514844674215, 'eval_precision@spa.rst.rststb': 0.05847692406977174, 'eval_recall@spa.rst.rststb': 0.07587367823200218, 'eval_loss@spa.rst.rststb': 2.530768394470215, 'eval_runtime': 4.8805, 'eval_samples_per_second': 78.475, 'eval_steps_per_second': 2.459, 'epoch': 3.0}
{'train@spa.rst.rststb_loss': 2.283796548843384, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.3575892857142857, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.07007316055716613, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.0941653022229778, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.09276104051062538, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.283796548843384, 'train@spa.rst.rststb_runtime': 27.1696, 'train@spa.rst.rststb_samples_per_second': 82.445, 'train@spa.rst.rststb_steps_per_second': 2.576, 'epoch': 4.0}
{'loss': 2.3831, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.460556745529175, 'eval_accuracy@spa.rst.rststb': 0.32114882506527415, 'eval_f1@spa.rst.rststb': 0.07621387499465607, 'eval_precision@spa.rst.rststb': 0.06876391317352173, 'eval_recall@spa.rst.rststb': 0.09864444938250375, 'eval_loss@spa.rst.rststb': 2.460556745529175, 'eval_runtime': 4.9095, 'eval_samples_per_second': 78.011, 'eval_steps_per_second': 2.444, 'epoch': 4.0}
{'train@spa.rst.rststb_loss': 2.202831506729126, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.38660714285714287, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.08513947216004995, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.08551016974401424, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.10855207480437808, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.202831506729126, 'train@spa.rst.rststb_runtime': 27.3433, 'train@spa.rst.rststb_samples_per_second': 81.921, 'train@spa.rst.rststb_steps_per_second': 2.56, 'epoch': 5.0}
{'loss': 2.2895, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.405005693435669, 'eval_accuracy@spa.rst.rststb': 0.34464751958224543, 'eval_f1@spa.rst.rststb': 0.09895581993819193, 'eval_precision@spa.rst.rststb': 0.09474895491358173, 'eval_recall@spa.rst.rststb': 0.12289562042619816, 'eval_loss@spa.rst.rststb': 2.405005931854248, 'eval_runtime': 4.9635, 'eval_samples_per_second': 77.163, 'eval_steps_per_second': 2.418, 'epoch': 5.0}
{'train@spa.rst.rststb_loss': 2.1358463764190674, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.3986607142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.09205032427479613, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.10467234295150842, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.11605071894526872, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.1358466148376465, 'train@spa.rst.rststb_runtime': 27.301, 'train@spa.rst.rststb_samples_per_second': 82.048, 'train@spa.rst.rststb_steps_per_second': 2.564, 'epoch': 6.0}
{'loss': 2.2248, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.3556957244873047, 'eval_accuracy@spa.rst.rststb': 0.3524804177545692, 'eval_f1@spa.rst.rststb': 0.10016581304417516, 'eval_precision@spa.rst.rststb': 0.09661063846710563, 'eval_recall@spa.rst.rststb': 0.12542226274370452, 'eval_loss@spa.rst.rststb': 2.3556954860687256, 'eval_runtime': 4.9301, 'eval_samples_per_second': 77.686, 'eval_steps_per_second': 2.434, 'epoch': 6.0}
{'train@spa.rst.rststb_loss': 2.079176664352417, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4120535714285714, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10121831664624238, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.16865400559745874, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.12517668668788853, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.079176425933838, 'train@spa.rst.rststb_runtime': 27.2561, 'train@spa.rst.rststb_samples_per_second': 82.184, 'train@spa.rst.rststb_steps_per_second': 2.568, 'epoch': 7.0}
{'loss': 2.163, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.3139145374298096, 'eval_accuracy@spa.rst.rststb': 0.3733681462140992, 'eval_f1@spa.rst.rststb': 0.10677530426285733, 'eval_precision@spa.rst.rststb': 0.09925781828690328, 'eval_recall@spa.rst.rststb': 0.13544304864259848, 'eval_loss@spa.rst.rststb': 2.3139140605926514, 'eval_runtime': 4.9025, 'eval_samples_per_second': 78.123, 'eval_steps_per_second': 2.448, 'epoch': 7.0}
{'train@spa.rst.rststb_loss': 2.036531686782837, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4191964285714286, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10648799901428807, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.1807601856364128, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.12956497965044317, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.036531686782837, 'train@spa.rst.rststb_runtime': 27.1718, 'train@spa.rst.rststb_samples_per_second': 82.438, 'train@spa.rst.rststb_steps_per_second': 2.576, 'epoch': 8.0}
{'loss': 2.1093, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.281860589981079, 'eval_accuracy@spa.rst.rststb': 0.3838120104438642, 'eval_f1@spa.rst.rststb': 0.11137773744496525, 'eval_precision@spa.rst.rststb': 0.10507550846425526, 'eval_recall@spa.rst.rststb': 0.1380387657094382, 'eval_loss@spa.rst.rststb': 2.281860589981079, 'eval_runtime': 4.9, 'eval_samples_per_second': 78.163, 'eval_steps_per_second': 2.449, 'epoch': 8.0}
{'train@spa.rst.rststb_loss': 2.0049383640289307, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.42410714285714285, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10767462381968669, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.16136131479179813, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13250127431865658, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0049381256103516, 'train@spa.rst.rststb_runtime': 27.1445, 'train@spa.rst.rststb_samples_per_second': 82.521, 'train@spa.rst.rststb_steps_per_second': 2.579, 'epoch': 9.0}
{'loss': 2.068, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.255803346633911, 'eval_accuracy@spa.rst.rststb': 0.3838120104438642, 'eval_f1@spa.rst.rststb': 0.10954726205485567, 'eval_precision@spa.rst.rststb': 0.09993698802772526, 'eval_recall@spa.rst.rststb': 0.138575121803586, 'eval_loss@spa.rst.rststb': 2.255803346633911, 'eval_runtime': 4.8941, 'eval_samples_per_second': 78.258, 'eval_steps_per_second': 2.452, 'epoch': 9.0}
{'train@spa.rst.rststb_loss': 1.9833213090896606, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4316964285714286, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.11161118568642932, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.15541897252731754, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13560764785160997, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.9833210706710815, 'train@spa.rst.rststb_runtime': 27.1211, 'train@spa.rst.rststb_samples_per_second': 82.593, 'train@spa.rst.rststb_steps_per_second': 2.581, 'epoch': 10.0}
{'loss': 2.0468, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.24149227142334, 'eval_accuracy@spa.rst.rststb': 0.38642297650130547, 'eval_f1@spa.rst.rststb': 0.11354256434474432, 'eval_precision@spa.rst.rststb': 0.10230689978308519, 'eval_recall@spa.rst.rststb': 0.14400990441228165, 'eval_loss@spa.rst.rststb': 2.241492748260498, 'eval_runtime': 4.8834, 'eval_samples_per_second': 78.428, 'eval_steps_per_second': 2.457, 'epoch': 10.0}
{'train@spa.rst.rststb_loss': 1.9705156087875366, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.43348214285714287, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.11263341748854032, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.15199980908422078, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13665519157475242, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.9705156087875366, 'train@spa.rst.rststb_runtime': 27.1585, 'train@spa.rst.rststb_samples_per_second': 82.479, 'train@spa.rst.rststb_steps_per_second': 2.577, 'epoch': 11.0}
{'loss': 2.029, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.2304859161376953, 'eval_accuracy@spa.rst.rststb': 0.391644908616188, 'eval_f1@spa.rst.rststb': 0.11537655073124181, 'eval_precision@spa.rst.rststb': 0.10445571953424855, 'eval_recall@spa.rst.rststb': 0.1453077629457015, 'eval_loss@spa.rst.rststb': 2.2304859161376953, 'eval_runtime': 4.8902, 'eval_samples_per_second': 78.319, 'eval_steps_per_second': 2.454, 'epoch': 11.0}
{'train@spa.rst.rststb_loss': 1.9664311408996582, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4361607142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.11457395181914867, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.1577459625299301, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.1378837617120991, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.966430902481079, 'train@spa.rst.rststb_runtime': 27.1265, 'train@spa.rst.rststb_samples_per_second': 82.576, 'train@spa.rst.rststb_steps_per_second': 2.581, 'epoch': 12.0}
{'loss': 2.0166, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.22735333442688, 'eval_accuracy@spa.rst.rststb': 0.39425587467362927, 'eval_f1@spa.rst.rststb': 0.11517856780769932, 'eval_precision@spa.rst.rststb': 0.10370176327066102, 'eval_recall@spa.rst.rststb': 0.14595669221241142, 'eval_loss@spa.rst.rststb': 2.22735333442688, 'eval_runtime': 4.915, 'eval_samples_per_second': 77.925, 'eval_steps_per_second': 2.442, 'epoch': 12.0}
{'train_runtime': 1067.4823, 'train_samples_per_second': 25.181, 'train_steps_per_second': 0.787, 'train_loss': 2.2914430527460006, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.2914
  train_runtime            = 0:17:47.48
  train_samples_per_second =     25.181
  train_steps_per_second   =      0.787
{'train@spa.rst.sctb_loss': 3.8781449794769287, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.0387243735763098, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.029254658385093165, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.026451063829787236, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.03492063492063492, 'train@spa.rst.sctb_loss@spa.rst.sctb': 3.878145217895508, 'train@spa.rst.sctb_runtime': 5.5608, 'train@spa.rst.sctb_samples_per_second': 78.945, 'train@spa.rst.sctb_steps_per_second': 2.518, 'epoch': 1.0}
{'loss': 4.2381, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.8738462924957275, 'eval_accuracy@spa.rst.sctb': 0.031914893617021274, 'eval_f1@spa.rst.sctb': 0.022186147186147184, 'eval_precision@spa.rst.sctb': 0.019528619528619527, 'eval_recall@spa.rst.sctb': 0.07777777777777778, 'eval_loss@spa.rst.sctb': 3.8738458156585693, 'eval_runtime': 1.4391, 'eval_samples_per_second': 65.321, 'eval_steps_per_second': 2.085, 'epoch': 1.0}
{'train@spa.rst.sctb_loss': 3.333672046661377, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.04328018223234624, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.033351196172248805, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.05054082714740191, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.03725550435227855, 'train@spa.rst.sctb_loss@spa.rst.sctb': 3.333671808242798, 'train@spa.rst.sctb_runtime': 5.5796, 'train@spa.rst.sctb_samples_per_second': 78.68, 'train@spa.rst.sctb_steps_per_second': 2.509, 'epoch': 2.0}
{'loss': 3.5848, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.307839870452881, 'eval_accuracy@spa.rst.sctb': 0.031914893617021274, 'eval_f1@spa.rst.sctb': 0.02503654970760234, 'eval_precision@spa.rst.sctb': 0.027151211361737676, 'eval_recall@spa.rst.sctb': 0.07368421052631578, 'eval_loss@spa.rst.sctb': 3.3078391551971436, 'eval_runtime': 1.428, 'eval_samples_per_second': 65.826, 'eval_steps_per_second': 2.101, 'epoch': 2.0}
{'train@spa.rst.sctb_loss': 3.0113353729248047, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.06150341685649203, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03837057010785824, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.04556521739130434, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.03922171018945212, 'train@spa.rst.sctb_loss@spa.rst.sctb': 3.0113351345062256, 'train@spa.rst.sctb_runtime': 5.6177, 'train@spa.rst.sctb_samples_per_second': 78.146, 'train@spa.rst.sctb_steps_per_second': 2.492, 'epoch': 3.0}
{'loss': 3.2013, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.981724262237549, 'eval_accuracy@spa.rst.sctb': 0.0425531914893617, 'eval_f1@spa.rst.sctb': 0.03794060660257844, 'eval_precision@spa.rst.sctb': 0.06746031746031747, 'eval_recall@spa.rst.sctb': 0.08070175438596491, 'eval_loss@spa.rst.sctb': 2.9817237854003906, 'eval_runtime': 1.4436, 'eval_samples_per_second': 65.116, 'eval_steps_per_second': 2.078, 'epoch': 3.0}
{'train@spa.rst.sctb_loss': 2.7912352085113525, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.31662870159453305, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.06462767766065963, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.06463157894736841, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.06556067588325652, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.7912349700927734, 'train@spa.rst.sctb_runtime': 5.5572, 'train@spa.rst.sctb_samples_per_second': 78.997, 'train@spa.rst.sctb_steps_per_second': 2.519, 'epoch': 4.0}
{'loss': 2.9274, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.7650279998779297, 'eval_accuracy@spa.rst.sctb': 0.3191489361702128, 'eval_f1@spa.rst.sctb': 0.09315622108019768, 'eval_precision@spa.rst.sctb': 0.10278880070546736, 'eval_recall@spa.rst.sctb': 0.1388623072833599, 'eval_loss@spa.rst.sctb': 2.7650279998779297, 'eval_runtime': 1.4342, 'eval_samples_per_second': 65.543, 'eval_steps_per_second': 2.092, 'epoch': 4.0}
{'train@spa.rst.sctb_loss': 2.6099722385406494, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4031890660592255, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.06618398425208374, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.060381112097596265, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.0770015360983103, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.6099729537963867, 'train@spa.rst.sctb_runtime': 5.5691, 'train@spa.rst.sctb_samples_per_second': 78.828, 'train@spa.rst.sctb_steps_per_second': 2.514, 'epoch': 5.0}
{'loss': 2.7307, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.5926337242126465, 'eval_accuracy@spa.rst.sctb': 0.425531914893617, 'eval_f1@spa.rst.sctb': 0.09186913446273186, 'eval_precision@spa.rst.sctb': 0.08817616959064328, 'eval_recall@spa.rst.sctb': 0.10430622009569378, 'eval_loss@spa.rst.sctb': 2.5926342010498047, 'eval_runtime': 1.4174, 'eval_samples_per_second': 66.32, 'eval_steps_per_second': 2.117, 'epoch': 5.0}
{'train@spa.rst.sctb_loss': 2.462050199508667, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4054669703872437, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.06695204139310405, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.06154457496116729, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.07792196620583718, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.462050676345825, 'train@spa.rst.sctb_runtime': 5.5853, 'train@spa.rst.sctb_samples_per_second': 78.599, 'train@spa.rst.sctb_steps_per_second': 2.507, 'epoch': 6.0}
{'loss': 2.5839, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.4589579105377197, 'eval_accuracy@spa.rst.sctb': 0.4574468085106383, 'eval_f1@spa.rst.sctb': 0.09479786550442824, 'eval_precision@spa.rst.sctb': 0.08462752525252526, 'eval_recall@spa.rst.sctb': 0.11307814992025518, 'eval_loss@spa.rst.sctb': 2.4589579105377197, 'eval_runtime': 1.4547, 'eval_samples_per_second': 64.617, 'eval_steps_per_second': 2.062, 'epoch': 6.0}
{'train@spa.rst.sctb_loss': 2.343329906463623, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4145785876993166, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.0689846060561062, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.062395136778115506, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.08111705069124424, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.343329906463623, 'train@spa.rst.sctb_runtime': 5.5613, 'train@spa.rst.sctb_samples_per_second': 78.939, 'train@spa.rst.sctb_steps_per_second': 2.517, 'epoch': 7.0}
{'loss': 2.4358, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.3564326763153076, 'eval_accuracy@spa.rst.sctb': 0.4574468085106383, 'eval_f1@spa.rst.sctb': 0.09785632466044837, 'eval_precision@spa.rst.sctb': 0.091862922705314, 'eval_recall@spa.rst.sctb': 0.11307814992025518, 'eval_loss@spa.rst.sctb': 2.356433391571045, 'eval_runtime': 1.4218, 'eval_samples_per_second': 66.113, 'eval_steps_per_second': 2.11, 'epoch': 7.0}
{'train@spa.rst.sctb_loss': 2.2591347694396973, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.428246013667426, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.0709578299521313, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.06390974493132767, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.08369769585253456, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.2591350078582764, 'train@spa.rst.sctb_runtime': 5.5778, 'train@spa.rst.sctb_samples_per_second': 78.705, 'train@spa.rst.sctb_steps_per_second': 2.51, 'epoch': 8.0}
{'loss': 2.3343, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.286102294921875, 'eval_accuracy@spa.rst.sctb': 0.4574468085106383, 'eval_f1@spa.rst.sctb': 0.0896604938271605, 'eval_precision@spa.rst.sctb': 0.08699633699633699, 'eval_recall@spa.rst.sctb': 0.1048910154173312, 'eval_loss@spa.rst.sctb': 2.286102533340454, 'eval_runtime': 1.4361, 'eval_samples_per_second': 65.456, 'eval_steps_per_second': 2.089, 'epoch': 8.0}
{'train@spa.rst.sctb_loss': 2.2027034759521484, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4305239179954442, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.07131729764234657, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.06412087177430138, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.08429124423963133, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.2027034759521484, 'train@spa.rst.sctb_runtime': 5.5742, 'train@spa.rst.sctb_samples_per_second': 78.756, 'train@spa.rst.sctb_steps_per_second': 2.512, 'epoch': 9.0}
{'loss': 2.2874, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.2398860454559326, 'eval_accuracy@spa.rst.sctb': 0.44680851063829785, 'eval_f1@spa.rst.sctb': 0.08541666666666667, 'eval_precision@spa.rst.sctb': 0.07685524352191019, 'eval_recall@spa.rst.sctb': 0.10320751373382953, 'eval_loss@spa.rst.sctb': 2.2398858070373535, 'eval_runtime': 1.4137, 'eval_samples_per_second': 66.491, 'eval_steps_per_second': 2.122, 'epoch': 9.0}
{'train@spa.rst.sctb_loss': 2.1664648056030273, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.43735763097949887, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.0744424839911283, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.10586274509803921, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.08478515892709443, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.1664648056030273, 'train@spa.rst.sctb_runtime': 5.5494, 'train@spa.rst.sctb_samples_per_second': 79.107, 'train@spa.rst.sctb_steps_per_second': 2.523, 'epoch': 10.0}
{'loss': 2.2378, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.211941719055176, 'eval_accuracy@spa.rst.sctb': 0.44680851063829785, 'eval_f1@spa.rst.sctb': 0.08981599886331344, 'eval_precision@spa.rst.sctb': 0.08045440398381574, 'eval_recall@spa.rst.sctb': 0.10927854395346656, 'eval_loss@spa.rst.sctb': 2.211942195892334, 'eval_runtime': 1.4492, 'eval_samples_per_second': 64.864, 'eval_steps_per_second': 2.07, 'epoch': 10.0}
{'train@spa.rst.sctb_loss': 2.1463100910186768, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4419134396355353, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.07398938978952303, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.10605228758169936, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.08417071960297764, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.1463098526000977, 'train@spa.rst.sctb_runtime': 5.6165, 'train@spa.rst.sctb_samples_per_second': 78.162, 'train@spa.rst.sctb_steps_per_second': 2.493, 'epoch': 11.0}
{'loss': 2.205, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.1962592601776123, 'eval_accuracy@spa.rst.sctb': 0.4574468085106383, 'eval_f1@spa.rst.sctb': 0.10288789428815004, 'eval_precision@spa.rst.sctb': 0.1490818549642079, 'eval_recall@spa.rst.sctb': 0.11462613753635426, 'eval_loss@spa.rst.sctb': 2.196258783340454, 'eval_runtime': 1.427, 'eval_samples_per_second': 65.874, 'eval_steps_per_second': 2.102, 'epoch': 11.0}
{'train@spa.rst.sctb_loss': 2.1398749351501465, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4419134396355353, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.07398938978952303, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.10605228758169936, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.08417071960297764, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.1398746967315674, 'train@spa.rst.sctb_runtime': 5.5742, 'train@spa.rst.sctb_samples_per_second': 78.756, 'train@spa.rst.sctb_steps_per_second': 2.512, 'epoch': 12.0}
{'loss': 2.184, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.1915104389190674, 'eval_accuracy@spa.rst.sctb': 0.4574468085106383, 'eval_f1@spa.rst.sctb': 0.10288789428815004, 'eval_precision@spa.rst.sctb': 0.1490818549642079, 'eval_recall@spa.rst.sctb': 0.11462613753635426, 'eval_loss@spa.rst.sctb': 2.1915102005004883, 'eval_runtime': 1.4233, 'eval_samples_per_second': 66.046, 'eval_steps_per_second': 2.108, 'epoch': 12.0}
{'train_runtime': 218.3966, 'train_samples_per_second': 24.121, 'train_steps_per_second': 0.769, 'train_loss': 2.745869681948707, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.2914
  train_runtime            = 0:17:47.48
  train_samples_per_second =     25.181
  train_steps_per_second   =      0.787
-------------------------------------------------------------------
Lang1:  tur.pdtb.tdb    Lang2:  spa.rst.sctb
Saving run to:  runs/full_shot/FullShot=v4_tur.pdtb.tdb_spa.rst.sctb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2451 examples
read 312 examples
read 422 examples
read 439 examples
read 94 examples
read 159 examples
Total prediction labels:  48
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (tr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (tr): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=48, bias=True)
    )
  )
)
{'train@tur.pdtb.tdb_loss': 2.9851486682891846, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01741967515426626, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010891738953044898, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.9851484298706055, 'train@tur.pdtb.tdb_runtime': 29.6359, 'train@tur.pdtb.tdb_samples_per_second': 82.704, 'train@tur.pdtb.tdb_steps_per_second': 2.598, 'epoch': 1.0}
{'loss': 3.4659, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.9290997982025146, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.9291000366210938, 'eval_runtime': 4.1718, 'eval_samples_per_second': 74.789, 'eval_steps_per_second': 2.397, 'epoch': 1.0}
{'train@tur.pdtb.tdb_loss': 2.4742591381073, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01741967515426626, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010891738953044898, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.4742591381073, 'train@tur.pdtb.tdb_runtime': 29.8045, 'train@tur.pdtb.tdb_samples_per_second': 82.236, 'train@tur.pdtb.tdb_steps_per_second': 2.583, 'epoch': 2.0}
{'loss': 2.6871, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.366307497024536, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.366307497024536, 'eval_runtime': 4.2447, 'eval_samples_per_second': 73.503, 'eval_steps_per_second': 2.356, 'epoch': 2.0}
{'train@tur.pdtb.tdb_loss': 2.3872060775756836, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01741967515426626, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010891738953044898, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.3872063159942627, 'train@tur.pdtb.tdb_runtime': 29.9409, 'train@tur.pdtb.tdb_samples_per_second': 81.861, 'train@tur.pdtb.tdb_steps_per_second': 2.572, 'epoch': 3.0}
{'loss': 2.4516, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.3117434978485107, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.3117434978485107, 'eval_runtime': 4.2187, 'eval_samples_per_second': 73.957, 'eval_steps_per_second': 2.37, 'epoch': 3.0}
{'train@tur.pdtb.tdb_loss': 2.3264572620391846, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.27498980008159934, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.03362467466293562, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.0498869340144298, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.05475560205671279, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.3264575004577637, 'train@tur.pdtb.tdb_runtime': 29.8112, 'train@tur.pdtb.tdb_samples_per_second': 82.217, 'train@tur.pdtb.tdb_steps_per_second': 2.583, 'epoch': 4.0}
{'loss': 2.3828, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.2773847579956055, 'eval_accuracy@tur.pdtb.tdb': 0.26282051282051283, 'eval_f1@tur.pdtb.tdb': 0.02287638287638288, 'eval_precision@tur.pdtb.tdb': 0.018008474576271187, 'eval_recall@tur.pdtb.tdb': 0.04648540686811417, 'eval_loss@tur.pdtb.tdb': 2.2773849964141846, 'eval_runtime': 4.2562, 'eval_samples_per_second': 73.305, 'eval_steps_per_second': 2.35, 'epoch': 4.0}
{'train@tur.pdtb.tdb_loss': 2.2821848392486572, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.29294165646674825, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.050416163324178226, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.06028114164308423, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.06586339364005116, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.2821848392486572, 'train@tur.pdtb.tdb_runtime': 29.8146, 'train@tur.pdtb.tdb_samples_per_second': 82.208, 'train@tur.pdtb.tdb_steps_per_second': 2.583, 'epoch': 5.0}
{'loss': 2.3373, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.24605393409729, 'eval_accuracy@tur.pdtb.tdb': 0.2692307692307692, 'eval_f1@tur.pdtb.tdb': 0.030255888713849835, 'eval_precision@tur.pdtb.tdb': 0.03830876064265823, 'eval_recall@tur.pdtb.tdb': 0.05009186556347065, 'eval_loss@tur.pdtb.tdb': 2.24605393409729, 'eval_runtime': 4.1941, 'eval_samples_per_second': 74.39, 'eval_steps_per_second': 2.384, 'epoch': 5.0}
{'train@tur.pdtb.tdb_loss': 2.2319936752319336, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.32313341493268055, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.08539426590510026, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.09716155047673451, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.09762106051459145, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.2319939136505127, 'train@tur.pdtb.tdb_runtime': 29.8158, 'train@tur.pdtb.tdb_samples_per_second': 82.205, 'train@tur.pdtb.tdb_steps_per_second': 2.583, 'epoch': 6.0}
{'loss': 2.2934, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.2144157886505127, 'eval_accuracy@tur.pdtb.tdb': 0.30128205128205127, 'eval_f1@tur.pdtb.tdb': 0.07612198746527105, 'eval_precision@tur.pdtb.tdb': 0.0783636861223068, 'eval_recall@tur.pdtb.tdb': 0.10232396687423959, 'eval_loss@tur.pdtb.tdb': 2.2144157886505127, 'eval_runtime': 7.8147, 'eval_samples_per_second': 39.925, 'eval_steps_per_second': 1.28, 'epoch': 6.0}
{'train@tur.pdtb.tdb_loss': 2.2008206844329834, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3349653202774378, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09142654728873975, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.1026551366915303, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.10963514615669145, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.2008209228515625, 'train@tur.pdtb.tdb_runtime': 29.8442, 'train@tur.pdtb.tdb_samples_per_second': 82.126, 'train@tur.pdtb.tdb_steps_per_second': 2.58, 'epoch': 7.0}
{'loss': 2.2556, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.197894334793091, 'eval_accuracy@tur.pdtb.tdb': 0.2948717948717949, 'eval_f1@tur.pdtb.tdb': 0.07586456511805159, 'eval_precision@tur.pdtb.tdb': 0.07200367214211664, 'eval_recall@tur.pdtb.tdb': 0.10209220202518203, 'eval_loss@tur.pdtb.tdb': 2.197894811630249, 'eval_runtime': 4.1788, 'eval_samples_per_second': 74.663, 'eval_steps_per_second': 2.393, 'epoch': 7.0}
{'train@tur.pdtb.tdb_loss': 2.187330961227417, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3423092615259078, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09440487879509325, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.11110539956289223, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11184330564620418, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.187330961227417, 'train@tur.pdtb.tdb_runtime': 29.8518, 'train@tur.pdtb.tdb_samples_per_second': 82.106, 'train@tur.pdtb.tdb_steps_per_second': 2.579, 'epoch': 8.0}
{'loss': 2.2384, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.1855432987213135, 'eval_accuracy@tur.pdtb.tdb': 0.2948717948717949, 'eval_f1@tur.pdtb.tdb': 0.07480187808000481, 'eval_precision@tur.pdtb.tdb': 0.07402485295456757, 'eval_recall@tur.pdtb.tdb': 0.1013946898187184, 'eval_loss@tur.pdtb.tdb': 2.1855430603027344, 'eval_runtime': 4.1992, 'eval_samples_per_second': 74.299, 'eval_steps_per_second': 2.381, 'epoch': 8.0}
{'train@tur.pdtb.tdb_loss': 2.160288095474243, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3419012647898817, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09296086310299313, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.09523346157700194, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11431286195911856, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.160287857055664, 'train@tur.pdtb.tdb_runtime': 29.8574, 'train@tur.pdtb.tdb_samples_per_second': 82.09, 'train@tur.pdtb.tdb_steps_per_second': 2.579, 'epoch': 9.0}
{'loss': 2.2103, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.1643736362457275, 'eval_accuracy@tur.pdtb.tdb': 0.3076923076923077, 'eval_f1@tur.pdtb.tdb': 0.07830022092640389, 'eval_precision@tur.pdtb.tdb': 0.07423814108596717, 'eval_recall@tur.pdtb.tdb': 0.10743979560806972, 'eval_loss@tur.pdtb.tdb': 2.1643736362457275, 'eval_runtime': 4.2104, 'eval_samples_per_second': 74.102, 'eval_steps_per_second': 2.375, 'epoch': 9.0}
{'train@tur.pdtb.tdb_loss': 2.1474287509918213, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.34394124847001223, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09411574181603212, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.09375616569944764, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11532647234139429, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1474287509918213, 'train@tur.pdtb.tdb_runtime': 29.8455, 'train@tur.pdtb.tdb_samples_per_second': 82.123, 'train@tur.pdtb.tdb_steps_per_second': 2.58, 'epoch': 10.0}
{'loss': 2.1958, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.153362989425659, 'eval_accuracy@tur.pdtb.tdb': 0.3076923076923077, 'eval_f1@tur.pdtb.tdb': 0.07821614412523503, 'eval_precision@tur.pdtb.tdb': 0.07087990976322973, 'eval_recall@tur.pdtb.tdb': 0.10743979560806972, 'eval_loss@tur.pdtb.tdb': 2.153362989425659, 'eval_runtime': 4.1837, 'eval_samples_per_second': 74.574, 'eval_steps_per_second': 2.39, 'epoch': 10.0}
{'train@tur.pdtb.tdb_loss': 2.142923593521118, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3447572419420645, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.0944041184987151, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.09509272821289703, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11538888755899776, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1429238319396973, 'train@tur.pdtb.tdb_runtime': 29.8228, 'train@tur.pdtb.tdb_samples_per_second': 82.185, 'train@tur.pdtb.tdb_steps_per_second': 2.582, 'epoch': 11.0}
{'loss': 2.1954, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.1508073806762695, 'eval_accuracy@tur.pdtb.tdb': 0.30448717948717946, 'eval_f1@tur.pdtb.tdb': 0.07752620906029996, 'eval_precision@tur.pdtb.tdb': 0.07091198460285736, 'eval_recall@tur.pdtb.tdb': 0.1061028972123478, 'eval_loss@tur.pdtb.tdb': 2.1508071422576904, 'eval_runtime': 4.2108, 'eval_samples_per_second': 74.095, 'eval_steps_per_second': 2.375, 'epoch': 11.0}
{'train@tur.pdtb.tdb_loss': 2.1393136978149414, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.34761321909424725, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.0957170860705602, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.09563752648441554, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11691824893397197, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1393134593963623, 'train@tur.pdtb.tdb_runtime': 29.8144, 'train@tur.pdtb.tdb_samples_per_second': 82.209, 'train@tur.pdtb.tdb_steps_per_second': 2.583, 'epoch': 12.0}
{'loss': 2.1806, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.147813081741333, 'eval_accuracy@tur.pdtb.tdb': 0.3108974358974359, 'eval_f1@tur.pdtb.tdb': 0.08022885520065905, 'eval_precision@tur.pdtb.tdb': 0.073617734759952, 'eval_recall@tur.pdtb.tdb': 0.10870242187069601, 'eval_loss@tur.pdtb.tdb': 2.147813081741333, 'eval_runtime': 4.1907, 'eval_samples_per_second': 74.45, 'eval_steps_per_second': 2.386, 'epoch': 12.0}
{'train_runtime': 1161.6775, 'train_samples_per_second': 25.319, 'train_steps_per_second': 0.795, 'train_loss': 2.4078449397892148, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.4078
  train_runtime            = 0:19:21.67
  train_samples_per_second =     25.319
  train_steps_per_second   =      0.795
{'train@spa.rst.sctb_loss': 3.134136438369751, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.13211845102505695, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.023918697624087864, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.0380329696871008, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.08001361317893575, 'train@spa.rst.sctb_loss@spa.rst.sctb': 3.13413667678833, 'train@spa.rst.sctb_runtime': 5.6869, 'train@spa.rst.sctb_samples_per_second': 77.195, 'train@spa.rst.sctb_steps_per_second': 2.462, 'epoch': 1.0}
{'loss': 3.4207, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.1402406692504883, 'eval_accuracy@spa.rst.sctb': 0.0851063829787234, 'eval_f1@spa.rst.sctb': 0.014035087719298246, 'eval_precision@spa.rst.sctb': 0.010269576379974327, 'eval_recall@spa.rst.sctb': 0.0221606648199446, 'eval_loss@spa.rst.sctb': 3.14024019241333, 'eval_runtime': 1.605, 'eval_samples_per_second': 58.567, 'eval_steps_per_second': 1.869, 'epoch': 1.0}
{'train@spa.rst.sctb_loss': 2.7963380813598633, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.296127562642369, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.031916311300639655, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.027741473736619365, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.048879928315412186, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.796337842941284, 'train@spa.rst.sctb_runtime': 5.7196, 'train@spa.rst.sctb_samples_per_second': 76.753, 'train@spa.rst.sctb_steps_per_second': 2.448, 'epoch': 2.0}
{'loss': 2.9909, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.801990509033203, 'eval_accuracy@spa.rst.sctb': 0.2765957446808511, 'eval_f1@spa.rst.sctb': 0.042703024985094436, 'eval_precision@spa.rst.sctb': 0.03743872549019608, 'eval_recall@spa.rst.sctb': 0.06342058354442255, 'eval_loss@spa.rst.sctb': 2.801990509033203, 'eval_runtime': 1.5792, 'eval_samples_per_second': 59.525, 'eval_steps_per_second': 1.9, 'epoch': 2.0}
{'train@spa.rst.sctb_loss': 2.5530202388763428, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.37813211845102507, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03844027220804472, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.029859398849360102, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.053942652329749104, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.5530202388763428, 'train@spa.rst.sctb_runtime': 5.7189, 'train@spa.rst.sctb_samples_per_second': 76.763, 'train@spa.rst.sctb_steps_per_second': 2.448, 'epoch': 3.0}
{'loss': 2.7314, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.561269998550415, 'eval_accuracy@spa.rst.sctb': 0.40425531914893614, 'eval_f1@spa.rst.sctb': 0.05784313725490196, 'eval_precision@spa.rst.sctb': 0.045119122917984394, 'eval_recall@spa.rst.sctb': 0.08087062576226663, 'eval_loss@spa.rst.sctb': 2.561269998550415, 'eval_runtime': 1.5716, 'eval_samples_per_second': 59.812, 'eval_steps_per_second': 1.909, 'epoch': 3.0}
{'train@spa.rst.sctb_loss': 2.4095122814178467, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3826879271070615, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03662634408602151, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.030059357324880354, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.0517741935483871, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.409512519836426, 'train@spa.rst.sctb_runtime': 5.7108, 'train@spa.rst.sctb_samples_per_second': 76.872, 'train@spa.rst.sctb_steps_per_second': 2.452, 'epoch': 4.0}
{'loss': 2.5103, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.423553228378296, 'eval_accuracy@spa.rst.sctb': 0.3723404255319149, 'eval_f1@spa.rst.sctb': 0.04640385565991103, 'eval_precision@spa.rst.sctb': 0.03876892529163564, 'eval_recall@spa.rst.sctb': 0.06764236795196547, 'eval_loss@spa.rst.sctb': 2.423553228378296, 'eval_runtime': 1.5526, 'eval_samples_per_second': 60.542, 'eval_steps_per_second': 1.932, 'epoch': 4.0}
{'train@spa.rst.sctb_loss': 2.3305418491363525, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3804100227790433, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03434849371161158, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.030067966903073284, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04996415770609319, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.3305420875549316, 'train@spa.rst.sctb_runtime': 5.7364, 'train@spa.rst.sctb_samples_per_second': 76.529, 'train@spa.rst.sctb_steps_per_second': 2.441, 'epoch': 5.0}
{'loss': 2.3965, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.3520030975341797, 'eval_accuracy@spa.rst.sctb': 0.3829787234042553, 'eval_f1@spa.rst.sctb': 0.047729618163054696, 'eval_precision@spa.rst.sctb': 0.04133847271102173, 'eval_recall@spa.rst.sctb': 0.06942489914626138, 'eval_loss@spa.rst.sctb': 2.3520028591156006, 'eval_runtime': 1.5614, 'eval_samples_per_second': 60.201, 'eval_steps_per_second': 1.921, 'epoch': 5.0}
{'train@spa.rst.sctb_loss': 2.2825515270233154, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3826879271070615, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.0349476911976912, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.031120796831179343, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.050412186379928316, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.2825512886047363, 'train@spa.rst.sctb_runtime': 5.7105, 'train@spa.rst.sctb_samples_per_second': 76.875, 'train@spa.rst.sctb_steps_per_second': 2.452, 'epoch': 6.0}
{'loss': 2.3284, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.3140616416931152, 'eval_accuracy@spa.rst.sctb': 0.3829787234042553, 'eval_f1@spa.rst.sctb': 0.048140635564570655, 'eval_precision@spa.rst.sctb': 0.0440693254300625, 'eval_recall@spa.rst.sctb': 0.06942489914626138, 'eval_loss@spa.rst.sctb': 2.314061403274536, 'eval_runtime': 1.5572, 'eval_samples_per_second': 60.365, 'eval_steps_per_second': 1.927, 'epoch': 6.0}
{'train@spa.rst.sctb_loss': 2.247236728668213, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.39635535307517084, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03781594669117647, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03238232761713425, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.053270609318996416, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.247236490249634, 'train@spa.rst.sctb_runtime': 5.7437, 'train@spa.rst.sctb_samples_per_second': 76.432, 'train@spa.rst.sctb_steps_per_second': 2.437, 'epoch': 7.0}
{'loss': 2.3086, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.286574125289917, 'eval_accuracy@spa.rst.sctb': 0.3829787234042553, 'eval_f1@spa.rst.sctb': 0.05010016878835204, 'eval_precision@spa.rst.sctb': 0.043802521008403365, 'eval_recall@spa.rst.sctb': 0.07073834318416361, 'eval_loss@spa.rst.sctb': 2.286574125289917, 'eval_runtime': 1.5466, 'eval_samples_per_second': 60.778, 'eval_steps_per_second': 1.94, 'epoch': 7.0}
{'train@spa.rst.sctb_loss': 2.2237308025360107, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4009111617312073, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03916026424538734, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.032569681276577826, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05467741935483871, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.223731279373169, 'train@spa.rst.sctb_runtime': 5.7318, 'train@spa.rst.sctb_samples_per_second': 76.59, 'train@spa.rst.sctb_steps_per_second': 2.443, 'epoch': 8.0}
{'loss': 2.26, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.2699244022369385, 'eval_accuracy@spa.rst.sctb': 0.3829787234042553, 'eval_f1@spa.rst.sctb': 0.049663108486637894, 'eval_precision@spa.rst.sctb': 0.04176093514328808, 'eval_recall@spa.rst.sctb': 0.07073834318416361, 'eval_loss@spa.rst.sctb': 2.2699241638183594, 'eval_runtime': 1.5913, 'eval_samples_per_second': 59.071, 'eval_steps_per_second': 1.885, 'epoch': 8.0}
{'train@spa.rst.sctb_loss': 2.2061867713928223, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4031890660592255, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.0396045918367347, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.032430926916221035, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.055295698924731185, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.2061870098114014, 'train@spa.rst.sctb_runtime': 5.6991, 'train@spa.rst.sctb_samples_per_second': 77.03, 'train@spa.rst.sctb_steps_per_second': 2.457, 'epoch': 9.0}
{'loss': 2.2503, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.256798028945923, 'eval_accuracy@spa.rst.sctb': 0.39361702127659576, 'eval_f1@spa.rst.sctb': 0.052762923351158654, 'eval_precision@spa.rst.sctb': 0.04444344582752888, 'eval_recall@spa.rst.sctb': 0.07383431841636177, 'eval_loss@spa.rst.sctb': 2.256798028945923, 'eval_runtime': 1.5833, 'eval_samples_per_second': 59.369, 'eval_steps_per_second': 1.895, 'epoch': 9.0}
{'train@spa.rst.sctb_loss': 2.1939351558685303, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4054669703872437, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03987824236398207, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03236103138155377, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05574372759856631, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.1939351558685303, 'train@spa.rst.sctb_runtime': 5.6814, 'train@spa.rst.sctb_samples_per_second': 77.27, 'train@spa.rst.sctb_steps_per_second': 2.464, 'epoch': 10.0}
{'loss': 2.2343, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.24808669090271, 'eval_accuracy@spa.rst.sctb': 0.39361702127659576, 'eval_f1@spa.rst.sctb': 0.05234491457401674, 'eval_precision@spa.rst.sctb': 0.04288957688338493, 'eval_recall@spa.rst.sctb': 0.07383431841636177, 'eval_loss@spa.rst.sctb': 2.2480862140655518, 'eval_runtime': 1.6026, 'eval_samples_per_second': 58.654, 'eval_steps_per_second': 1.872, 'epoch': 10.0}
{'train@spa.rst.sctb_loss': 2.1871683597564697, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.40774487471526194, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.04024001997565467, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03257974956088164, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05619175627240144, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.187168598175049, 'train@spa.rst.sctb_runtime': 5.7137, 'train@spa.rst.sctb_samples_per_second': 76.833, 'train@spa.rst.sctb_steps_per_second': 2.45, 'epoch': 11.0}
{'loss': 2.2396, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.243156671524048, 'eval_accuracy@spa.rst.sctb': 0.39361702127659576, 'eval_f1@spa.rst.sctb': 0.05234491457401674, 'eval_precision@spa.rst.sctb': 0.04288957688338493, 'eval_recall@spa.rst.sctb': 0.07383431841636177, 'eval_loss@spa.rst.sctb': 2.243157148361206, 'eval_runtime': 1.5861, 'eval_samples_per_second': 59.265, 'eval_steps_per_second': 1.891, 'epoch': 11.0}
{'train@spa.rst.sctb_loss': 2.184743881225586, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.41002277904328016, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.040627593360995855, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03288269902038059, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05663978494623656, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.184743881225586, 'train@spa.rst.sctb_runtime': 5.7077, 'train@spa.rst.sctb_samples_per_second': 76.914, 'train@spa.rst.sctb_steps_per_second': 2.453, 'epoch': 12.0}
{'loss': 2.2194, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.241609573364258, 'eval_accuracy@spa.rst.sctb': 0.39361702127659576, 'eval_f1@spa.rst.sctb': 0.05234491457401674, 'eval_precision@spa.rst.sctb': 0.04288957688338493, 'eval_recall@spa.rst.sctb': 0.07383431841636177, 'eval_loss@spa.rst.sctb': 2.2416093349456787, 'eval_runtime': 1.5504, 'eval_samples_per_second': 60.628, 'eval_steps_per_second': 1.935, 'epoch': 12.0}
{'train_runtime': 221.8176, 'train_samples_per_second': 23.749, 'train_steps_per_second': 0.757, 'train_loss': 2.490855126153855, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.4078
  train_runtime            = 0:19:21.67
  train_samples_per_second =     25.319
  train_steps_per_second   =      0.795
-------------------------------------------------------------------
Lang1:  zho.rst.sctb    Lang2:  spa.rst.sctb
Saving run to:  runs/full_shot/FullShot=v4_zho.rst.sctb_spa.rst.sctb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 439 examples
read 94 examples
read 159 examples
read 439 examples
read 94 examples
read 159 examples
Total prediction labels:  26
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (zh): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=26, bias=True)
    )
  )
)
{'train@zho.rst.sctb_loss': 3.01120924949646, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3348519362186788, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.020098136399682793, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.0513408609738885, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038866396761133605, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.011209726333618, 'train@zho.rst.sctb_runtime': 5.3809, 'train@zho.rst.sctb_samples_per_second': 81.585, 'train@zho.rst.sctb_steps_per_second': 2.602, 'epoch': 1.0}
{'loss': 3.1458, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.0247695446014404, 'eval_accuracy@zho.rst.sctb': 0.3617021276595745, 'eval_f1@zho.rst.sctb': 0.03824501831829148, 'eval_precision@zho.rst.sctb': 0.07093821510297482, 'eval_recall@zho.rst.sctb': 0.058823529411764705, 'eval_loss@zho.rst.sctb': 3.0247693061828613, 'eval_runtime': 1.3583, 'eval_samples_per_second': 69.205, 'eval_steps_per_second': 2.209, 'epoch': 1.0}
{'train@zho.rst.sctb_loss': 2.8236286640167236, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.33712984054669703, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.020866713061116596, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.038520348153375676, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.039271255060728746, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.8236281871795654, 'train@zho.rst.sctb_runtime': 5.4146, 'train@zho.rst.sctb_samples_per_second': 81.077, 'train@zho.rst.sctb_steps_per_second': 2.586, 'epoch': 2.0}
{'loss': 2.9343, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.8480935096740723, 'eval_accuracy@zho.rst.sctb': 0.35106382978723405, 'eval_f1@zho.rst.sctb': 0.032795321637426905, 'eval_precision@zho.rst.sctb': 0.07074136955291455, 'eval_recall@zho.rst.sctb': 0.05572755417956656, 'eval_loss@zho.rst.sctb': 2.848093271255493, 'eval_runtime': 1.3821, 'eval_samples_per_second': 68.012, 'eval_steps_per_second': 2.171, 'epoch': 2.0}
{'train@zho.rst.sctb_loss': 2.6615703105926514, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.33712984054669703, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.020866713061116596, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.038520348153375676, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.039271255060728746, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.6615705490112305, 'train@zho.rst.sctb_runtime': 5.4259, 'train@zho.rst.sctb_samples_per_second': 80.908, 'train@zho.rst.sctb_steps_per_second': 2.58, 'epoch': 3.0}
{'loss': 2.7826, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.696760892868042, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.696760416030884, 'eval_runtime': 1.373, 'eval_samples_per_second': 68.464, 'eval_steps_per_second': 2.185, 'epoch': 3.0}
{'train@zho.rst.sctb_loss': 2.532724380493164, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3348519362186788, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.02005677652438983, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03208061960922373, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038866396761133605, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.532724618911743, 'train@zho.rst.sctb_runtime': 5.4398, 'train@zho.rst.sctb_samples_per_second': 80.702, 'train@zho.rst.sctb_steps_per_second': 2.574, 'epoch': 4.0}
{'loss': 2.6239, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.579197406768799, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 2.579197645187378, 'eval_runtime': 1.4124, 'eval_samples_per_second': 66.555, 'eval_steps_per_second': 2.124, 'epoch': 4.0}
{'train@zho.rst.sctb_loss': 2.43707013130188, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.33712984054669703, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.020884069076840164, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.0321396993810787, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.039271255060728746, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.43707013130188, 'train@zho.rst.sctb_runtime': 5.4207, 'train@zho.rst.sctb_samples_per_second': 80.986, 'train@zho.rst.sctb_steps_per_second': 2.583, 'epoch': 5.0}
{'loss': 2.5127, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.495753765106201, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 2.495753288269043, 'eval_runtime': 1.3823, 'eval_samples_per_second': 68.003, 'eval_steps_per_second': 2.17, 'epoch': 5.0}
{'train@zho.rst.sctb_loss': 2.367037057876587, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.33712984054669703, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.020884069076840164, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.0321396993810787, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.039271255060728746, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.367037296295166, 'train@zho.rst.sctb_runtime': 5.439, 'train@zho.rst.sctb_samples_per_second': 80.714, 'train@zho.rst.sctb_steps_per_second': 2.574, 'epoch': 6.0}
{'loss': 2.4388, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.4394288063049316, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 2.4394285678863525, 'eval_runtime': 1.404, 'eval_samples_per_second': 66.951, 'eval_steps_per_second': 2.137, 'epoch': 6.0}
{'train@zho.rst.sctb_loss': 2.319145679473877, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.33940774487471526, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.02168168191869008, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03219932492449813, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.03967611336032389, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.319145917892456, 'train@zho.rst.sctb_runtime': 5.4746, 'train@zho.rst.sctb_samples_per_second': 80.189, 'train@zho.rst.sctb_steps_per_second': 2.557, 'epoch': 7.0}
{'loss': 2.3733, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.402897357940674, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.03185595567867036, 'eval_precision@zho.rst.sctb': 0.044050343249427915, 'eval_recall@zho.rst.sctb': 0.0540828173374613, 'eval_loss@zho.rst.sctb': 2.4028971195220947, 'eval_runtime': 1.3803, 'eval_samples_per_second': 68.103, 'eval_steps_per_second': 2.174, 'epoch': 7.0}
{'train@zho.rst.sctb_loss': 2.2906250953674316, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.33940774487471526, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.02168168191869008, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03219932492449813, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.03967611336032389, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.2906253337860107, 'train@zho.rst.sctb_runtime': 5.4496, 'train@zho.rst.sctb_samples_per_second': 80.557, 'train@zho.rst.sctb_steps_per_second': 2.569, 'epoch': 8.0}
{'loss': 2.3353, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.3826417922973633, 'eval_accuracy@zho.rst.sctb': 0.35106382978723405, 'eval_f1@zho.rst.sctb': 0.03705605477107403, 'eval_precision@zho.rst.sctb': 0.053017158280316175, 'eval_recall@zho.rst.sctb': 0.057178792569659444, 'eval_loss@zho.rst.sctb': 2.3826417922973633, 'eval_runtime': 1.3705, 'eval_samples_per_second': 68.589, 'eval_steps_per_second': 2.189, 'epoch': 8.0}
{'train@zho.rst.sctb_loss': 2.270601272583008, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3462414578587699, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.0244980509207157, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.035496607217918696, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.041032111363762405, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.270601511001587, 'train@zho.rst.sctb_runtime': 5.4718, 'train@zho.rst.sctb_samples_per_second': 80.23, 'train@zho.rst.sctb_steps_per_second': 2.559, 'epoch': 9.0}
{'loss': 2.3085, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.3690085411071777, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.03566768160069595, 'eval_precision@zho.rst.sctb': 0.03879361324659965, 'eval_recall@zho.rst.sctb': 0.05553405572755418, 'eval_loss@zho.rst.sctb': 2.369008779525757, 'eval_runtime': 1.4254, 'eval_samples_per_second': 65.948, 'eval_steps_per_second': 2.105, 'epoch': 9.0}
{'train@zho.rst.sctb_loss': 2.257603645324707, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3530751708428246, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.026479038278448306, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03461436953123414, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.042246686262547836, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.257603645324707, 'train@zho.rst.sctb_runtime': 5.4335, 'train@zho.rst.sctb_samples_per_second': 80.796, 'train@zho.rst.sctb_steps_per_second': 2.577, 'epoch': 10.0}
{'loss': 2.2927, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.360825777053833, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.038810260946483856, 'eval_precision@zho.rst.sctb': 0.040100250626566414, 'eval_recall@zho.rst.sctb': 0.05698529411764706, 'eval_loss@zho.rst.sctb': 2.360825538635254, 'eval_runtime': 1.3898, 'eval_samples_per_second': 67.635, 'eval_steps_per_second': 2.159, 'epoch': 10.0}
{'train@zho.rst.sctb_loss': 2.25089955329895, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3621867881548975, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.02897316809494866, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03681743632621559, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.0438661194609284, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.250899314880371, 'train@zho.rst.sctb_runtime': 5.4194, 'train@zho.rst.sctb_samples_per_second': 81.005, 'train@zho.rst.sctb_steps_per_second': 2.583, 'epoch': 11.0}
{'loss': 2.278, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.3568036556243896, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.03850133809099019, 'eval_precision@zho.rst.sctb': 0.03748470012239902, 'eval_recall@zho.rst.sctb': 0.05698529411764706, 'eval_loss@zho.rst.sctb': 2.356804132461548, 'eval_runtime': 1.3876, 'eval_samples_per_second': 67.743, 'eval_steps_per_second': 2.162, 'epoch': 11.0}
{'train@zho.rst.sctb_loss': 2.2485995292663574, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.36446469248291574, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.029578264872382517, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.037476830398517146, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04427097776052354, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.2485995292663574, 'train@zho.rst.sctb_runtime': 5.4669, 'train@zho.rst.sctb_samples_per_second': 80.302, 'train@zho.rst.sctb_steps_per_second': 2.561, 'epoch': 12.0}
{'loss': 2.2701, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.355499744415283, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.03850133809099019, 'eval_precision@zho.rst.sctb': 0.03748470012239902, 'eval_recall@zho.rst.sctb': 0.05698529411764706, 'eval_loss@zho.rst.sctb': 2.355499505996704, 'eval_runtime': 1.3586, 'eval_samples_per_second': 69.188, 'eval_steps_per_second': 2.208, 'epoch': 12.0}
{'train_runtime': 213.453, 'train_samples_per_second': 24.68, 'train_steps_per_second': 0.787, 'train_loss': 2.52467071442377, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.5247
  train_runtime            = 0:03:33.45
  train_samples_per_second =      24.68
  train_steps_per_second   =      0.787
{'train@spa.rst.sctb_loss': 3.2306928634643555, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.018223234624145785, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.0020833333333333333, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.0010718113612004287, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.037037037037037035, 'train@spa.rst.sctb_loss@spa.rst.sctb': 3.2306926250457764, 'train@spa.rst.sctb_runtime': 5.533, 'train@spa.rst.sctb_samples_per_second': 79.342, 'train@spa.rst.sctb_steps_per_second': 2.53, 'epoch': 1.0}
{'loss': 3.4121, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.2533373832702637, 'eval_accuracy@spa.rst.sctb': 0.010638297872340425, 'eval_f1@spa.rst.sctb': 0.0015015015015015015, 'eval_precision@spa.rst.sctb': 0.00076103500761035, 'eval_recall@spa.rst.sctb': 0.05555555555555555, 'eval_loss@spa.rst.sctb': 3.253337860107422, 'eval_runtime': 1.4059, 'eval_samples_per_second': 66.859, 'eval_steps_per_second': 2.134, 'epoch': 1.0}
{'train@spa.rst.sctb_loss': 2.9323768615722656, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.025056947608200455, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.005708665564192601, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03466197111636939, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.062192353643966546, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.9323766231536865, 'train@spa.rst.sctb_runtime': 5.543, 'train@spa.rst.sctb_samples_per_second': 79.2, 'train@spa.rst.sctb_steps_per_second': 2.526, 'epoch': 2.0}
{'loss': 3.0932, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.940174102783203, 'eval_accuracy@spa.rst.sctb': 0.02127659574468085, 'eval_f1@spa.rst.sctb': 0.00519155030433226, 'eval_precision@spa.rst.sctb': 0.012268518518518519, 'eval_recall@spa.rst.sctb': 0.057239057239057235, 'eval_loss@spa.rst.sctb': 2.940173864364624, 'eval_runtime': 1.3884, 'eval_samples_per_second': 67.706, 'eval_steps_per_second': 2.161, 'epoch': 2.0}
{'train@spa.rst.sctb_loss': 2.636575937271118, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.37813211845102507, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03456182065217391, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.037855366027007814, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04934587813620072, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.63657546043396, 'train@spa.rst.sctb_runtime': 5.5366, 'train@spa.rst.sctb_samples_per_second': 79.291, 'train@spa.rst.sctb_steps_per_second': 2.529, 'epoch': 3.0}
{'loss': 2.8235, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.648566722869873, 'eval_accuracy@spa.rst.sctb': 0.3404255319148936, 'eval_f1@spa.rst.sctb': 0.04382476693342134, 'eval_precision@spa.rst.sctb': 0.03969392635102822, 'eval_recall@spa.rst.sctb': 0.062294774369077766, 'eval_loss@spa.rst.sctb': 2.648566484451294, 'eval_runtime': 1.3955, 'eval_samples_per_second': 67.358, 'eval_steps_per_second': 2.15, 'epoch': 3.0}
{'train@spa.rst.sctb_loss': 2.424445867538452, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.36674259681093396, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.030594535183907163, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03761536225565002, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04676523297491039, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.424445390701294, 'train@spa.rst.sctb_runtime': 5.5295, 'train@spa.rst.sctb_samples_per_second': 79.393, 'train@spa.rst.sctb_steps_per_second': 2.532, 'epoch': 4.0}
{'loss': 2.556, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.4537532329559326, 'eval_accuracy@spa.rst.sctb': 0.3617021276595745, 'eval_f1@spa.rst.sctb': 0.046393093046237066, 'eval_precision@spa.rst.sctb': 0.04453781512605042, 'eval_recall@spa.rst.sctb': 0.06585983675766957, 'eval_loss@spa.rst.sctb': 2.4537529945373535, 'eval_runtime': 1.4007, 'eval_samples_per_second': 67.108, 'eval_steps_per_second': 2.142, 'epoch': 4.0}
{'train@spa.rst.sctb_loss': 2.3040971755981445, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.357630979498861, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02721933371846717, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.038942557897475936, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.044802867383512544, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.304097890853882, 'train@spa.rst.sctb_runtime': 5.548, 'train@spa.rst.sctb_samples_per_second': 79.127, 'train@spa.rst.sctb_steps_per_second': 2.523, 'epoch': 5.0}
{'loss': 2.398, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.34051251411438, 'eval_accuracy@spa.rst.sctb': 0.3723404255319149, 'eval_f1@spa.rst.sctb': 0.041794024078348195, 'eval_precision@spa.rst.sctb': 0.050980392156862744, 'eval_recall@spa.rst.sctb': 0.06501547987616098, 'eval_loss@spa.rst.sctb': 2.340512752532959, 'eval_runtime': 1.4006, 'eval_samples_per_second': 67.115, 'eval_steps_per_second': 2.142, 'epoch': 5.0}
{'train@spa.rst.sctb_loss': 2.237157106399536, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.357630979498861, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02721933371846717, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.038942557897475936, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.044802867383512544, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.2371573448181152, 'train@spa.rst.sctb_runtime': 5.5705, 'train@spa.rst.sctb_samples_per_second': 78.808, 'train@spa.rst.sctb_steps_per_second': 2.513, 'epoch': 6.0}
{'loss': 2.3195, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.2813291549682617, 'eval_accuracy@spa.rst.sctb': 0.3723404255319149, 'eval_f1@spa.rst.sctb': 0.042263305322128845, 'eval_precision@spa.rst.sctb': 0.079923273657289, 'eval_recall@spa.rst.sctb': 0.06501547987616098, 'eval_loss@spa.rst.sctb': 2.2813291549682617, 'eval_runtime': 1.4299, 'eval_samples_per_second': 65.741, 'eval_steps_per_second': 2.098, 'epoch': 6.0}
{'train@spa.rst.sctb_loss': 2.194554567337036, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.37585421412300685, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03310158079625292, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.038130782169890666, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04855734767025089, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.194554328918457, 'train@spa.rst.sctb_runtime': 5.5285, 'train@spa.rst.sctb_samples_per_second': 79.407, 'train@spa.rst.sctb_steps_per_second': 2.532, 'epoch': 7.0}
{'loss': 2.2585, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.246032476425171, 'eval_accuracy@spa.rst.sctb': 0.3829787234042553, 'eval_f1@spa.rst.sctb': 0.049065377785100964, 'eval_precision@spa.rst.sctb': 0.05129958960328317, 'eval_recall@spa.rst.sctb': 0.06942489914626138, 'eval_loss@spa.rst.sctb': 2.246032953262329, 'eval_runtime': 1.4163, 'eval_samples_per_second': 66.37, 'eval_steps_per_second': 2.118, 'epoch': 7.0}
{'train@spa.rst.sctb_loss': 2.1686978340148926, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3804100227790433, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03441919191919191, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.035715811965811965, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04962365591397849, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.1686980724334717, 'train@spa.rst.sctb_runtime': 5.5653, 'train@spa.rst.sctb_samples_per_second': 78.881, 'train@spa.rst.sctb_steps_per_second': 2.516, 'epoch': 8.0}
{'loss': 2.2116, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.2267212867736816, 'eval_accuracy@spa.rst.sctb': 0.3829787234042553, 'eval_f1@spa.rst.sctb': 0.049065377785100964, 'eval_precision@spa.rst.sctb': 0.05129958960328317, 'eval_recall@spa.rst.sctb': 0.06942489914626138, 'eval_loss@spa.rst.sctb': 2.2267215251922607, 'eval_runtime': 1.4367, 'eval_samples_per_second': 65.427, 'eval_steps_per_second': 2.088, 'epoch': 8.0}
{'train@spa.rst.sctb_loss': 2.150184392929077, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3826879271070615, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03534610353848272, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03365255036722593, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.050412186379928316, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.150184392929077, 'train@spa.rst.sctb_runtime': 5.534, 'train@spa.rst.sctb_samples_per_second': 79.328, 'train@spa.rst.sctb_steps_per_second': 2.53, 'epoch': 9.0}
{'loss': 2.2067, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.212311267852783, 'eval_accuracy@spa.rst.sctb': 0.39361702127659576, 'eval_f1@spa.rst.sctb': 0.05246094901267315, 'eval_precision@spa.rst.sctb': 0.05182072829131652, 'eval_recall@spa.rst.sctb': 0.07252087437845951, 'eval_loss@spa.rst.sctb': 2.212311029434204, 'eval_runtime': 1.402, 'eval_samples_per_second': 67.046, 'eval_steps_per_second': 2.14, 'epoch': 9.0}
{'train@spa.rst.sctb_loss': 2.1370179653167725, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.39863325740318906, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03919193012414268, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.035656497955425574, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05388888888888888, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.1370182037353516, 'train@spa.rst.sctb_runtime': 5.5518, 'train@spa.rst.sctb_samples_per_second': 79.073, 'train@spa.rst.sctb_steps_per_second': 2.522, 'epoch': 10.0}
{'loss': 2.1942, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.2025537490844727, 'eval_accuracy@spa.rst.sctb': 0.39361702127659576, 'eval_f1@spa.rst.sctb': 0.05405056759545923, 'eval_precision@spa.rst.sctb': 0.04966203005418692, 'eval_recall@spa.rst.sctb': 0.07383431841636177, 'eval_loss@spa.rst.sctb': 2.2025539875030518, 'eval_runtime': 1.4071, 'eval_samples_per_second': 66.805, 'eval_steps_per_second': 2.132, 'epoch': 10.0}
{'train@spa.rst.sctb_loss': 2.1299703121185303, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4031890660592255, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.039992552927408775, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03590301974448316, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05478494623655914, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.129969835281372, 'train@spa.rst.sctb_runtime': 5.553, 'train@spa.rst.sctb_samples_per_second': 79.056, 'train@spa.rst.sctb_steps_per_second': 2.521, 'epoch': 11.0}
{'loss': 2.1794, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.197059392929077, 'eval_accuracy@spa.rst.sctb': 0.39361702127659576, 'eval_f1@spa.rst.sctb': 0.05405056759545923, 'eval_precision@spa.rst.sctb': 0.04966203005418692, 'eval_recall@spa.rst.sctb': 0.07383431841636177, 'eval_loss@spa.rst.sctb': 2.1970598697662354, 'eval_runtime': 1.4035, 'eval_samples_per_second': 66.978, 'eval_steps_per_second': 2.138, 'epoch': 11.0}
{'train@spa.rst.sctb_loss': 2.127603530883789, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4031890660592255, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.039992552927408775, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03590301974448316, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05478494623655914, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.127603054046631, 'train@spa.rst.sctb_runtime': 5.5558, 'train@spa.rst.sctb_samples_per_second': 79.016, 'train@spa.rst.sctb_steps_per_second': 2.52, 'epoch': 12.0}
{'loss': 2.1782, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.1955111026763916, 'eval_accuracy@spa.rst.sctb': 0.39361702127659576, 'eval_f1@spa.rst.sctb': 0.05405056759545923, 'eval_precision@spa.rst.sctb': 0.04966203005418692, 'eval_recall@spa.rst.sctb': 0.07383431841636177, 'eval_loss@spa.rst.sctb': 2.1955106258392334, 'eval_runtime': 1.3981, 'eval_samples_per_second': 67.234, 'eval_steps_per_second': 2.146, 'epoch': 12.0}
{'train_runtime': 217.7701, 'train_samples_per_second': 24.191, 'train_steps_per_second': 0.771, 'train_loss': 2.485915558678763, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.5247
  train_runtime            = 0:03:33.45
  train_samples_per_second =      24.68
  train_steps_per_second   =      0.787

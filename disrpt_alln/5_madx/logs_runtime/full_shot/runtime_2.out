-------------------------------------------------------------------
Lang1:  deu.rst.pcc    Lang2:  eng.rst.gum
Saving run to:  runs/full_shot/FullShot=v4_deu.rst.pcc_eng.rst.gum_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2164 examples
read 241 examples
read 260 examples
read 13897 examples
read 2149 examples
read 2091 examples
Total prediction labels:  32
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=32, bias=True)
    )
  )
)
{'train@deu.rst.pcc_loss': 3.1309711933135986, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.10489833641404805, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.014395922258864598, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.02598877551344981, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.04180462324129585, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.1309709548950195, 'train@deu.rst.pcc_runtime': 27.3768, 'train@deu.rst.pcc_samples_per_second': 79.045, 'train@deu.rst.pcc_steps_per_second': 2.484, 'epoch': 1.0}
{'loss': 3.3133, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.1513423919677734, 'eval_accuracy@deu.rst.pcc': 0.11618257261410789, 'eval_f1@deu.rst.pcc': 0.019060599317608663, 'eval_precision@deu.rst.pcc': 0.03426466102341188, 'eval_recall@deu.rst.pcc': 0.04503713878713878, 'eval_loss@deu.rst.pcc': 3.1513423919677734, 'eval_runtime': 3.3488, 'eval_samples_per_second': 71.965, 'eval_steps_per_second': 2.389, 'epoch': 1.0}
{'train@deu.rst.pcc_loss': 2.9396371841430664, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.12014787430683918, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.024794241236324236, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.04338494069905682, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.04919471448105195, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.9396371841430664, 'train@deu.rst.pcc_runtime': 26.5874, 'train@deu.rst.pcc_samples_per_second': 81.392, 'train@deu.rst.pcc_steps_per_second': 2.558, 'epoch': 2.0}
{'loss': 3.0324, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.9703927040100098, 'eval_accuracy@deu.rst.pcc': 0.13692946058091288, 'eval_f1@deu.rst.pcc': 0.024430022253405338, 'eval_precision@deu.rst.pcc': 0.020213716384236614, 'eval_recall@deu.rst.pcc': 0.05584808709808709, 'eval_loss@deu.rst.pcc': 2.9703927040100098, 'eval_runtime': 3.3581, 'eval_samples_per_second': 71.766, 'eval_steps_per_second': 2.382, 'epoch': 2.0}
{'train@deu.rst.pcc_loss': 2.8736727237701416, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.1423290203327172, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.049926211382330624, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.08937304501981172, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.066013028401663, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.8736727237701416, 'train@deu.rst.pcc_runtime': 26.5618, 'train@deu.rst.pcc_samples_per_second': 81.47, 'train@deu.rst.pcc_steps_per_second': 2.56, 'epoch': 3.0}
{'loss': 2.9339, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.9202253818511963, 'eval_accuracy@deu.rst.pcc': 0.13278008298755187, 'eval_f1@deu.rst.pcc': 0.03443087643785513, 'eval_precision@deu.rst.pcc': 0.03548208896058276, 'eval_recall@deu.rst.pcc': 0.06234101546601547, 'eval_loss@deu.rst.pcc': 2.920225143432617, 'eval_runtime': 3.3042, 'eval_samples_per_second': 72.937, 'eval_steps_per_second': 2.421, 'epoch': 3.0}
{'train@deu.rst.pcc_loss': 2.82322096824646, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.18022181146025879, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.07195320048305995, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.07476839871389099, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.099829440003295, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.82322096824646, 'train@deu.rst.pcc_runtime': 26.5841, 'train@deu.rst.pcc_samples_per_second': 81.402, 'train@deu.rst.pcc_steps_per_second': 2.558, 'epoch': 4.0}
{'loss': 2.8828, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.8821070194244385, 'eval_accuracy@deu.rst.pcc': 0.16182572614107885, 'eval_f1@deu.rst.pcc': 0.05839795979393774, 'eval_precision@deu.rst.pcc': 0.055969379498791265, 'eval_recall@deu.rst.pcc': 0.0987993487993488, 'eval_loss@deu.rst.pcc': 2.8821072578430176, 'eval_runtime': 3.255, 'eval_samples_per_second': 74.04, 'eval_steps_per_second': 2.458, 'epoch': 4.0}
{'train@deu.rst.pcc_loss': 2.780750274658203, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.19824399260628467, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.07735261223894187, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.0715501954886995, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.11634139875862412, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.780750274658203, 'train@deu.rst.pcc_runtime': 26.7068, 'train@deu.rst.pcc_samples_per_second': 81.028, 'train@deu.rst.pcc_steps_per_second': 2.546, 'epoch': 5.0}
{'loss': 2.8345, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.848940372467041, 'eval_accuracy@deu.rst.pcc': 0.17012448132780084, 'eval_f1@deu.rst.pcc': 0.06961866633447673, 'eval_precision@deu.rst.pcc': 0.08838282711562806, 'eval_recall@deu.rst.pcc': 0.10638609076109078, 'eval_loss@deu.rst.pcc': 2.848940372467041, 'eval_runtime': 3.3354, 'eval_samples_per_second': 72.255, 'eval_steps_per_second': 2.399, 'epoch': 5.0}
{'train@deu.rst.pcc_loss': 2.744351387023926, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.20194085027726433, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.07741993823782341, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.07450615802630706, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.12185242065521046, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.744351387023926, 'train@deu.rst.pcc_runtime': 26.5237, 'train@deu.rst.pcc_samples_per_second': 81.587, 'train@deu.rst.pcc_steps_per_second': 2.564, 'epoch': 6.0}
{'loss': 2.7972, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.8174855709075928, 'eval_accuracy@deu.rst.pcc': 0.17427385892116182, 'eval_f1@deu.rst.pcc': 0.0711861926485895, 'eval_precision@deu.rst.pcc': 0.09207587240503677, 'eval_recall@deu.rst.pcc': 0.1224689662189662, 'eval_loss@deu.rst.pcc': 2.8174853324890137, 'eval_runtime': 3.2659, 'eval_samples_per_second': 73.793, 'eval_steps_per_second': 2.45, 'epoch': 6.0}
{'train@deu.rst.pcc_loss': 2.717621326446533, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.20425138632162662, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.07721522496231295, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.08632621391156867, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.12227202937864634, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.717621326446533, 'train@deu.rst.pcc_runtime': 26.5751, 'train@deu.rst.pcc_samples_per_second': 81.429, 'train@deu.rst.pcc_steps_per_second': 2.559, 'epoch': 7.0}
{'loss': 2.7709, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.7955024242401123, 'eval_accuracy@deu.rst.pcc': 0.17427385892116182, 'eval_f1@deu.rst.pcc': 0.06607709697846226, 'eval_precision@deu.rst.pcc': 0.06995091896407685, 'eval_recall@deu.rst.pcc': 0.1210381054131054, 'eval_loss@deu.rst.pcc': 2.7955029010772705, 'eval_runtime': 3.3514, 'eval_samples_per_second': 71.911, 'eval_steps_per_second': 2.387, 'epoch': 7.0}
{'train@deu.rst.pcc_loss': 2.6973228454589844, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.20425138632162662, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.07817776091201609, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.09236256118234995, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.12327291902915441, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6973230838775635, 'train@deu.rst.pcc_runtime': 26.5922, 'train@deu.rst.pcc_samples_per_second': 81.377, 'train@deu.rst.pcc_steps_per_second': 2.557, 'epoch': 8.0}
{'loss': 2.7399, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.7789905071258545, 'eval_accuracy@deu.rst.pcc': 0.16182572614107885, 'eval_f1@deu.rst.pcc': 0.06317314399155831, 'eval_precision@deu.rst.pcc': 0.06947374732993013, 'eval_recall@deu.rst.pcc': 0.11663741351241351, 'eval_loss@deu.rst.pcc': 2.7789905071258545, 'eval_runtime': 3.3335, 'eval_samples_per_second': 72.297, 'eval_steps_per_second': 2.4, 'epoch': 8.0}
{'train@deu.rst.pcc_loss': 2.680814266204834, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2121072088724584, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08352598808797823, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.0933233153111859, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.12823822996476522, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.680814027786255, 'train@deu.rst.pcc_runtime': 26.676, 'train@deu.rst.pcc_samples_per_second': 81.122, 'train@deu.rst.pcc_steps_per_second': 2.549, 'epoch': 9.0}
{'loss': 2.7245, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.768150568008423, 'eval_accuracy@deu.rst.pcc': 0.17012448132780084, 'eval_f1@deu.rst.pcc': 0.06813873249631763, 'eval_precision@deu.rst.pcc': 0.08756732172680448, 'eval_recall@deu.rst.pcc': 0.12037672975172975, 'eval_loss@deu.rst.pcc': 2.768150568008423, 'eval_runtime': 3.2777, 'eval_samples_per_second': 73.527, 'eval_steps_per_second': 2.441, 'epoch': 9.0}
{'train@deu.rst.pcc_loss': 2.669949769973755, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.21487985212569316, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.0862342117467804, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.09680503204394116, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.1308029369737903, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.669949769973755, 'train@deu.rst.pcc_runtime': 26.6535, 'train@deu.rst.pcc_samples_per_second': 81.19, 'train@deu.rst.pcc_steps_per_second': 2.551, 'epoch': 10.0}
{'loss': 2.7109, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.7570016384124756, 'eval_accuracy@deu.rst.pcc': 0.16597510373443983, 'eval_f1@deu.rst.pcc': 0.06552378384478362, 'eval_precision@deu.rst.pcc': 0.07194861787253092, 'eval_recall@deu.rst.pcc': 0.11895222832722833, 'eval_loss@deu.rst.pcc': 2.7570009231567383, 'eval_runtime': 3.3285, 'eval_samples_per_second': 72.405, 'eval_steps_per_second': 2.403, 'epoch': 10.0}
{'train@deu.rst.pcc_loss': 2.6634273529052734, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.21950092421441775, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08786057892376149, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.09807843692621279, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.1329721862706607, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6634273529052734, 'train@deu.rst.pcc_runtime': 26.5798, 'train@deu.rst.pcc_samples_per_second': 81.415, 'train@deu.rst.pcc_steps_per_second': 2.558, 'epoch': 11.0}
{'loss': 2.6997, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.7518608570098877, 'eval_accuracy@deu.rst.pcc': 0.17427385892116182, 'eval_f1@deu.rst.pcc': 0.06895844831293467, 'eval_precision@deu.rst.pcc': 0.0790541913386741, 'eval_recall@deu.rst.pcc': 0.12186482498982498, 'eval_loss@deu.rst.pcc': 2.7518603801727295, 'eval_runtime': 3.2777, 'eval_samples_per_second': 73.527, 'eval_steps_per_second': 2.441, 'epoch': 11.0}
{'train@deu.rst.pcc_loss': 2.661210775375366, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.21857670979667282, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08819089502236169, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.09805738389408437, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.13306539349361018, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.661210775375366, 'train@deu.rst.pcc_runtime': 26.5473, 'train@deu.rst.pcc_samples_per_second': 81.515, 'train@deu.rst.pcc_steps_per_second': 2.561, 'epoch': 12.0}
{'loss': 2.692, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.7495994567871094, 'eval_accuracy@deu.rst.pcc': 0.16597510373443983, 'eval_f1@deu.rst.pcc': 0.06511230824758579, 'eval_precision@deu.rst.pcc': 0.07383342204680389, 'eval_recall@deu.rst.pcc': 0.11723519536019537, 'eval_loss@deu.rst.pcc': 2.7495994567871094, 'eval_runtime': 3.3039, 'eval_samples_per_second': 72.945, 'eval_steps_per_second': 2.421, 'epoch': 12.0}
{'train_runtime': 1038.1278, 'train_samples_per_second': 25.014, 'train_steps_per_second': 0.786, 'train_loss': 2.844333798277612, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.8443
  train_runtime            = 0:17:18.12
  train_samples_per_second =     25.014
  train_steps_per_second   =      0.786
{'train@eng.rst.gum_loss': 2.3727619647979736, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.307620349715766, 'train@eng.rst.gum_f1@eng.rst.gum': 0.06451982269454792, 'train@eng.rst.gum_precision@eng.rst.gum': 0.07091282441586846, 'train@eng.rst.gum_recall@eng.rst.gum': 0.08647041719539802, 'train@eng.rst.gum_loss@eng.rst.gum': 2.3727619647979736, 'train@eng.rst.gum_runtime': 165.5699, 'train@eng.rst.gum_samples_per_second': 83.934, 'train@eng.rst.gum_steps_per_second': 2.627, 'epoch': 1.0}
{'loss': 2.7115, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.453589677810669, 'eval_accuracy@eng.rst.gum': 0.29176361098185205, 'eval_f1@eng.rst.gum': 0.06217619631928841, 'eval_precision@eng.rst.gum': 0.06178812646325851, 'eval_recall@eng.rst.gum': 0.08843442548273146, 'eval_loss@eng.rst.gum': 2.453589677810669, 'eval_runtime': 25.9306, 'eval_samples_per_second': 82.875, 'eval_steps_per_second': 2.622, 'epoch': 1.0}
{'train@eng.rst.gum_loss': 1.9830946922302246, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.41462186083327335, 'train@eng.rst.gum_f1@eng.rst.gum': 0.1824898503321268, 'train@eng.rst.gum_precision@eng.rst.gum': 0.2899273910974394, 'train@eng.rst.gum_recall@eng.rst.gum': 0.19377691227063784, 'train@eng.rst.gum_loss@eng.rst.gum': 1.9830946922302246, 'train@eng.rst.gum_runtime': 165.4793, 'train@eng.rst.gum_samples_per_second': 83.98, 'train@eng.rst.gum_steps_per_second': 2.629, 'epoch': 2.0}
{'loss': 2.2217, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.092960834503174, 'eval_accuracy@eng.rst.gum': 0.3838994881340158, 'eval_f1@eng.rst.gum': 0.16962842071446688, 'eval_precision@eng.rst.gum': 0.2566042841483932, 'eval_recall@eng.rst.gum': 0.1888725752164282, 'eval_loss@eng.rst.gum': 2.0929605960845947, 'eval_runtime': 25.9722, 'eval_samples_per_second': 82.742, 'eval_steps_per_second': 2.618, 'epoch': 2.0}
{'train@eng.rst.gum_loss': 1.7606685161590576, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.492336475498309, 'train@eng.rst.gum_f1@eng.rst.gum': 0.27775171576283114, 'train@eng.rst.gum_precision@eng.rst.gum': 0.41310344172840846, 'train@eng.rst.gum_recall@eng.rst.gum': 0.287430289768121, 'train@eng.rst.gum_loss@eng.rst.gum': 1.7606685161590576, 'train@eng.rst.gum_runtime': 165.0721, 'train@eng.rst.gum_samples_per_second': 84.187, 'train@eng.rst.gum_steps_per_second': 2.635, 'epoch': 3.0}
{'loss': 1.9467, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.8985217809677124, 'eval_accuracy@eng.rst.gum': 0.4578873894834807, 'eval_f1@eng.rst.gum': 0.2636043385136146, 'eval_precision@eng.rst.gum': 0.32378785283440387, 'eval_recall@eng.rst.gum': 0.27684322951308027, 'eval_loss@eng.rst.gum': 1.8985216617584229, 'eval_runtime': 25.87, 'eval_samples_per_second': 83.069, 'eval_steps_per_second': 2.629, 'epoch': 3.0}
{'train@eng.rst.gum_loss': 1.6485505104064941, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5152191120385695, 'train@eng.rst.gum_f1@eng.rst.gum': 0.30979722136517435, 'train@eng.rst.gum_precision@eng.rst.gum': 0.42583381683548027, 'train@eng.rst.gum_recall@eng.rst.gum': 0.31587979139928185, 'train@eng.rst.gum_loss@eng.rst.gum': 1.6485505104064941, 'train@eng.rst.gum_runtime': 165.3348, 'train@eng.rst.gum_samples_per_second': 84.054, 'train@eng.rst.gum_steps_per_second': 2.631, 'epoch': 4.0}
{'loss': 1.7827, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.8158432245254517, 'eval_accuracy@eng.rst.gum': 0.47417403443462075, 'eval_f1@eng.rst.gum': 0.2901068212824546, 'eval_precision@eng.rst.gum': 0.3260700425698091, 'eval_recall@eng.rst.gum': 0.30095464673856037, 'eval_loss@eng.rst.gum': 1.8158434629440308, 'eval_runtime': 25.9702, 'eval_samples_per_second': 82.749, 'eval_steps_per_second': 2.618, 'epoch': 4.0}
{'train@eng.rst.gum_loss': 1.5783467292785645, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5284593797222422, 'train@eng.rst.gum_f1@eng.rst.gum': 0.35707091623326565, 'train@eng.rst.gum_precision@eng.rst.gum': 0.4734918181682164, 'train@eng.rst.gum_recall@eng.rst.gum': 0.360254482549052, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5783467292785645, 'train@eng.rst.gum_runtime': 165.4817, 'train@eng.rst.gum_samples_per_second': 83.979, 'train@eng.rst.gum_steps_per_second': 2.629, 'epoch': 5.0}
{'loss': 1.6884, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.7586032152175903, 'eval_accuracy@eng.rst.gum': 0.4960446719404374, 'eval_f1@eng.rst.gum': 0.3445320939776262, 'eval_precision@eng.rst.gum': 0.44271359188268916, 'eval_recall@eng.rst.gum': 0.35343355240891267, 'eval_loss@eng.rst.gum': 1.7586030960083008, 'eval_runtime': 26.0328, 'eval_samples_per_second': 82.55, 'eval_steps_per_second': 2.612, 'epoch': 5.0}
{'train@eng.rst.gum_loss': 1.5293699502944946, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5443620925379579, 'train@eng.rst.gum_f1@eng.rst.gum': 0.38731122830409037, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5072375423747689, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3870567929515995, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5293699502944946, 'train@eng.rst.gum_runtime': 165.2048, 'train@eng.rst.gum_samples_per_second': 84.12, 'train@eng.rst.gum_steps_per_second': 2.633, 'epoch': 6.0}
{'loss': 1.6309, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.7239407300949097, 'eval_accuracy@eng.rst.gum': 0.5123313168915775, 'eval_f1@eng.rst.gum': 0.3770333546444675, 'eval_precision@eng.rst.gum': 0.4998353458392774, 'eval_recall@eng.rst.gum': 0.3815814832809205, 'eval_loss@eng.rst.gum': 1.7239408493041992, 'eval_runtime': 25.892, 'eval_samples_per_second': 82.998, 'eval_steps_per_second': 2.626, 'epoch': 6.0}
{'train@eng.rst.gum_loss': 1.4961390495300293, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5488234870835432, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3950759807875564, 'train@eng.rst.gum_precision@eng.rst.gum': 0.49866898580313335, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3991893983664458, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4961386919021606, 'train@eng.rst.gum_runtime': 165.2751, 'train@eng.rst.gum_samples_per_second': 84.084, 'train@eng.rst.gum_steps_per_second': 2.632, 'epoch': 7.0}
{'loss': 1.5924, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.703379511833191, 'eval_accuracy@eng.rst.gum': 0.5104699860400186, 'eval_f1@eng.rst.gum': 0.3780478417832237, 'eval_precision@eng.rst.gum': 0.49333320504864575, 'eval_recall@eng.rst.gum': 0.3842866138774495, 'eval_loss@eng.rst.gum': 1.7033796310424805, 'eval_runtime': 25.93, 'eval_samples_per_second': 82.877, 'eval_steps_per_second': 2.622, 'epoch': 7.0}
{'train@eng.rst.gum_loss': 1.4743179082870483, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5554436209253796, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4072684418959541, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5107294673585309, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4075658045815354, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4743177890777588, 'train@eng.rst.gum_runtime': 165.6463, 'train@eng.rst.gum_samples_per_second': 83.896, 'train@eng.rst.gum_steps_per_second': 2.626, 'epoch': 8.0}
{'loss': 1.5613, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.6884002685546875, 'eval_accuracy@eng.rst.gum': 0.5202419730107026, 'eval_f1@eng.rst.gum': 0.3888146249353639, 'eval_precision@eng.rst.gum': 0.5055838232358489, 'eval_recall@eng.rst.gum': 0.3952706381204562, 'eval_loss@eng.rst.gum': 1.6884002685546875, 'eval_runtime': 26.0426, 'eval_samples_per_second': 82.519, 'eval_steps_per_second': 2.611, 'epoch': 8.0}
{'train@eng.rst.gum_loss': 1.455586314201355, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5640785781103835, 'train@eng.rst.gum_f1@eng.rst.gum': 0.42017903757535036, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5091716187323448, 'train@eng.rst.gum_recall@eng.rst.gum': 0.42345363924268753, 'train@eng.rst.gum_loss@eng.rst.gum': 1.455586314201355, 'train@eng.rst.gum_runtime': 165.065, 'train@eng.rst.gum_samples_per_second': 84.191, 'train@eng.rst.gum_steps_per_second': 2.635, 'epoch': 9.0}
{'loss': 1.5384, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.671951174736023, 'eval_accuracy@eng.rst.gum': 0.5160539785946953, 'eval_f1@eng.rst.gum': 0.3921623260887629, 'eval_precision@eng.rst.gum': 0.48039837474484764, 'eval_recall@eng.rst.gum': 0.4024217460366519, 'eval_loss@eng.rst.gum': 1.6719512939453125, 'eval_runtime': 25.9588, 'eval_samples_per_second': 82.785, 'eval_steps_per_second': 2.62, 'epoch': 9.0}
{'train@eng.rst.gum_loss': 1.442086935043335, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5664531913362596, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4248867145827905, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5149813314692232, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4226716356208556, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4420868158340454, 'train@eng.rst.gum_runtime': 165.2746, 'train@eng.rst.gum_samples_per_second': 84.084, 'train@eng.rst.gum_steps_per_second': 2.632, 'epoch': 10.0}
{'loss': 1.5238, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.6651475429534912, 'eval_accuracy@eng.rst.gum': 0.5239646347138204, 'eval_f1@eng.rst.gum': 0.399835757163198, 'eval_precision@eng.rst.gum': 0.49272035849413853, 'eval_recall@eng.rst.gum': 0.4046108321901272, 'eval_loss@eng.rst.gum': 1.6651475429534912, 'eval_runtime': 26.0616, 'eval_samples_per_second': 82.458, 'eval_steps_per_second': 2.609, 'epoch': 10.0}
{'train@eng.rst.gum_loss': 1.435549020767212, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5670288551485932, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4268753148572365, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5139260002763851, 'train@eng.rst.gum_recall@eng.rst.gum': 0.42811797644574706, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4355489015579224, 'train@eng.rst.gum_runtime': 165.6666, 'train@eng.rst.gum_samples_per_second': 83.885, 'train@eng.rst.gum_steps_per_second': 2.626, 'epoch': 11.0}
{'loss': 1.5168, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.6591403484344482, 'eval_accuracy@eng.rst.gum': 0.5188459748720335, 'eval_f1@eng.rst.gum': 0.39685278805296437, 'eval_precision@eng.rst.gum': 0.4846078020154834, 'eval_recall@eng.rst.gum': 0.4059238326333383, 'eval_loss@eng.rst.gum': 1.6591401100158691, 'eval_runtime': 26.0657, 'eval_samples_per_second': 82.446, 'eval_steps_per_second': 2.609, 'epoch': 11.0}
{'train@eng.rst.gum_loss': 1.4322218894958496, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5694034683744693, 'train@eng.rst.gum_f1@eng.rst.gum': 0.42865512608510065, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5177741179921057, 'train@eng.rst.gum_recall@eng.rst.gum': 0.42840622110481447, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4322220087051392, 'train@eng.rst.gum_runtime': 165.1886, 'train@eng.rst.gum_samples_per_second': 84.128, 'train@eng.rst.gum_steps_per_second': 2.633, 'epoch': 12.0}
{'loss': 1.5054, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.6571540832519531, 'eval_accuracy@eng.rst.gum': 0.5221033038622616, 'eval_f1@eng.rst.gum': 0.398686539802773, 'eval_precision@eng.rst.gum': 0.486169112946125, 'eval_recall@eng.rst.gum': 0.4061019495625812, 'eval_loss@eng.rst.gum': 1.6571539640426636, 'eval_runtime': 26.0404, 'eval_samples_per_second': 82.526, 'eval_steps_per_second': 2.611, 'epoch': 12.0}
{'train_runtime': 6524.5678, 'train_samples_per_second': 25.559, 'train_steps_per_second': 0.8, 'train_loss': 1.7683403395144877, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.8443
  train_runtime            = 0:17:18.12
  train_samples_per_second =     25.014
  train_steps_per_second   =      0.786
-------------------------------------------------------------------
Lang1:  eng.pdtb.pdtb    Lang2:  eng.rst.gum
Saving run to:  runs/full_shot/FullShot=v4_eng.pdtb.pdtb_eng.rst.gum_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 43920 examples
read 1674 examples
read 2257 examples
read 13897 examples
read 2149 examples
read 2091 examples
Total prediction labels:  46
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=46, bias=True)
    )
  )
)
{'train@eng.pdtb.pdtb_loss': 1.3112739324569702, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.5872495446265938, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.24794807372489885, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.32168981713252576, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.24425419325194647, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.3112739324569702, 'train@eng.pdtb.pdtb_runtime': 522.0974, 'train@eng.pdtb.pdtb_samples_per_second': 84.122, 'train@eng.pdtb.pdtb_steps_per_second': 2.63, 'epoch': 1.0}
{'loss': 1.9039, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.2309428453445435, 'eval_accuracy@eng.pdtb.pdtb': 0.6135005973715651, 'eval_f1@eng.pdtb.pdtb': 0.29270176487677774, 'eval_precision@eng.pdtb.pdtb': 0.3330480294190183, 'eval_recall@eng.pdtb.pdtb': 0.2898724573584661, 'eval_loss@eng.pdtb.pdtb': 1.2309428453445435, 'eval_runtime': 20.5474, 'eval_samples_per_second': 81.47, 'eval_steps_per_second': 2.579, 'epoch': 1.0}
{'train@eng.pdtb.pdtb_loss': 1.1216715574264526, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.630464480874317, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.33305227286909256, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.4219486978616011, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.327176683832147, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.1216715574264526, 'train@eng.pdtb.pdtb_runtime': 522.3302, 'train@eng.pdtb.pdtb_samples_per_second': 84.085, 'train@eng.pdtb.pdtb_steps_per_second': 2.629, 'epoch': 2.0}
{'loss': 1.2562, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.0465431213378906, 'eval_accuracy@eng.pdtb.pdtb': 0.6618876941457587, 'eval_f1@eng.pdtb.pdtb': 0.3903617435129078, 'eval_precision@eng.pdtb.pdtb': 0.44127184639088934, 'eval_recall@eng.pdtb.pdtb': 0.38101323763217304, 'eval_loss@eng.pdtb.pdtb': 1.0465428829193115, 'eval_runtime': 20.4824, 'eval_samples_per_second': 81.729, 'eval_steps_per_second': 2.588, 'epoch': 2.0}
{'train@eng.pdtb.pdtb_loss': 1.064273476600647, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6488615664845173, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4226665712396562, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.4653359964452708, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4059570883865459, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.0642735958099365, 'train@eng.pdtb.pdtb_runtime': 522.1662, 'train@eng.pdtb.pdtb_samples_per_second': 84.111, 'train@eng.pdtb.pdtb_steps_per_second': 2.629, 'epoch': 3.0}
{'loss': 1.1419, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.000012993812561, 'eval_accuracy@eng.pdtb.pdtb': 0.6780167264038232, 'eval_f1@eng.pdtb.pdtb': 0.46816606053638754, 'eval_precision@eng.pdtb.pdtb': 0.5265744912124, 'eval_recall@eng.pdtb.pdtb': 0.44597251432847995, 'eval_loss@eng.pdtb.pdtb': 1.000012993812561, 'eval_runtime': 20.5057, 'eval_samples_per_second': 81.636, 'eval_steps_per_second': 2.585, 'epoch': 3.0}
{'train@eng.pdtb.pdtb_loss': 1.01381254196167, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6623178506375228, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.44127121388456286, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.4718418295514705, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.43221021444207747, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.0138124227523804, 'train@eng.pdtb.pdtb_runtime': 521.766, 'train@eng.pdtb.pdtb_samples_per_second': 84.176, 'train@eng.pdtb.pdtb_steps_per_second': 2.631, 'epoch': 4.0}
{'loss': 1.0878, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 0.9571578502655029, 'eval_accuracy@eng.pdtb.pdtb': 0.6816009557945042, 'eval_f1@eng.pdtb.pdtb': 0.494315666418417, 'eval_precision@eng.pdtb.pdtb': 0.5532781177458111, 'eval_recall@eng.pdtb.pdtb': 0.4753145725387341, 'eval_loss@eng.pdtb.pdtb': 0.9571579098701477, 'eval_runtime': 20.486, 'eval_samples_per_second': 81.714, 'eval_steps_per_second': 2.587, 'epoch': 4.0}
{'train@eng.pdtb.pdtb_loss': 0.9905592799186707, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6690573770491803, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4511491773976315, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5209746124828082, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.44460301820116455, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9905592799186707, 'train@eng.pdtb.pdtb_runtime': 522.0139, 'train@eng.pdtb.pdtb_samples_per_second': 84.136, 'train@eng.pdtb.pdtb_steps_per_second': 2.63, 'epoch': 5.0}
{'loss': 1.0572, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 0.9382768273353577, 'eval_accuracy@eng.pdtb.pdtb': 0.6893667861409797, 'eval_f1@eng.pdtb.pdtb': 0.5301182345681206, 'eval_precision@eng.pdtb.pdtb': 0.6180091011911469, 'eval_recall@eng.pdtb.pdtb': 0.5117466362215161, 'eval_loss@eng.pdtb.pdtb': 0.9382768273353577, 'eval_runtime': 23.0343, 'eval_samples_per_second': 72.674, 'eval_steps_per_second': 2.301, 'epoch': 5.0}
{'train@eng.pdtb.pdtb_loss': 0.9681995511054993, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6751593806921676, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.45812730224804415, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5200466850094095, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4533360408571466, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9681995511054993, 'train@eng.pdtb.pdtb_runtime': 522.4478, 'train@eng.pdtb.pdtb_samples_per_second': 84.066, 'train@eng.pdtb.pdtb_steps_per_second': 2.628, 'epoch': 6.0}
{'loss': 1.0329, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 0.9269272685050964, 'eval_accuracy@eng.pdtb.pdtb': 0.6845878136200717, 'eval_f1@eng.pdtb.pdtb': 0.5332400325863801, 'eval_precision@eng.pdtb.pdtb': 0.6086886443353171, 'eval_recall@eng.pdtb.pdtb': 0.5207776746826831, 'eval_loss@eng.pdtb.pdtb': 0.9269272685050964, 'eval_runtime': 20.4317, 'eval_samples_per_second': 81.931, 'eval_steps_per_second': 2.594, 'epoch': 6.0}
{'train@eng.pdtb.pdtb_loss': 0.9564723372459412, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6771630236794172, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.46236781035667757, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5303578013939854, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4532142336322845, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9564723372459412, 'train@eng.pdtb.pdtb_runtime': 515.7467, 'train@eng.pdtb.pdtb_samples_per_second': 85.158, 'train@eng.pdtb.pdtb_steps_per_second': 2.662, 'epoch': 7.0}
{'loss': 1.0173, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 0.914440393447876, 'eval_accuracy@eng.pdtb.pdtb': 0.6857825567502986, 'eval_f1@eng.pdtb.pdtb': 0.5396971786854362, 'eval_precision@eng.pdtb.pdtb': 0.6285016891268642, 'eval_recall@eng.pdtb.pdtb': 0.5203217130015956, 'eval_loss@eng.pdtb.pdtb': 0.914440393447876, 'eval_runtime': 20.08, 'eval_samples_per_second': 83.367, 'eval_steps_per_second': 2.639, 'epoch': 7.0}
{'train@eng.pdtb.pdtb_loss': 0.9446706175804138, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6813296903460838, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4682850593097032, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5243767777316265, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.46582559001792595, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9446706175804138, 'train@eng.pdtb.pdtb_runtime': 515.6886, 'train@eng.pdtb.pdtb_samples_per_second': 85.168, 'train@eng.pdtb.pdtb_steps_per_second': 2.662, 'epoch': 8.0}
{'loss': 1.0044, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 0.9098454713821411, 'eval_accuracy@eng.pdtb.pdtb': 0.6953405017921147, 'eval_f1@eng.pdtb.pdtb': 0.5530495260517021, 'eval_precision@eng.pdtb.pdtb': 0.6280835413122566, 'eval_recall@eng.pdtb.pdtb': 0.5400910757991702, 'eval_loss@eng.pdtb.pdtb': 0.9098455309867859, 'eval_runtime': 20.0878, 'eval_samples_per_second': 83.334, 'eval_steps_per_second': 2.638, 'epoch': 8.0}
{'train@eng.pdtb.pdtb_loss': 0.9357814788818359, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6845400728597449, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.47139456128090257, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5255391879812689, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.46667948129140574, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9357814788818359, 'train@eng.pdtb.pdtb_runtime': 515.4603, 'train@eng.pdtb.pdtb_samples_per_second': 85.205, 'train@eng.pdtb.pdtb_steps_per_second': 2.664, 'epoch': 9.0}
{'loss': 0.9931, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 0.907585620880127, 'eval_accuracy@eng.pdtb.pdtb': 0.6941457586618877, 'eval_f1@eng.pdtb.pdtb': 0.5507952670311413, 'eval_precision@eng.pdtb.pdtb': 0.6275723541376172, 'eval_recall@eng.pdtb.pdtb': 0.5348835628295183, 'eval_loss@eng.pdtb.pdtb': 0.9075855612754822, 'eval_runtime': 20.0315, 'eval_samples_per_second': 83.568, 'eval_steps_per_second': 2.646, 'epoch': 9.0}
{'train@eng.pdtb.pdtb_loss': 0.9318343997001648, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6843123861566485, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4709977181626847, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5269533778785941, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4663131825961297, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9318343997001648, 'train@eng.pdtb.pdtb_runtime': 515.6694, 'train@eng.pdtb.pdtb_samples_per_second': 85.171, 'train@eng.pdtb.pdtb_steps_per_second': 2.663, 'epoch': 10.0}
{'loss': 0.9876, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 0.902798056602478, 'eval_accuracy@eng.pdtb.pdtb': 0.6923536439665472, 'eval_f1@eng.pdtb.pdtb': 0.5577028912602453, 'eval_precision@eng.pdtb.pdtb': 0.6377868708656724, 'eval_recall@eng.pdtb.pdtb': 0.5442896308003246, 'eval_loss@eng.pdtb.pdtb': 0.9027979969978333, 'eval_runtime': 20.0874, 'eval_samples_per_second': 83.336, 'eval_steps_per_second': 2.638, 'epoch': 10.0}
{'train@eng.pdtb.pdtb_loss': 0.9274120330810547, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6875227686703097, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4936904855317638, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5722133518366772, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4821861537148151, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9274121522903442, 'train@eng.pdtb.pdtb_runtime': 515.8818, 'train@eng.pdtb.pdtb_samples_per_second': 85.136, 'train@eng.pdtb.pdtb_steps_per_second': 2.661, 'epoch': 11.0}
{'loss': 0.9812, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.9022247195243835, 'eval_accuracy@eng.pdtb.pdtb': 0.6983273596176822, 'eval_f1@eng.pdtb.pdtb': 0.561935496440464, 'eval_precision@eng.pdtb.pdtb': 0.6322287731891196, 'eval_recall@eng.pdtb.pdtb': 0.5494652722203945, 'eval_loss@eng.pdtb.pdtb': 0.9022247195243835, 'eval_runtime': 20.0719, 'eval_samples_per_second': 83.4, 'eval_steps_per_second': 2.641, 'epoch': 11.0}
{'train@eng.pdtb.pdtb_loss': 0.9261711835861206, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6875227686703097, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.49371296829612377, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5737330852748557, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4807421096367737, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9261711835861206, 'train@eng.pdtb.pdtb_runtime': 515.6598, 'train@eng.pdtb.pdtb_samples_per_second': 85.172, 'train@eng.pdtb.pdtb_steps_per_second': 2.663, 'epoch': 12.0}
{'loss': 0.9761, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.9004144072532654, 'eval_accuracy@eng.pdtb.pdtb': 0.6965352449223416, 'eval_f1@eng.pdtb.pdtb': 0.556939270174478, 'eval_precision@eng.pdtb.pdtb': 0.6293288453178354, 'eval_recall@eng.pdtb.pdtb': 0.5432924833026677, 'eval_loss@eng.pdtb.pdtb': 0.9004144072532654, 'eval_runtime': 20.2111, 'eval_samples_per_second': 82.826, 'eval_steps_per_second': 2.622, 'epoch': 12.0}
{'train_runtime': 19656.0489, 'train_samples_per_second': 26.813, 'train_steps_per_second': 0.838, 'train_loss': 1.119963517898667, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =       1.12
  train_runtime            = 5:27:36.04
  train_samples_per_second =     26.813
  train_steps_per_second   =      0.838
{'train@eng.rst.gum_loss': 2.032759666442871, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.4205943728862344, 'train@eng.rst.gum_f1@eng.rst.gum': 0.20889480263426474, 'train@eng.rst.gum_precision@eng.rst.gum': 0.3089414077397627, 'train@eng.rst.gum_recall@eng.rst.gum': 0.22700636819392458, 'train@eng.rst.gum_loss@eng.rst.gum': 2.032759666442871, 'train@eng.rst.gum_runtime': 163.3812, 'train@eng.rst.gum_samples_per_second': 85.059, 'train@eng.rst.gum_steps_per_second': 2.662, 'epoch': 1.0}
{'loss': 2.6882, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.136638879776001, 'eval_accuracy@eng.rst.gum': 0.3922754769660307, 'eval_f1@eng.rst.gum': 0.18169121644572278, 'eval_precision@eng.rst.gum': 0.1924657342388667, 'eval_recall@eng.rst.gum': 0.20846364262210984, 'eval_loss@eng.rst.gum': 2.136638879776001, 'eval_runtime': 25.6888, 'eval_samples_per_second': 83.655, 'eval_steps_per_second': 2.647, 'epoch': 1.0}
{'train@eng.rst.gum_loss': 1.7372307777404785, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.4924084334748507, 'train@eng.rst.gum_f1@eng.rst.gum': 0.32747093927369725, 'train@eng.rst.gum_precision@eng.rst.gum': 0.43011998218910164, 'train@eng.rst.gum_recall@eng.rst.gum': 0.33066111339568033, 'train@eng.rst.gum_loss@eng.rst.gum': 1.7372307777404785, 'train@eng.rst.gum_runtime': 163.3469, 'train@eng.rst.gum_samples_per_second': 85.077, 'train@eng.rst.gum_steps_per_second': 2.663, 'epoch': 2.0}
{'loss': 1.9308, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.8855977058410645, 'eval_accuracy@eng.rst.gum': 0.4634713820381573, 'eval_f1@eng.rst.gum': 0.3113310681832402, 'eval_precision@eng.rst.gum': 0.4012727022053005, 'eval_recall@eng.rst.gum': 0.31798752394644764, 'eval_loss@eng.rst.gum': 1.8855977058410645, 'eval_runtime': 25.644, 'eval_samples_per_second': 83.801, 'eval_steps_per_second': 2.652, 'epoch': 2.0}
{'train@eng.rst.gum_loss': 1.604466438293457, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5234223213643232, 'train@eng.rst.gum_f1@eng.rst.gum': 0.38534344046670344, 'train@eng.rst.gum_precision@eng.rst.gum': 0.4498867126666731, 'train@eng.rst.gum_recall@eng.rst.gum': 0.39662210081312316, 'train@eng.rst.gum_loss@eng.rst.gum': 1.604466438293457, 'train@eng.rst.gum_runtime': 163.0529, 'train@eng.rst.gum_samples_per_second': 85.23, 'train@eng.rst.gum_steps_per_second': 2.668, 'epoch': 3.0}
{'loss': 1.7437, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.7676182985305786, 'eval_accuracy@eng.rst.gum': 0.5002326663564448, 'eval_f1@eng.rst.gum': 0.3616516819191074, 'eval_precision@eng.rst.gum': 0.38782081386939515, 'eval_recall@eng.rst.gum': 0.38226792051974795, 'eval_loss@eng.rst.gum': 1.7676182985305786, 'eval_runtime': 25.5791, 'eval_samples_per_second': 84.014, 'eval_steps_per_second': 2.658, 'epoch': 3.0}
{'train@eng.rst.gum_loss': 1.5387414693832397, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5382456645319134, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4036488780226456, 'train@eng.rst.gum_precision@eng.rst.gum': 0.4894085375974064, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4034052170474661, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5387413501739502, 'train@eng.rst.gum_runtime': 163.3381, 'train@eng.rst.gum_samples_per_second': 85.081, 'train@eng.rst.gum_steps_per_second': 2.663, 'epoch': 4.0}
{'loss': 1.644, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.7331316471099854, 'eval_accuracy@eng.rst.gum': 0.498371335504886, 'eval_f1@eng.rst.gum': 0.3615721204141404, 'eval_precision@eng.rst.gum': 0.4140076473861459, 'eval_recall@eng.rst.gum': 0.3719646665478824, 'eval_loss@eng.rst.gum': 1.7331316471099854, 'eval_runtime': 25.6281, 'eval_samples_per_second': 83.853, 'eval_steps_per_second': 2.653, 'epoch': 4.0}
{'train@eng.rst.gum_loss': 1.4823886156082153, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5535727135352954, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4358452899255497, 'train@eng.rst.gum_precision@eng.rst.gum': 0.4911136178995065, 'train@eng.rst.gum_recall@eng.rst.gum': 0.43745224768503127, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4823886156082153, 'train@eng.rst.gum_runtime': 163.3663, 'train@eng.rst.gum_samples_per_second': 85.067, 'train@eng.rst.gum_steps_per_second': 2.663, 'epoch': 5.0}
{'loss': 1.5874, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.6824942827224731, 'eval_accuracy@eng.rst.gum': 0.5090739879013495, 'eval_f1@eng.rst.gum': 0.3798053481814225, 'eval_precision@eng.rst.gum': 0.4171906304808671, 'eval_recall@eng.rst.gum': 0.3941629900077458, 'eval_loss@eng.rst.gum': 1.6824944019317627, 'eval_runtime': 25.667, 'eval_samples_per_second': 83.726, 'eval_steps_per_second': 2.649, 'epoch': 5.0}
{'train@eng.rst.gum_loss': 1.4485337734222412, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5623515866733827, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4487607186577426, 'train@eng.rst.gum_precision@eng.rst.gum': 0.507297090755879, 'train@eng.rst.gum_recall@eng.rst.gum': 0.44774485290332783, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4485337734222412, 'train@eng.rst.gum_runtime': 163.0538, 'train@eng.rst.gum_samples_per_second': 85.23, 'train@eng.rst.gum_steps_per_second': 2.668, 'epoch': 6.0}
{'loss': 1.5481, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.665511131286621, 'eval_accuracy@eng.rst.gum': 0.5127966496044672, 'eval_f1@eng.rst.gum': 0.38479362872724787, 'eval_precision@eng.rst.gum': 0.4368407658101192, 'eval_recall@eng.rst.gum': 0.3971697336996299, 'eval_loss@eng.rst.gum': 1.665511131286621, 'eval_runtime': 25.611, 'eval_samples_per_second': 83.909, 'eval_steps_per_second': 2.655, 'epoch': 6.0}
{'train@eng.rst.gum_loss': 1.424841284751892, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5672447290782183, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4526098091690961, 'train@eng.rst.gum_precision@eng.rst.gum': 0.507368338135226, 'train@eng.rst.gum_recall@eng.rst.gum': 0.45124322157964003, 'train@eng.rst.gum_loss@eng.rst.gum': 1.424841284751892, 'train@eng.rst.gum_runtime': 163.3751, 'train@eng.rst.gum_samples_per_second': 85.062, 'train@eng.rst.gum_steps_per_second': 2.663, 'epoch': 7.0}
{'loss': 1.5107, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.6529767513275146, 'eval_accuracy@eng.rst.gum': 0.5155886458818055, 'eval_f1@eng.rst.gum': 0.3894437559024179, 'eval_precision@eng.rst.gum': 0.4544457543381346, 'eval_recall@eng.rst.gum': 0.3979848234827919, 'eval_loss@eng.rst.gum': 1.6529767513275146, 'eval_runtime': 25.6254, 'eval_samples_per_second': 83.862, 'eval_steps_per_second': 2.654, 'epoch': 7.0}
{'train@eng.rst.gum_loss': 1.4087333679199219, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5731452831546376, 'train@eng.rst.gum_f1@eng.rst.gum': 0.46003507411427763, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5094543790863784, 'train@eng.rst.gum_recall@eng.rst.gum': 0.45671543153861416, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4087333679199219, 'train@eng.rst.gum_runtime': 163.397, 'train@eng.rst.gum_samples_per_second': 85.051, 'train@eng.rst.gum_steps_per_second': 2.662, 'epoch': 8.0}
{'loss': 1.4894, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.6460222005844116, 'eval_accuracy@eng.rst.gum': 0.5188459748720335, 'eval_f1@eng.rst.gum': 0.4009187805997338, 'eval_precision@eng.rst.gum': 0.468889951975777, 'eval_recall@eng.rst.gum': 0.40544503222808514, 'eval_loss@eng.rst.gum': 1.646022081375122, 'eval_runtime': 25.6697, 'eval_samples_per_second': 83.717, 'eval_steps_per_second': 2.649, 'epoch': 8.0}
{'train@eng.rst.gum_loss': 1.3968063592910767, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5758077282866806, 'train@eng.rst.gum_f1@eng.rst.gum': 0.46518714611289214, 'train@eng.rst.gum_precision@eng.rst.gum': 0.514987574765006, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4667106949548305, 'train@eng.rst.gum_loss@eng.rst.gum': 1.396806240081787, 'train@eng.rst.gum_runtime': 163.1349, 'train@eng.rst.gum_samples_per_second': 85.187, 'train@eng.rst.gum_steps_per_second': 2.667, 'epoch': 9.0}
{'loss': 1.471, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.6339534521102905, 'eval_accuracy@eng.rst.gum': 0.5207073057235924, 'eval_f1@eng.rst.gum': 0.4084930668998723, 'eval_precision@eng.rst.gum': 0.46989187917990277, 'eval_recall@eng.rst.gum': 0.4137882709063092, 'eval_loss@eng.rst.gum': 1.6339534521102905, 'eval_runtime': 25.5607, 'eval_samples_per_second': 84.074, 'eval_steps_per_second': 2.66, 'epoch': 9.0}
{'train@eng.rst.gum_loss': 1.383685827255249, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.580484996761891, 'train@eng.rst.gum_f1@eng.rst.gum': 0.46806496497926586, 'train@eng.rst.gum_precision@eng.rst.gum': 0.520539128186303, 'train@eng.rst.gum_recall@eng.rst.gum': 0.46375810551921637, 'train@eng.rst.gum_loss@eng.rst.gum': 1.383685827255249, 'train@eng.rst.gum_runtime': 163.1883, 'train@eng.rst.gum_samples_per_second': 85.159, 'train@eng.rst.gum_steps_per_second': 2.666, 'epoch': 10.0}
{'loss': 1.4654, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.6258877515792847, 'eval_accuracy@eng.rst.gum': 0.5216379711493718, 'eval_f1@eng.rst.gum': 0.407047703055231, 'eval_precision@eng.rst.gum': 0.46664793612031413, 'eval_recall@eng.rst.gum': 0.4076752312256097, 'eval_loss@eng.rst.gum': 1.6258877515792847, 'eval_runtime': 25.6274, 'eval_samples_per_second': 83.856, 'eval_steps_per_second': 2.653, 'epoch': 10.0}
{'train@eng.rst.gum_loss': 1.3785203695297241, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5807008706915161, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4725562781343168, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5227365920206019, 'train@eng.rst.gum_recall@eng.rst.gum': 0.470536999036542, 'train@eng.rst.gum_loss@eng.rst.gum': 1.3785203695297241, 'train@eng.rst.gum_runtime': 163.4987, 'train@eng.rst.gum_samples_per_second': 84.998, 'train@eng.rst.gum_steps_per_second': 2.661, 'epoch': 11.0}
{'loss': 1.4544, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.6207643747329712, 'eval_accuracy@eng.rst.gum': 0.5253606328524896, 'eval_f1@eng.rst.gum': 0.4156971162940017, 'eval_precision@eng.rst.gum': 0.47632747880150067, 'eval_recall@eng.rst.gum': 0.4165528018207995, 'eval_loss@eng.rst.gum': 1.6207642555236816, 'eval_runtime': 25.6754, 'eval_samples_per_second': 83.699, 'eval_steps_per_second': 2.648, 'epoch': 11.0}
{'train@eng.rst.gum_loss': 1.3753819465637207, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5822119881988919, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4724470987105735, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5246414286558858, 'train@eng.rst.gum_recall@eng.rst.gum': 0.46906557894681944, 'train@eng.rst.gum_loss@eng.rst.gum': 1.3753823041915894, 'train@eng.rst.gum_runtime': 163.2008, 'train@eng.rst.gum_samples_per_second': 85.153, 'train@eng.rst.gum_steps_per_second': 2.665, 'epoch': 12.0}
{'loss': 1.4466, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.6194729804992676, 'eval_accuracy@eng.rst.gum': 0.5248953001395998, 'eval_f1@eng.rst.gum': 0.4124265078916384, 'eval_precision@eng.rst.gum': 0.4731999407082708, 'eval_recall@eng.rst.gum': 0.4128527827839231, 'eval_loss@eng.rst.gum': 1.6194730997085571, 'eval_runtime': 25.6498, 'eval_samples_per_second': 83.782, 'eval_steps_per_second': 2.651, 'epoch': 12.0}
{'train_runtime': 6407.5064, 'train_samples_per_second': 26.026, 'train_steps_per_second': 0.815, 'train_loss': 1.6649808569429478, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =       1.12
  train_runtime            = 5:27:36.04
  train_samples_per_second =     26.813
  train_steps_per_second   =      0.838
-------------------------------------------------------------------
Lang1:  eng.rst.rstdt    Lang2:  eng.rst.gum
Saving run to:  runs/full_shot/FullShot=v4_eng.rst.rstdt_eng.rst.gum_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 16002 examples
read 1621 examples
read 2155 examples
read 13897 examples
read 2149 examples
read 2091 examples
Total prediction labels:  32
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=32, bias=True)
    )
  )
)
{'train@eng.rst.rstdt_loss': 1.764591932296753, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.5069991251093613, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.08243700316032554, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.0701875434595459, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.10927607802221889, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.7645920515060425, 'train@eng.rst.rstdt_runtime': 188.1902, 'train@eng.rst.rstdt_samples_per_second': 85.031, 'train@eng.rst.rstdt_steps_per_second': 2.662, 'epoch': 1.0}
{'loss': 2.1744, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.7417118549346924, 'eval_accuracy@eng.rst.rstdt': 0.5194324491054905, 'eval_f1@eng.rst.rstdt': 0.08257749572858189, 'eval_precision@eng.rst.rstdt': 0.06776085532098508, 'eval_recall@eng.rst.rstdt': 0.1080816913184252, 'eval_loss@eng.rst.rstdt': 1.7417117357254028, 'eval_runtime': 19.3519, 'eval_samples_per_second': 83.764, 'eval_steps_per_second': 2.635, 'epoch': 1.0}
{'train@eng.rst.rstdt_loss': 1.4462181329727173, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.5924259467566554, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.17977807447941796, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.2755730649875869, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.1941287765704476, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.4462180137634277, 'train@eng.rst.rstdt_runtime': 188.1966, 'train@eng.rst.rstdt_samples_per_second': 85.028, 'train@eng.rst.rstdt_steps_per_second': 2.662, 'epoch': 2.0}
{'loss': 1.635, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.4474647045135498, 'eval_accuracy@eng.rst.rstdt': 0.5983960518198643, 'eval_f1@eng.rst.rstdt': 0.18289298292863898, 'eval_precision@eng.rst.rstdt': 0.25119207662270737, 'eval_recall@eng.rst.rstdt': 0.19260313299937887, 'eval_loss@eng.rst.rstdt': 1.447464942932129, 'eval_runtime': 19.354, 'eval_samples_per_second': 83.755, 'eval_steps_per_second': 2.635, 'epoch': 2.0}
{'train@eng.rst.rstdt_loss': 1.3336433172225952, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6222972128483939, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.2535369462724392, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.3730010093865449, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.25406967567153615, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.3336433172225952, 'train@eng.rst.rstdt_runtime': 187.9509, 'train@eng.rst.rstdt_samples_per_second': 85.139, 'train@eng.rst.rstdt_steps_per_second': 2.666, 'epoch': 3.0}
{'loss': 1.4432, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.3650072813034058, 'eval_accuracy@eng.rst.rstdt': 0.621221468229488, 'eval_f1@eng.rst.rstdt': 0.2348811174009399, 'eval_precision@eng.rst.rstdt': 0.2885659448898154, 'eval_recall@eng.rst.rstdt': 0.24322620099477124, 'eval_loss@eng.rst.rstdt': 1.3650071620941162, 'eval_runtime': 19.3369, 'eval_samples_per_second': 83.829, 'eval_steps_per_second': 2.637, 'epoch': 3.0}
{'train@eng.rst.rstdt_loss': 1.2633533477783203, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6411073615798025, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3231515042340865, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.4592179251126904, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3050375313634539, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.2633534669876099, 'train@eng.rst.rstdt_runtime': 188.1934, 'train@eng.rst.rstdt_samples_per_second': 85.03, 'train@eng.rst.rstdt_steps_per_second': 2.662, 'epoch': 4.0}
{'loss': 1.3533, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.319549560546875, 'eval_accuracy@eng.rst.rstdt': 0.6397285626156693, 'eval_f1@eng.rst.rstdt': 0.31782384575351347, 'eval_precision@eng.rst.rstdt': 0.3985205009622324, 'eval_recall@eng.rst.rstdt': 0.3100927045169481, 'eval_loss@eng.rst.rstdt': 1.319549560546875, 'eval_runtime': 19.3972, 'eval_samples_per_second': 83.569, 'eval_steps_per_second': 2.629, 'epoch': 4.0}
{'train@eng.rst.rstdt_loss': 1.2134953737258911, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6551056117985252, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.34824903101127724, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.508293337362843, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3241511406546726, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.2134953737258911, 'train@eng.rst.rstdt_runtime': 187.9554, 'train@eng.rst.rstdt_samples_per_second': 85.137, 'train@eng.rst.rstdt_steps_per_second': 2.666, 'epoch': 5.0}
{'loss': 1.2918, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.2790583372116089, 'eval_accuracy@eng.rst.rstdt': 0.6434299814929056, 'eval_f1@eng.rst.rstdt': 0.31775023897817056, 'eval_precision@eng.rst.rstdt': 0.3828100154894324, 'eval_recall@eng.rst.rstdt': 0.3127832173666936, 'eval_loss@eng.rst.rstdt': 1.2790584564208984, 'eval_runtime': 19.3345, 'eval_samples_per_second': 83.84, 'eval_steps_per_second': 2.638, 'epoch': 5.0}
{'train@eng.rst.rstdt_loss': 1.1771172285079956, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6618547681539807, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.37390697304800824, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5155559221233438, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3431514932232816, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1771172285079956, 'train@eng.rst.rstdt_runtime': 188.2508, 'train@eng.rst.rstdt_samples_per_second': 85.004, 'train@eng.rst.rstdt_steps_per_second': 2.661, 'epoch': 6.0}
{'loss': 1.2473, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.2496048212051392, 'eval_accuracy@eng.rst.rstdt': 0.6428130783466995, 'eval_f1@eng.rst.rstdt': 0.33092108169180307, 'eval_precision@eng.rst.rstdt': 0.46769800284397034, 'eval_recall@eng.rst.rstdt': 0.3219825099854552, 'eval_loss@eng.rst.rstdt': 1.2496048212051392, 'eval_runtime': 19.3733, 'eval_samples_per_second': 83.672, 'eval_steps_per_second': 2.632, 'epoch': 6.0}
{'train@eng.rst.rstdt_loss': 1.156984567642212, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6662917135358081, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.38357013980405974, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5797426669583429, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3490004629452413, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1569846868515015, 'train@eng.rst.rstdt_runtime': 188.2205, 'train@eng.rst.rstdt_samples_per_second': 85.017, 'train@eng.rst.rstdt_steps_per_second': 2.662, 'epoch': 7.0}
{'loss': 1.2266, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.232787013053894, 'eval_accuracy@eng.rst.rstdt': 0.6477483035163479, 'eval_f1@eng.rst.rstdt': 0.33545723044843107, 'eval_precision@eng.rst.rstdt': 0.4973906024644722, 'eval_recall@eng.rst.rstdt': 0.32345208621836674, 'eval_loss@eng.rst.rstdt': 1.232787013053894, 'eval_runtime': 19.3702, 'eval_samples_per_second': 83.685, 'eval_steps_per_second': 2.633, 'epoch': 7.0}
{'train@eng.rst.rstdt_loss': 1.1362227201461792, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6679165104361955, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3963140038015922, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5786241826368678, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3636354096848257, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1362227201461792, 'train@eng.rst.rstdt_runtime': 187.8893, 'train@eng.rst.rstdt_samples_per_second': 85.167, 'train@eng.rst.rstdt_steps_per_second': 2.666, 'epoch': 8.0}
{'loss': 1.2, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.224890112876892, 'eval_accuracy@eng.rst.rstdt': 0.64898210980876, 'eval_f1@eng.rst.rstdt': 0.3518183110728555, 'eval_precision@eng.rst.rstdt': 0.5205165991601718, 'eval_recall@eng.rst.rstdt': 0.33997633820716483, 'eval_loss@eng.rst.rstdt': 1.2248902320861816, 'eval_runtime': 19.3161, 'eval_samples_per_second': 83.92, 'eval_steps_per_second': 2.64, 'epoch': 8.0}
{'train@eng.rst.rstdt_loss': 1.1241461038589478, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6711036120484939, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4037952629849032, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6410036489380065, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.36830122465167275, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1241461038589478, 'train@eng.rst.rstdt_runtime': 188.2972, 'train@eng.rst.rstdt_samples_per_second': 84.983, 'train@eng.rst.rstdt_steps_per_second': 2.661, 'epoch': 9.0}
{'loss': 1.179, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.213377833366394, 'eval_accuracy@eng.rst.rstdt': 0.6477483035163479, 'eval_f1@eng.rst.rstdt': 0.3503437425015453, 'eval_precision@eng.rst.rstdt': 0.5142754881959355, 'eval_recall@eng.rst.rstdt': 0.3399992781631715, 'eval_loss@eng.rst.rstdt': 1.213377594947815, 'eval_runtime': 19.3583, 'eval_samples_per_second': 83.737, 'eval_steps_per_second': 2.635, 'epoch': 9.0}
{'train@eng.rst.rstdt_loss': 1.1170978546142578, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6702912135983002, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.41370269403819687, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5978635547480159, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.378443289619437, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1170978546142578, 'train@eng.rst.rstdt_runtime': 188.2715, 'train@eng.rst.rstdt_samples_per_second': 84.994, 'train@eng.rst.rstdt_steps_per_second': 2.661, 'epoch': 10.0}
{'loss': 1.1688, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.2165751457214355, 'eval_accuracy@eng.rst.rstdt': 0.6440468846391116, 'eval_f1@eng.rst.rstdt': 0.35564276965548336, 'eval_precision@eng.rst.rstdt': 0.4840385876411329, 'eval_recall@eng.rst.rstdt': 0.3435445090741987, 'eval_loss@eng.rst.rstdt': 1.2165753841400146, 'eval_runtime': 19.3878, 'eval_samples_per_second': 83.609, 'eval_steps_per_second': 2.631, 'epoch': 10.0}
{'train@eng.rst.rstdt_loss': 1.1103945970535278, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6728533933258343, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4136944434558642, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6060281469447186, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.37624398311242674, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1103945970535278, 'train@eng.rst.rstdt_runtime': 187.8625, 'train@eng.rst.rstdt_samples_per_second': 85.179, 'train@eng.rst.rstdt_steps_per_second': 2.667, 'epoch': 11.0}
{'loss': 1.1641, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.208656668663025, 'eval_accuracy@eng.rst.rstdt': 0.6477483035163479, 'eval_f1@eng.rst.rstdt': 0.3595678484200479, 'eval_precision@eng.rst.rstdt': 0.5171856088094356, 'eval_recall@eng.rst.rstdt': 0.34642979973921895, 'eval_loss@eng.rst.rstdt': 1.208656668663025, 'eval_runtime': 19.3247, 'eval_samples_per_second': 83.882, 'eval_steps_per_second': 2.639, 'epoch': 11.0}
{'train@eng.rst.rstdt_loss': 1.1087634563446045, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6730408698912635, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4135605656683379, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6094480400787459, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3760229376589335, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.108763337135315, 'train@eng.rst.rstdt_runtime': 188.2751, 'train@eng.rst.rstdt_samples_per_second': 84.993, 'train@eng.rst.rstdt_steps_per_second': 2.661, 'epoch': 12.0}
{'loss': 1.16, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.2066009044647217, 'eval_accuracy@eng.rst.rstdt': 0.6502159161011721, 'eval_f1@eng.rst.rstdt': 0.3608320849501925, 'eval_precision@eng.rst.rstdt': 0.5188387015342133, 'eval_recall@eng.rst.rstdt': 0.3473333869805671, 'eval_loss@eng.rst.rstdt': 1.2066010236740112, 'eval_runtime': 19.4185, 'eval_samples_per_second': 83.477, 'eval_steps_per_second': 2.626, 'epoch': 12.0}
{'train_runtime': 7254.0747, 'train_samples_per_second': 26.471, 'train_steps_per_second': 0.829, 'train_loss': 1.353632060829513, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.3536
  train_runtime            = 2:00:54.07
  train_samples_per_second =     26.471
  train_steps_per_second   =      0.829
{'train@eng.rst.gum_loss': 2.0142579078674316, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.4216737425343599, 'train@eng.rst.gum_f1@eng.rst.gum': 0.19615866384973427, 'train@eng.rst.gum_precision@eng.rst.gum': 0.30202883784101736, 'train@eng.rst.gum_recall@eng.rst.gum': 0.2017048995813946, 'train@eng.rst.gum_loss@eng.rst.gum': 2.0142579078674316, 'train@eng.rst.gum_runtime': 163.2921, 'train@eng.rst.gum_samples_per_second': 85.105, 'train@eng.rst.gum_steps_per_second': 2.664, 'epoch': 1.0}
{'loss': 2.6271, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.1420230865478516, 'eval_accuracy@eng.rst.gum': 0.40111679851093535, 'eval_f1@eng.rst.gum': 0.18922367730594794, 'eval_precision@eng.rst.gum': 0.28230038828530885, 'eval_recall@eng.rst.gum': 0.20029279122293817, 'eval_loss@eng.rst.gum': 2.1420230865478516, 'eval_runtime': 25.5074, 'eval_samples_per_second': 84.25, 'eval_steps_per_second': 2.666, 'epoch': 1.0}
{'train@eng.rst.gum_loss': 1.746300220489502, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.49694178599697775, 'train@eng.rst.gum_f1@eng.rst.gum': 0.2881427943841444, 'train@eng.rst.gum_precision@eng.rst.gum': 0.3332094389666052, 'train@eng.rst.gum_recall@eng.rst.gum': 0.2955774927829615, 'train@eng.rst.gum_loss@eng.rst.gum': 1.746300220489502, 'train@eng.rst.gum_runtime': 163.1317, 'train@eng.rst.gum_samples_per_second': 85.189, 'train@eng.rst.gum_steps_per_second': 2.667, 'epoch': 2.0}
{'loss': 1.9318, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.8998717069625854, 'eval_accuracy@eng.rst.gum': 0.45974872033503955, 'eval_f1@eng.rst.gum': 0.26227129505760244, 'eval_precision@eng.rst.gum': 0.3106983068667665, 'eval_recall@eng.rst.gum': 0.276899910535427, 'eval_loss@eng.rst.gum': 1.8998719453811646, 'eval_runtime': 25.5169, 'eval_samples_per_second': 84.219, 'eval_steps_per_second': 2.665, 'epoch': 2.0}
{'train@eng.rst.gum_loss': 1.6136058568954468, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5288911275814924, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3531973578408207, 'train@eng.rst.gum_precision@eng.rst.gum': 0.43235398126613944, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3567957315102637, 'train@eng.rst.gum_loss@eng.rst.gum': 1.6136060953140259, 'train@eng.rst.gum_runtime': 163.4567, 'train@eng.rst.gum_samples_per_second': 85.019, 'train@eng.rst.gum_steps_per_second': 2.661, 'epoch': 3.0}
{'loss': 1.753, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.7830604314804077, 'eval_accuracy@eng.rst.gum': 0.48534201954397393, 'eval_f1@eng.rst.gum': 0.33038020762047476, 'eval_precision@eng.rst.gum': 0.3877015564604483, 'eval_recall@eng.rst.gum': 0.34089942444470117, 'eval_loss@eng.rst.gum': 1.7830605506896973, 'eval_runtime': 25.5687, 'eval_samples_per_second': 84.048, 'eval_steps_per_second': 2.66, 'epoch': 3.0}
{'train@eng.rst.gum_loss': 1.5450537204742432, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5427070590774987, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3832309914499238, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5115850499697386, 'train@eng.rst.gum_recall@eng.rst.gum': 0.37858662119087044, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5450537204742432, 'train@eng.rst.gum_runtime': 163.3261, 'train@eng.rst.gum_samples_per_second': 85.087, 'train@eng.rst.gum_steps_per_second': 2.663, 'epoch': 4.0}
{'loss': 1.6584, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.733978033065796, 'eval_accuracy@eng.rst.gum': 0.5002326663564448, 'eval_f1@eng.rst.gum': 0.35970062643482204, 'eval_precision@eng.rst.gum': 0.46203298338478305, 'eval_recall@eng.rst.gum': 0.36678105435302005, 'eval_loss@eng.rst.gum': 1.733978271484375, 'eval_runtime': 25.5911, 'eval_samples_per_second': 83.974, 'eval_steps_per_second': 2.657, 'epoch': 4.0}
{'train@eng.rst.gum_loss': 1.4881463050842285, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5588256458228394, 'train@eng.rst.gum_f1@eng.rst.gum': 0.422595276845202, 'train@eng.rst.gum_precision@eng.rst.gum': 0.4969534123816458, 'train@eng.rst.gum_recall@eng.rst.gum': 0.41897572743777606, 'train@eng.rst.gum_loss@eng.rst.gum': 1.488146424293518, 'train@eng.rst.gum_runtime': 163.1606, 'train@eng.rst.gum_samples_per_second': 85.174, 'train@eng.rst.gum_steps_per_second': 2.666, 'epoch': 5.0}
{'loss': 1.5921, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.687421202659607, 'eval_accuracy@eng.rst.gum': 0.5062819916240112, 'eval_f1@eng.rst.gum': 0.3862628930750769, 'eval_precision@eng.rst.gum': 0.4737986200406934, 'eval_recall@eng.rst.gum': 0.3957670164513765, 'eval_loss@eng.rst.gum': 1.6874210834503174, 'eval_runtime': 25.5735, 'eval_samples_per_second': 84.032, 'eval_steps_per_second': 2.659, 'epoch': 5.0}
{'train@eng.rst.gum_loss': 1.4524085521697998, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5695473843275527, 'train@eng.rst.gum_f1@eng.rst.gum': 0.43641229051722535, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5090480478196557, 'train@eng.rst.gum_recall@eng.rst.gum': 0.43202135912789785, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4524085521697998, 'train@eng.rst.gum_runtime': 163.4998, 'train@eng.rst.gum_samples_per_second': 84.997, 'train@eng.rst.gum_steps_per_second': 2.661, 'epoch': 6.0}
{'loss': 1.5501, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.666133999824524, 'eval_accuracy@eng.rst.gum': 0.5160539785946953, 'eval_f1@eng.rst.gum': 0.40119709526894076, 'eval_precision@eng.rst.gum': 0.4837881914495631, 'eval_recall@eng.rst.gum': 0.409406164250457, 'eval_loss@eng.rst.gum': 1.6661341190338135, 'eval_runtime': 25.5847, 'eval_samples_per_second': 83.995, 'eval_steps_per_second': 2.658, 'epoch': 6.0}
{'train@eng.rst.gum_loss': 1.4237953424453735, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5744405267323883, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4440348971866958, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5227548018654973, 'train@eng.rst.gum_recall@eng.rst.gum': 0.43875038026407054, 'train@eng.rst.gum_loss@eng.rst.gum': 1.423795223236084, 'train@eng.rst.gum_runtime': 163.3831, 'train@eng.rst.gum_samples_per_second': 85.058, 'train@eng.rst.gum_steps_per_second': 2.662, 'epoch': 7.0}
{'loss': 1.517, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.6491600275039673, 'eval_accuracy@eng.rst.gum': 0.5179153094462541, 'eval_f1@eng.rst.gum': 0.4063257712046653, 'eval_precision@eng.rst.gum': 0.49178807995047574, 'eval_recall@eng.rst.gum': 0.4119512187266019, 'eval_loss@eng.rst.gum': 1.6491600275039673, 'eval_runtime': 25.6361, 'eval_samples_per_second': 83.827, 'eval_steps_per_second': 2.653, 'epoch': 7.0}
{'train@eng.rst.gum_loss': 1.4071840047836304, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5796934590199324, 'train@eng.rst.gum_f1@eng.rst.gum': 0.453633146512772, 'train@eng.rst.gum_precision@eng.rst.gum': 0.524193461403785, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4475851019414962, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4071842432022095, 'train@eng.rst.gum_runtime': 163.1521, 'train@eng.rst.gum_samples_per_second': 85.178, 'train@eng.rst.gum_steps_per_second': 2.666, 'epoch': 8.0}
{'loss': 1.4942, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.6419076919555664, 'eval_accuracy@eng.rst.gum': 0.5225686365751512, 'eval_f1@eng.rst.gum': 0.4085617153955228, 'eval_precision@eng.rst.gum': 0.4890133872484178, 'eval_recall@eng.rst.gum': 0.41416127248741835, 'eval_loss@eng.rst.gum': 1.641907811164856, 'eval_runtime': 25.5454, 'eval_samples_per_second': 84.125, 'eval_steps_per_second': 2.662, 'epoch': 8.0}
{'train@eng.rst.gum_loss': 1.3925672769546509, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5836511477297258, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4623425066526347, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5243264062821078, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4599947158980691, 'train@eng.rst.gum_loss@eng.rst.gum': 1.3925672769546509, 'train@eng.rst.gum_runtime': 163.4568, 'train@eng.rst.gum_samples_per_second': 85.019, 'train@eng.rst.gum_steps_per_second': 2.661, 'epoch': 9.0}
{'loss': 1.4727, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.6273847818374634, 'eval_accuracy@eng.rst.gum': 0.5253606328524896, 'eval_f1@eng.rst.gum': 0.4150587520616386, 'eval_precision@eng.rst.gum': 0.48604144878608874, 'eval_recall@eng.rst.gum': 0.42320385844315206, 'eval_loss@eng.rst.gum': 1.6273847818374634, 'eval_runtime': 25.5857, 'eval_samples_per_second': 83.992, 'eval_steps_per_second': 2.658, 'epoch': 9.0}
{'train@eng.rst.gum_loss': 1.3806804418563843, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5876088364395193, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4649715864385696, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5299999562039857, 'train@eng.rst.gum_recall@eng.rst.gum': 0.45802237512609106, 'train@eng.rst.gum_loss@eng.rst.gum': 1.3806805610656738, 'train@eng.rst.gum_runtime': 163.4258, 'train@eng.rst.gum_samples_per_second': 85.036, 'train@eng.rst.gum_steps_per_second': 2.662, 'epoch': 10.0}
{'loss': 1.4601, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.6241624355316162, 'eval_accuracy@eng.rst.gum': 0.5253606328524896, 'eval_f1@eng.rst.gum': 0.41382924396718557, 'eval_precision@eng.rst.gum': 0.48060702925538046, 'eval_recall@eng.rst.gum': 0.41783063891345285, 'eval_loss@eng.rst.gum': 1.6241624355316162, 'eval_runtime': 25.5681, 'eval_samples_per_second': 84.05, 'eval_steps_per_second': 2.66, 'epoch': 10.0}
{'train@eng.rst.gum_loss': 1.375535488128662, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5886882060876448, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4675421761782518, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5259079139439855, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4643211636689703, 'train@eng.rst.gum_loss@eng.rst.gum': 1.375535488128662, 'train@eng.rst.gum_runtime': 163.0769, 'train@eng.rst.gum_samples_per_second': 85.217, 'train@eng.rst.gum_steps_per_second': 2.667, 'epoch': 11.0}
{'loss': 1.4565, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.6187704801559448, 'eval_accuracy@eng.rst.gum': 0.5248953001395998, 'eval_f1@eng.rst.gum': 0.4162932758815825, 'eval_precision@eng.rst.gum': 0.47546443073353944, 'eval_recall@eng.rst.gum': 0.423995472236908, 'eval_loss@eng.rst.gum': 1.6187704801559448, 'eval_runtime': 25.5615, 'eval_samples_per_second': 84.072, 'eval_steps_per_second': 2.66, 'epoch': 11.0}
{'train@eng.rst.gum_loss': 1.3730593919754028, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5894077858530619, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4683850204042115, 'train@eng.rst.gum_precision@eng.rst.gum': 0.530611767844875, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4628053539004798, 'train@eng.rst.gum_loss@eng.rst.gum': 1.3730593919754028, 'train@eng.rst.gum_runtime': 163.3946, 'train@eng.rst.gum_samples_per_second': 85.052, 'train@eng.rst.gum_steps_per_second': 2.662, 'epoch': 12.0}
{'loss': 1.4485, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.6184682846069336, 'eval_accuracy@eng.rst.gum': 0.5276872964169381, 'eval_f1@eng.rst.gum': 0.41625688347981554, 'eval_precision@eng.rst.gum': 0.47756958940359345, 'eval_recall@eng.rst.gum': 0.42246387223204473, 'eval_loss@eng.rst.gum': 1.6184682846069336, 'eval_runtime': 25.496, 'eval_samples_per_second': 84.288, 'eval_steps_per_second': 2.667, 'epoch': 12.0}
{'train_runtime': 6406.4674, 'train_samples_per_second': 26.031, 'train_steps_per_second': 0.815, 'train_loss': 1.6634587664257063, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.3536
  train_runtime            = 2:00:54.07
  train_samples_per_second =     26.471
  train_steps_per_second   =      0.829
-------------------------------------------------------------------
Lang1:  eng.sdrt.stac    Lang2:  eng.rst.gum
Saving run to:  runs/full_shot/FullShot=v4_eng.sdrt.stac_eng.rst.gum_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 9580 examples
read 1145 examples
read 1510 examples
read 13897 examples
read 2149 examples
read 2091 examples
Total prediction labels:  39
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=39, bias=True)
    )
  )
)
{'train@eng.sdrt.stac_loss': 2.0868961811065674, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.35521920668058454, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.07146079241085006, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.0777864640683484, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.11218258544125043, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 2.0868964195251465, 'train@eng.sdrt.stac_runtime': 112.6638, 'train@eng.sdrt.stac_samples_per_second': 85.032, 'train@eng.sdrt.stac_steps_per_second': 2.663, 'epoch': 1.0}
{'loss': 2.5364, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.0470588207244873, 'eval_accuracy@eng.sdrt.stac': 0.36069868995633186, 'eval_f1@eng.sdrt.stac': 0.07182478069546734, 'eval_precision@eng.sdrt.stac': 0.08725192077513055, 'eval_recall@eng.sdrt.stac': 0.1132102147019232, 'eval_loss@eng.sdrt.stac': 2.047058582305908, 'eval_runtime': 13.8473, 'eval_samples_per_second': 82.688, 'eval_steps_per_second': 2.6, 'epoch': 1.0}
{'train@eng.sdrt.stac_loss': 1.8472548723220825, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.43674321503131525, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.13206688380751647, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.13896765394072028, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.1797894491071772, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.8472546339035034, 'train@eng.sdrt.stac_runtime': 112.4095, 'train@eng.sdrt.stac_samples_per_second': 85.224, 'train@eng.sdrt.stac_steps_per_second': 2.669, 'epoch': 2.0}
{'loss': 2.0058, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.8051843643188477, 'eval_accuracy@eng.sdrt.stac': 0.43580786026200874, 'eval_f1@eng.sdrt.stac': 0.1261964061579582, 'eval_precision@eng.sdrt.stac': 0.1356304955303601, 'eval_recall@eng.sdrt.stac': 0.1764167005936358, 'eval_loss@eng.sdrt.stac': 1.8051841259002686, 'eval_runtime': 13.8295, 'eval_samples_per_second': 82.794, 'eval_steps_per_second': 2.603, 'epoch': 2.0}
{'train@eng.sdrt.stac_loss': 1.739440679550171, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4532359081419624, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.15130891347190767, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.2009125527465156, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.1938993285987907, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.739440679550171, 'train@eng.sdrt.stac_runtime': 112.5347, 'train@eng.sdrt.stac_samples_per_second': 85.129, 'train@eng.sdrt.stac_steps_per_second': 2.666, 'epoch': 3.0}
{'loss': 1.8256, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.6940289735794067, 'eval_accuracy@eng.sdrt.stac': 0.46550218340611355, 'eval_f1@eng.sdrt.stac': 0.15091036564270682, 'eval_precision@eng.sdrt.stac': 0.1466951980205542, 'eval_recall@eng.sdrt.stac': 0.19353146816955608, 'eval_loss@eng.sdrt.stac': 1.6940289735794067, 'eval_runtime': 13.7957, 'eval_samples_per_second': 82.997, 'eval_steps_per_second': 2.61, 'epoch': 3.0}
{'train@eng.sdrt.stac_loss': 1.6598000526428223, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4838204592901879, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.20577829322939667, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.22281289868355378, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2335143376600851, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.6597999334335327, 'train@eng.sdrt.stac_runtime': 112.6243, 'train@eng.sdrt.stac_samples_per_second': 85.062, 'train@eng.sdrt.stac_steps_per_second': 2.664, 'epoch': 4.0}
{'loss': 1.7357, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.606526494026184, 'eval_accuracy@eng.sdrt.stac': 0.4873362445414847, 'eval_f1@eng.sdrt.stac': 0.1958026695562997, 'eval_precision@eng.sdrt.stac': 0.24607446572113223, 'eval_recall@eng.sdrt.stac': 0.2214897348381083, 'eval_loss@eng.sdrt.stac': 1.606526494026184, 'eval_runtime': 13.79, 'eval_samples_per_second': 83.031, 'eval_steps_per_second': 2.611, 'epoch': 4.0}
{'train@eng.sdrt.stac_loss': 1.605239748954773, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4978079331941545, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2257910224097871, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.24744035096516978, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2504456527009921, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.605239748954773, 'train@eng.sdrt.stac_runtime': 112.5508, 'train@eng.sdrt.stac_samples_per_second': 85.117, 'train@eng.sdrt.stac_steps_per_second': 2.665, 'epoch': 5.0}
{'loss': 1.6708, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.5523380041122437, 'eval_accuracy@eng.sdrt.stac': 0.508296943231441, 'eval_f1@eng.sdrt.stac': 0.21970350023252222, 'eval_precision@eng.sdrt.stac': 0.28021190813158187, 'eval_recall@eng.sdrt.stac': 0.2395386626770466, 'eval_loss@eng.sdrt.stac': 1.5523380041122437, 'eval_runtime': 13.7975, 'eval_samples_per_second': 82.986, 'eval_steps_per_second': 2.609, 'epoch': 5.0}
{'train@eng.sdrt.stac_loss': 1.557473063468933, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5082463465553236, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.24948514585811074, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.3667759761904932, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.26446709320033557, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.557473063468933, 'train@eng.sdrt.stac_runtime': 112.5765, 'train@eng.sdrt.stac_samples_per_second': 85.098, 'train@eng.sdrt.stac_steps_per_second': 2.665, 'epoch': 6.0}
{'loss': 1.6209, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.5267622470855713, 'eval_accuracy@eng.sdrt.stac': 0.5179039301310043, 'eval_f1@eng.sdrt.stac': 0.2434158640348139, 'eval_precision@eng.sdrt.stac': 0.29460092827846407, 'eval_recall@eng.sdrt.stac': 0.2584413712200251, 'eval_loss@eng.sdrt.stac': 1.5267622470855713, 'eval_runtime': 13.7981, 'eval_samples_per_second': 82.982, 'eval_steps_per_second': 2.609, 'epoch': 6.0}
{'train@eng.sdrt.stac_loss': 1.538278579711914, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5180584551148225, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2917162093874471, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.38484926160228816, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.30554361655109674, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5382784605026245, 'train@eng.sdrt.stac_runtime': 112.5472, 'train@eng.sdrt.stac_samples_per_second': 85.12, 'train@eng.sdrt.stac_steps_per_second': 2.666, 'epoch': 7.0}
{'loss': 1.5831, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.5103490352630615, 'eval_accuracy@eng.sdrt.stac': 0.525764192139738, 'eval_f1@eng.sdrt.stac': 0.2581807000430998, 'eval_precision@eng.sdrt.stac': 0.3405682115441898, 'eval_recall@eng.sdrt.stac': 0.2718281401979048, 'eval_loss@eng.sdrt.stac': 1.5103490352630615, 'eval_runtime': 13.8315, 'eval_samples_per_second': 82.782, 'eval_steps_per_second': 2.603, 'epoch': 7.0}
{'train@eng.sdrt.stac_loss': 1.5015214681625366, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5303757828810021, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.31129780028863524, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.3789734991813167, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3228314599320399, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5015214681625366, 'train@eng.sdrt.stac_runtime': 112.6571, 'train@eng.sdrt.stac_samples_per_second': 85.037, 'train@eng.sdrt.stac_steps_per_second': 2.663, 'epoch': 8.0}
{'loss': 1.5556, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.4724903106689453, 'eval_accuracy@eng.sdrt.stac': 0.5379912663755458, 'eval_f1@eng.sdrt.stac': 0.2787791241244909, 'eval_precision@eng.sdrt.stac': 0.4186628944870493, 'eval_recall@eng.sdrt.stac': 0.29101368355541185, 'eval_loss@eng.sdrt.stac': 1.4724903106689453, 'eval_runtime': 13.8211, 'eval_samples_per_second': 82.845, 'eval_steps_per_second': 2.605, 'epoch': 8.0}
{'train@eng.sdrt.stac_loss': 1.4844346046447754, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5370563674321504, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.33249561818952633, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.44242852783481673, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3405602429906893, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4844344854354858, 'train@eng.sdrt.stac_runtime': 112.5863, 'train@eng.sdrt.stac_samples_per_second': 85.09, 'train@eng.sdrt.stac_steps_per_second': 2.665, 'epoch': 9.0}
{'loss': 1.5353, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.4626328945159912, 'eval_accuracy@eng.sdrt.stac': 0.5353711790393013, 'eval_f1@eng.sdrt.stac': 0.2959469486193895, 'eval_precision@eng.sdrt.stac': 0.4196799560852254, 'eval_recall@eng.sdrt.stac': 0.3024165408780274, 'eval_loss@eng.sdrt.stac': 1.4626328945159912, 'eval_runtime': 13.8176, 'eval_samples_per_second': 82.865, 'eval_steps_per_second': 2.605, 'epoch': 9.0}
{'train@eng.sdrt.stac_loss': 1.4676189422607422, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5409185803757829, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.34271847415757345, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.43457967036923206, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.35051725205528383, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4676190614700317, 'train@eng.sdrt.stac_runtime': 112.4672, 'train@eng.sdrt.stac_samples_per_second': 85.18, 'train@eng.sdrt.stac_steps_per_second': 2.667, 'epoch': 10.0}
{'loss': 1.5214, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.4474117755889893, 'eval_accuracy@eng.sdrt.stac': 0.5441048034934498, 'eval_f1@eng.sdrt.stac': 0.32113938652708396, 'eval_precision@eng.sdrt.stac': 0.42548910805394735, 'eval_recall@eng.sdrt.stac': 0.3261017333761768, 'eval_loss@eng.sdrt.stac': 1.4474116563796997, 'eval_runtime': 13.7979, 'eval_samples_per_second': 82.984, 'eval_steps_per_second': 2.609, 'epoch': 10.0}
{'train@eng.sdrt.stac_loss': 1.4605692625045776, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5433194154488518, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3473801568166307, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.4323022096200369, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3558851854286519, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4605693817138672, 'train@eng.sdrt.stac_runtime': 112.6868, 'train@eng.sdrt.stac_samples_per_second': 85.014, 'train@eng.sdrt.stac_steps_per_second': 2.662, 'epoch': 11.0}
{'loss': 1.508, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.4424277544021606, 'eval_accuracy@eng.sdrt.stac': 0.5432314410480349, 'eval_f1@eng.sdrt.stac': 0.32324053114905293, 'eval_precision@eng.sdrt.stac': 0.4304936513434123, 'eval_recall@eng.sdrt.stac': 0.32737305131941113, 'eval_loss@eng.sdrt.stac': 1.4424278736114502, 'eval_runtime': 15.3021, 'eval_samples_per_second': 74.827, 'eval_steps_per_second': 2.353, 'epoch': 11.0}
{'train@eng.sdrt.stac_loss': 1.4593733549118042, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5436325678496868, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3473119074447566, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.43071280359674347, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.35617082269141986, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4593734741210938, 'train@eng.sdrt.stac_runtime': 112.4754, 'train@eng.sdrt.stac_samples_per_second': 85.174, 'train@eng.sdrt.stac_steps_per_second': 2.667, 'epoch': 12.0}
{'loss': 1.5042, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.4424265623092651, 'eval_accuracy@eng.sdrt.stac': 0.5449781659388646, 'eval_f1@eng.sdrt.stac': 0.33534543747572854, 'eval_precision@eng.sdrt.stac': 0.4307691145184871, 'eval_recall@eng.sdrt.stac': 0.3394896231307892, 'eval_loss@eng.sdrt.stac': 1.4424265623092651, 'eval_runtime': 13.7801, 'eval_samples_per_second': 83.091, 'eval_steps_per_second': 2.612, 'epoch': 12.0}
{'train_runtime': 4369.8705, 'train_samples_per_second': 26.307, 'train_steps_per_second': 0.824, 'train_loss': 1.7168974558512369, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.7169
  train_runtime            = 1:12:49.87
  train_samples_per_second =     26.307
  train_steps_per_second   =      0.824
{'train@eng.rst.gum_loss': 2.410625696182251, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.27293660502266676, 'train@eng.rst.gum_f1@eng.rst.gum': 0.05543400908544594, 'train@eng.rst.gum_precision@eng.rst.gum': 0.15123001288633434, 'train@eng.rst.gum_recall@eng.rst.gum': 0.07681051168362804, 'train@eng.rst.gum_loss@eng.rst.gum': 2.410625696182251, 'train@eng.rst.gum_runtime': 163.2591, 'train@eng.rst.gum_samples_per_second': 85.122, 'train@eng.rst.gum_steps_per_second': 2.664, 'epoch': 1.0}
{'loss': 2.8268, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.4950809478759766, 'eval_accuracy@eng.rst.gum': 0.2624476500697999, 'eval_f1@eng.rst.gum': 0.04956230373817688, 'eval_precision@eng.rst.gum': 0.0952895666598634, 'eval_recall@eng.rst.gum': 0.07144017251766002, 'eval_loss@eng.rst.gum': 2.4950807094573975, 'eval_runtime': 25.5843, 'eval_samples_per_second': 83.997, 'eval_steps_per_second': 2.658, 'epoch': 1.0}
{'train@eng.rst.gum_loss': 2.0263733863830566, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.4064906094840613, 'train@eng.rst.gum_f1@eng.rst.gum': 0.2253516993482288, 'train@eng.rst.gum_precision@eng.rst.gum': 0.3459650422173799, 'train@eng.rst.gum_recall@eng.rst.gum': 0.24310215447538153, 'train@eng.rst.gum_loss@eng.rst.gum': 2.0263736248016357, 'train@eng.rst.gum_runtime': 163.2203, 'train@eng.rst.gum_samples_per_second': 85.143, 'train@eng.rst.gum_steps_per_second': 2.665, 'epoch': 2.0}
{'loss': 2.2721, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.1260056495666504, 'eval_accuracy@eng.rst.gum': 0.401582131223825, 'eval_f1@eng.rst.gum': 0.22290969171995337, 'eval_precision@eng.rst.gum': 0.274079159247906, 'eval_recall@eng.rst.gum': 0.2514899536591355, 'eval_loss@eng.rst.gum': 2.1260054111480713, 'eval_runtime': 25.6053, 'eval_samples_per_second': 83.928, 'eval_steps_per_second': 2.656, 'epoch': 2.0}
{'train@eng.rst.gum_loss': 1.8149546384811401, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.4772253004245521, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3052409985322646, 'train@eng.rst.gum_precision@eng.rst.gum': 0.37198460944840156, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3195959723123346, 'train@eng.rst.gum_loss@eng.rst.gum': 1.8149546384811401, 'train@eng.rst.gum_runtime': 163.394, 'train@eng.rst.gum_samples_per_second': 85.052, 'train@eng.rst.gum_steps_per_second': 2.662, 'epoch': 3.0}
{'loss': 1.9989, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.9363130331039429, 'eval_accuracy@eng.rst.gum': 0.4434620753838995, 'eval_f1@eng.rst.gum': 0.28516072028289197, 'eval_precision@eng.rst.gum': 0.30829223897391905, 'eval_recall@eng.rst.gum': 0.3060785159908158, 'eval_loss@eng.rst.gum': 1.936313271522522, 'eval_runtime': 25.5878, 'eval_samples_per_second': 83.985, 'eval_steps_per_second': 2.658, 'epoch': 3.0}
{'train@eng.rst.gum_loss': 1.686455249786377, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.49938835719939556, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3289469899747354, 'train@eng.rst.gum_precision@eng.rst.gum': 0.48575604746267503, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3369999437953671, 'train@eng.rst.gum_loss@eng.rst.gum': 1.686455249786377, 'train@eng.rst.gum_runtime': 163.465, 'train@eng.rst.gum_samples_per_second': 85.015, 'train@eng.rst.gum_steps_per_second': 2.661, 'epoch': 4.0}
{'loss': 1.8354, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.8417472839355469, 'eval_accuracy@eng.rst.gum': 0.4695207073057236, 'eval_f1@eng.rst.gum': 0.3084408489386795, 'eval_precision@eng.rst.gum': 0.34048306223644514, 'eval_recall@eng.rst.gum': 0.32461762349885726, 'eval_loss@eng.rst.gum': 1.8417472839355469, 'eval_runtime': 25.6065, 'eval_samples_per_second': 83.924, 'eval_steps_per_second': 2.656, 'epoch': 4.0}
{'train@eng.rst.gum_loss': 1.6119189262390137, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5219831618334893, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3795004985378374, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5192116999985803, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3827073227483757, 'train@eng.rst.gum_loss@eng.rst.gum': 1.6119189262390137, 'train@eng.rst.gum_runtime': 163.2459, 'train@eng.rst.gum_samples_per_second': 85.129, 'train@eng.rst.gum_steps_per_second': 2.665, 'epoch': 5.0}
{'loss': 1.7333, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.7870283126831055, 'eval_accuracy@eng.rst.gum': 0.4811540251279665, 'eval_f1@eng.rst.gum': 0.3434365543161955, 'eval_precision@eng.rst.gum': 0.3804978988114453, 'eval_recall@eng.rst.gum': 0.35568751707115687, 'eval_loss@eng.rst.gum': 1.787028193473816, 'eval_runtime': 25.5639, 'eval_samples_per_second': 84.064, 'eval_steps_per_second': 2.66, 'epoch': 5.0}
{'train@eng.rst.gum_loss': 1.5575600862503052, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5352953874937036, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4011456050023195, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5664862592926933, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3997972070632423, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5575602054595947, 'train@eng.rst.gum_runtime': 163.5149, 'train@eng.rst.gum_samples_per_second': 84.989, 'train@eng.rst.gum_steps_per_second': 2.66, 'epoch': 6.0}
{'loss': 1.6671, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.749675989151001, 'eval_accuracy@eng.rst.gum': 0.4965100046533271, 'eval_f1@eng.rst.gum': 0.36958241276815407, 'eval_precision@eng.rst.gum': 0.5083683494737713, 'eval_recall@eng.rst.gum': 0.37577232954406126, 'eval_loss@eng.rst.gum': 1.7496761083602905, 'eval_runtime': 25.6942, 'eval_samples_per_second': 83.637, 'eval_steps_per_second': 2.647, 'epoch': 6.0}
{'train@eng.rst.gum_loss': 1.516892671585083, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5455853781391667, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4175306576368249, 'train@eng.rst.gum_precision@eng.rst.gum': 0.56419815209275, 'train@eng.rst.gum_recall@eng.rst.gum': 0.41332066566774, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5168927907943726, 'train@eng.rst.gum_runtime': 163.1555, 'train@eng.rst.gum_samples_per_second': 85.176, 'train@eng.rst.gum_steps_per_second': 2.666, 'epoch': 7.0}
{'loss': 1.6195, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.7227458953857422, 'eval_accuracy@eng.rst.gum': 0.5002326663564448, 'eval_f1@eng.rst.gum': 0.38128209805123686, 'eval_precision@eng.rst.gum': 0.4982294640219942, 'eval_recall@eng.rst.gum': 0.3857777700546339, 'eval_loss@eng.rst.gum': 1.7227457761764526, 'eval_runtime': 25.5951, 'eval_samples_per_second': 83.961, 'eval_steps_per_second': 2.657, 'epoch': 7.0}
{'train@eng.rst.gum_loss': 1.4958794116973877, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5506943944736274, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4232931766365571, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5637839542425057, 'train@eng.rst.gum_recall@eng.rst.gum': 0.41868774288981864, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4958794116973877, 'train@eng.rst.gum_runtime': 163.6176, 'train@eng.rst.gum_samples_per_second': 84.936, 'train@eng.rst.gum_steps_per_second': 2.659, 'epoch': 8.0}
{'loss': 1.5888, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.7140780687332153, 'eval_accuracy@eng.rst.gum': 0.5053513261982318, 'eval_f1@eng.rst.gum': 0.3882313851945717, 'eval_precision@eng.rst.gum': 0.5048147045633873, 'eval_recall@eng.rst.gum': 0.3903231799355849, 'eval_loss@eng.rst.gum': 1.7140780687332153, 'eval_runtime': 25.6883, 'eval_samples_per_second': 83.657, 'eval_steps_per_second': 2.647, 'epoch': 8.0}
{'train@eng.rst.gum_loss': 1.4752546548843384, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5582499820105059, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4316891079896224, 'train@eng.rst.gum_precision@eng.rst.gum': 0.563858922651226, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4297205173623882, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4752546548843384, 'train@eng.rst.gum_runtime': 163.4557, 'train@eng.rst.gum_samples_per_second': 85.02, 'train@eng.rst.gum_steps_per_second': 2.661, 'epoch': 9.0}
{'loss': 1.5634, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.6960324048995972, 'eval_accuracy@eng.rst.gum': 0.5034899953466728, 'eval_f1@eng.rst.gum': 0.390546873532907, 'eval_precision@eng.rst.gum': 0.46617920025145415, 'eval_recall@eng.rst.gum': 0.39523598498934276, 'eval_loss@eng.rst.gum': 1.6960325241088867, 'eval_runtime': 25.6147, 'eval_samples_per_second': 83.897, 'eval_steps_per_second': 2.655, 'epoch': 9.0}
{'train@eng.rst.gum_loss': 1.4604960680007935, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5613441750017989, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4377856156422063, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5434140368922872, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4314226377612437, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4604960680007935, 'train@eng.rst.gum_runtime': 163.1448, 'train@eng.rst.gum_samples_per_second': 85.182, 'train@eng.rst.gum_steps_per_second': 2.666, 'epoch': 10.0}
{'loss': 1.5508, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.6876590251922607, 'eval_accuracy@eng.rst.gum': 0.50814332247557, 'eval_f1@eng.rst.gum': 0.3968864018218663, 'eval_precision@eng.rst.gum': 0.4721839429677335, 'eval_recall@eng.rst.gum': 0.3990189260900319, 'eval_loss@eng.rst.gum': 1.6876590251922607, 'eval_runtime': 25.5623, 'eval_samples_per_second': 84.069, 'eval_steps_per_second': 2.66, 'epoch': 10.0}
{'train@eng.rst.gum_loss': 1.4541113376617432, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5636468302511334, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4412555392983983, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5403728943146388, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4376499414535798, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4541113376617432, 'train@eng.rst.gum_runtime': 163.4423, 'train@eng.rst.gum_samples_per_second': 85.027, 'train@eng.rst.gum_steps_per_second': 2.661, 'epoch': 11.0}
{'loss': 1.5394, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.6818978786468506, 'eval_accuracy@eng.rst.gum': 0.5072126570497906, 'eval_f1@eng.rst.gum': 0.40128504512798807, 'eval_precision@eng.rst.gum': 0.47021646775808257, 'eval_recall@eng.rst.gum': 0.4051961006774843, 'eval_loss@eng.rst.gum': 1.6818976402282715, 'eval_runtime': 25.6894, 'eval_samples_per_second': 83.653, 'eval_steps_per_second': 2.647, 'epoch': 11.0}
{'train@eng.rst.gum_loss': 1.451559066772461, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.562783334532633, 'train@eng.rst.gum_f1@eng.rst.gum': 0.44104484093471064, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5453289205835649, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4364243551195339, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4515589475631714, 'train@eng.rst.gum_runtime': 163.5533, 'train@eng.rst.gum_samples_per_second': 84.969, 'train@eng.rst.gum_steps_per_second': 2.66, 'epoch': 12.0}
{'loss': 1.5263, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.6816054582595825, 'eval_accuracy@eng.rst.gum': 0.5067473243369008, 'eval_f1@eng.rst.gum': 0.3998895024583522, 'eval_precision@eng.rst.gum': 0.46591594510177264, 'eval_recall@eng.rst.gum': 0.40357021952032923, 'eval_loss@eng.rst.gum': 1.681605577468872, 'eval_runtime': 25.6487, 'eval_samples_per_second': 83.786, 'eval_steps_per_second': 2.651, 'epoch': 12.0}
{'train_runtime': 6409.7596, 'train_samples_per_second': 26.017, 'train_steps_per_second': 0.814, 'train_loss': 1.8101436454217552, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.7169
  train_runtime            = 1:12:49.87
  train_samples_per_second =     26.307
  train_steps_per_second   =      0.824
-------------------------------------------------------------------
Lang1:  fas.rst.prstc    Lang2:  eng.rst.gum
Saving run to:  runs/full_shot/FullShot=v4_fas.rst.prstc_eng.rst.gum_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 4100 examples
read 499 examples
read 592 examples
read 13897 examples
read 2149 examples
read 2091 examples
Total prediction labels:  32
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (ar): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=32, bias=True)
    )
  )
)
{'train@fas.rst.prstc_loss': 2.4116644859313965, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.23780487804878048, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.02260214430599826, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.013988522238163558, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.058823529411764705, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.4116644859313965, 'train@fas.rst.prstc_runtime': 48.2946, 'train@fas.rst.prstc_samples_per_second': 84.896, 'train@fas.rst.prstc_steps_per_second': 2.671, 'epoch': 1.0}
{'loss': 2.766, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.3374645709991455, 'eval_accuracy@fas.rst.prstc': 0.24448897795591182, 'eval_f1@fas.rst.prstc': 0.026194310252281264, 'eval_precision@fas.rst.prstc': 0.016299265197060788, 'eval_recall@fas.rst.prstc': 0.06666666666666667, 'eval_loss@fas.rst.prstc': 2.3374645709991455, 'eval_runtime': 6.1795, 'eval_samples_per_second': 80.751, 'eval_steps_per_second': 2.589, 'epoch': 1.0}
{'train@fas.rst.prstc_loss': 2.357722043991089, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.26365853658536587, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04332877210210696, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03259840039280953, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07086721649925655, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3577218055725098, 'train@fas.rst.prstc_runtime': 48.3151, 'train@fas.rst.prstc_samples_per_second': 84.86, 'train@fas.rst.prstc_steps_per_second': 2.67, 'epoch': 2.0}
{'loss': 2.4086, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.2733631134033203, 'eval_accuracy@fas.rst.prstc': 0.3026052104208417, 'eval_f1@fas.rst.prstc': 0.05459603289791969, 'eval_precision@fas.rst.prstc': 0.0410912394254244, 'eval_recall@fas.rst.prstc': 0.08544072226181991, 'eval_loss@fas.rst.prstc': 2.273362874984741, 'eval_runtime': 6.1588, 'eval_samples_per_second': 81.023, 'eval_steps_per_second': 2.598, 'epoch': 2.0}
{'train@fas.rst.prstc_loss': 2.3365044593811035, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.25585365853658537, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.03328385576826402, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03500386877967119, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.06415284062342885, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3365044593811035, 'train@fas.rst.prstc_runtime': 48.3156, 'train@fas.rst.prstc_samples_per_second': 84.859, 'train@fas.rst.prstc_steps_per_second': 2.67, 'epoch': 3.0}
{'loss': 2.359, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.2523810863494873, 'eval_accuracy@fas.rst.prstc': 0.26452905811623245, 'eval_f1@fas.rst.prstc': 0.03866320615574585, 'eval_precision@fas.rst.prstc': 0.04255112797807294, 'eval_recall@fas.rst.prstc': 0.07256355428842955, 'eval_loss@fas.rst.prstc': 2.2523813247680664, 'eval_runtime': 6.1594, 'eval_samples_per_second': 81.014, 'eval_steps_per_second': 2.598, 'epoch': 3.0}
{'train@fas.rst.prstc_loss': 2.3111813068389893, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.24170731707317072, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.025136141269583922, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.037136130272018286, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.059954430217258756, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3111815452575684, 'train@fas.rst.prstc_runtime': 48.2978, 'train@fas.rst.prstc_samples_per_second': 84.89, 'train@fas.rst.prstc_steps_per_second': 2.671, 'epoch': 4.0}
{'loss': 2.3462, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.2312841415405273, 'eval_accuracy@fas.rst.prstc': 0.24849699398797595, 'eval_f1@fas.rst.prstc': 0.028581354103815845, 'eval_precision@fas.rst.prstc': 0.060842293906810035, 'eval_recall@fas.rst.prstc': 0.06782608695652173, 'eval_loss@fas.rst.prstc': 2.2312839031219482, 'eval_runtime': 6.1538, 'eval_samples_per_second': 81.088, 'eval_steps_per_second': 2.6, 'epoch': 4.0}
{'train@fas.rst.prstc_loss': 2.2471795082092285, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.30829268292682926, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.051394763338182665, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.038456815463904614, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.0817699474770814, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.2471792697906494, 'train@fas.rst.prstc_runtime': 48.3502, 'train@fas.rst.prstc_samples_per_second': 84.798, 'train@fas.rst.prstc_steps_per_second': 2.668, 'epoch': 5.0}
{'loss': 2.3127, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.1547911167144775, 'eval_accuracy@fas.rst.prstc': 0.33266533066132264, 'eval_f1@fas.rst.prstc': 0.06187526067893511, 'eval_precision@fas.rst.prstc': 0.04776384535005225, 'eval_recall@fas.rst.prstc': 0.09353765740080779, 'eval_loss@fas.rst.prstc': 2.1547911167144775, 'eval_runtime': 6.1568, 'eval_samples_per_second': 81.049, 'eval_steps_per_second': 2.599, 'epoch': 5.0}
{'train@fas.rst.prstc_loss': 2.190797805786133, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.32097560975609757, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.05458970645131815, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.042679836029816245, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08571676133628575, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.190798044204712, 'train@fas.rst.prstc_runtime': 48.3842, 'train@fas.rst.prstc_samples_per_second': 84.738, 'train@fas.rst.prstc_steps_per_second': 2.666, 'epoch': 6.0}
{'loss': 2.2442, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.092691421508789, 'eval_accuracy@fas.rst.prstc': 0.3486973947895792, 'eval_f1@fas.rst.prstc': 0.06573466117935346, 'eval_precision@fas.rst.prstc': 0.0524581993969749, 'eval_recall@fas.rst.prstc': 0.09820860061772392, 'eval_loss@fas.rst.prstc': 2.092691421508789, 'eval_runtime': 6.2006, 'eval_samples_per_second': 80.476, 'eval_steps_per_second': 2.58, 'epoch': 6.0}
{'train@fas.rst.prstc_loss': 2.1791539192199707, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3336585365853659, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.05729660340117419, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.046026907512430035, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08935999058652376, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.17915415763855, 'train@fas.rst.prstc_runtime': 48.3916, 'train@fas.rst.prstc_samples_per_second': 84.725, 'train@fas.rst.prstc_steps_per_second': 2.666, 'epoch': 7.0}
{'loss': 2.2114, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.0682296752929688, 'eval_accuracy@fas.rst.prstc': 0.3667334669338677, 'eval_f1@fas.rst.prstc': 0.07027777777777779, 'eval_precision@fas.rst.prstc': 0.058613780413003476, 'eval_recall@fas.rst.prstc': 0.10342599192207175, 'eval_loss@fas.rst.prstc': 2.0682291984558105, 'eval_runtime': 6.1525, 'eval_samples_per_second': 81.105, 'eval_steps_per_second': 2.601, 'epoch': 7.0}
{'train@fas.rst.prstc_loss': 2.1597418785095215, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3404878048780488, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.05866146325320753, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.04752411560755844, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.09124247189328542, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1597418785095215, 'train@fas.rst.prstc_runtime': 48.4015, 'train@fas.rst.prstc_samples_per_second': 84.708, 'train@fas.rst.prstc_steps_per_second': 2.665, 'epoch': 8.0}
{'loss': 2.1965, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.0551035404205322, 'eval_accuracy@fas.rst.prstc': 0.3727454909819639, 'eval_f1@fas.rst.prstc': 0.07163757341766952, 'eval_precision@fas.rst.prstc': 0.06044276834974509, 'eval_recall@fas.rst.prstc': 0.10516512235685435, 'eval_loss@fas.rst.prstc': 2.0551035404205322, 'eval_runtime': 6.1646, 'eval_samples_per_second': 80.946, 'eval_steps_per_second': 2.595, 'epoch': 8.0}
{'train@fas.rst.prstc_loss': 2.150501251220703, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.34512195121951217, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.059112804301364806, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.04712488055241077, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.09236117796818671, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.150501012802124, 'train@fas.rst.prstc_runtime': 48.3403, 'train@fas.rst.prstc_samples_per_second': 84.815, 'train@fas.rst.prstc_steps_per_second': 2.669, 'epoch': 9.0}
{'loss': 2.1796, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.043997287750244, 'eval_accuracy@fas.rst.prstc': 0.374749498997996, 'eval_f1@fas.rst.prstc': 0.07149375677735906, 'eval_precision@fas.rst.prstc': 0.05883671574479374, 'eval_recall@fas.rst.prstc': 0.10564504632929438, 'eval_loss@fas.rst.prstc': 2.043997287750244, 'eval_runtime': 6.1565, 'eval_samples_per_second': 81.052, 'eval_steps_per_second': 2.599, 'epoch': 9.0}
{'train@fas.rst.prstc_loss': 2.1401543617248535, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.34512195121951217, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.059548471785514825, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.0483515951737888, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.09248077190505226, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1401541233062744, 'train@fas.rst.prstc_runtime': 48.3671, 'train@fas.rst.prstc_samples_per_second': 84.768, 'train@fas.rst.prstc_steps_per_second': 2.667, 'epoch': 10.0}
{'loss': 2.1771, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.034846305847168, 'eval_accuracy@fas.rst.prstc': 0.37675350701402804, 'eval_f1@fas.rst.prstc': 0.07250472191648662, 'eval_precision@fas.rst.prstc': 0.06072969308517195, 'eval_recall@fas.rst.prstc': 0.1062247564742219, 'eval_loss@fas.rst.prstc': 2.034846544265747, 'eval_runtime': 6.1587, 'eval_samples_per_second': 81.024, 'eval_steps_per_second': 2.598, 'epoch': 10.0}
{'train@fas.rst.prstc_loss': 2.1345276832580566, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3468292682926829, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.059642151188411055, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.04807856462003512, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.09291229421392125, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1345276832580566, 'train@fas.rst.prstc_runtime': 48.346, 'train@fas.rst.prstc_samples_per_second': 84.805, 'train@fas.rst.prstc_steps_per_second': 2.668, 'epoch': 11.0}
{'loss': 2.1662, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.0306906700134277, 'eval_accuracy@fas.rst.prstc': 0.37074148296593185, 'eval_f1@fas.rst.prstc': 0.07123997312676558, 'eval_precision@fas.rst.prstc': 0.05917382479063328, 'eval_recall@fas.rst.prstc': 0.1044856260394393, 'eval_loss@fas.rst.prstc': 2.0306906700134277, 'eval_runtime': 6.1534, 'eval_samples_per_second': 81.094, 'eval_steps_per_second': 2.6, 'epoch': 11.0}
{'train@fas.rst.prstc_loss': 2.1337385177612305, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.34609756097560973, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.05931004459029799, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.04740262158413044, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.09266690200357285, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1337382793426514, 'train@fas.rst.prstc_runtime': 48.3268, 'train@fas.rst.prstc_samples_per_second': 84.839, 'train@fas.rst.prstc_steps_per_second': 2.669, 'epoch': 12.0}
{'loss': 2.16, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.030768632888794, 'eval_accuracy@fas.rst.prstc': 0.37074148296593185, 'eval_f1@fas.rst.prstc': 0.07083122318775338, 'eval_precision@fas.rst.prstc': 0.05823791222578813, 'eval_recall@fas.rst.prstc': 0.1044856260394393, 'eval_loss@fas.rst.prstc': 2.030768632888794, 'eval_runtime': 6.1619, 'eval_samples_per_second': 80.981, 'eval_steps_per_second': 2.597, 'epoch': 12.0}
{'train_runtime': 1877.7872, 'train_samples_per_second': 26.201, 'train_steps_per_second': 0.824, 'train_loss': 2.293968121826803, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =      2.294
  train_runtime            = 0:31:17.78
  train_samples_per_second =     26.201
  train_steps_per_second   =      0.824
{'train@eng.rst.gum_loss': 2.5075690746307373, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.2573936820896596, 'train@eng.rst.gum_f1@eng.rst.gum': 0.050976865853762064, 'train@eng.rst.gum_precision@eng.rst.gum': 0.06227239261642622, 'train@eng.rst.gum_recall@eng.rst.gum': 0.0730155223975149, 'train@eng.rst.gum_loss@eng.rst.gum': 2.507568836212158, 'train@eng.rst.gum_runtime': 163.1954, 'train@eng.rst.gum_samples_per_second': 85.156, 'train@eng.rst.gum_steps_per_second': 2.666, 'epoch': 1.0}
{'loss': 2.7551, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.6001574993133545, 'eval_accuracy@eng.rst.gum': 0.2424383434155421, 'eval_f1@eng.rst.gum': 0.0410778108501012, 'eval_precision@eng.rst.gum': 0.04565279861055883, 'eval_recall@eng.rst.gum': 0.06864747428525834, 'eval_loss@eng.rst.gum': 2.6001574993133545, 'eval_runtime': 25.5039, 'eval_samples_per_second': 84.262, 'eval_steps_per_second': 2.666, 'epoch': 1.0}
{'train@eng.rst.gum_loss': 2.112902879714966, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.3749730157587969, 'train@eng.rst.gum_f1@eng.rst.gum': 0.12018324427257267, 'train@eng.rst.gum_precision@eng.rst.gum': 0.18917038936656314, 'train@eng.rst.gum_recall@eng.rst.gum': 0.14285701469558815, 'train@eng.rst.gum_loss@eng.rst.gum': 2.112902879714966, 'train@eng.rst.gum_runtime': 163.407, 'train@eng.rst.gum_samples_per_second': 85.045, 'train@eng.rst.gum_steps_per_second': 2.662, 'epoch': 2.0}
{'loss': 2.3744, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.2259206771850586, 'eval_accuracy@eng.rst.gum': 0.3480688692415077, 'eval_f1@eng.rst.gum': 0.11103708652017684, 'eval_precision@eng.rst.gum': 0.1955594926463326, 'eval_recall@eng.rst.gum': 0.136607809567213, 'eval_loss@eng.rst.gum': 2.2259206771850586, 'eval_runtime': 25.5687, 'eval_samples_per_second': 84.048, 'eval_steps_per_second': 2.66, 'epoch': 2.0}
{'train@eng.rst.gum_loss': 1.82510244846344, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.4650644023890048, 'train@eng.rst.gum_f1@eng.rst.gum': 0.24812082233531338, 'train@eng.rst.gum_precision@eng.rst.gum': 0.37137437785737637, 'train@eng.rst.gum_recall@eng.rst.gum': 0.26276053459477583, 'train@eng.rst.gum_loss@eng.rst.gum': 1.8251023292541504, 'train@eng.rst.gum_runtime': 163.3006, 'train@eng.rst.gum_samples_per_second': 85.101, 'train@eng.rst.gum_steps_per_second': 2.664, 'epoch': 3.0}
{'loss': 2.0382, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.9490985870361328, 'eval_accuracy@eng.rst.gum': 0.44625407166123776, 'eval_f1@eng.rst.gum': 0.2524663453153159, 'eval_precision@eng.rst.gum': 0.30627789187398063, 'eval_recall@eng.rst.gum': 0.2640835079238443, 'eval_loss@eng.rst.gum': 1.9490981101989746, 'eval_runtime': 25.5116, 'eval_samples_per_second': 84.236, 'eval_steps_per_second': 2.665, 'epoch': 3.0}
{'train@eng.rst.gum_loss': 1.705264925956726, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.4934158451464345, 'train@eng.rst.gum_f1@eng.rst.gum': 0.2848773231411334, 'train@eng.rst.gum_precision@eng.rst.gum': 0.41093514973025813, 'train@eng.rst.gum_recall@eng.rst.gum': 0.2981523801669131, 'train@eng.rst.gum_loss@eng.rst.gum': 1.7052648067474365, 'train@eng.rst.gum_runtime': 163.1516, 'train@eng.rst.gum_samples_per_second': 85.178, 'train@eng.rst.gum_steps_per_second': 2.666, 'epoch': 4.0}
{'loss': 1.8444, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.8528923988342285, 'eval_accuracy@eng.rst.gum': 0.467194043741275, 'eval_f1@eng.rst.gum': 0.28027480902434637, 'eval_precision@eng.rst.gum': 0.3177919900429423, 'eval_recall@eng.rst.gum': 0.2943587335535537, 'eval_loss@eng.rst.gum': 1.852892279624939, 'eval_runtime': 25.4921, 'eval_samples_per_second': 84.301, 'eval_steps_per_second': 2.667, 'epoch': 4.0}
{'train@eng.rst.gum_loss': 1.6230101585388184, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5114053392818594, 'train@eng.rst.gum_f1@eng.rst.gum': 0.32655886277810325, 'train@eng.rst.gum_precision@eng.rst.gum': 0.43287718301422534, 'train@eng.rst.gum_recall@eng.rst.gum': 0.33730551383417845, 'train@eng.rst.gum_loss@eng.rst.gum': 1.6230100393295288, 'train@eng.rst.gum_runtime': 163.5171, 'train@eng.rst.gum_samples_per_second': 84.988, 'train@eng.rst.gum_steps_per_second': 2.66, 'epoch': 5.0}
{'loss': 1.7398, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.783466100692749, 'eval_accuracy@eng.rst.gum': 0.48255002326663565, 'eval_f1@eng.rst.gum': 0.3141951041070819, 'eval_precision@eng.rst.gum': 0.32762280113992537, 'eval_recall@eng.rst.gum': 0.3287943722710793, 'eval_loss@eng.rst.gum': 1.783466100692749, 'eval_runtime': 25.5887, 'eval_samples_per_second': 83.982, 'eval_steps_per_second': 2.657, 'epoch': 5.0}
{'train@eng.rst.gum_loss': 1.566593885421753, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5280276318629921, 'train@eng.rst.gum_f1@eng.rst.gum': 0.36799158573513757, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5072571585881444, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3698717100486876, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5665940046310425, 'train@eng.rst.gum_runtime': 163.3352, 'train@eng.rst.gum_samples_per_second': 85.083, 'train@eng.rst.gum_steps_per_second': 2.663, 'epoch': 6.0}
{'loss': 1.6742, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.7422595024108887, 'eval_accuracy@eng.rst.gum': 0.49697533736621685, 'eval_f1@eng.rst.gum': 0.3563434882501123, 'eval_precision@eng.rst.gum': 0.39769072568565217, 'eval_recall@eng.rst.gum': 0.3657171954354323, 'eval_loss@eng.rst.gum': 1.7422597408294678, 'eval_runtime': 25.5335, 'eval_samples_per_second': 84.164, 'eval_steps_per_second': 2.663, 'epoch': 6.0}
{'train@eng.rst.gum_loss': 1.531734585762024, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5399006979923725, 'train@eng.rst.gum_f1@eng.rst.gum': 0.38762244970159704, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5351269355719717, 'train@eng.rst.gum_recall@eng.rst.gum': 0.38844243072135604, 'train@eng.rst.gum_loss@eng.rst.gum': 1.531734824180603, 'train@eng.rst.gum_runtime': 163.1003, 'train@eng.rst.gum_samples_per_second': 85.205, 'train@eng.rst.gum_steps_per_second': 2.667, 'epoch': 7.0}
{'loss': 1.6307, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.720171570777893, 'eval_accuracy@eng.rst.gum': 0.4993020009306654, 'eval_f1@eng.rst.gum': 0.36313366207497827, 'eval_precision@eng.rst.gum': 0.4572483446265986, 'eval_recall@eng.rst.gum': 0.3735323931657259, 'eval_loss@eng.rst.gum': 1.720171570777893, 'eval_runtime': 25.5251, 'eval_samples_per_second': 84.192, 'eval_steps_per_second': 2.664, 'epoch': 7.0}
{'train@eng.rst.gum_loss': 1.506273865699768, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5470964956465424, 'train@eng.rst.gum_f1@eng.rst.gum': 0.40097985449591833, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5304235208287186, 'train@eng.rst.gum_recall@eng.rst.gum': 0.40041424249925045, 'train@eng.rst.gum_loss@eng.rst.gum': 1.506273865699768, 'train@eng.rst.gum_runtime': 163.3467, 'train@eng.rst.gum_samples_per_second': 85.077, 'train@eng.rst.gum_steps_per_second': 2.663, 'epoch': 8.0}
{'loss': 1.598, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.7038325071334839, 'eval_accuracy@eng.rst.gum': 0.5044206607724523, 'eval_f1@eng.rst.gum': 0.36985566386792734, 'eval_precision@eng.rst.gum': 0.45436641208117334, 'eval_recall@eng.rst.gum': 0.38176954601280877, 'eval_loss@eng.rst.gum': 1.7038323879241943, 'eval_runtime': 25.5783, 'eval_samples_per_second': 84.017, 'eval_steps_per_second': 2.659, 'epoch': 8.0}
{'train@eng.rst.gum_loss': 1.4846420288085938, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5543642512772541, 'train@eng.rst.gum_f1@eng.rst.gum': 0.41292407595421743, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5135304731042635, 'train@eng.rst.gum_recall@eng.rst.gum': 0.41441163628191363, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4846421480178833, 'train@eng.rst.gum_runtime': 163.4215, 'train@eng.rst.gum_samples_per_second': 85.038, 'train@eng.rst.gum_steps_per_second': 2.662, 'epoch': 9.0}
{'loss': 1.5687, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.6872475147247314, 'eval_accuracy@eng.rst.gum': 0.5062819916240112, 'eval_f1@eng.rst.gum': 0.37681988740173156, 'eval_precision@eng.rst.gum': 0.4570678864460618, 'eval_recall@eng.rst.gum': 0.39319580170668783, 'eval_loss@eng.rst.gum': 1.687247633934021, 'eval_runtime': 25.5235, 'eval_samples_per_second': 84.197, 'eval_steps_per_second': 2.664, 'epoch': 9.0}
{'train@eng.rst.gum_loss': 1.4694998264312744, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5588976037993811, 'train@eng.rst.gum_f1@eng.rst.gum': 0.42003012613397256, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5197470495266053, 'train@eng.rst.gum_recall@eng.rst.gum': 0.41754314944579696, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4694998264312744, 'train@eng.rst.gum_runtime': 163.0795, 'train@eng.rst.gum_samples_per_second': 85.216, 'train@eng.rst.gum_steps_per_second': 2.667, 'epoch': 10.0}
{'loss': 1.5557, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.6787434816360474, 'eval_accuracy@eng.rst.gum': 0.5118659841786878, 'eval_f1@eng.rst.gum': 0.38299786137825165, 'eval_precision@eng.rst.gum': 0.47183100979974807, 'eval_recall@eng.rst.gum': 0.3951804617116094, 'eval_loss@eng.rst.gum': 1.678743600845337, 'eval_runtime': 25.5155, 'eval_samples_per_second': 84.223, 'eval_steps_per_second': 2.665, 'epoch': 10.0}
{'train@eng.rst.gum_loss': 1.4626251459121704, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5598330574944232, 'train@eng.rst.gum_f1@eng.rst.gum': 0.42426238456215826, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5126024889545121, 'train@eng.rst.gum_recall@eng.rst.gum': 0.42418251930413, 'train@eng.rst.gum_loss@eng.rst.gum': 1.46262526512146, 'train@eng.rst.gum_runtime': 163.3111, 'train@eng.rst.gum_samples_per_second': 85.095, 'train@eng.rst.gum_steps_per_second': 2.664, 'epoch': 11.0}
{'loss': 1.5497, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.6727323532104492, 'eval_accuracy@eng.rst.gum': 0.5053513261982318, 'eval_f1@eng.rst.gum': 0.3803159792959855, 'eval_precision@eng.rst.gum': 0.4503706644954893, 'eval_recall@eng.rst.gum': 0.39486581944065396, 'eval_loss@eng.rst.gum': 1.6727323532104492, 'eval_runtime': 25.5326, 'eval_samples_per_second': 84.167, 'eval_steps_per_second': 2.663, 'epoch': 11.0}
{'train@eng.rst.gum_loss': 1.4598112106323242, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5593293516586314, 'train@eng.rst.gum_f1@eng.rst.gum': 0.42291606176154123, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5153814179419044, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4218260849462941, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4598113298416138, 'train@eng.rst.gum_runtime': 163.2309, 'train@eng.rst.gum_samples_per_second': 85.137, 'train@eng.rst.gum_steps_per_second': 2.665, 'epoch': 12.0}
{'loss': 1.5351, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.6721493005752563, 'eval_accuracy@eng.rst.gum': 0.5090739879013495, 'eval_f1@eng.rst.gum': 0.384898156699493, 'eval_precision@eng.rst.gum': 0.4640088061512229, 'eval_recall@eng.rst.gum': 0.3968523641697467, 'eval_loss@eng.rst.gum': 1.6721493005752563, 'eval_runtime': 25.5527, 'eval_samples_per_second': 84.101, 'eval_steps_per_second': 2.661, 'epoch': 12.0}
{'train_runtime': 6413.4846, 'train_samples_per_second': 26.002, 'train_steps_per_second': 0.814, 'train_loss': 1.822007827466475, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =      2.294
  train_runtime            = 0:31:17.78
  train_samples_per_second =     26.201
  train_steps_per_second   =      0.824
-------------------------------------------------------------------
Lang1:  fra.sdrt.annodis    Lang2:  eng.rst.gum
Saving run to:  runs/full_shot/FullShot=v4_fra.sdrt.annodis_eng.rst.gum_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2185 examples
read 528 examples
read 625 examples
read 13897 examples
read 2149 examples
read 2091 examples
Total prediction labels:  36
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (fr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=36, bias=True)
    )
  )
)
{'train@fra.sdrt.annodis_loss': 2.6657533645629883, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.2320366132723112, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.044809012303629006, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.04022484150640783, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.06671676841819879, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.6657536029815674, 'train@fra.sdrt.annodis_runtime': 26.2804, 'train@fra.sdrt.annodis_samples_per_second': 83.142, 'train@fra.sdrt.annodis_steps_per_second': 2.626, 'epoch': 1.0}
{'loss': 3.1464, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.673563241958618, 'eval_accuracy@fra.sdrt.annodis': 0.2481060606060606, 'eval_f1@fra.sdrt.annodis': 0.04630524058567731, 'eval_precision@fra.sdrt.annodis': 0.04327246827246827, 'eval_recall@fra.sdrt.annodis': 0.07084637423855146, 'eval_loss@fra.sdrt.annodis': 2.673563003540039, 'eval_runtime': 6.6238, 'eval_samples_per_second': 79.712, 'eval_steps_per_second': 2.566, 'epoch': 1.0}
{'train@fra.sdrt.annodis_loss': 2.3816335201263428, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.28558352402745996, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.061026373599983724, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.048530080878192536, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.0851428241600757, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.3816335201263428, 'train@fra.sdrt.annodis_runtime': 26.4039, 'train@fra.sdrt.annodis_samples_per_second': 82.753, 'train@fra.sdrt.annodis_steps_per_second': 2.613, 'epoch': 2.0}
{'loss': 2.522, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.395061492919922, 'eval_accuracy@fra.sdrt.annodis': 0.2821969696969697, 'eval_f1@fra.sdrt.annodis': 0.0589551793956034, 'eval_precision@fra.sdrt.annodis': 0.0480859010270775, 'eval_recall@fra.sdrt.annodis': 0.08339266923552142, 'eval_loss@fra.sdrt.annodis': 2.395061492919922, 'eval_runtime': 6.6338, 'eval_samples_per_second': 79.592, 'eval_steps_per_second': 2.563, 'epoch': 2.0}
{'train@fra.sdrt.annodis_loss': 2.301004648208618, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.28695652173913044, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.06069685451577132, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.05474765882774194, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.08369339826966611, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.301004409790039, 'train@fra.sdrt.annodis_runtime': 26.3676, 'train@fra.sdrt.annodis_samples_per_second': 82.867, 'train@fra.sdrt.annodis_steps_per_second': 2.617, 'epoch': 3.0}
{'loss': 2.3621, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.3207485675811768, 'eval_accuracy@fra.sdrt.annodis': 0.2689393939393939, 'eval_f1@fra.sdrt.annodis': 0.05414525550479713, 'eval_precision@fra.sdrt.annodis': 0.04689708048892264, 'eval_recall@fra.sdrt.annodis': 0.07790795295122051, 'eval_loss@fra.sdrt.annodis': 2.320748805999756, 'eval_runtime': 6.6323, 'eval_samples_per_second': 79.61, 'eval_steps_per_second': 2.563, 'epoch': 3.0}
{'train@fra.sdrt.annodis_loss': 2.243929147720337, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.30160183066361557, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.06570459191918893, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.09106427166910697, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.08839202623413543, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.243929147720337, 'train@fra.sdrt.annodis_runtime': 26.3851, 'train@fra.sdrt.annodis_samples_per_second': 82.812, 'train@fra.sdrt.annodis_steps_per_second': 2.615, 'epoch': 4.0}
{'loss': 2.3145, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.2686612606048584, 'eval_accuracy@fra.sdrt.annodis': 0.2803030303030303, 'eval_f1@fra.sdrt.annodis': 0.05798006328810956, 'eval_precision@fra.sdrt.annodis': 0.04950393239625168, 'eval_recall@fra.sdrt.annodis': 0.08122556616845299, 'eval_loss@fra.sdrt.annodis': 2.2686612606048584, 'eval_runtime': 6.6611, 'eval_samples_per_second': 79.267, 'eval_steps_per_second': 2.552, 'epoch': 4.0}
{'train@fra.sdrt.annodis_loss': 2.1945009231567383, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.34324942791762014, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.08829709282223336, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12751730623038202, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.11070424384142252, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.194500684738159, 'train@fra.sdrt.annodis_runtime': 26.4023, 'train@fra.sdrt.annodis_samples_per_second': 82.758, 'train@fra.sdrt.annodis_steps_per_second': 2.613, 'epoch': 5.0}
{'loss': 2.2535, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.230567216873169, 'eval_accuracy@fra.sdrt.annodis': 0.3106060606060606, 'eval_f1@fra.sdrt.annodis': 0.0699244121226999, 'eval_precision@fra.sdrt.annodis': 0.10701795507475022, 'eval_recall@fra.sdrt.annodis': 0.0956143874210374, 'eval_loss@fra.sdrt.annodis': 2.23056697845459, 'eval_runtime': 6.6364, 'eval_samples_per_second': 79.562, 'eval_steps_per_second': 2.562, 'epoch': 5.0}
{'train@fra.sdrt.annodis_loss': 2.148251533508301, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.37894736842105264, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.11255783478825526, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.1310293677240875, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1372376764135031, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.14825177192688, 'train@fra.sdrt.annodis_runtime': 26.408, 'train@fra.sdrt.annodis_samples_per_second': 82.74, 'train@fra.sdrt.annodis_steps_per_second': 2.613, 'epoch': 6.0}
{'loss': 2.2057, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.191166639328003, 'eval_accuracy@fra.sdrt.annodis': 0.32765151515151514, 'eval_f1@fra.sdrt.annodis': 0.08293410017887783, 'eval_precision@fra.sdrt.annodis': 0.10403019232384311, 'eval_recall@fra.sdrt.annodis': 0.1065266884006543, 'eval_loss@fra.sdrt.annodis': 2.191166639328003, 'eval_runtime': 6.6638, 'eval_samples_per_second': 79.234, 'eval_steps_per_second': 2.551, 'epoch': 6.0}
{'train@fra.sdrt.annodis_loss': 2.104454278945923, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4045766590389016, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.12478441244496298, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.1307614397345205, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1529686520924749, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.104454517364502, 'train@fra.sdrt.annodis_runtime': 26.4353, 'train@fra.sdrt.annodis_samples_per_second': 82.654, 'train@fra.sdrt.annodis_steps_per_second': 2.61, 'epoch': 7.0}
{'loss': 2.1678, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.1566505432128906, 'eval_accuracy@fra.sdrt.annodis': 0.3465909090909091, 'eval_f1@fra.sdrt.annodis': 0.09390265719714871, 'eval_precision@fra.sdrt.annodis': 0.11012031965520339, 'eval_recall@fra.sdrt.annodis': 0.11718041524459553, 'eval_loss@fra.sdrt.annodis': 2.1566505432128906, 'eval_runtime': 6.6492, 'eval_samples_per_second': 79.408, 'eval_steps_per_second': 2.557, 'epoch': 7.0}
{'train@fra.sdrt.annodis_loss': 2.0664854049682617, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4073226544622426, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.12630072461002978, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12753436781102362, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.15709384669571824, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.0664854049682617, 'train@fra.sdrt.annodis_runtime': 26.4117, 'train@fra.sdrt.annodis_samples_per_second': 82.728, 'train@fra.sdrt.annodis_steps_per_second': 2.612, 'epoch': 8.0}
{'loss': 2.1351, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.1264147758483887, 'eval_accuracy@fra.sdrt.annodis': 0.3484848484848485, 'eval_f1@fra.sdrt.annodis': 0.09854892136929447, 'eval_precision@fra.sdrt.annodis': 0.10399666973567558, 'eval_recall@fra.sdrt.annodis': 0.12169566704981485, 'eval_loss@fra.sdrt.annodis': 2.1264147758483887, 'eval_runtime': 6.6601, 'eval_samples_per_second': 79.278, 'eval_steps_per_second': 2.553, 'epoch': 8.0}
{'train@fra.sdrt.annodis_loss': 2.037445306777954, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.41418764302059496, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.128438064856747, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.1279684469772356, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1605035654602216, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.037445068359375, 'train@fra.sdrt.annodis_runtime': 26.4087, 'train@fra.sdrt.annodis_samples_per_second': 82.738, 'train@fra.sdrt.annodis_steps_per_second': 2.613, 'epoch': 9.0}
{'loss': 2.0983, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.102264165878296, 'eval_accuracy@fra.sdrt.annodis': 0.3446969696969697, 'eval_f1@fra.sdrt.annodis': 0.09546712668863894, 'eval_precision@fra.sdrt.annodis': 0.09630271363625277, 'eval_recall@fra.sdrt.annodis': 0.11991259588437878, 'eval_loss@fra.sdrt.annodis': 2.102264165878296, 'eval_runtime': 6.6493, 'eval_samples_per_second': 79.407, 'eval_steps_per_second': 2.557, 'epoch': 9.0}
{'train@fra.sdrt.annodis_loss': 2.0160086154937744, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4196796338672769, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.13155577846137242, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.1286457464897868, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.16314509031973426, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.0160086154937744, 'train@fra.sdrt.annodis_runtime': 26.4549, 'train@fra.sdrt.annodis_samples_per_second': 82.593, 'train@fra.sdrt.annodis_steps_per_second': 2.608, 'epoch': 10.0}
{'loss': 2.0675, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.085364580154419, 'eval_accuracy@fra.sdrt.annodis': 0.3541666666666667, 'eval_f1@fra.sdrt.annodis': 0.10175829602825928, 'eval_precision@fra.sdrt.annodis': 0.10230124830706616, 'eval_recall@fra.sdrt.annodis': 0.12498175304976655, 'eval_loss@fra.sdrt.annodis': 2.0853641033172607, 'eval_runtime': 6.6534, 'eval_samples_per_second': 79.358, 'eval_steps_per_second': 2.555, 'epoch': 10.0}
{'train@fra.sdrt.annodis_loss': 2.003427505493164, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4242562929061785, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.13358059442617903, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.1291560741704244, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1661472218500916, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.003427505493164, 'train@fra.sdrt.annodis_runtime': 26.4499, 'train@fra.sdrt.annodis_samples_per_second': 82.609, 'train@fra.sdrt.annodis_steps_per_second': 2.609, 'epoch': 11.0}
{'loss': 2.0491, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.074658155441284, 'eval_accuracy@fra.sdrt.annodis': 0.3541666666666667, 'eval_f1@fra.sdrt.annodis': 0.1030545828785071, 'eval_precision@fra.sdrt.annodis': 0.1008531665908635, 'eval_recall@fra.sdrt.annodis': 0.12682246844546918, 'eval_loss@fra.sdrt.annodis': 2.074657917022705, 'eval_runtime': 6.6627, 'eval_samples_per_second': 79.248, 'eval_steps_per_second': 2.552, 'epoch': 11.0}
{'train@fra.sdrt.annodis_loss': 1.9990354776382446, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4260869565217391, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.13460563784717566, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.1290278993417026, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.16703870418212272, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.9990354776382446, 'train@fra.sdrt.annodis_runtime': 26.4102, 'train@fra.sdrt.annodis_samples_per_second': 82.733, 'train@fra.sdrt.annodis_steps_per_second': 2.613, 'epoch': 12.0}
{'loss': 2.0418, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.071370840072632, 'eval_accuracy@fra.sdrt.annodis': 0.3522727272727273, 'eval_f1@fra.sdrt.annodis': 0.10262968276766618, 'eval_precision@fra.sdrt.annodis': 0.10041077825731291, 'eval_recall@fra.sdrt.annodis': 0.12630325764588454, 'eval_loss@fra.sdrt.annodis': 2.071370840072632, 'eval_runtime': 6.6441, 'eval_samples_per_second': 79.469, 'eval_steps_per_second': 2.559, 'epoch': 12.0}
{'train_runtime': 1060.7264, 'train_samples_per_second': 24.719, 'train_steps_per_second': 0.781, 'train_loss': 2.2803175184461804, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.2803
  train_runtime            = 0:17:40.72
  train_samples_per_second =     24.719
  train_steps_per_second   =      0.781
{'train@eng.rst.gum_loss': 2.31868577003479, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.311937828308268, 'train@eng.rst.gum_f1@eng.rst.gum': 0.06890376714317258, 'train@eng.rst.gum_precision@eng.rst.gum': 0.17338865871115353, 'train@eng.rst.gum_recall@eng.rst.gum': 0.09485387880466221, 'train@eng.rst.gum_loss@eng.rst.gum': 2.31868577003479, 'train@eng.rst.gum_runtime': 163.4161, 'train@eng.rst.gum_samples_per_second': 85.041, 'train@eng.rst.gum_steps_per_second': 2.662, 'epoch': 1.0}
{'loss': 2.699, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.399423122406006, 'eval_accuracy@eng.rst.gum': 0.29874360167519776, 'eval_f1@eng.rst.gum': 0.0675897809084024, 'eval_precision@eng.rst.gum': 0.08080977153981266, 'eval_recall@eng.rst.gum': 0.09919445364324966, 'eval_loss@eng.rst.gum': 2.399423360824585, 'eval_runtime': 25.5681, 'eval_samples_per_second': 84.05, 'eval_steps_per_second': 2.66, 'epoch': 1.0}
{'train@eng.rst.gum_loss': 1.9427049160003662, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.4291573720946967, 'train@eng.rst.gum_f1@eng.rst.gum': 0.19995982894514794, 'train@eng.rst.gum_precision@eng.rst.gum': 0.2583804504434545, 'train@eng.rst.gum_recall@eng.rst.gum': 0.21245287996471718, 'train@eng.rst.gum_loss@eng.rst.gum': 1.9427049160003662, 'train@eng.rst.gum_runtime': 163.0917, 'train@eng.rst.gum_samples_per_second': 85.21, 'train@eng.rst.gum_steps_per_second': 2.667, 'epoch': 2.0}
{'loss': 2.1767, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.0640475749969482, 'eval_accuracy@eng.rst.gum': 0.39041414611447184, 'eval_f1@eng.rst.gum': 0.18525069917589526, 'eval_precision@eng.rst.gum': 0.23505116404522594, 'eval_recall@eng.rst.gum': 0.20444447933265614, 'eval_loss@eng.rst.gum': 2.0640475749969482, 'eval_runtime': 25.5695, 'eval_samples_per_second': 84.046, 'eval_steps_per_second': 2.659, 'epoch': 2.0}
{'train@eng.rst.gum_loss': 1.7275954484939575, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.4956465424192272, 'train@eng.rst.gum_f1@eng.rst.gum': 0.2905313163562381, 'train@eng.rst.gum_precision@eng.rst.gum': 0.41394680761036595, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3005554557782033, 'train@eng.rst.gum_loss@eng.rst.gum': 1.7275956869125366, 'train@eng.rst.gum_runtime': 163.5037, 'train@eng.rst.gum_samples_per_second': 84.995, 'train@eng.rst.gum_steps_per_second': 2.66, 'epoch': 3.0}
{'loss': 1.9009, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.8761616945266724, 'eval_accuracy@eng.rst.gum': 0.45556072591903213, 'eval_f1@eng.rst.gum': 0.2740252977409886, 'eval_precision@eng.rst.gum': 0.3259940577702726, 'eval_recall@eng.rst.gum': 0.2886716747172862, 'eval_loss@eng.rst.gum': 1.8761615753173828, 'eval_runtime': 25.6169, 'eval_samples_per_second': 83.89, 'eval_steps_per_second': 2.654, 'epoch': 3.0}
{'train@eng.rst.gum_loss': 1.6201515197753906, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5192487587249046, 'train@eng.rst.gum_f1@eng.rst.gum': 0.32304230782737386, 'train@eng.rst.gum_precision@eng.rst.gum': 0.47106238474845286, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3295152299187852, 'train@eng.rst.gum_loss@eng.rst.gum': 1.620151400566101, 'train@eng.rst.gum_runtime': 163.4319, 'train@eng.rst.gum_samples_per_second': 85.032, 'train@eng.rst.gum_steps_per_second': 2.662, 'epoch': 4.0}
{'loss': 1.7488, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.793314814567566, 'eval_accuracy@eng.rst.gum': 0.4755700325732899, 'eval_f1@eng.rst.gum': 0.2928896923348878, 'eval_precision@eng.rst.gum': 0.3251362326424339, 'eval_recall@eng.rst.gum': 0.30879460510230833, 'eval_loss@eng.rst.gum': 1.793314814567566, 'eval_runtime': 25.5218, 'eval_samples_per_second': 84.202, 'eval_steps_per_second': 2.664, 'epoch': 4.0}
{'train@eng.rst.gum_loss': 1.5526769161224365, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5375260847664963, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3855576655270108, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5627734734830798, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3851568724657181, 'train@eng.rst.gum_loss@eng.rst.gum': 1.552677035331726, 'train@eng.rst.gum_runtime': 163.1488, 'train@eng.rst.gum_samples_per_second': 85.18, 'train@eng.rst.gum_steps_per_second': 2.666, 'epoch': 5.0}
{'loss': 1.6606, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.7382583618164062, 'eval_accuracy@eng.rst.gum': 0.49092601209865055, 'eval_f1@eng.rst.gum': 0.3443885497101967, 'eval_precision@eng.rst.gum': 0.36190846554100853, 'eval_recall@eng.rst.gum': 0.35906548199042415, 'eval_loss@eng.rst.gum': 1.7382584810256958, 'eval_runtime': 25.5768, 'eval_samples_per_second': 84.022, 'eval_steps_per_second': 2.659, 'epoch': 5.0}
{'train@eng.rst.gum_loss': 1.5072708129882812, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5511981003094193, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4064668477838155, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5446825188886187, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4032393509143378, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5072708129882812, 'train@eng.rst.gum_runtime': 163.2468, 'train@eng.rst.gum_samples_per_second': 85.129, 'train@eng.rst.gum_steps_per_second': 2.665, 'epoch': 6.0}
{'loss': 1.6097, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.706325888633728, 'eval_accuracy@eng.rst.gum': 0.50814332247557, 'eval_f1@eng.rst.gum': 0.3733044901939677, 'eval_precision@eng.rst.gum': 0.45201029145104027, 'eval_recall@eng.rst.gum': 0.3819739650887678, 'eval_loss@eng.rst.gum': 1.7063260078430176, 'eval_runtime': 25.5726, 'eval_samples_per_second': 84.035, 'eval_steps_per_second': 2.659, 'epoch': 6.0}
{'train@eng.rst.gum_loss': 1.4744162559509277, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5599769734475066, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4199074202357347, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5403789323266854, 'train@eng.rst.gum_recall@eng.rst.gum': 0.41743900160067954, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4744162559509277, 'train@eng.rst.gum_runtime': 163.477, 'train@eng.rst.gum_samples_per_second': 85.009, 'train@eng.rst.gum_steps_per_second': 2.661, 'epoch': 7.0}
{'loss': 1.5652, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.6865873336791992, 'eval_accuracy@eng.rst.gum': 0.5118659841786878, 'eval_f1@eng.rst.gum': 0.3858157682671389, 'eval_precision@eng.rst.gum': 0.4934183756205293, 'eval_recall@eng.rst.gum': 0.39326338782697734, 'eval_loss@eng.rst.gum': 1.6865874528884888, 'eval_runtime': 25.5752, 'eval_samples_per_second': 84.027, 'eval_steps_per_second': 2.659, 'epoch': 7.0}
{'train@eng.rst.gum_loss': 1.452047348022461, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5658775275239261, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4283480574927889, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5370231193104403, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4247257857957979, 'train@eng.rst.gum_loss@eng.rst.gum': 1.452047348022461, 'train@eng.rst.gum_runtime': 163.0788, 'train@eng.rst.gum_samples_per_second': 85.216, 'train@eng.rst.gum_steps_per_second': 2.667, 'epoch': 8.0}
{'loss': 1.5377, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.6727180480957031, 'eval_accuracy@eng.rst.gum': 0.5104699860400186, 'eval_f1@eng.rst.gum': 0.3878263515382205, 'eval_precision@eng.rst.gum': 0.48220833857174566, 'eval_recall@eng.rst.gum': 0.3948817094129967, 'eval_loss@eng.rst.gum': 1.6727180480957031, 'eval_runtime': 25.5853, 'eval_samples_per_second': 83.993, 'eval_steps_per_second': 2.658, 'epoch': 8.0}
{'train@eng.rst.gum_loss': 1.4337691068649292, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5706267539756782, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4377936242167753, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5297533969894134, 'train@eng.rst.gum_recall@eng.rst.gum': 0.43650455812198385, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4337689876556396, 'train@eng.rst.gum_runtime': 163.4041, 'train@eng.rst.gum_samples_per_second': 85.047, 'train@eng.rst.gum_steps_per_second': 2.662, 'epoch': 9.0}
{'loss': 1.5121, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.6572242975234985, 'eval_accuracy@eng.rst.gum': 0.5132619823173569, 'eval_f1@eng.rst.gum': 0.3975518733122776, 'eval_precision@eng.rst.gum': 0.48375639320574626, 'eval_recall@eng.rst.gum': 0.40875051406275215, 'eval_loss@eng.rst.gum': 1.6572242975234985, 'eval_runtime': 25.6566, 'eval_samples_per_second': 83.76, 'eval_steps_per_second': 2.65, 'epoch': 9.0}
{'train@eng.rst.gum_loss': 1.4197344779968262, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5753759804274304, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4414430720025159, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5330014480214645, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4368933473820214, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4197344779968262, 'train@eng.rst.gum_runtime': 163.4145, 'train@eng.rst.gum_samples_per_second': 85.041, 'train@eng.rst.gum_steps_per_second': 2.662, 'epoch': 10.0}
{'loss': 1.5006, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.6485358476638794, 'eval_accuracy@eng.rst.gum': 0.5165193113075849, 'eval_f1@eng.rst.gum': 0.4012947853471509, 'eval_precision@eng.rst.gum': 0.49336010312046563, 'eval_recall@eng.rst.gum': 0.40846558812766176, 'eval_loss@eng.rst.gum': 1.648535966873169, 'eval_runtime': 25.6158, 'eval_samples_per_second': 83.894, 'eval_steps_per_second': 2.655, 'epoch': 10.0}
{'train@eng.rst.gum_loss': 1.4135915040969849, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5751601064978052, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4430886472246098, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5249038273912873, 'train@eng.rst.gum_recall@eng.rst.gum': 0.44154579260874627, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4135915040969849, 'train@eng.rst.gum_runtime': 163.0504, 'train@eng.rst.gum_samples_per_second': 85.231, 'train@eng.rst.gum_steps_per_second': 2.668, 'epoch': 11.0}
{'loss': 1.4962, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.6441304683685303, 'eval_accuracy@eng.rst.gum': 0.5202419730107026, 'eval_f1@eng.rst.gum': 0.4085262371584754, 'eval_precision@eng.rst.gum': 0.499045754172659, 'eval_recall@eng.rst.gum': 0.4164072820379489, 'eval_loss@eng.rst.gum': 1.6441305875778198, 'eval_runtime': 25.5477, 'eval_samples_per_second': 84.117, 'eval_steps_per_second': 2.662, 'epoch': 11.0}
{'train@eng.rst.gum_loss': 1.4104121923446655, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5762394761459307, 'train@eng.rst.gum_f1@eng.rst.gum': 0.44504431687494356, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5307910979483164, 'train@eng.rst.gum_recall@eng.rst.gum': 0.441860804778479, 'train@eng.rst.gum_loss@eng.rst.gum': 1.410412311553955, 'train@eng.rst.gum_runtime': 163.1943, 'train@eng.rst.gum_samples_per_second': 85.156, 'train@eng.rst.gum_steps_per_second': 2.666, 'epoch': 12.0}
{'loss': 1.4813, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.642825722694397, 'eval_accuracy@eng.rst.gum': 0.5197766402978129, 'eval_f1@eng.rst.gum': 0.40582632594348095, 'eval_precision@eng.rst.gum': 0.49767338933235966, 'eval_recall@eng.rst.gum': 0.41305598564297524, 'eval_loss@eng.rst.gum': 1.6428256034851074, 'eval_runtime': 25.5874, 'eval_samples_per_second': 83.987, 'eval_steps_per_second': 2.658, 'epoch': 12.0}
{'train_runtime': 6412.1429, 'train_samples_per_second': 26.008, 'train_steps_per_second': 0.814, 'train_loss': 1.7407386516702585, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.2803
  train_runtime            = 0:17:40.72
  train_samples_per_second =     24.719
  train_steps_per_second   =      0.781
-------------------------------------------------------------------
Lang1:  nld.rst.nldt    Lang2:  eng.rst.gum
Saving run to:  runs/full_shot/FullShot=v4_nld.rst.nldt_eng.rst.gum_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 1608 examples
read 331 examples
read 326 examples
read 13897 examples
read 2149 examples
read 2091 examples
Total prediction labels:  38
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=38, bias=True)
    )
  )
)
{'train@nld.rst.nldt_loss': 3.1696102619171143, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26430348258706465, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01306566650270536, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.00825948383084577, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03125, 'train@nld.rst.nldt_loss@nld.rst.nldt': 3.1696102619171143, 'train@nld.rst.nldt_runtime': 19.4307, 'train@nld.rst.nldt_samples_per_second': 82.755, 'train@nld.rst.nldt_steps_per_second': 2.625, 'epoch': 1.0}
{'loss': 3.4459, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.13219952583313, 'eval_accuracy@nld.rst.nldt': 0.27794561933534745, 'eval_f1@nld.rst.nldt': 0.016110673321075214, 'eval_precision@nld.rst.nldt': 0.010294282197605462, 'eval_recall@nld.rst.nldt': 0.037037037037037035, 'eval_loss@nld.rst.nldt': 3.132200241088867, 'eval_runtime': 4.2905, 'eval_samples_per_second': 77.147, 'eval_steps_per_second': 2.564, 'epoch': 1.0}
{'train@nld.rst.nldt_loss': 2.870323657989502, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26430348258706465, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01306566650270536, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.00825948383084577, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03125, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.870323657989502, 'train@nld.rst.nldt_runtime': 19.5082, 'train@nld.rst.nldt_samples_per_second': 82.427, 'train@nld.rst.nldt_steps_per_second': 2.614, 'epoch': 2.0}
{'loss': 3.0121, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.80051589012146, 'eval_accuracy@nld.rst.nldt': 0.27794561933534745, 'eval_f1@nld.rst.nldt': 0.016110673321075214, 'eval_precision@nld.rst.nldt': 0.010294282197605462, 'eval_recall@nld.rst.nldt': 0.037037037037037035, 'eval_loss@nld.rst.nldt': 2.80051589012146, 'eval_runtime': 4.2926, 'eval_samples_per_second': 77.109, 'eval_steps_per_second': 2.563, 'epoch': 2.0}
{'train@nld.rst.nldt_loss': 2.77984619140625, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26616915422885573, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01572634879925421, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.022917980243799915, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.032518090569561155, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.77984619140625, 'train@nld.rst.nldt_runtime': 19.5267, 'train@nld.rst.nldt_samples_per_second': 82.349, 'train@nld.rst.nldt_steps_per_second': 2.612, 'epoch': 3.0}
{'loss': 2.849, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.7152984142303467, 'eval_accuracy@nld.rst.nldt': 0.28700906344410876, 'eval_f1@nld.rst.nldt': 0.025360060447779748, 'eval_precision@nld.rst.nldt': 0.053662046504582284, 'eval_recall@nld.rst.nldt': 0.042210464432686653, 'eval_loss@nld.rst.nldt': 2.7152981758117676, 'eval_runtime': 4.3169, 'eval_samples_per_second': 76.676, 'eval_steps_per_second': 2.548, 'epoch': 3.0}
{'train@nld.rst.nldt_loss': 2.7331461906433105, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2810945273631841, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.02532836377698615, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.02564651337197478, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.04074346405228758, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.7331461906433105, 'train@nld.rst.nldt_runtime': 19.5438, 'train@nld.rst.nldt_samples_per_second': 82.277, 'train@nld.rst.nldt_steps_per_second': 2.61, 'epoch': 4.0}
{'loss': 2.7599, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.677098512649536, 'eval_accuracy@nld.rst.nldt': 0.29607250755287007, 'eval_f1@nld.rst.nldt': 0.0316047258165933, 'eval_precision@nld.rst.nldt': 0.03556352050975707, 'eval_recall@nld.rst.nldt': 0.049038928507527536, 'eval_loss@nld.rst.nldt': 2.6770989894866943, 'eval_runtime': 4.317, 'eval_samples_per_second': 76.673, 'eval_steps_per_second': 2.548, 'epoch': 4.0}
{'train@nld.rst.nldt_loss': 2.698638677597046, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2916666666666667, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.030253520944919066, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.026318995328420004, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.0464735060690943, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.698639154434204, 'train@nld.rst.nldt_runtime': 19.5408, 'train@nld.rst.nldt_samples_per_second': 82.289, 'train@nld.rst.nldt_steps_per_second': 2.61, 'epoch': 5.0}
{'loss': 2.735, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.646167516708374, 'eval_accuracy@nld.rst.nldt': 0.311178247734139, 'eval_f1@nld.rst.nldt': 0.03828072066934851, 'eval_precision@nld.rst.nldt': 0.03904991948470209, 'eval_recall@nld.rst.nldt': 0.05832758224062572, 'eval_loss@nld.rst.nldt': 2.646167516708374, 'eval_runtime': 4.2843, 'eval_samples_per_second': 77.259, 'eval_steps_per_second': 2.568, 'epoch': 5.0}
{'train@nld.rst.nldt_loss': 2.666987657546997, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3009950248756219, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.033875327636773894, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.02799549574244918, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.051999299719887954, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.666987657546997, 'train@nld.rst.nldt_runtime': 19.5599, 'train@nld.rst.nldt_samples_per_second': 82.209, 'train@nld.rst.nldt_steps_per_second': 2.607, 'epoch': 6.0}
{'loss': 2.7097, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.620929479598999, 'eval_accuracy@nld.rst.nldt': 0.311178247734139, 'eval_f1@nld.rst.nldt': 0.03700645950387953, 'eval_precision@nld.rst.nldt': 0.03390852974186308, 'eval_recall@nld.rst.nldt': 0.05998261891981699, 'eval_loss@nld.rst.nldt': 2.620929479598999, 'eval_runtime': 4.3117, 'eval_samples_per_second': 76.768, 'eval_steps_per_second': 2.551, 'epoch': 6.0}
{'train@nld.rst.nldt_loss': 2.640282154083252, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30597014925373134, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03566872240264378, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.027976767704754515, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05555497198879551, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.640282392501831, 'train@nld.rst.nldt_runtime': 19.544, 'train@nld.rst.nldt_samples_per_second': 82.276, 'train@nld.rst.nldt_steps_per_second': 2.609, 'epoch': 7.0}
{'loss': 2.6798, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.5982961654663086, 'eval_accuracy@nld.rst.nldt': 0.31722054380664655, 'eval_f1@nld.rst.nldt': 0.03993421993421994, 'eval_precision@nld.rst.nldt': 0.036418584430701055, 'eval_recall@nld.rst.nldt': 0.06275464560488715, 'eval_loss@nld.rst.nldt': 2.5982961654663086, 'eval_runtime': 4.2878, 'eval_samples_per_second': 77.196, 'eval_steps_per_second': 2.565, 'epoch': 7.0}
{'train@nld.rst.nldt_loss': 2.618635416030884, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30970149253731344, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.036328341807038145, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.027624675350914896, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05814950980392157, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.618635416030884, 'train@nld.rst.nldt_runtime': 19.5253, 'train@nld.rst.nldt_samples_per_second': 82.355, 'train@nld.rst.nldt_steps_per_second': 2.612, 'epoch': 8.0}
{'loss': 2.6573, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.5809338092803955, 'eval_accuracy@nld.rst.nldt': 0.3293051359516616, 'eval_f1@nld.rst.nldt': 0.04319343223761945, 'eval_precision@nld.rst.nldt': 0.03721595992127393, 'eval_recall@nld.rst.nldt': 0.06998568616926104, 'eval_loss@nld.rst.nldt': 2.5809338092803955, 'eval_runtime': 4.3211, 'eval_samples_per_second': 76.602, 'eval_steps_per_second': 2.546, 'epoch': 8.0}
{'train@nld.rst.nldt_loss': 2.6054930686950684, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30845771144278605, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.0360741078048132, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.027191764534593387, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05765289449112978, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6054933071136475, 'train@nld.rst.nldt_runtime': 19.5447, 'train@nld.rst.nldt_samples_per_second': 82.273, 'train@nld.rst.nldt_steps_per_second': 2.609, 'epoch': 9.0}
{'loss': 2.6352, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.5703630447387695, 'eval_accuracy@nld.rst.nldt': 0.32628398791540786, 'eval_f1@nld.rst.nldt': 0.04259358615722154, 'eval_precision@nld.rst.nldt': 0.03732302235758075, 'eval_recall@nld.rst.nldt': 0.06692866089001355, 'eval_loss@nld.rst.nldt': 2.5703630447387695, 'eval_runtime': 4.2969, 'eval_samples_per_second': 77.031, 'eval_steps_per_second': 2.56, 'epoch': 9.0}
{'train@nld.rst.nldt_loss': 2.595953941345215, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30970149253731344, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03665908381841251, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.0278105599442292, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.058265639589169, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.5959537029266357, 'train@nld.rst.nldt_runtime': 19.5782, 'train@nld.rst.nldt_samples_per_second': 82.132, 'train@nld.rst.nldt_steps_per_second': 2.605, 'epoch': 10.0}
{'loss': 2.6273, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.5624427795410156, 'eval_accuracy@nld.rst.nldt': 0.32628398791540786, 'eval_f1@nld.rst.nldt': 0.04259358615722154, 'eval_precision@nld.rst.nldt': 0.03732302235758075, 'eval_recall@nld.rst.nldt': 0.06692866089001355, 'eval_loss@nld.rst.nldt': 2.5624427795410156, 'eval_runtime': 4.3186, 'eval_samples_per_second': 76.645, 'eval_steps_per_second': 2.547, 'epoch': 10.0}
{'train@nld.rst.nldt_loss': 2.589461088180542, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30970149253731344, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03647810466580373, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.027470455019662762, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05844012605042016, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.589460849761963, 'train@nld.rst.nldt_runtime': 19.579, 'train@nld.rst.nldt_samples_per_second': 82.129, 'train@nld.rst.nldt_steps_per_second': 2.605, 'epoch': 11.0}
{'loss': 2.6127, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.5573360919952393, 'eval_accuracy@nld.rst.nldt': 0.3323262839879154, 'eval_f1@nld.rst.nldt': 0.044435296303959655, 'eval_precision@nld.rst.nldt': 0.03845362197764944, 'eval_recall@nld.rst.nldt': 0.0710438872274621, 'eval_loss@nld.rst.nldt': 2.5573363304138184, 'eval_runtime': 4.2828, 'eval_samples_per_second': 77.287, 'eval_steps_per_second': 2.568, 'epoch': 11.0}
{'train@nld.rst.nldt_loss': 2.586643695831299, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31094527363184077, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.036756048031357125, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.027618740760478773, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05899451447245564, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.586643695831299, 'train@nld.rst.nldt_runtime': 19.5318, 'train@nld.rst.nldt_samples_per_second': 82.327, 'train@nld.rst.nldt_steps_per_second': 2.611, 'epoch': 12.0}
{'loss': 2.617, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.5555357933044434, 'eval_accuracy@nld.rst.nldt': 0.3323262839879154, 'eval_f1@nld.rst.nldt': 0.04393544205750778, 'eval_precision@nld.rst.nldt': 0.037379125005867715, 'eval_recall@nld.rst.nldt': 0.0710438872274621, 'eval_loss@nld.rst.nldt': 2.5555357933044434, 'eval_runtime': 4.3096, 'eval_samples_per_second': 76.805, 'eval_steps_per_second': 2.552, 'epoch': 12.0}
{'train_runtime': 775.006, 'train_samples_per_second': 24.898, 'train_steps_per_second': 0.79, 'train_loss': 2.7784006704691966, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.7784
  train_runtime            = 0:12:55.00
  train_samples_per_second =     24.898
  train_steps_per_second   =       0.79
{'train@eng.rst.gum_loss': 2.5064914226531982, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.2574656400662013, 'train@eng.rst.gum_f1@eng.rst.gum': 0.04670537242962207, 'train@eng.rst.gum_precision@eng.rst.gum': 0.06533274640234271, 'train@eng.rst.gum_recall@eng.rst.gum': 0.0633597424806999, 'train@eng.rst.gum_loss@eng.rst.gum': 2.5064916610717773, 'train@eng.rst.gum_runtime': 163.2407, 'train@eng.rst.gum_samples_per_second': 85.132, 'train@eng.rst.gum_steps_per_second': 2.665, 'epoch': 1.0}
{'loss': 2.8123, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.5854179859161377, 'eval_accuracy@eng.rst.gum': 0.2447650069799907, 'eval_f1@eng.rst.gum': 0.044525017238660425, 'eval_precision@eng.rst.gum': 0.048566771104689224, 'eval_recall@eng.rst.gum': 0.06412369583850829, 'eval_loss@eng.rst.gum': 2.585418224334717, 'eval_runtime': 25.5751, 'eval_samples_per_second': 84.027, 'eval_steps_per_second': 2.659, 'epoch': 1.0}
{'train@eng.rst.gum_loss': 2.0366804599761963, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.4059149456717277, 'train@eng.rst.gum_f1@eng.rst.gum': 0.1724499768667866, 'train@eng.rst.gum_precision@eng.rst.gum': 0.2515686443496058, 'train@eng.rst.gum_recall@eng.rst.gum': 0.185492549202108, 'train@eng.rst.gum_loss@eng.rst.gum': 2.0366804599761963, 'train@eng.rst.gum_runtime': 163.4033, 'train@eng.rst.gum_samples_per_second': 85.047, 'train@eng.rst.gum_steps_per_second': 2.662, 'epoch': 2.0}
{'loss': 2.3305, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.1368260383605957, 'eval_accuracy@eng.rst.gum': 0.3755234993020009, 'eval_f1@eng.rst.gum': 0.1611290258785662, 'eval_precision@eng.rst.gum': 0.23878730945442997, 'eval_recall@eng.rst.gum': 0.17786023270724827, 'eval_loss@eng.rst.gum': 2.1368257999420166, 'eval_runtime': 27.0786, 'eval_samples_per_second': 79.362, 'eval_steps_per_second': 2.511, 'epoch': 2.0}
{'train@eng.rst.gum_loss': 1.7861682176589966, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.4854285097503058, 'train@eng.rst.gum_f1@eng.rst.gum': 0.27638031224768506, 'train@eng.rst.gum_precision@eng.rst.gum': 0.36068520690664935, 'train@eng.rst.gum_recall@eng.rst.gum': 0.2887467375971455, 'train@eng.rst.gum_loss@eng.rst.gum': 1.7861682176589966, 'train@eng.rst.gum_runtime': 163.3701, 'train@eng.rst.gum_samples_per_second': 85.065, 'train@eng.rst.gum_steps_per_second': 2.663, 'epoch': 3.0}
{'loss': 1.9785, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.919874668121338, 'eval_accuracy@eng.rst.gum': 0.4532340623545835, 'eval_f1@eng.rst.gum': 0.26148005853255635, 'eval_precision@eng.rst.gum': 0.3090583910415412, 'eval_recall@eng.rst.gum': 0.2792018396823933, 'eval_loss@eng.rst.gum': 1.9198744297027588, 'eval_runtime': 25.5998, 'eval_samples_per_second': 83.946, 'eval_steps_per_second': 2.656, 'epoch': 3.0}
{'train@eng.rst.gum_loss': 1.6689907312393188, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5080952723609412, 'train@eng.rst.gum_f1@eng.rst.gum': 0.30557877774645925, 'train@eng.rst.gum_precision@eng.rst.gum': 0.4051975395171496, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3173109994393792, 'train@eng.rst.gum_loss@eng.rst.gum': 1.6689906120300293, 'train@eng.rst.gum_runtime': 163.2944, 'train@eng.rst.gum_samples_per_second': 85.104, 'train@eng.rst.gum_steps_per_second': 2.664, 'epoch': 4.0}
{'loss': 1.8041, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.8355255126953125, 'eval_accuracy@eng.rst.gum': 0.47650069799906936, 'eval_f1@eng.rst.gum': 0.29012656283521915, 'eval_precision@eng.rst.gum': 0.3635342493655463, 'eval_recall@eng.rst.gum': 0.30920928174265166, 'eval_loss@eng.rst.gum': 1.8355257511138916, 'eval_runtime': 25.6093, 'eval_samples_per_second': 83.915, 'eval_steps_per_second': 2.655, 'epoch': 4.0}
{'train@eng.rst.gum_loss': 1.5927855968475342, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5232064474346981, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3461809840160077, 'train@eng.rst.gum_precision@eng.rst.gum': 0.4681714382696897, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3572454038782987, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5927857160568237, 'train@eng.rst.gum_runtime': 163.3452, 'train@eng.rst.gum_samples_per_second': 85.078, 'train@eng.rst.gum_steps_per_second': 2.663, 'epoch': 5.0}
{'loss': 1.7026, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.775578260421753, 'eval_accuracy@eng.rst.gum': 0.48348068869241506, 'eval_f1@eng.rst.gum': 0.3132105883017503, 'eval_precision@eng.rst.gum': 0.368849103192455, 'eval_recall@eng.rst.gum': 0.33597893512388755, 'eval_loss@eng.rst.gum': 1.775578260421753, 'eval_runtime': 25.5786, 'eval_samples_per_second': 84.016, 'eval_steps_per_second': 2.658, 'epoch': 5.0}
{'train@eng.rst.gum_loss': 1.5415865182876587, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5380297906022883, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3802758034155518, 'train@eng.rst.gum_precision@eng.rst.gum': 0.48642506335197094, 'train@eng.rst.gum_recall@eng.rst.gum': 0.38437282666702893, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5415866374969482, 'train@eng.rst.gum_runtime': 163.1739, 'train@eng.rst.gum_samples_per_second': 85.167, 'train@eng.rst.gum_steps_per_second': 2.666, 'epoch': 6.0}
{'loss': 1.6438, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.7398120164871216, 'eval_accuracy@eng.rst.gum': 0.49790600279199626, 'eval_f1@eng.rst.gum': 0.35113523792849893, 'eval_precision@eng.rst.gum': 0.3778589997990528, 'eval_recall@eng.rst.gum': 0.36637134683956324, 'eval_loss@eng.rst.gum': 1.7398121356964111, 'eval_runtime': 25.6139, 'eval_samples_per_second': 83.9, 'eval_steps_per_second': 2.655, 'epoch': 6.0}
{'train@eng.rst.gum_loss': 1.5074493885040283, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5456573361157084, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3938715719969773, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5479477622776292, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4005635461387485, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5074495077133179, 'train@eng.rst.gum_runtime': 163.4557, 'train@eng.rst.gum_samples_per_second': 85.02, 'train@eng.rst.gum_steps_per_second': 2.661, 'epoch': 7.0}
{'loss': 1.5995, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.7205170392990112, 'eval_accuracy@eng.rst.gum': 0.49790600279199626, 'eval_f1@eng.rst.gum': 0.35384524339447765, 'eval_precision@eng.rst.gum': 0.4420000063402527, 'eval_recall@eng.rst.gum': 0.37554618787288735, 'eval_loss@eng.rst.gum': 1.7205170392990112, 'eval_runtime': 25.6516, 'eval_samples_per_second': 83.776, 'eval_steps_per_second': 2.651, 'epoch': 7.0}
{'train@eng.rst.gum_loss': 1.482690691947937, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5532848816291286, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4033126504195492, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5284906954010122, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4072697592502756, 'train@eng.rst.gum_loss@eng.rst.gum': 1.482690691947937, 'train@eng.rst.gum_runtime': 163.361, 'train@eng.rst.gum_samples_per_second': 85.069, 'train@eng.rst.gum_steps_per_second': 2.663, 'epoch': 8.0}
{'loss': 1.5682, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.7051513195037842, 'eval_accuracy@eng.rst.gum': 0.504885993485342, 'eval_f1@eng.rst.gum': 0.36803978881021676, 'eval_precision@eng.rst.gum': 0.46172705031193745, 'eval_recall@eng.rst.gum': 0.3860669644614768, 'eval_loss@eng.rst.gum': 1.7051513195037842, 'eval_runtime': 25.5863, 'eval_samples_per_second': 83.99, 'eval_steps_per_second': 2.658, 'epoch': 8.0}
{'train@eng.rst.gum_loss': 1.4620566368103027, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5612722170252572, 'train@eng.rst.gum_f1@eng.rst.gum': 0.42126485712103706, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5294642120970072, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4251550486357822, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4620565176010132, 'train@eng.rst.gum_runtime': 163.0893, 'train@eng.rst.gum_samples_per_second': 85.211, 'train@eng.rst.gum_steps_per_second': 2.667, 'epoch': 9.0}
{'loss': 1.5418, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.6885854005813599, 'eval_accuracy@eng.rst.gum': 0.5058166589111215, 'eval_f1@eng.rst.gum': 0.37750214528576403, 'eval_precision@eng.rst.gum': 0.4283443030287325, 'eval_recall@eng.rst.gum': 0.3989442314278181, 'eval_loss@eng.rst.gum': 1.6885851621627808, 'eval_runtime': 25.5836, 'eval_samples_per_second': 83.999, 'eval_steps_per_second': 2.658, 'epoch': 9.0}
{'train@eng.rst.gum_loss': 1.447828769683838, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5634309563215082, 'train@eng.rst.gum_f1@eng.rst.gum': 0.42466601623246764, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5341299933020281, 'train@eng.rst.gum_recall@eng.rst.gum': 0.42370632835210253, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4478288888931274, 'train@eng.rst.gum_runtime': 163.4788, 'train@eng.rst.gum_samples_per_second': 85.008, 'train@eng.rst.gum_steps_per_second': 2.661, 'epoch': 10.0}
{'loss': 1.5271, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.6803498268127441, 'eval_accuracy@eng.rst.gum': 0.5104699860400186, 'eval_f1@eng.rst.gum': 0.3800034365573636, 'eval_precision@eng.rst.gum': 0.43461578163440207, 'eval_recall@eng.rst.gum': 0.3976405953798244, 'eval_loss@eng.rst.gum': 1.6803497076034546, 'eval_runtime': 25.6197, 'eval_samples_per_second': 83.881, 'eval_steps_per_second': 2.654, 'epoch': 10.0}
{'train@eng.rst.gum_loss': 1.4404428005218506, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5660934014535511, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4298544515637052, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5303402675684497, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4313068270411313, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4404428005218506, 'train@eng.rst.gum_runtime': 163.4063, 'train@eng.rst.gum_samples_per_second': 85.046, 'train@eng.rst.gum_steps_per_second': 2.662, 'epoch': 11.0}
{'loss': 1.522, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.6747459173202515, 'eval_accuracy@eng.rst.gum': 0.5095393206142392, 'eval_f1@eng.rst.gum': 0.3854660916777502, 'eval_precision@eng.rst.gum': 0.4383031730644049, 'eval_recall@eng.rst.gum': 0.403327464714387, 'eval_loss@eng.rst.gum': 1.674745798110962, 'eval_runtime': 25.5635, 'eval_samples_per_second': 84.065, 'eval_steps_per_second': 2.66, 'epoch': 11.0}
{'train@eng.rst.gum_loss': 1.437430739402771, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5671008131251349, 'train@eng.rst.gum_f1@eng.rst.gum': 0.43047269511207414, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5351656979953933, 'train@eng.rst.gum_recall@eng.rst.gum': 0.43050653608126926, 'train@eng.rst.gum_loss@eng.rst.gum': 1.437430739402771, 'train@eng.rst.gum_runtime': 163.0558, 'train@eng.rst.gum_samples_per_second': 85.228, 'train@eng.rst.gum_steps_per_second': 2.668, 'epoch': 12.0}
{'loss': 1.5094, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.673519492149353, 'eval_accuracy@eng.rst.gum': 0.5104699860400186, 'eval_f1@eng.rst.gum': 0.38600853362064114, 'eval_precision@eng.rst.gum': 0.43785947175747153, 'eval_recall@eng.rst.gum': 0.40347690573413997, 'eval_loss@eng.rst.gum': 1.6735196113586426, 'eval_runtime': 25.5768, 'eval_samples_per_second': 84.022, 'eval_steps_per_second': 2.659, 'epoch': 12.0}
{'train_runtime': 6412.9934, 'train_samples_per_second': 26.004, 'train_steps_per_second': 0.814, 'train_loss': 1.7949824307613447, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.7784
  train_runtime            = 0:12:55.00
  train_samples_per_second =     24.898
  train_steps_per_second   =       0.79
-------------------------------------------------------------------
Lang1:  por.rst.cstn    Lang2:  eng.rst.gum
Saving run to:  runs/full_shot/FullShot=v4_por.rst.cstn_eng.rst.gum_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 4148 examples
read 573 examples
read 272 examples
read 13897 examples
read 2149 examples
read 2091 examples
Total prediction labels:  37
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (pt): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (pt): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=37, bias=True)
    )
  )
)
{'train@por.rst.cstn_loss': 2.449023962020874, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.2774831243972999, 'train@por.rst.cstn_f1@por.rst.cstn': 0.013575674655595393, 'train@por.rst.cstn_precision@por.rst.cstn': 0.008671347637415621, 'train@por.rst.cstn_recall@por.rst.cstn': 0.03125, 'train@por.rst.cstn_loss@por.rst.cstn': 2.449023962020874, 'train@por.rst.cstn_runtime': 51.1188, 'train@por.rst.cstn_samples_per_second': 81.144, 'train@por.rst.cstn_steps_per_second': 2.543, 'epoch': 1.0}
{'loss': 2.9701, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.5117499828338623, 'eval_accuracy@por.rst.cstn': 0.28097731239092494, 'eval_f1@por.rst.cstn': 0.019940549913301957, 'eval_precision@por.rst.cstn': 0.012771696017769315, 'eval_recall@por.rst.cstn': 0.045454545454545456, 'eval_loss@por.rst.cstn': 2.511749744415283, 'eval_runtime': 7.1552, 'eval_samples_per_second': 80.082, 'eval_steps_per_second': 2.516, 'epoch': 1.0}
{'train@por.rst.cstn_loss': 2.2327475547790527, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.3690935390549662, 'train@por.rst.cstn_f1@por.rst.cstn': 0.05410682607898855, 'train@por.rst.cstn_precision@por.rst.cstn': 0.08349318072200405, 'train@por.rst.cstn_recall@por.rst.cstn': 0.06080381956833393, 'train@por.rst.cstn_loss@por.rst.cstn': 2.2327475547790527, 'train@por.rst.cstn_runtime': 49.8907, 'train@por.rst.cstn_samples_per_second': 83.142, 'train@por.rst.cstn_steps_per_second': 2.606, 'epoch': 2.0}
{'loss': 2.3807, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.3359415531158447, 'eval_accuracy@por.rst.cstn': 0.31762652705061084, 'eval_f1@por.rst.cstn': 0.06453616189950327, 'eval_precision@por.rst.cstn': 0.10574425574425575, 'eval_recall@por.rst.cstn': 0.07580613859478016, 'eval_loss@por.rst.cstn': 2.3359415531158447, 'eval_runtime': 7.1881, 'eval_samples_per_second': 79.715, 'eval_steps_per_second': 2.504, 'epoch': 2.0}
{'train@por.rst.cstn_loss': 2.00860595703125, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.46986499517839925, 'train@por.rst.cstn_f1@por.rst.cstn': 0.07669089048251879, 'train@por.rst.cstn_precision@por.rst.cstn': 0.07672306361806147, 'train@por.rst.cstn_recall@por.rst.cstn': 0.08983998766810133, 'train@por.rst.cstn_loss@por.rst.cstn': 2.008606195449829, 'train@por.rst.cstn_runtime': 49.8362, 'train@por.rst.cstn_samples_per_second': 83.233, 'train@por.rst.cstn_steps_per_second': 2.609, 'epoch': 3.0}
{'loss': 2.1782, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.134718656539917, 'eval_accuracy@por.rst.cstn': 0.38219895287958117, 'eval_f1@por.rst.cstn': 0.10144646847215198, 'eval_precision@por.rst.cstn': 0.10471001059152446, 'eval_recall@por.rst.cstn': 0.1262314517879319, 'eval_loss@por.rst.cstn': 2.134719133377075, 'eval_runtime': 7.2208, 'eval_samples_per_second': 79.354, 'eval_steps_per_second': 2.493, 'epoch': 3.0}
{'train@por.rst.cstn_loss': 1.8344212770462036, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5248312439729991, 'train@por.rst.cstn_f1@por.rst.cstn': 0.08847836436698678, 'train@por.rst.cstn_precision@por.rst.cstn': 0.1381873387265199, 'train@por.rst.cstn_recall@por.rst.cstn': 0.10376536467909186, 'train@por.rst.cstn_loss@por.rst.cstn': 1.8344213962554932, 'train@por.rst.cstn_runtime': 49.8384, 'train@por.rst.cstn_samples_per_second': 83.229, 'train@por.rst.cstn_steps_per_second': 2.608, 'epoch': 4.0}
{'loss': 1.9833, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.980974555015564, 'eval_accuracy@por.rst.cstn': 0.43455497382198954, 'eval_f1@por.rst.cstn': 0.11539076307229736, 'eval_precision@por.rst.cstn': 0.14219181444603168, 'eval_recall@por.rst.cstn': 0.14576199751985064, 'eval_loss@por.rst.cstn': 1.980974555015564, 'eval_runtime': 7.2082, 'eval_samples_per_second': 79.493, 'eval_steps_per_second': 2.497, 'epoch': 4.0}
{'train@por.rst.cstn_loss': 1.7236549854278564, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5429122468659595, 'train@por.rst.cstn_f1@por.rst.cstn': 0.11534218219291034, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13972416925626777, 'train@por.rst.cstn_recall@por.rst.cstn': 0.12487150894114772, 'train@por.rst.cstn_loss@por.rst.cstn': 1.7236549854278564, 'train@por.rst.cstn_runtime': 49.8739, 'train@por.rst.cstn_samples_per_second': 83.17, 'train@por.rst.cstn_steps_per_second': 2.607, 'epoch': 5.0}
{'loss': 1.8416, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.8787089586257935, 'eval_accuracy@por.rst.cstn': 0.45898778359511344, 'eval_f1@por.rst.cstn': 0.14966118008200183, 'eval_precision@por.rst.cstn': 0.20753417459516196, 'eval_recall@por.rst.cstn': 0.17276112637564686, 'eval_loss@por.rst.cstn': 1.8787089586257935, 'eval_runtime': 7.1867, 'eval_samples_per_second': 79.731, 'eval_steps_per_second': 2.505, 'epoch': 5.0}
{'train@por.rst.cstn_loss': 1.652318000793457, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5662970106075217, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1257581317525509, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13820554690934767, 'train@por.rst.cstn_recall@por.rst.cstn': 0.13620873285043666, 'train@por.rst.cstn_loss@por.rst.cstn': 1.6523181200027466, 'train@por.rst.cstn_runtime': 49.8163, 'train@por.rst.cstn_samples_per_second': 83.266, 'train@por.rst.cstn_steps_per_second': 2.61, 'epoch': 6.0}
{'loss': 1.753, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.8073492050170898, 'eval_accuracy@por.rst.cstn': 0.4712041884816754, 'eval_f1@por.rst.cstn': 0.16179421621951712, 'eval_precision@por.rst.cstn': 0.2012157634032634, 'eval_recall@por.rst.cstn': 0.182513371102928, 'eval_loss@por.rst.cstn': 1.8073492050170898, 'eval_runtime': 7.1787, 'eval_samples_per_second': 79.82, 'eval_steps_per_second': 2.507, 'epoch': 6.0}
{'train@por.rst.cstn_loss': 1.6081615686416626, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5781099324975892, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1333538218719898, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13608898374071446, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1438672750237918, 'train@por.rst.cstn_loss@por.rst.cstn': 1.6081613302230835, 'train@por.rst.cstn_runtime': 49.798, 'train@por.rst.cstn_samples_per_second': 83.297, 'train@por.rst.cstn_steps_per_second': 2.611, 'epoch': 7.0}
{'loss': 1.6896, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.7684974670410156, 'eval_accuracy@por.rst.cstn': 0.49040139616055844, 'eval_f1@por.rst.cstn': 0.17630141788307485, 'eval_precision@por.rst.cstn': 0.1881190764917956, 'eval_recall@por.rst.cstn': 0.19079757874744352, 'eval_loss@por.rst.cstn': 1.768497347831726, 'eval_runtime': 7.2201, 'eval_samples_per_second': 79.362, 'eval_steps_per_second': 2.493, 'epoch': 7.0}
{'train@por.rst.cstn_loss': 1.5800206661224365, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5824493731918997, 'train@por.rst.cstn_f1@por.rst.cstn': 0.13661942968678256, 'train@por.rst.cstn_precision@por.rst.cstn': 0.16819901527868683, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14686937218910903, 'train@por.rst.cstn_loss@por.rst.cstn': 1.580020785331726, 'train@por.rst.cstn_runtime': 49.7874, 'train@por.rst.cstn_samples_per_second': 83.314, 'train@por.rst.cstn_steps_per_second': 2.611, 'epoch': 8.0}
{'loss': 1.6609, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.7467457056045532, 'eval_accuracy@por.rst.cstn': 0.493891797556719, 'eval_f1@por.rst.cstn': 0.17885102900315883, 'eval_precision@por.rst.cstn': 0.18931817659485645, 'eval_recall@por.rst.cstn': 0.19339955545229104, 'eval_loss@por.rst.cstn': 1.7467455863952637, 'eval_runtime': 7.1862, 'eval_samples_per_second': 79.736, 'eval_steps_per_second': 2.505, 'epoch': 8.0}
{'train@por.rst.cstn_loss': 1.559597373008728, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5877531340405014, 'train@por.rst.cstn_f1@por.rst.cstn': 0.13914394149672005, 'train@por.rst.cstn_precision@por.rst.cstn': 0.1695684963803855, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14889455907125082, 'train@por.rst.cstn_loss@por.rst.cstn': 1.559597373008728, 'train@por.rst.cstn_runtime': 49.7542, 'train@por.rst.cstn_samples_per_second': 83.37, 'train@por.rst.cstn_steps_per_second': 2.613, 'epoch': 9.0}
{'loss': 1.6326, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.728989601135254, 'eval_accuracy@por.rst.cstn': 0.493891797556719, 'eval_f1@por.rst.cstn': 0.18182174502039006, 'eval_precision@por.rst.cstn': 0.19190722166181476, 'eval_recall@por.rst.cstn': 0.19332283127269606, 'eval_loss@por.rst.cstn': 1.728989601135254, 'eval_runtime': 7.1983, 'eval_samples_per_second': 79.602, 'eval_steps_per_second': 2.501, 'epoch': 9.0}
{'train@por.rst.cstn_loss': 1.545527696609497, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5918514946962391, 'train@por.rst.cstn_f1@por.rst.cstn': 0.14194372711483716, 'train@por.rst.cstn_precision@por.rst.cstn': 0.15949398444854287, 'train@por.rst.cstn_recall@por.rst.cstn': 0.15295669224940572, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5455278158187866, 'train@por.rst.cstn_runtime': 49.793, 'train@por.rst.cstn_samples_per_second': 83.305, 'train@por.rst.cstn_steps_per_second': 2.611, 'epoch': 10.0}
{'loss': 1.6047, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.7136809825897217, 'eval_accuracy@por.rst.cstn': 0.518324607329843, 'eval_f1@por.rst.cstn': 0.1967035459755584, 'eval_precision@por.rst.cstn': 0.2047695872623316, 'eval_recall@por.rst.cstn': 0.21006240598501888, 'eval_loss@por.rst.cstn': 1.7136807441711426, 'eval_runtime': 7.1998, 'eval_samples_per_second': 79.585, 'eval_steps_per_second': 2.5, 'epoch': 10.0}
{'train@por.rst.cstn_loss': 1.5371595621109009, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5925747348119575, 'train@por.rst.cstn_f1@por.rst.cstn': 0.14260379220577435, 'train@por.rst.cstn_precision@por.rst.cstn': 0.15270466675193564, 'train@por.rst.cstn_recall@por.rst.cstn': 0.15337812161344455, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5371595621109009, 'train@por.rst.cstn_runtime': 49.8436, 'train@por.rst.cstn_samples_per_second': 83.22, 'train@por.rst.cstn_steps_per_second': 2.608, 'epoch': 11.0}
{'loss': 1.6002, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.708301305770874, 'eval_accuracy@por.rst.cstn': 0.5148342059336823, 'eval_f1@por.rst.cstn': 0.19601403157679453, 'eval_precision@por.rst.cstn': 0.2034615674307827, 'eval_recall@por.rst.cstn': 0.20890932565573717, 'eval_loss@por.rst.cstn': 1.708301305770874, 'eval_runtime': 7.206, 'eval_samples_per_second': 79.517, 'eval_steps_per_second': 2.498, 'epoch': 11.0}
{'train@por.rst.cstn_loss': 1.5343875885009766, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.593297974927676, 'train@por.rst.cstn_f1@por.rst.cstn': 0.14281202568292056, 'train@por.rst.cstn_precision@por.rst.cstn': 0.15274914710766024, 'train@por.rst.cstn_recall@por.rst.cstn': 0.15356100681040452, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5343875885009766, 'train@por.rst.cstn_runtime': 49.8319, 'train@por.rst.cstn_samples_per_second': 83.24, 'train@por.rst.cstn_steps_per_second': 2.609, 'epoch': 12.0}
{'loss': 1.594, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.704984426498413, 'eval_accuracy@por.rst.cstn': 0.518324607329843, 'eval_f1@por.rst.cstn': 0.19817062727053322, 'eval_precision@por.rst.cstn': 0.20546694020913805, 'eval_recall@por.rst.cstn': 0.21068848084398326, 'eval_loss@por.rst.cstn': 1.7049845457077026, 'eval_runtime': 7.2091, 'eval_samples_per_second': 79.483, 'eval_steps_per_second': 2.497, 'epoch': 12.0}
{'train_runtime': 1951.3553, 'train_samples_per_second': 25.508, 'train_steps_per_second': 0.799, 'train_loss': 1.9074049338316306, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.9074
  train_runtime            = 0:32:31.35
  train_samples_per_second =     25.508
  train_steps_per_second   =      0.799
{'train@eng.rst.gum_loss': 2.1538703441619873, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.3931783838238469, 'train@eng.rst.gum_f1@eng.rst.gum': 0.154402687313936, 'train@eng.rst.gum_precision@eng.rst.gum': 0.18992702318298052, 'train@eng.rst.gum_recall@eng.rst.gum': 0.17015621026674094, 'train@eng.rst.gum_loss@eng.rst.gum': 2.1538705825805664, 'train@eng.rst.gum_runtime': 163.3207, 'train@eng.rst.gum_samples_per_second': 85.09, 'train@eng.rst.gum_steps_per_second': 2.663, 'epoch': 1.0}
{'loss': 2.6635, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.243476152420044, 'eval_accuracy@eng.rst.gum': 0.3769194974406701, 'eval_f1@eng.rst.gum': 0.15337248588954702, 'eval_precision@eng.rst.gum': 0.20484044049566127, 'eval_recall@eng.rst.gum': 0.17021985339038134, 'eval_loss@eng.rst.gum': 2.243476152420044, 'eval_runtime': 25.631, 'eval_samples_per_second': 83.844, 'eval_steps_per_second': 2.653, 'epoch': 1.0}
{'train@eng.rst.gum_loss': 1.8681097030639648, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.45707706699287615, 'train@eng.rst.gum_f1@eng.rst.gum': 0.23270183840066636, 'train@eng.rst.gum_precision@eng.rst.gum': 0.3348927144874343, 'train@eng.rst.gum_recall@eng.rst.gum': 0.24246674637410734, 'train@eng.rst.gum_loss@eng.rst.gum': 1.868109941482544, 'train@eng.rst.gum_runtime': 163.0284, 'train@eng.rst.gum_samples_per_second': 85.243, 'train@eng.rst.gum_steps_per_second': 2.668, 'epoch': 2.0}
{'loss': 2.0696, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.9980498552322388, 'eval_accuracy@eng.rst.gum': 0.42810609585853887, 'eval_f1@eng.rst.gum': 0.22829359864246818, 'eval_precision@eng.rst.gum': 0.28771697984556127, 'eval_recall@eng.rst.gum': 0.23755925728391966, 'eval_loss@eng.rst.gum': 1.9980500936508179, 'eval_runtime': 25.5374, 'eval_samples_per_second': 84.151, 'eval_steps_per_second': 2.663, 'epoch': 2.0}
{'train@eng.rst.gum_loss': 1.7047103643417358, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5031301719795639, 'train@eng.rst.gum_f1@eng.rst.gum': 0.29947718077892377, 'train@eng.rst.gum_precision@eng.rst.gum': 0.3960328393264568, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3079486130906595, 'train@eng.rst.gum_loss@eng.rst.gum': 1.7047104835510254, 'train@eng.rst.gum_runtime': 163.4188, 'train@eng.rst.gum_samples_per_second': 85.039, 'train@eng.rst.gum_steps_per_second': 2.662, 'epoch': 3.0}
{'loss': 1.862, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.85337495803833, 'eval_accuracy@eng.rst.gum': 0.46533271288971617, 'eval_f1@eng.rst.gum': 0.2795072834542305, 'eval_precision@eng.rst.gum': 0.31569200099411304, 'eval_recall@eng.rst.gum': 0.2925762680688295, 'eval_loss@eng.rst.gum': 1.85337495803833, 'eval_runtime': 25.5908, 'eval_samples_per_second': 83.975, 'eval_steps_per_second': 2.657, 'epoch': 3.0}
{'train@eng.rst.gum_loss': 1.6133745908737183, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5202561703964884, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3307624851717604, 'train@eng.rst.gum_precision@eng.rst.gum': 0.4701682563614122, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3318129613863335, 'train@eng.rst.gum_loss@eng.rst.gum': 1.6133747100830078, 'train@eng.rst.gum_runtime': 163.4286, 'train@eng.rst.gum_samples_per_second': 85.034, 'train@eng.rst.gum_steps_per_second': 2.662, 'epoch': 4.0}
{'loss': 1.7389, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.7844339609146118, 'eval_accuracy@eng.rst.gum': 0.4820846905537459, 'eval_f1@eng.rst.gum': 0.31013404187542715, 'eval_precision@eng.rst.gum': 0.38292676583977775, 'eval_recall@eng.rst.gum': 0.31494671740535923, 'eval_loss@eng.rst.gum': 1.7844338417053223, 'eval_runtime': 25.5821, 'eval_samples_per_second': 84.004, 'eval_steps_per_second': 2.658, 'epoch': 4.0}
{'train@eng.rst.gum_loss': 1.5521451234817505, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5360149672591207, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3807764523352497, 'train@eng.rst.gum_precision@eng.rst.gum': 0.4798653788830575, 'train@eng.rst.gum_recall@eng.rst.gum': 0.37818891320658254, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5521448850631714, 'train@eng.rst.gum_runtime': 163.1146, 'train@eng.rst.gum_samples_per_second': 85.198, 'train@eng.rst.gum_steps_per_second': 2.667, 'epoch': 5.0}
{'loss': 1.6614, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.733933448791504, 'eval_accuracy@eng.rst.gum': 0.5011633317822243, 'eval_f1@eng.rst.gum': 0.3582257148349175, 'eval_precision@eng.rst.gum': 0.41529509424172667, 'eval_recall@eng.rst.gum': 0.3638738418218593, 'eval_loss@eng.rst.gum': 1.7339335680007935, 'eval_runtime': 25.5692, 'eval_samples_per_second': 84.046, 'eval_steps_per_second': 2.659, 'epoch': 5.0}
{'train@eng.rst.gum_loss': 1.5090384483337402, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.547456285529251, 'train@eng.rst.gum_f1@eng.rst.gum': 0.40033044717679894, 'train@eng.rst.gum_precision@eng.rst.gum': 0.48762290259332414, 'train@eng.rst.gum_recall@eng.rst.gum': 0.39583876363069953, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5090383291244507, 'train@eng.rst.gum_runtime': 163.3998, 'train@eng.rst.gum_samples_per_second': 85.049, 'train@eng.rst.gum_steps_per_second': 2.662, 'epoch': 6.0}
{'loss': 1.6116, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.7008596658706665, 'eval_accuracy@eng.rst.gum': 0.5086086551884598, 'eval_f1@eng.rst.gum': 0.3709909091830333, 'eval_precision@eng.rst.gum': 0.4408478123945915, 'eval_recall@eng.rst.gum': 0.3772702805260916, 'eval_loss@eng.rst.gum': 1.7008599042892456, 'eval_runtime': 25.641, 'eval_samples_per_second': 83.811, 'eval_steps_per_second': 2.652, 'epoch': 6.0}
{'train@eng.rst.gum_loss': 1.4801064729690552, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5542922933007124, 'train@eng.rst.gum_f1@eng.rst.gum': 0.41396769356990076, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5384005231748138, 'train@eng.rst.gum_recall@eng.rst.gum': 0.41065718996535183, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4801065921783447, 'train@eng.rst.gum_runtime': 163.3801, 'train@eng.rst.gum_samples_per_second': 85.059, 'train@eng.rst.gum_steps_per_second': 2.663, 'epoch': 7.0}
{'loss': 1.5731, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.683587908744812, 'eval_accuracy@eng.rst.gum': 0.5109353187529083, 'eval_f1@eng.rst.gum': 0.38170788341037687, 'eval_precision@eng.rst.gum': 0.4664088134364373, 'eval_recall@eng.rst.gum': 0.3873370137016171, 'eval_loss@eng.rst.gum': 1.683587670326233, 'eval_runtime': 25.6194, 'eval_samples_per_second': 83.882, 'eval_steps_per_second': 2.654, 'epoch': 7.0}
{'train@eng.rst.gum_loss': 1.4587101936340332, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.560624595236382, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4224714608922743, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5408782555228935, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4169614328870804, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4587098360061646, 'train@eng.rst.gum_runtime': 163.0397, 'train@eng.rst.gum_samples_per_second': 85.237, 'train@eng.rst.gum_steps_per_second': 2.668, 'epoch': 8.0}
{'loss': 1.5452, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.6713157892227173, 'eval_accuracy@eng.rst.gum': 0.5165193113075849, 'eval_f1@eng.rst.gum': 0.388661314922283, 'eval_precision@eng.rst.gum': 0.47945490316776823, 'eval_recall@eng.rst.gum': 0.3925291720296517, 'eval_loss@eng.rst.gum': 1.6713159084320068, 'eval_runtime': 25.5325, 'eval_samples_per_second': 84.167, 'eval_steps_per_second': 2.663, 'epoch': 8.0}
{'train@eng.rst.gum_loss': 1.4424265623092651, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5658055695473844, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4334113720978423, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5201017065764143, 'train@eng.rst.gum_recall@eng.rst.gum': 0.43092555236327157, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4424266815185547, 'train@eng.rst.gum_runtime': 163.4765, 'train@eng.rst.gum_samples_per_second': 85.009, 'train@eng.rst.gum_steps_per_second': 2.661, 'epoch': 9.0}
{'loss': 1.5247, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.6564887762069702, 'eval_accuracy@eng.rst.gum': 0.5183806421591438, 'eval_f1@eng.rst.gum': 0.3943334951672607, 'eval_precision@eng.rst.gum': 0.46325558623994817, 'eval_recall@eng.rst.gum': 0.40367498311959, 'eval_loss@eng.rst.gum': 1.6564887762069702, 'eval_runtime': 25.6212, 'eval_samples_per_second': 83.876, 'eval_steps_per_second': 2.654, 'epoch': 9.0}
{'train@eng.rst.gum_loss': 1.4283719062805176, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5700510901633447, 'train@eng.rst.gum_f1@eng.rst.gum': 0.43998039931806565, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5775249639781486, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4323341374987887, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4283720254898071, 'train@eng.rst.gum_runtime': 163.486, 'train@eng.rst.gum_samples_per_second': 85.004, 'train@eng.rst.gum_steps_per_second': 2.661, 'epoch': 10.0}
{'loss': 1.5113, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.6490213871002197, 'eval_accuracy@eng.rst.gum': 0.5207073057235924, 'eval_f1@eng.rst.gum': 0.3966083795443474, 'eval_precision@eng.rst.gum': 0.45377251470399566, 'eval_recall@eng.rst.gum': 0.4005322685065872, 'eval_loss@eng.rst.gum': 1.6490215063095093, 'eval_runtime': 25.6256, 'eval_samples_per_second': 83.861, 'eval_steps_per_second': 2.654, 'epoch': 10.0}
{'train@eng.rst.gum_loss': 1.4224421977996826, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5714182917176369, 'train@eng.rst.gum_f1@eng.rst.gum': 0.44343767551318675, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5491514120340412, 'train@eng.rst.gum_recall@eng.rst.gum': 0.438010867873659, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4224423170089722, 'train@eng.rst.gum_runtime': 163.0278, 'train@eng.rst.gum_samples_per_second': 85.243, 'train@eng.rst.gum_steps_per_second': 2.668, 'epoch': 11.0}
{'loss': 1.5072, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.6435519456863403, 'eval_accuracy@eng.rst.gum': 0.5258259655653792, 'eval_f1@eng.rst.gum': 0.40279741026941357, 'eval_precision@eng.rst.gum': 0.4714841899971905, 'eval_recall@eng.rst.gum': 0.40857470069244706, 'eval_loss@eng.rst.gum': 1.6435519456863403, 'eval_runtime': 25.5261, 'eval_samples_per_second': 84.188, 'eval_steps_per_second': 2.664, 'epoch': 11.0}
{'train@eng.rst.gum_loss': 1.4200773239135742, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5717061236238037, 'train@eng.rst.gum_f1@eng.rst.gum': 0.44346981998151075, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5500125497144753, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4376451751970735, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4200774431228638, 'train@eng.rst.gum_runtime': 163.4318, 'train@eng.rst.gum_samples_per_second': 85.032, 'train@eng.rst.gum_steps_per_second': 2.662, 'epoch': 12.0}
{'loss': 1.4962, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.643013596534729, 'eval_accuracy@eng.rst.gum': 0.5230339692880409, 'eval_f1@eng.rst.gum': 0.4013744026538825, 'eval_precision@eng.rst.gum': 0.4573345749118554, 'eval_recall@eng.rst.gum': 0.40694031447059936, 'eval_loss@eng.rst.gum': 1.6430137157440186, 'eval_runtime': 25.6389, 'eval_samples_per_second': 83.818, 'eval_steps_per_second': 2.652, 'epoch': 12.0}
{'train_runtime': 6428.1143, 'train_samples_per_second': 25.943, 'train_steps_per_second': 0.812, 'train_loss': 1.7303895329150203, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.9074
  train_runtime            = 0:32:31.35
  train_samples_per_second =     25.508
  train_steps_per_second   =      0.799
-------------------------------------------------------------------
Lang1:  rus.rst.rrt    Lang2:  eng.rst.gum
Saving run to:  runs/full_shot/FullShot=v4_rus.rst.rrt_eng.rst.gum_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 28822 examples
read 2855 examples
read 2843 examples
read 13897 examples
read 2149 examples
read 2091 examples
Total prediction labels:  28
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (ru): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=28, bias=True)
    )
  )
)
{'train@rus.rst.rrt_loss': 1.7139577865600586, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.4970508639233919, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.18088061464228058, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.2212799408773704, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.1978899214833728, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.7139577865600586, 'train@rus.rst.rrt_runtime': 344.5954, 'train@rus.rst.rrt_samples_per_second': 83.64, 'train@rus.rst.rrt_steps_per_second': 2.615, 'epoch': 1.0}
{'loss': 2.1372, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.7525361776351929, 'eval_accuracy@rus.rst.rrt': 0.4742556917688266, 'eval_f1@rus.rst.rrt': 0.19929488632274417, 'eval_precision@rus.rst.rrt': 0.20642690145756398, 'eval_recall@rus.rst.rrt': 0.21737057444375427, 'eval_loss@rus.rst.rrt': 1.7525362968444824, 'eval_runtime': 34.426, 'eval_samples_per_second': 82.931, 'eval_steps_per_second': 2.614, 'epoch': 1.0}
{'train@rus.rst.rrt_loss': 1.502801537513733, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5411491221983207, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.2234644119609876, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.31841587301491525, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.22951293351276336, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.502801537513733, 'train@rus.rst.rrt_runtime': 345.3634, 'train@rus.rst.rrt_samples_per_second': 83.454, 'train@rus.rst.rrt_steps_per_second': 2.609, 'epoch': 2.0}
{'loss': 1.6392, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.5676687955856323, 'eval_accuracy@rus.rst.rrt': 0.5173380035026269, 'eval_f1@rus.rst.rrt': 0.24814165668628987, 'eval_precision@rus.rst.rrt': 0.3587374382565577, 'eval_recall@rus.rst.rrt': 0.25453017689736984, 'eval_loss@rus.rst.rrt': 1.5676687955856323, 'eval_runtime': 34.479, 'eval_samples_per_second': 82.804, 'eval_steps_per_second': 2.61, 'epoch': 2.0}
{'train@rus.rst.rrt_loss': 1.4211299419403076, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5663035181458608, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.2771951221701091, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4482859495626426, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.27266498838111675, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.4211299419403076, 'train@rus.rst.rrt_runtime': 345.3367, 'train@rus.rst.rrt_samples_per_second': 83.461, 'train@rus.rst.rrt_steps_per_second': 2.609, 'epoch': 3.0}
{'loss': 1.5105, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.4913392066955566, 'eval_accuracy@rus.rst.rrt': 0.5453590192644483, 'eval_f1@rus.rst.rrt': 0.3063803600856203, 'eval_precision@rus.rst.rrt': 0.349122723236153, 'eval_recall@rus.rst.rrt': 0.3047118312816596, 'eval_loss@rus.rst.rrt': 1.4913392066955566, 'eval_runtime': 34.5174, 'eval_samples_per_second': 82.712, 'eval_steps_per_second': 2.607, 'epoch': 3.0}
{'train@rus.rst.rrt_loss': 1.3692988157272339, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5812920685587398, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3100246594838247, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4453690524673799, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.2931593276535765, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3692989349365234, 'train@rus.rst.rrt_runtime': 344.4765, 'train@rus.rst.rrt_samples_per_second': 83.669, 'train@rus.rst.rrt_steps_per_second': 2.616, 'epoch': 4.0}
{'loss': 1.4472, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.4433480501174927, 'eval_accuracy@rus.rst.rrt': 0.5583187390542907, 'eval_f1@rus.rst.rrt': 0.3486117838564896, 'eval_precision@rus.rst.rrt': 0.5005164038707081, 'eval_recall@rus.rst.rrt': 0.33175019503511394, 'eval_loss@rus.rst.rrt': 1.443347692489624, 'eval_runtime': 34.4152, 'eval_samples_per_second': 82.957, 'eval_steps_per_second': 2.615, 'epoch': 4.0}
{'train@rus.rst.rrt_loss': 1.3368099927902222, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5874679064603427, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3219192706252086, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.43686652927667496, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3044303296190003, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3368102312088013, 'train@rus.rst.rrt_runtime': 345.6568, 'train@rus.rst.rrt_samples_per_second': 83.383, 'train@rus.rst.rrt_steps_per_second': 2.607, 'epoch': 5.0}
{'loss': 1.4087, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.4196217060089111, 'eval_accuracy@rus.rst.rrt': 0.5663747810858144, 'eval_f1@rus.rst.rrt': 0.3651735770065837, 'eval_precision@rus.rst.rrt': 0.49441650027504663, 'eval_recall@rus.rst.rrt': 0.3481360459279068, 'eval_loss@rus.rst.rrt': 1.4196215867996216, 'eval_runtime': 34.4758, 'eval_samples_per_second': 82.812, 'eval_steps_per_second': 2.611, 'epoch': 5.0}
{'train@rus.rst.rrt_loss': 1.3129081726074219, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5959683575046839, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.33785002258773883, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4470315728561484, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.31879843131273744, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.312908411026001, 'train@rus.rst.rrt_runtime': 344.6384, 'train@rus.rst.rrt_samples_per_second': 83.63, 'train@rus.rst.rrt_steps_per_second': 2.614, 'epoch': 6.0}
{'loss': 1.3791, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.3996728658676147, 'eval_accuracy@rus.rst.rrt': 0.5733800350262697, 'eval_f1@rus.rst.rrt': 0.3881925881007135, 'eval_precision@rus.rst.rrt': 0.5061278486322104, 'eval_recall@rus.rst.rrt': 0.3703956275651117, 'eval_loss@rus.rst.rrt': 1.3996728658676147, 'eval_runtime': 34.4491, 'eval_samples_per_second': 82.876, 'eval_steps_per_second': 2.613, 'epoch': 6.0}
{'train@rus.rst.rrt_loss': 1.2967206239700317, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5994032336409687, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.34989696922550545, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4329678956427326, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.33064466339788157, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2967205047607422, 'train@rus.rst.rrt_runtime': 344.2535, 'train@rus.rst.rrt_samples_per_second': 83.723, 'train@rus.rst.rrt_steps_per_second': 2.617, 'epoch': 7.0}
{'loss': 1.3575, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.3917698860168457, 'eval_accuracy@rus.rst.rrt': 0.5775831873905429, 'eval_f1@rus.rst.rrt': 0.4002601793710067, 'eval_precision@rus.rst.rrt': 0.4911040679262313, 'eval_recall@rus.rst.rrt': 0.3859786142204629, 'eval_loss@rus.rst.rrt': 1.3917698860168457, 'eval_runtime': 34.3735, 'eval_samples_per_second': 83.058, 'eval_steps_per_second': 2.618, 'epoch': 7.0}
{'train@rus.rst.rrt_loss': 1.2839782238006592, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6021788911248352, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.35612184900863225, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.42571941625363047, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3391363145119437, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2839781045913696, 'train@rus.rst.rrt_runtime': 344.9941, 'train@rus.rst.rrt_samples_per_second': 83.543, 'train@rus.rst.rrt_steps_per_second': 2.612, 'epoch': 8.0}
{'loss': 1.3401, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.381752848625183, 'eval_accuracy@rus.rst.rrt': 0.5786339754816112, 'eval_f1@rus.rst.rrt': 0.40250572711421984, 'eval_precision@rus.rst.rrt': 0.4807826794806614, 'eval_recall@rus.rst.rrt': 0.390548169273567, 'eval_loss@rus.rst.rrt': 1.381752848625183, 'eval_runtime': 34.4829, 'eval_samples_per_second': 82.795, 'eval_steps_per_second': 2.61, 'epoch': 8.0}
{'train@rus.rst.rrt_loss': 1.2714701890945435, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6064464645062799, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36141206778370205, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4360853417293651, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34260546061875596, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2714699506759644, 'train@rus.rst.rrt_runtime': 345.5294, 'train@rus.rst.rrt_samples_per_second': 83.414, 'train@rus.rst.rrt_steps_per_second': 2.608, 'epoch': 9.0}
{'loss': 1.3313, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.3734530210494995, 'eval_accuracy@rus.rst.rrt': 0.5800350262697023, 'eval_f1@rus.rst.rrt': 0.4090679734408972, 'eval_precision@rus.rst.rrt': 0.4932510187142301, 'eval_recall@rus.rst.rrt': 0.3948315625331966, 'eval_loss@rus.rst.rrt': 1.3734527826309204, 'eval_runtime': 34.4197, 'eval_samples_per_second': 82.947, 'eval_steps_per_second': 2.615, 'epoch': 9.0}
{'train@rus.rst.rrt_loss': 1.265450119972229, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6080424675595032, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36357054287887186, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.46494631396629305, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34192230481564834, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2654500007629395, 'train@rus.rst.rrt_runtime': 344.3883, 'train@rus.rst.rrt_samples_per_second': 83.69, 'train@rus.rst.rrt_steps_per_second': 2.616, 'epoch': 10.0}
{'loss': 1.3209, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.3716373443603516, 'eval_accuracy@rus.rst.rrt': 0.5835376532399299, 'eval_f1@rus.rst.rrt': 0.4141246909917288, 'eval_precision@rus.rst.rrt': 0.5054628266775624, 'eval_recall@rus.rst.rrt': 0.3961020350987972, 'eval_loss@rus.rst.rrt': 1.3716373443603516, 'eval_runtime': 34.4357, 'eval_samples_per_second': 82.908, 'eval_steps_per_second': 2.614, 'epoch': 10.0}
{'train@rus.rst.rrt_loss': 1.2614185810089111, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6097425577683714, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3667359366435417, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.46756507215232185, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3472584751320819, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2614188194274902, 'train@rus.rst.rrt_runtime': 345.3082, 'train@rus.rst.rrt_samples_per_second': 83.467, 'train@rus.rst.rrt_steps_per_second': 2.609, 'epoch': 11.0}
{'loss': 1.3156, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.3714783191680908, 'eval_accuracy@rus.rst.rrt': 0.5831873905429071, 'eval_f1@rus.rst.rrt': 0.4136004726511412, 'eval_precision@rus.rst.rrt': 0.4954460570735908, 'eval_recall@rus.rst.rrt': 0.39894947295363836, 'eval_loss@rus.rst.rrt': 1.3714780807495117, 'eval_runtime': 34.5387, 'eval_samples_per_second': 82.661, 'eval_steps_per_second': 2.606, 'epoch': 11.0}
{'train@rus.rst.rrt_loss': 1.2597575187683105, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6100895149538547, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36677263305585994, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.46209222389365046, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34759068246250324, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2597575187683105, 'train@rus.rst.rrt_runtime': 344.8007, 'train@rus.rst.rrt_samples_per_second': 83.59, 'train@rus.rst.rrt_steps_per_second': 2.613, 'epoch': 12.0}
{'loss': 1.3099, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.369118094444275, 'eval_accuracy@rus.rst.rrt': 0.5828371278458844, 'eval_f1@rus.rst.rrt': 0.4144910187412797, 'eval_precision@rus.rst.rrt': 0.4933040214738914, 'eval_recall@rus.rst.rrt': 0.40077265490440306, 'eval_loss@rus.rst.rrt': 1.369118094444275, 'eval_runtime': 34.4645, 'eval_samples_per_second': 82.839, 'eval_steps_per_second': 2.611, 'epoch': 12.0}
{'train_runtime': 13315.3137, 'train_samples_per_second': 25.975, 'train_steps_per_second': 0.812, 'train_loss': 1.4581018829627959, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.4581
  train_runtime            = 3:41:55.31
  train_samples_per_second =     25.975
  train_steps_per_second   =      0.812
{'train@eng.rst.gum_loss': 1.959754228591919, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.4300928257897388, 'train@eng.rst.gum_f1@eng.rst.gum': 0.2115239723645227, 'train@eng.rst.gum_precision@eng.rst.gum': 0.32587917288377605, 'train@eng.rst.gum_recall@eng.rst.gum': 0.22477696521529872, 'train@eng.rst.gum_loss@eng.rst.gum': 1.959754228591919, 'train@eng.rst.gum_runtime': 163.3064, 'train@eng.rst.gum_samples_per_second': 85.098, 'train@eng.rst.gum_steps_per_second': 2.664, 'epoch': 1.0}
{'loss': 2.4882, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.0530169010162354, 'eval_accuracy@eng.rst.gum': 0.40483946021405304, 'eval_f1@eng.rst.gum': 0.20794513810157317, 'eval_precision@eng.rst.gum': 0.32131768081932627, 'eval_recall@eng.rst.gum': 0.22245781494378528, 'eval_loss@eng.rst.gum': 2.0530171394348145, 'eval_runtime': 25.542, 'eval_samples_per_second': 84.136, 'eval_steps_per_second': 2.662, 'epoch': 1.0}
{'train@eng.rst.gum_loss': 1.6739305257797241, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5118370871411095, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3404313333372557, 'train@eng.rst.gum_precision@eng.rst.gum': 0.49104513716472753, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3461356790867375, 'train@eng.rst.gum_loss@eng.rst.gum': 1.6739307641983032, 'train@eng.rst.gum_runtime': 162.8896, 'train@eng.rst.gum_samples_per_second': 85.315, 'train@eng.rst.gum_steps_per_second': 2.671, 'epoch': 2.0}
{'loss': 1.8662, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.801296591758728, 'eval_accuracy@eng.rst.gum': 0.48348068869241506, 'eval_f1@eng.rst.gum': 0.3279705996095545, 'eval_precision@eng.rst.gum': 0.44821436287969146, 'eval_recall@eng.rst.gum': 0.337862522602245, 'eval_loss@eng.rst.gum': 1.801296591758728, 'eval_runtime': 25.4818, 'eval_samples_per_second': 84.335, 'eval_steps_per_second': 2.669, 'epoch': 2.0}
{'train@eng.rst.gum_loss': 1.5442142486572266, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5459451680218752, 'train@eng.rst.gum_f1@eng.rst.gum': 0.40425430026169346, 'train@eng.rst.gum_precision@eng.rst.gum': 0.49686516987428936, 'train@eng.rst.gum_recall@eng.rst.gum': 0.41286872311415285, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5442142486572266, 'train@eng.rst.gum_runtime': 163.1257, 'train@eng.rst.gum_samples_per_second': 85.192, 'train@eng.rst.gum_steps_per_second': 2.667, 'epoch': 3.0}
{'loss': 1.6781, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.6946357488632202, 'eval_accuracy@eng.rst.gum': 0.5072126570497906, 'eval_f1@eng.rst.gum': 0.38037472499132347, 'eval_precision@eng.rst.gum': 0.46672550159003123, 'eval_recall@eng.rst.gum': 0.3977377731598491, 'eval_loss@eng.rst.gum': 1.6946357488632202, 'eval_runtime': 25.4768, 'eval_samples_per_second': 84.351, 'eval_steps_per_second': 2.669, 'epoch': 3.0}
{'train@eng.rst.gum_loss': 1.4849483966827393, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5552997049722962, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4178757210285949, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5209029432595779, 'train@eng.rst.gum_recall@eng.rst.gum': 0.41889205812013824, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4849482774734497, 'train@eng.rst.gum_runtime': 163.4659, 'train@eng.rst.gum_samples_per_second': 85.015, 'train@eng.rst.gum_steps_per_second': 2.661, 'epoch': 4.0}
{'loss': 1.5947, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.6632839441299438, 'eval_accuracy@eng.rst.gum': 0.50814332247557, 'eval_f1@eng.rst.gum': 0.3802836064784374, 'eval_precision@eng.rst.gum': 0.4558187337626804, 'eval_recall@eng.rst.gum': 0.391965490927648, 'eval_loss@eng.rst.gum': 1.663284182548523, 'eval_runtime': 25.5803, 'eval_samples_per_second': 84.01, 'eval_steps_per_second': 2.658, 'epoch': 4.0}
{'train@eng.rst.gum_loss': 1.43727707862854, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5677484349140102, 'train@eng.rst.gum_f1@eng.rst.gum': 0.45197682099963027, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5108032100224221, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4520672936748754, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4372769594192505, 'train@eng.rst.gum_runtime': 162.9402, 'train@eng.rst.gum_samples_per_second': 85.289, 'train@eng.rst.gum_steps_per_second': 2.67, 'epoch': 5.0}
{'loss': 1.5343, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.6271371841430664, 'eval_accuracy@eng.rst.gum': 0.5267566309911587, 'eval_f1@eng.rst.gum': 0.41807652436568427, 'eval_precision@eng.rst.gum': 0.4737644767133739, 'eval_recall@eng.rst.gum': 0.42716843832204693, 'eval_loss@eng.rst.gum': 1.6271370649337769, 'eval_runtime': 25.5123, 'eval_samples_per_second': 84.234, 'eval_steps_per_second': 2.665, 'epoch': 5.0}
{'train@eng.rst.gum_loss': 1.402618408203125, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5750881485212636, 'train@eng.rst.gum_f1@eng.rst.gum': 0.46036871769371585, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5212695148515359, 'train@eng.rst.gum_recall@eng.rst.gum': 0.45579094959342753, 'train@eng.rst.gum_loss@eng.rst.gum': 1.402618408203125, 'train@eng.rst.gum_runtime': 163.2247, 'train@eng.rst.gum_samples_per_second': 85.14, 'train@eng.rst.gum_steps_per_second': 2.665, 'epoch': 6.0}
{'loss': 1.4999, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.6082109212875366, 'eval_accuracy@eng.rst.gum': 0.5286179618427176, 'eval_f1@eng.rst.gum': 0.42046904007913727, 'eval_precision@eng.rst.gum': 0.47452868177778457, 'eval_recall@eng.rst.gum': 0.4267750170291858, 'eval_loss@eng.rst.gum': 1.6082109212875366, 'eval_runtime': 25.4888, 'eval_samples_per_second': 84.312, 'eval_steps_per_second': 2.668, 'epoch': 6.0}
{'train@eng.rst.gum_loss': 1.381819248199463, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5789738792545154, 'train@eng.rst.gum_f1@eng.rst.gum': 0.46659125228005965, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5243502626274475, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4654868857772839, 'train@eng.rst.gum_loss@eng.rst.gum': 1.381819248199463, 'train@eng.rst.gum_runtime': 163.3383, 'train@eng.rst.gum_samples_per_second': 85.081, 'train@eng.rst.gum_steps_per_second': 2.663, 'epoch': 7.0}
{'loss': 1.4656, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.599951982498169, 'eval_accuracy@eng.rst.gum': 0.5267566309911587, 'eval_f1@eng.rst.gum': 0.4199487043456081, 'eval_precision@eng.rst.gum': 0.4739276161345143, 'eval_recall@eng.rst.gum': 0.42807891457679936, 'eval_loss@eng.rst.gum': 1.599951982498169, 'eval_runtime': 25.5783, 'eval_samples_per_second': 84.017, 'eval_steps_per_second': 2.659, 'epoch': 7.0}
{'train@eng.rst.gum_loss': 1.3665913343429565, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5837950636828092, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4693721449989653, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5239272294252284, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4664011550653058, 'train@eng.rst.gum_loss@eng.rst.gum': 1.3665913343429565, 'train@eng.rst.gum_runtime': 163.0536, 'train@eng.rst.gum_samples_per_second': 85.23, 'train@eng.rst.gum_steps_per_second': 2.668, 'epoch': 8.0}
{'loss': 1.4496, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.5957515239715576, 'eval_accuracy@eng.rst.gum': 0.5281526291298279, 'eval_f1@eng.rst.gum': 0.4206845135108129, 'eval_precision@eng.rst.gum': 0.47041391473591826, 'eval_recall@eng.rst.gum': 0.42794678874963016, 'eval_loss@eng.rst.gum': 1.5957512855529785, 'eval_runtime': 25.5013, 'eval_samples_per_second': 84.27, 'eval_steps_per_second': 2.667, 'epoch': 8.0}
{'train@eng.rst.gum_loss': 1.3531451225280762, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5896236597826869, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4799590333049335, 'train@eng.rst.gum_precision@eng.rst.gum': 0.527805039740886, 'train@eng.rst.gum_recall@eng.rst.gum': 0.47690205901316274, 'train@eng.rst.gum_loss@eng.rst.gum': 1.3531452417373657, 'train@eng.rst.gum_runtime': 163.5308, 'train@eng.rst.gum_samples_per_second': 84.981, 'train@eng.rst.gum_steps_per_second': 2.66, 'epoch': 9.0}
{'loss': 1.4283, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.5842467546463013, 'eval_accuracy@eng.rst.gum': 0.5281526291298279, 'eval_f1@eng.rst.gum': 0.4245007761436861, 'eval_precision@eng.rst.gum': 0.4695807496524304, 'eval_recall@eng.rst.gum': 0.4322286608255415, 'eval_loss@eng.rst.gum': 1.5842467546463013, 'eval_runtime': 25.5616, 'eval_samples_per_second': 84.071, 'eval_steps_per_second': 2.66, 'epoch': 9.0}
{'train@eng.rst.gum_loss': 1.343441367149353, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5896956177592286, 'train@eng.rst.gum_f1@eng.rst.gum': 0.48134715761071084, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5442187741025295, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4755362644517067, 'train@eng.rst.gum_loss@eng.rst.gum': 1.3434414863586426, 'train@eng.rst.gum_runtime': 163.683, 'train@eng.rst.gum_samples_per_second': 84.902, 'train@eng.rst.gum_steps_per_second': 2.658, 'epoch': 10.0}
{'loss': 1.422, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.581221580505371, 'eval_accuracy@eng.rst.gum': 0.5342019543973942, 'eval_f1@eng.rst.gum': 0.4288962841778087, 'eval_precision@eng.rst.gum': 0.47184818743824763, 'eval_recall@eng.rst.gum': 0.43201935134515795, 'eval_loss@eng.rst.gum': 1.5812216997146606, 'eval_runtime': 25.581, 'eval_samples_per_second': 84.008, 'eval_steps_per_second': 2.658, 'epoch': 10.0}
{'train@eng.rst.gum_loss': 1.3394503593444824, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5899114916888537, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4814187866600186, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5250818992018534, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4793558139328517, 'train@eng.rst.gum_loss@eng.rst.gum': 1.3394503593444824, 'train@eng.rst.gum_runtime': 163.1367, 'train@eng.rst.gum_samples_per_second': 85.186, 'train@eng.rst.gum_steps_per_second': 2.666, 'epoch': 11.0}
{'loss': 1.4197, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.576712727546692, 'eval_accuracy@eng.rst.gum': 0.532805956258725, 'eval_f1@eng.rst.gum': 0.4303365161679245, 'eval_precision@eng.rst.gum': 0.4724218624180715, 'eval_recall@eng.rst.gum': 0.4371019409579804, 'eval_loss@eng.rst.gum': 1.5767126083374023, 'eval_runtime': 25.5301, 'eval_samples_per_second': 84.175, 'eval_steps_per_second': 2.664, 'epoch': 11.0}
{'train@eng.rst.gum_loss': 1.3364841938018799, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5908469453838958, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4821573077596148, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5303505560598935, 'train@eng.rst.gum_recall@eng.rst.gum': 0.477566583637974, 'train@eng.rst.gum_loss@eng.rst.gum': 1.3364840745925903, 'train@eng.rst.gum_runtime': 163.1782, 'train@eng.rst.gum_samples_per_second': 85.165, 'train@eng.rst.gum_steps_per_second': 2.666, 'epoch': 12.0}
{'loss': 1.4088, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.5761638879776, 'eval_accuracy@eng.rst.gum': 0.5351326198231736, 'eval_f1@eng.rst.gum': 0.43199646294681177, 'eval_precision@eng.rst.gum': 0.47221820557534455, 'eval_recall@eng.rst.gum': 0.43549772117249, 'eval_loss@eng.rst.gum': 1.5761640071868896, 'eval_runtime': 25.5442, 'eval_samples_per_second': 84.129, 'eval_steps_per_second': 2.662, 'epoch': 12.0}
{'train_runtime': 6414.574, 'train_samples_per_second': 25.998, 'train_steps_per_second': 0.814, 'train_loss': 1.604621133036997, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.4581
  train_runtime            = 3:41:55.31
  train_samples_per_second =     25.975
  train_steps_per_second   =      0.812
-------------------------------------------------------------------
Lang1:  spa.rst.rststb    Lang2:  eng.rst.gum
Saving run to:  runs/full_shot/FullShot=v4_spa.rst.rststb_eng.rst.gum_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2240 examples
read 383 examples
read 426 examples
read 13897 examples
read 2149 examples
read 2091 examples
Total prediction labels:  32
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=32, bias=True)
    )
  )
)
{'train@spa.rst.rststb_loss': 2.7294325828552246, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.23883928571428573, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.029623126511118102, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.03927139676912582, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.04606786862316275, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.7294323444366455, 'train@spa.rst.rststb_runtime': 26.9335, 'train@spa.rst.rststb_samples_per_second': 83.168, 'train@spa.rst.rststb_steps_per_second': 2.599, 'epoch': 1.0}
{'loss': 3.0947, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.789945125579834, 'eval_accuracy@spa.rst.rststb': 0.20887728459530025, 'eval_f1@spa.rst.rststb': 0.018111219099763275, 'eval_precision@spa.rst.rststb': 0.03485738703130008, 'eval_recall@spa.rst.rststb': 0.04456497469623913, 'eval_loss@spa.rst.rststb': 2.789945363998413, 'eval_runtime': 4.8961, 'eval_samples_per_second': 78.226, 'eval_steps_per_second': 2.451, 'epoch': 1.0}
{'train@spa.rst.rststb_loss': 2.5316083431243896, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.278125, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.041655276772012896, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.043470339937376826, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.0569244540335929, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.5316081047058105, 'train@spa.rst.rststb_runtime': 27.0613, 'train@spa.rst.rststb_samples_per_second': 82.775, 'train@spa.rst.rststb_steps_per_second': 2.587, 'epoch': 2.0}
{'loss': 2.6503, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.6623337268829346, 'eval_accuracy@spa.rst.rststb': 0.22715404699738903, 'eval_f1@spa.rst.rststb': 0.030694485648572457, 'eval_precision@spa.rst.rststb': 0.0490601904289399, 'eval_recall@spa.rst.rststb': 0.05133990078625734, 'eval_loss@spa.rst.rststb': 2.6623342037200928, 'eval_runtime': 4.9084, 'eval_samples_per_second': 78.029, 'eval_steps_per_second': 2.445, 'epoch': 2.0}
{'train@spa.rst.rststb_loss': 2.4143552780151367, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.3254464285714286, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.050105774967395614, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.052001057976069257, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.07098327889400097, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.414355516433716, 'train@spa.rst.rststb_runtime': 27.0885, 'train@spa.rst.rststb_samples_per_second': 82.692, 'train@spa.rst.rststb_steps_per_second': 2.584, 'epoch': 3.0}
{'loss': 2.5097, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.574338674545288, 'eval_accuracy@spa.rst.rststb': 0.2845953002610966, 'eval_f1@spa.rst.rststb': 0.0534901676041167, 'eval_precision@spa.rst.rststb': 0.04640583627083219, 'eval_recall@spa.rst.rststb': 0.0746446010802544, 'eval_loss@spa.rst.rststb': 2.574338674545288, 'eval_runtime': 4.8954, 'eval_samples_per_second': 78.237, 'eval_steps_per_second': 2.451, 'epoch': 3.0}
{'train@spa.rst.rststb_loss': 2.3194921016693115, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.3464285714285714, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.07119287546795708, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.08892910656015172, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.0857435866536898, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.3194921016693115, 'train@spa.rst.rststb_runtime': 27.0485, 'train@spa.rst.rststb_samples_per_second': 82.814, 'train@spa.rst.rststb_steps_per_second': 2.588, 'epoch': 4.0}
{'loss': 2.4158, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.5035483837127686, 'eval_accuracy@spa.rst.rststb': 0.30809399477806787, 'eval_f1@spa.rst.rststb': 0.07900202526593861, 'eval_precision@spa.rst.rststb': 0.11073267833010701, 'eval_recall@spa.rst.rststb': 0.09230460086902968, 'eval_loss@spa.rst.rststb': 2.5035483837127686, 'eval_runtime': 4.9087, 'eval_samples_per_second': 78.025, 'eval_steps_per_second': 2.445, 'epoch': 4.0}
{'train@spa.rst.rststb_loss': 2.2354636192321777, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.38526785714285716, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.09164820029673956, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.08652409690705766, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.1101309879278828, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.235463857650757, 'train@spa.rst.rststb_runtime': 27.0788, 'train@spa.rst.rststb_samples_per_second': 82.721, 'train@spa.rst.rststb_steps_per_second': 2.585, 'epoch': 5.0}
{'loss': 2.3271, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.4434316158294678, 'eval_accuracy@spa.rst.rststb': 0.33159268929503916, 'eval_f1@spa.rst.rststb': 0.09331621426433968, 'eval_precision@spa.rst.rststb': 0.09749134386815546, 'eval_recall@spa.rst.rststb': 0.11091044466749869, 'eval_loss@spa.rst.rststb': 2.443431854248047, 'eval_runtime': 4.8808, 'eval_samples_per_second': 78.471, 'eval_steps_per_second': 2.459, 'epoch': 5.0}
{'train@spa.rst.rststb_loss': 2.168949604034424, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.39732142857142855, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.09607686243621814, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.12174149526635655, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.11928196414036195, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.168949604034424, 'train@spa.rst.rststb_runtime': 27.0829, 'train@spa.rst.rststb_samples_per_second': 82.709, 'train@spa.rst.rststb_steps_per_second': 2.585, 'epoch': 6.0}
{'loss': 2.2544, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.395207166671753, 'eval_accuracy@spa.rst.rststb': 0.3524804177545692, 'eval_f1@spa.rst.rststb': 0.10043644503664441, 'eval_precision@spa.rst.rststb': 0.09681885211685662, 'eval_recall@spa.rst.rststb': 0.12398204088521078, 'eval_loss@spa.rst.rststb': 2.395207166671753, 'eval_runtime': 4.8716, 'eval_samples_per_second': 78.619, 'eval_steps_per_second': 2.463, 'epoch': 6.0}
{'train@spa.rst.rststb_loss': 2.112168550491333, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4089285714285714, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10109220612997531, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.15169810413091403, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.12621282287259689, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.112168550491333, 'train@spa.rst.rststb_runtime': 27.0293, 'train@spa.rst.rststb_samples_per_second': 82.873, 'train@spa.rst.rststb_steps_per_second': 2.59, 'epoch': 7.0}
{'loss': 2.2015, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.3531644344329834, 'eval_accuracy@spa.rst.rststb': 0.370757180156658, 'eval_f1@spa.rst.rststb': 0.10567482116272937, 'eval_precision@spa.rst.rststb': 0.09467415648278886, 'eval_recall@spa.rst.rststb': 0.13501955908867416, 'eval_loss@spa.rst.rststb': 2.3531644344329834, 'eval_runtime': 4.8764, 'eval_samples_per_second': 78.541, 'eval_steps_per_second': 2.461, 'epoch': 7.0}
{'train@spa.rst.rststb_loss': 2.069946765899658, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4160714285714286, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10497569238804402, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.16106336648513442, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.12983871235161132, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0699470043182373, 'train@spa.rst.rststb_runtime': 26.989, 'train@spa.rst.rststb_samples_per_second': 82.997, 'train@spa.rst.rststb_steps_per_second': 2.594, 'epoch': 8.0}
{'loss': 2.1425, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.320584297180176, 'eval_accuracy@spa.rst.rststb': 0.3785900783289817, 'eval_f1@spa.rst.rststb': 0.10784997959970999, 'eval_precision@spa.rst.rststb': 0.09794029066359426, 'eval_recall@spa.rst.rststb': 0.13686777535462016, 'eval_loss@spa.rst.rststb': 2.320584774017334, 'eval_runtime': 4.8736, 'eval_samples_per_second': 78.587, 'eval_steps_per_second': 2.462, 'epoch': 8.0}
{'train@spa.rst.rststb_loss': 2.038377046585083, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.42232142857142857, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.1074210066366524, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.15232281006836793, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13449091652411602, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.038376808166504, 'train@spa.rst.rststb_runtime': 27.0371, 'train@spa.rst.rststb_samples_per_second': 82.849, 'train@spa.rst.rststb_steps_per_second': 2.589, 'epoch': 9.0}
{'loss': 2.1135, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.294132947921753, 'eval_accuracy@spa.rst.rststb': 0.37597911227154046, 'eval_f1@spa.rst.rststb': 0.11362949395339639, 'eval_precision@spa.rst.rststb': 0.14002741880774, 'eval_recall@spa.rst.rststb': 0.14317677156231093, 'eval_loss@spa.rst.rststb': 2.294133186340332, 'eval_runtime': 4.8847, 'eval_samples_per_second': 78.408, 'eval_steps_per_second': 2.457, 'epoch': 9.0}
{'train@spa.rst.rststb_loss': 2.0163002014160156, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4263392857142857, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.11106089457400391, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.15476320767095264, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13696149771192115, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0163002014160156, 'train@spa.rst.rststb_runtime': 27.023, 'train@spa.rst.rststb_samples_per_second': 82.892, 'train@spa.rst.rststb_steps_per_second': 2.59, 'epoch': 10.0}
{'loss': 2.082, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.279766082763672, 'eval_accuracy@spa.rst.rststb': 0.3785900783289817, 'eval_f1@spa.rst.rststb': 0.11236217156712504, 'eval_precision@spa.rst.rststb': 0.1380656332838408, 'eval_recall@spa.rst.rststb': 0.14382570082902085, 'eval_loss@spa.rst.rststb': 2.279766321182251, 'eval_runtime': 4.8745, 'eval_samples_per_second': 78.572, 'eval_steps_per_second': 2.462, 'epoch': 10.0}
{'train@spa.rst.rststb_loss': 2.003622055053711, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.43080357142857145, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.11401479809063578, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.16228829929090674, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13940744359855228, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.003621816635132, 'train@spa.rst.rststb_runtime': 27.015, 'train@spa.rst.rststb_samples_per_second': 82.917, 'train@spa.rst.rststb_steps_per_second': 2.591, 'epoch': 11.0}
{'loss': 2.0637, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.2691049575805664, 'eval_accuracy@spa.rst.rststb': 0.3785900783289817, 'eval_f1@spa.rst.rststb': 0.1126094528099497, 'eval_precision@spa.rst.rststb': 0.13825197895153352, 'eval_recall@spa.rst.rststb': 0.14382570082902085, 'eval_loss@spa.rst.rststb': 2.2691054344177246, 'eval_runtime': 4.8861, 'eval_samples_per_second': 78.385, 'eval_steps_per_second': 2.456, 'epoch': 11.0}
{'train@spa.rst.rststb_loss': 1.9996074438095093, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.43214285714285716, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.1144642090373729, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.16300939941542758, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.1395501153518904, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.999607801437378, 'train@spa.rst.rststb_runtime': 27.0163, 'train@spa.rst.rststb_samples_per_second': 82.913, 'train@spa.rst.rststb_steps_per_second': 2.591, 'epoch': 12.0}
{'loss': 2.0533, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.2665555477142334, 'eval_accuracy@spa.rst.rststb': 0.3785900783289817, 'eval_f1@spa.rst.rststb': 0.11269300091342119, 'eval_precision@spa.rst.rststb': 0.139307914756206, 'eval_recall@spa.rst.rststb': 0.14282326353049107, 'eval_loss@spa.rst.rststb': 2.2665555477142334, 'eval_runtime': 4.8844, 'eval_samples_per_second': 78.413, 'eval_steps_per_second': 2.457, 'epoch': 12.0}
{'train_runtime': 1064.1731, 'train_samples_per_second': 25.259, 'train_steps_per_second': 0.789, 'train_loss': 2.325702231270926, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.3257
  train_runtime            = 0:17:44.17
  train_samples_per_second =     25.259
  train_steps_per_second   =      0.789
{'train@eng.rst.gum_loss': 2.3861865997314453, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.2942361660790099, 'train@eng.rst.gum_f1@eng.rst.gum': 0.0629448497780334, 'train@eng.rst.gum_precision@eng.rst.gum': 0.08749601918038967, 'train@eng.rst.gum_recall@eng.rst.gum': 0.08499020200600056, 'train@eng.rst.gum_loss@eng.rst.gum': 2.3861873149871826, 'train@eng.rst.gum_runtime': 163.0796, 'train@eng.rst.gum_samples_per_second': 85.216, 'train@eng.rst.gum_steps_per_second': 2.667, 'epoch': 1.0}
{'loss': 2.7552, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.4651718139648438, 'eval_accuracy@eng.rst.gum': 0.2829222894369474, 'eval_f1@eng.rst.gum': 0.06454212767686116, 'eval_precision@eng.rst.gum': 0.10019527482223958, 'eval_recall@eng.rst.gum': 0.0905695991282205, 'eval_loss@eng.rst.gum': 2.4651718139648438, 'eval_runtime': 25.5324, 'eval_samples_per_second': 84.168, 'eval_steps_per_second': 2.663, 'epoch': 1.0}
{'train@eng.rst.gum_loss': 1.920023798942566, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.44268547168453626, 'train@eng.rst.gum_f1@eng.rst.gum': 0.2169421170807226, 'train@eng.rst.gum_precision@eng.rst.gum': 0.34206435424383114, 'train@eng.rst.gum_recall@eng.rst.gum': 0.22509596731556994, 'train@eng.rst.gum_loss@eng.rst.gum': 1.920023798942566, 'train@eng.rst.gum_runtime': 163.3778, 'train@eng.rst.gum_samples_per_second': 85.061, 'train@eng.rst.gum_steps_per_second': 2.663, 'epoch': 2.0}
{'loss': 2.2019, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.0335865020751953, 'eval_accuracy@eng.rst.gum': 0.4006514657980456, 'eval_f1@eng.rst.gum': 0.19718326020398624, 'eval_precision@eng.rst.gum': 0.2859161489914283, 'eval_recall@eng.rst.gum': 0.2102909412531976, 'eval_loss@eng.rst.gum': 2.033586263656616, 'eval_runtime': 25.5931, 'eval_samples_per_second': 83.968, 'eval_steps_per_second': 2.657, 'epoch': 2.0}
{'train@eng.rst.gum_loss': 1.7052260637283325, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5052169532992732, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3039187457506393, 'train@eng.rst.gum_precision@eng.rst.gum': 0.40147490316805623, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3114299956323308, 'train@eng.rst.gum_loss@eng.rst.gum': 1.7052263021469116, 'train@eng.rst.gum_runtime': 162.9842, 'train@eng.rst.gum_samples_per_second': 85.266, 'train@eng.rst.gum_steps_per_second': 2.669, 'epoch': 3.0}
{'loss': 1.882, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.8446249961853027, 'eval_accuracy@eng.rst.gum': 0.46812470916705445, 'eval_f1@eng.rst.gum': 0.288867440800793, 'eval_precision@eng.rst.gum': 0.32557831844485213, 'eval_recall@eng.rst.gum': 0.3002517734143557, 'eval_loss@eng.rst.gum': 1.8446251153945923, 'eval_runtime': 25.5323, 'eval_samples_per_second': 84.168, 'eval_steps_per_second': 2.663, 'epoch': 3.0}
{'train@eng.rst.gum_loss': 1.6081006526947021, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5197524645606966, 'train@eng.rst.gum_f1@eng.rst.gum': 0.33130294836216106, 'train@eng.rst.gum_precision@eng.rst.gum': 0.4408159770835244, 'train@eng.rst.gum_recall@eng.rst.gum': 0.33457047885280194, 'train@eng.rst.gum_loss@eng.rst.gum': 1.6081005334854126, 'train@eng.rst.gum_runtime': 162.95, 'train@eng.rst.gum_samples_per_second': 85.284, 'train@eng.rst.gum_steps_per_second': 2.67, 'epoch': 4.0}
{'loss': 1.7336, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.7748185396194458, 'eval_accuracy@eng.rst.gum': 0.4872033503955328, 'eval_f1@eng.rst.gum': 0.32205152107982893, 'eval_precision@eng.rst.gum': 0.3874994503175242, 'eval_recall@eng.rst.gum': 0.3265273928957817, 'eval_loss@eng.rst.gum': 1.7748184204101562, 'eval_runtime': 25.505, 'eval_samples_per_second': 84.258, 'eval_steps_per_second': 2.666, 'epoch': 4.0}
{'train@eng.rst.gum_loss': 1.5417550802230835, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5384615384615384, 'train@eng.rst.gum_f1@eng.rst.gum': 0.38502323529505506, 'train@eng.rst.gum_precision@eng.rst.gum': 0.524453934254041, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3881694863644534, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5417548418045044, 'train@eng.rst.gum_runtime': 163.4284, 'train@eng.rst.gum_samples_per_second': 85.034, 'train@eng.rst.gum_steps_per_second': 2.662, 'epoch': 5.0}
{'loss': 1.6477, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.7207882404327393, 'eval_accuracy@eng.rst.gum': 0.5006979990693345, 'eval_f1@eng.rst.gum': 0.3625234112689785, 'eval_precision@eng.rst.gum': 0.3996339347679467, 'eval_recall@eng.rst.gum': 0.3749004034925964, 'eval_loss@eng.rst.gum': 1.7207882404327393, 'eval_runtime': 25.5829, 'eval_samples_per_second': 84.001, 'eval_steps_per_second': 2.658, 'epoch': 5.0}
{'train@eng.rst.gum_loss': 1.4955778121948242, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5506224364970858, 'train@eng.rst.gum_f1@eng.rst.gum': 0.40607301480477564, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5240990322215494, 'train@eng.rst.gum_recall@eng.rst.gum': 0.40407891291787906, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4955779314041138, 'train@eng.rst.gum_runtime': 162.9686, 'train@eng.rst.gum_samples_per_second': 85.274, 'train@eng.rst.gum_steps_per_second': 2.669, 'epoch': 6.0}
{'loss': 1.5953, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.6890416145324707, 'eval_accuracy@eng.rst.gum': 0.5141926477431363, 'eval_f1@eng.rst.gum': 0.3832082630311982, 'eval_precision@eng.rst.gum': 0.48254170560668364, 'eval_recall@eng.rst.gum': 0.39224702478870377, 'eval_loss@eng.rst.gum': 1.6890416145324707, 'eval_runtime': 25.5111, 'eval_samples_per_second': 84.238, 'eval_steps_per_second': 2.666, 'epoch': 6.0}
{'train@eng.rst.gum_loss': 1.463898777961731, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5586817298697561, 'train@eng.rst.gum_f1@eng.rst.gum': 0.418668456020639, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5338835297645695, 'train@eng.rst.gum_recall@eng.rst.gum': 0.41707365735893037, 'train@eng.rst.gum_loss@eng.rst.gum': 1.463898777961731, 'train@eng.rst.gum_runtime': 163.4091, 'train@eng.rst.gum_samples_per_second': 85.044, 'train@eng.rst.gum_steps_per_second': 2.662, 'epoch': 7.0}
{'loss': 1.5521, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.672616958618164, 'eval_accuracy@eng.rst.gum': 0.5137273150302466, 'eval_f1@eng.rst.gum': 0.38664478381728457, 'eval_precision@eng.rst.gum': 0.49184231843763365, 'eval_recall@eng.rst.gum': 0.3949597863095475, 'eval_loss@eng.rst.gum': 1.6726171970367432, 'eval_runtime': 25.5748, 'eval_samples_per_second': 84.028, 'eval_steps_per_second': 2.659, 'epoch': 7.0}
{'train@eng.rst.gum_loss': 1.4417641162872314, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5654457796646758, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4287063784259833, 'train@eng.rst.gum_precision@eng.rst.gum': 0.534100478422547, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4252007281742138, 'train@eng.rst.gum_loss@eng.rst.gum': 1.441764235496521, 'train@eng.rst.gum_runtime': 163.2458, 'train@eng.rst.gum_samples_per_second': 85.129, 'train@eng.rst.gum_steps_per_second': 2.665, 'epoch': 8.0}
{'loss': 1.5239, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.659084439277649, 'eval_accuracy@eng.rst.gum': 0.5169846440204746, 'eval_f1@eng.rst.gum': 0.3951265741423106, 'eval_precision@eng.rst.gum': 0.5060819556573183, 'eval_recall@eng.rst.gum': 0.40124761924102825, 'eval_loss@eng.rst.gum': 1.659084439277649, 'eval_runtime': 25.5019, 'eval_samples_per_second': 84.268, 'eval_steps_per_second': 2.666, 'epoch': 8.0}
{'train@eng.rst.gum_loss': 1.4233908653259277, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5706987119522199, 'train@eng.rst.gum_f1@eng.rst.gum': 0.43841269462226523, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5235747623086318, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4359893264580612, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4233909845352173, 'train@eng.rst.gum_runtime': 163.0927, 'train@eng.rst.gum_samples_per_second': 85.209, 'train@eng.rst.gum_steps_per_second': 2.667, 'epoch': 9.0}
{'loss': 1.501, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.6436024904251099, 'eval_accuracy@eng.rst.gum': 0.5174499767333643, 'eval_f1@eng.rst.gum': 0.4048256908810528, 'eval_precision@eng.rst.gum': 0.48945607559975096, 'eval_recall@eng.rst.gum': 0.4112671042228321, 'eval_loss@eng.rst.gum': 1.6436026096343994, 'eval_runtime': 25.4999, 'eval_samples_per_second': 84.275, 'eval_steps_per_second': 2.667, 'epoch': 9.0}
{'train@eng.rst.gum_loss': 1.4108999967575073, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5745844426854717, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4424940805195206, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5259718459349983, 'train@eng.rst.gum_recall@eng.rst.gum': 0.43658725011639654, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4108998775482178, 'train@eng.rst.gum_runtime': 163.3319, 'train@eng.rst.gum_samples_per_second': 85.084, 'train@eng.rst.gum_steps_per_second': 2.663, 'epoch': 10.0}
{'loss': 1.4899, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.6377668380737305, 'eval_accuracy@eng.rst.gum': 0.5225686365751512, 'eval_f1@eng.rst.gum': 0.40858787628746757, 'eval_precision@eng.rst.gum': 0.49463560217908376, 'eval_recall@eng.rst.gum': 0.41143113851879604, 'eval_loss@eng.rst.gum': 1.6377668380737305, 'eval_runtime': 25.5433, 'eval_samples_per_second': 84.132, 'eval_steps_per_second': 2.662, 'epoch': 10.0}
{'train@eng.rst.gum_loss': 1.404684066772461, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5742246528027631, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4450657406013594, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5206467558884862, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4423131804002196, 'train@eng.rst.gum_loss@eng.rst.gum': 1.404684066772461, 'train@eng.rst.gum_runtime': 163.1144, 'train@eng.rst.gum_samples_per_second': 85.198, 'train@eng.rst.gum_steps_per_second': 2.667, 'epoch': 11.0}
{'loss': 1.484, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.6330466270446777, 'eval_accuracy@eng.rst.gum': 0.5197766402978129, 'eval_f1@eng.rst.gum': 0.40962123362745, 'eval_precision@eng.rst.gum': 0.48633996846571553, 'eval_recall@eng.rst.gum': 0.4146915417578764, 'eval_loss@eng.rst.gum': 1.6330467462539673, 'eval_runtime': 25.539, 'eval_samples_per_second': 84.146, 'eval_steps_per_second': 2.663, 'epoch': 11.0}
{'train@eng.rst.gum_loss': 1.4018725156784058, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5747283586385551, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4456201508822531, 'train@eng.rst.gum_precision@eng.rst.gum': 0.524146267916911, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4411170271931335, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4018725156784058, 'train@eng.rst.gum_runtime': 163.0135, 'train@eng.rst.gum_samples_per_second': 85.251, 'train@eng.rst.gum_steps_per_second': 2.668, 'epoch': 12.0}
{'loss': 1.4723, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.6315795183181763, 'eval_accuracy@eng.rst.gum': 0.5234993020009306, 'eval_f1@eng.rst.gum': 0.41201809245780063, 'eval_precision@eng.rst.gum': 0.4891660454821764, 'eval_recall@eng.rst.gum': 0.4158548525211873, 'eval_loss@eng.rst.gum': 1.6315793991088867, 'eval_runtime': 25.4919, 'eval_samples_per_second': 84.301, 'eval_steps_per_second': 2.668, 'epoch': 12.0}
{'train_runtime': 6410.5269, 'train_samples_per_second': 26.014, 'train_steps_per_second': 0.814, 'train_loss': 1.7365823679956895, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.3257
  train_runtime            = 0:17:44.17
  train_samples_per_second =     25.259
  train_steps_per_second   =      0.789
-------------------------------------------------------------------
Lang1:  spa.rst.sctb    Lang2:  eng.rst.gum
Saving run to:  runs/full_shot/FullShot=v4_spa.rst.sctb_eng.rst.gum_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 439 examples
read 94 examples
read 159 examples
read 13897 examples
read 2149 examples
read 2091 examples
Total prediction labels:  29
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=29, bias=True)
    )
  )
)
{'train@spa.rst.sctb_loss': 3.1103081703186035, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.33940774487471526, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02372137666255314, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.028304370721432336, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.041899641577060935, 'train@spa.rst.sctb_loss@spa.rst.sctb': 3.1103084087371826, 'train@spa.rst.sctb_runtime': 5.4783, 'train@spa.rst.sctb_samples_per_second': 80.134, 'train@spa.rst.sctb_steps_per_second': 2.556, 'epoch': 1.0}
{'loss': 3.2609, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.1001551151275635, 'eval_accuracy@spa.rst.sctb': 0.3723404255319149, 'eval_f1@spa.rst.sctb': 0.0457516339869281, 'eval_precision@spa.rst.sctb': 0.047619047619047616, 'eval_recall@spa.rst.sctb': 0.06632892391406324, 'eval_loss@spa.rst.sctb': 3.1001551151275635, 'eval_runtime': 1.4052, 'eval_samples_per_second': 66.895, 'eval_steps_per_second': 2.135, 'epoch': 1.0}
{'train@spa.rst.sctb_loss': 2.8622968196868896, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3462414578587699, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.023104789861149944, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.031067588325652845, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04256272401433692, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.862297296524048, 'train@spa.rst.sctb_runtime': 5.4806, 'train@spa.rst.sctb_samples_per_second': 80.101, 'train@spa.rst.sctb_steps_per_second': 2.554, 'epoch': 2.0}
{'loss': 3.0062, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.851245880126953, 'eval_accuracy@spa.rst.sctb': 0.3723404255319149, 'eval_f1@spa.rst.sctb': 0.041794024078348195, 'eval_precision@spa.rst.sctb': 0.050980392156862744, 'eval_recall@spa.rst.sctb': 0.06501547987616098, 'eval_loss@spa.rst.sctb': 2.8512468338012695, 'eval_runtime': 1.4249, 'eval_samples_per_second': 65.969, 'eval_steps_per_second': 2.105, 'epoch': 2.0}
{'train@spa.rst.sctb_loss': 2.627331018447876, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3416856492027335, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.021258503401360544, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.014269406392694063, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.041666666666666664, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.627331495285034, 'train@spa.rst.sctb_runtime': 5.4875, 'train@spa.rst.sctb_samples_per_second': 80.0, 'train@spa.rst.sctb_steps_per_second': 2.551, 'epoch': 3.0}
{'loss': 2.7837, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.623781681060791, 'eval_accuracy@spa.rst.sctb': 0.3723404255319149, 'eval_f1@spa.rst.sctb': 0.042004485078488876, 'eval_precision@spa.rst.sctb': 0.06054729584141348, 'eval_recall@spa.rst.sctb': 0.06501547987616098, 'eval_loss@spa.rst.sctb': 2.623781442642212, 'eval_runtime': 1.4108, 'eval_samples_per_second': 66.629, 'eval_steps_per_second': 2.126, 'epoch': 3.0}
{'train@spa.rst.sctb_loss': 2.447878360748291, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3416856492027335, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02129471890971039, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.014302059496567507, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.041666666666666664, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.447878360748291, 'train@spa.rst.sctb_runtime': 5.4916, 'train@spa.rst.sctb_samples_per_second': 79.941, 'train@spa.rst.sctb_steps_per_second': 2.549, 'epoch': 4.0}
{'loss': 2.5717, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.4588539600372314, 'eval_accuracy@spa.rst.sctb': 0.3723404255319149, 'eval_f1@spa.rst.sctb': 0.042004485078488876, 'eval_precision@spa.rst.sctb': 0.06054729584141348, 'eval_recall@spa.rst.sctb': 0.06501547987616098, 'eval_loss@spa.rst.sctb': 2.4588534832000732, 'eval_runtime': 1.4148, 'eval_samples_per_second': 66.44, 'eval_steps_per_second': 2.12, 'epoch': 4.0}
{'train@spa.rst.sctb_loss': 2.332068681716919, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3439635535307517, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02231099656357388, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.020419973544973543, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.042114695340501794, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.332068681716919, 'train@spa.rst.sctb_runtime': 5.5199, 'train@spa.rst.sctb_samples_per_second': 79.53, 'train@spa.rst.sctb_steps_per_second': 2.536, 'epoch': 5.0}
{'loss': 2.4201, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.3538482189178467, 'eval_accuracy@spa.rst.sctb': 0.3723404255319149, 'eval_f1@spa.rst.sctb': 0.042004485078488876, 'eval_precision@spa.rst.sctb': 0.06054729584141348, 'eval_recall@spa.rst.sctb': 0.06501547987616098, 'eval_loss@spa.rst.sctb': 2.353848457336426, 'eval_runtime': 1.4159, 'eval_samples_per_second': 66.388, 'eval_steps_per_second': 2.119, 'epoch': 5.0}
{'train@spa.rst.sctb_loss': 2.262878179550171, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3462414578587699, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02316479496486654, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.02491782675947409, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04256272401433692, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.262878179550171, 'train@spa.rst.sctb_runtime': 5.5095, 'train@spa.rst.sctb_samples_per_second': 79.681, 'train@spa.rst.sctb_steps_per_second': 2.541, 'epoch': 6.0}
{'loss': 2.3337, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.2940151691436768, 'eval_accuracy@spa.rst.sctb': 0.3723404255319149, 'eval_f1@spa.rst.sctb': 0.042004485078488876, 'eval_precision@spa.rst.sctb': 0.06054729584141348, 'eval_recall@spa.rst.sctb': 0.06501547987616098, 'eval_loss@spa.rst.sctb': 2.2940146923065186, 'eval_runtime': 1.4313, 'eval_samples_per_second': 65.677, 'eval_steps_per_second': 2.096, 'epoch': 6.0}
{'train@spa.rst.sctb_loss': 2.2187178134918213, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.35079726651480636, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.026671845172819952, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.027404717112597548, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.043969534050179206, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.2187180519104004, 'train@spa.rst.sctb_runtime': 5.5379, 'train@spa.rst.sctb_samples_per_second': 79.271, 'train@spa.rst.sctb_steps_per_second': 2.528, 'epoch': 7.0}
{'loss': 2.2799, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.2580935955047607, 'eval_accuracy@spa.rst.sctb': 0.39361702127659576, 'eval_f1@spa.rst.sctb': 0.05246094901267315, 'eval_precision@spa.rst.sctb': 0.05182072829131652, 'eval_recall@spa.rst.sctb': 0.07252087437845951, 'eval_loss@spa.rst.sctb': 2.25809383392334, 'eval_runtime': 1.4242, 'eval_samples_per_second': 66.004, 'eval_steps_per_second': 2.106, 'epoch': 7.0}
{'train@spa.rst.sctb_loss': 2.1919443607330322, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3712984054669704, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03248781604683871, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03371702968477162, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.048001792114695345, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.1919443607330322, 'train@spa.rst.sctb_runtime': 5.5452, 'train@spa.rst.sctb_samples_per_second': 79.168, 'train@spa.rst.sctb_steps_per_second': 2.525, 'epoch': 8.0}
{'loss': 2.2342, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.237175941467285, 'eval_accuracy@spa.rst.sctb': 0.40425531914893614, 'eval_f1@spa.rst.sctb': 0.05508255933952528, 'eval_precision@spa.rst.sctb': 0.05038824646667784, 'eval_recall@spa.rst.sctb': 0.07561684961065766, 'eval_loss@spa.rst.sctb': 2.237175941467285, 'eval_runtime': 1.4105, 'eval_samples_per_second': 66.643, 'eval_steps_per_second': 2.127, 'epoch': 8.0}
{'train@spa.rst.sctb_loss': 2.1732332706451416, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3940774487471526, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03804755547078645, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03663419913419914, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05265232974910394, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.1732335090637207, 'train@spa.rst.sctb_runtime': 5.5298, 'train@spa.rst.sctb_samples_per_second': 79.388, 'train@spa.rst.sctb_steps_per_second': 2.532, 'epoch': 9.0}
{'loss': 2.2136, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.2220635414123535, 'eval_accuracy@spa.rst.sctb': 0.40425531914893614, 'eval_f1@spa.rst.sctb': 0.05437469105289175, 'eval_precision@spa.rst.sctb': 0.04735666418466121, 'eval_recall@spa.rst.sctb': 0.07561684961065766, 'eval_loss@spa.rst.sctb': 2.2220635414123535, 'eval_runtime': 1.4338, 'eval_samples_per_second': 65.559, 'eval_steps_per_second': 2.092, 'epoch': 9.0}
{'train@spa.rst.sctb_loss': 2.160583019256592, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4031890660592255, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.040001011020119305, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.036944444444444446, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05461469534050179, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.160583257675171, 'train@spa.rst.sctb_runtime': 5.5402, 'train@spa.rst.sctb_samples_per_second': 79.239, 'train@spa.rst.sctb_steps_per_second': 2.527, 'epoch': 10.0}
{'loss': 2.2073, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.212930679321289, 'eval_accuracy@spa.rst.sctb': 0.4148936170212766, 'eval_f1@spa.rst.sctb': 0.05710041592394533, 'eval_precision@spa.rst.sctb': 0.04866759538039815, 'eval_recall@spa.rst.sctb': 0.0787128248428558, 'eval_loss@spa.rst.sctb': 2.21293044090271, 'eval_runtime': 1.4225, 'eval_samples_per_second': 66.08, 'eval_steps_per_second': 2.109, 'epoch': 10.0}
{'train@spa.rst.sctb_loss': 2.153575897216797, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.41002277904328016, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.04117561300440126, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.037206413730803974, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.055958781362007166, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.153575897216797, 'train@spa.rst.sctb_runtime': 5.5383, 'train@spa.rst.sctb_samples_per_second': 79.266, 'train@spa.rst.sctb_steps_per_second': 2.528, 'epoch': 11.0}
{'loss': 2.1993, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.2073516845703125, 'eval_accuracy@spa.rst.sctb': 0.4148936170212766, 'eval_f1@spa.rst.sctb': 0.05710041592394533, 'eval_precision@spa.rst.sctb': 0.04866759538039815, 'eval_recall@spa.rst.sctb': 0.0787128248428558, 'eval_loss@spa.rst.sctb': 2.207350969314575, 'eval_runtime': 1.4219, 'eval_samples_per_second': 66.107, 'eval_steps_per_second': 2.11, 'epoch': 11.0}
{'train@spa.rst.sctb_loss': 2.1512649059295654, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4123006833712984, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.0415538362346873, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03729564032697547, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.056406810035842296, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.1512649059295654, 'train@spa.rst.sctb_runtime': 5.5534, 'train@spa.rst.sctb_samples_per_second': 79.051, 'train@spa.rst.sctb_steps_per_second': 2.521, 'epoch': 12.0}
{'loss': 2.1941, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.2058186531066895, 'eval_accuracy@spa.rst.sctb': 0.4148936170212766, 'eval_f1@spa.rst.sctb': 0.05710041592394533, 'eval_precision@spa.rst.sctb': 0.04866759538039815, 'eval_recall@spa.rst.sctb': 0.0787128248428558, 'eval_loss@spa.rst.sctb': 2.2058184146881104, 'eval_runtime': 1.4138, 'eval_samples_per_second': 66.485, 'eval_steps_per_second': 2.122, 'epoch': 12.0}
{'train_runtime': 216.7291, 'train_samples_per_second': 24.307, 'train_steps_per_second': 0.775, 'train_loss': 2.4754055681682767, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.4754
  train_runtime            = 0:03:36.72
  train_samples_per_second =     24.307
  train_steps_per_second   =      0.775
{'train@eng.rst.gum_loss': 2.5023908615112305, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.25501906886378356, 'train@eng.rst.gum_f1@eng.rst.gum': 0.04901724416857895, 'train@eng.rst.gum_precision@eng.rst.gum': 0.0590851309534207, 'train@eng.rst.gum_recall@eng.rst.gum': 0.06511948909079004, 'train@eng.rst.gum_loss@eng.rst.gum': 2.5023908615112305, 'train@eng.rst.gum_runtime': 163.4353, 'train@eng.rst.gum_samples_per_second': 85.031, 'train@eng.rst.gum_steps_per_second': 2.662, 'epoch': 1.0}
{'loss': 2.7506, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.5800817012786865, 'eval_accuracy@eng.rst.gum': 0.25779432294090276, 'eval_f1@eng.rst.gum': 0.05658754114030554, 'eval_precision@eng.rst.gum': 0.06279166140134464, 'eval_recall@eng.rst.gum': 0.07292854212399215, 'eval_loss@eng.rst.gum': 2.5800814628601074, 'eval_runtime': 25.6146, 'eval_samples_per_second': 83.897, 'eval_steps_per_second': 2.655, 'epoch': 1.0}
{'train@eng.rst.gum_loss': 2.0233874320983887, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.4122472476073973, 'train@eng.rst.gum_f1@eng.rst.gum': 0.18777505481472162, 'train@eng.rst.gum_precision@eng.rst.gum': 0.2394971803080734, 'train@eng.rst.gum_recall@eng.rst.gum': 0.19426379650516457, 'train@eng.rst.gum_loss@eng.rst.gum': 2.0233874320983887, 'train@eng.rst.gum_runtime': 163.1817, 'train@eng.rst.gum_samples_per_second': 85.163, 'train@eng.rst.gum_steps_per_second': 2.666, 'epoch': 2.0}
{'loss': 2.3196, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.1242587566375732, 'eval_accuracy@eng.rst.gum': 0.39274080967892044, 'eval_f1@eng.rst.gum': 0.1818090542969118, 'eval_precision@eng.rst.gum': 0.22281780228913647, 'eval_recall@eng.rst.gum': 0.19433288924856273, 'eval_loss@eng.rst.gum': 2.1242587566375732, 'eval_runtime': 25.51, 'eval_samples_per_second': 84.241, 'eval_steps_per_second': 2.666, 'epoch': 2.0}
{'train@eng.rst.gum_loss': 1.779686689376831, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.48103907318126216, 'train@eng.rst.gum_f1@eng.rst.gum': 0.27555649667265525, 'train@eng.rst.gum_precision@eng.rst.gum': 0.3929738501921866, 'train@eng.rst.gum_recall@eng.rst.gum': 0.2917753999616758, 'train@eng.rst.gum_loss@eng.rst.gum': 1.7796865701675415, 'train@eng.rst.gum_runtime': 163.0454, 'train@eng.rst.gum_samples_per_second': 85.234, 'train@eng.rst.gum_steps_per_second': 2.668, 'epoch': 3.0}
{'loss': 1.9689, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.9109798669815063, 'eval_accuracy@eng.rst.gum': 0.4523033969288041, 'eval_f1@eng.rst.gum': 0.2709185563660608, 'eval_precision@eng.rst.gum': 0.30268342380846275, 'eval_recall@eng.rst.gum': 0.29002079054811014, 'eval_loss@eng.rst.gum': 1.9109798669815063, 'eval_runtime': 25.5075, 'eval_samples_per_second': 84.25, 'eval_steps_per_second': 2.666, 'epoch': 3.0}
{'train@eng.rst.gum_loss': 1.6623759269714355, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5065841548535656, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3065480518761357, 'train@eng.rst.gum_precision@eng.rst.gum': 0.4230125222096504, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3196985088021312, 'train@eng.rst.gum_loss@eng.rst.gum': 1.6623759269714355, 'train@eng.rst.gum_runtime': 163.2443, 'train@eng.rst.gum_samples_per_second': 85.13, 'train@eng.rst.gum_steps_per_second': 2.665, 'epoch': 4.0}
{'loss': 1.7941, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.8226720094680786, 'eval_accuracy@eng.rst.gum': 0.47650069799906936, 'eval_f1@eng.rst.gum': 0.2989286660188212, 'eval_precision@eng.rst.gum': 0.33423184210311085, 'eval_recall@eng.rst.gum': 0.31399661744483703, 'eval_loss@eng.rst.gum': 1.8226720094680786, 'eval_runtime': 25.5199, 'eval_samples_per_second': 84.209, 'eval_steps_per_second': 2.665, 'epoch': 4.0}
{'train@eng.rst.gum_loss': 1.5888270139694214, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5286032956753256, 'train@eng.rst.gum_f1@eng.rst.gum': 0.35362946586620775, 'train@eng.rst.gum_precision@eng.rst.gum': 0.48623481549649233, 'train@eng.rst.gum_recall@eng.rst.gum': 0.36037179032093886, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5888270139694214, 'train@eng.rst.gum_runtime': 163.1988, 'train@eng.rst.gum_samples_per_second': 85.154, 'train@eng.rst.gum_steps_per_second': 2.665, 'epoch': 5.0}
{'loss': 1.697, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.766361117362976, 'eval_accuracy@eng.rst.gum': 0.48534201954397393, 'eval_f1@eng.rst.gum': 0.32889755769843293, 'eval_precision@eng.rst.gum': 0.3693005602604331, 'eval_recall@eng.rst.gum': 0.3422200978056404, 'eval_loss@eng.rst.gum': 1.7663609981536865, 'eval_runtime': 25.4886, 'eval_samples_per_second': 84.312, 'eval_steps_per_second': 2.668, 'epoch': 5.0}
{'train@eng.rst.gum_loss': 1.5387749671936035, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5405483197812477, 'train@eng.rst.gum_f1@eng.rst.gum': 0.38423461077419196, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5370355011488661, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3880582242911908, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5387749671936035, 'train@eng.rst.gum_runtime': 163.3453, 'train@eng.rst.gum_samples_per_second': 85.077, 'train@eng.rst.gum_steps_per_second': 2.663, 'epoch': 6.0}
{'loss': 1.6384, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.730400562286377, 'eval_accuracy@eng.rst.gum': 0.495114006514658, 'eval_f1@eng.rst.gum': 0.36094894690539653, 'eval_precision@eng.rst.gum': 0.4693142045051122, 'eval_recall@eng.rst.gum': 0.37405009383678967, 'eval_loss@eng.rst.gum': 1.7304004430770874, 'eval_runtime': 25.5661, 'eval_samples_per_second': 84.057, 'eval_steps_per_second': 2.66, 'epoch': 6.0}
{'train@eng.rst.gum_loss': 1.5034222602844238, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5485356551773765, 'train@eng.rst.gum_f1@eng.rst.gum': 0.39329688134824314, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5135426399878158, 'train@eng.rst.gum_recall@eng.rst.gum': 0.39908165305820614, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5034222602844238, 'train@eng.rst.gum_runtime': 163.2635, 'train@eng.rst.gum_samples_per_second': 85.12, 'train@eng.rst.gum_steps_per_second': 2.664, 'epoch': 7.0}
{'loss': 1.5963, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.709588885307312, 'eval_accuracy@eng.rst.gum': 0.5039553280595626, 'eval_f1@eng.rst.gum': 0.3744468241857201, 'eval_precision@eng.rst.gum': 0.5251901148446254, 'eval_recall@eng.rst.gum': 0.38783235131049054, 'eval_loss@eng.rst.gum': 1.709588885307312, 'eval_runtime': 25.5866, 'eval_samples_per_second': 83.989, 'eval_steps_per_second': 2.658, 'epoch': 7.0}
{'train@eng.rst.gum_loss': 1.4795734882354736, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5563790746204217, 'train@eng.rst.gum_f1@eng.rst.gum': 0.40342372920568226, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5280765226577638, 'train@eng.rst.gum_recall@eng.rst.gum': 0.40654351662259275, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4795734882354736, 'train@eng.rst.gum_runtime': 162.9258, 'train@eng.rst.gum_samples_per_second': 85.297, 'train@eng.rst.gum_steps_per_second': 2.67, 'epoch': 8.0}
{'loss': 1.5648, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.6937233209609985, 'eval_accuracy@eng.rst.gum': 0.5100046533271289, 'eval_f1@eng.rst.gum': 0.38189773752860473, 'eval_precision@eng.rst.gum': 0.504556646720785, 'eval_recall@eng.rst.gum': 0.39368195199612877, 'eval_loss@eng.rst.gum': 1.693723440170288, 'eval_runtime': 25.451, 'eval_samples_per_second': 84.437, 'eval_steps_per_second': 2.672, 'epoch': 8.0}
{'train@eng.rst.gum_loss': 1.4590815305709839, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5641505360869252, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4218353653935994, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5343554958562625, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4230506475251804, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4590816497802734, 'train@eng.rst.gum_runtime': 163.2177, 'train@eng.rst.gum_samples_per_second': 85.144, 'train@eng.rst.gum_steps_per_second': 2.665, 'epoch': 9.0}
{'loss': 1.5427, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.6771259307861328, 'eval_accuracy@eng.rst.gum': 0.5123313168915775, 'eval_f1@eng.rst.gum': 0.38932126844301956, 'eval_precision@eng.rst.gum': 0.47700082108547476, 'eval_recall@eng.rst.gum': 0.4015447680911403, 'eval_loss@eng.rst.gum': 1.6771260499954224, 'eval_runtime': 25.5551, 'eval_samples_per_second': 84.093, 'eval_steps_per_second': 2.661, 'epoch': 9.0}
{'train@eng.rst.gum_loss': 1.4449658393859863, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5683960567028855, 'train@eng.rst.gum_f1@eng.rst.gum': 0.424971588875426, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5299999154881375, 'train@eng.rst.gum_recall@eng.rst.gum': 0.42337009729672126, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4449659585952759, 'train@eng.rst.gum_runtime': 163.2366, 'train@eng.rst.gum_samples_per_second': 85.134, 'train@eng.rst.gum_steps_per_second': 2.665, 'epoch': 10.0}
{'loss': 1.5249, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.669481635093689, 'eval_accuracy@eng.rst.gum': 0.5109353187529083, 'eval_f1@eng.rst.gum': 0.3880287122112524, 'eval_precision@eng.rst.gum': 0.4743187723763012, 'eval_recall@eng.rst.gum': 0.3976935782588548, 'eval_loss@eng.rst.gum': 1.6694817543029785, 'eval_runtime': 25.5377, 'eval_samples_per_second': 84.15, 'eval_steps_per_second': 2.663, 'epoch': 10.0}
{'train@eng.rst.gum_loss': 1.4373576641082764, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5696913002806361, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4315825855153654, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5278642778110242, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4315437327042324, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4373575448989868, 'train@eng.rst.gum_runtime': 162.9775, 'train@eng.rst.gum_samples_per_second': 85.269, 'train@eng.rst.gum_steps_per_second': 2.669, 'epoch': 11.0}
{'loss': 1.5205, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.663029432296753, 'eval_accuracy@eng.rst.gum': 0.5141926477431363, 'eval_f1@eng.rst.gum': 0.39277314737238334, 'eval_precision@eng.rst.gum': 0.4729982357485916, 'eval_recall@eng.rst.gum': 0.4042075565315104, 'eval_loss@eng.rst.gum': 1.663029432296753, 'eval_runtime': 25.4748, 'eval_samples_per_second': 84.358, 'eval_steps_per_second': 2.669, 'epoch': 11.0}
{'train@eng.rst.gum_loss': 1.434458613395691, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5704108800460531, 'train@eng.rst.gum_f1@eng.rst.gum': 0.43216254664770565, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5299687990126407, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4312035247371726, 'train@eng.rst.gum_loss@eng.rst.gum': 1.434458613395691, 'train@eng.rst.gum_runtime': 163.3227, 'train@eng.rst.gum_samples_per_second': 85.089, 'train@eng.rst.gum_steps_per_second': 2.663, 'epoch': 12.0}
{'loss': 1.5052, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.6616706848144531, 'eval_accuracy@eng.rst.gum': 0.511400651465798, 'eval_f1@eng.rst.gum': 0.3894011036443556, 'eval_precision@eng.rst.gum': 0.4713768949861722, 'eval_recall@eng.rst.gum': 0.4007670623636515, 'eval_loss@eng.rst.gum': 1.6616706848144531, 'eval_runtime': 25.5456, 'eval_samples_per_second': 84.124, 'eval_steps_per_second': 2.662, 'epoch': 12.0}
{'train_runtime': 6411.8205, 'train_samples_per_second': 26.009, 'train_steps_per_second': 0.814, 'train_loss': 1.7852539296351173, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.4754
  train_runtime            = 0:03:36.72
  train_samples_per_second =     24.307
  train_steps_per_second   =      0.775
-------------------------------------------------------------------
Lang1:  tur.pdtb.tdb    Lang2:  eng.rst.gum
Saving run to:  runs/full_shot/FullShot=v4_tur.pdtb.tdb_eng.rst.gum_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2451 examples
read 312 examples
read 422 examples
read 13897 examples
read 2149 examples
read 2091 examples
Total prediction labels:  46
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (tr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (tr): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=46, bias=True)
    )
  )
)
{'train@tur.pdtb.tdb_loss': 2.8921523094177246, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01741967515426626, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010891738953044898, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.8921523094177246, 'train@tur.pdtb.tdb_runtime': 29.5333, 'train@tur.pdtb.tdb_samples_per_second': 82.991, 'train@tur.pdtb.tdb_steps_per_second': 2.607, 'epoch': 1.0}
{'loss': 3.3235, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.8176376819610596, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.8176379203796387, 'eval_runtime': 4.1525, 'eval_samples_per_second': 75.136, 'eval_steps_per_second': 2.408, 'epoch': 1.0}
{'train@tur.pdtb.tdb_loss': 2.463440418243408, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01741967515426626, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010891738953044898, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.4634406566619873, 'train@tur.pdtb.tdb_runtime': 29.6626, 'train@tur.pdtb.tdb_samples_per_second': 82.629, 'train@tur.pdtb.tdb_steps_per_second': 2.596, 'epoch': 2.0}
{'loss': 2.6464, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.3407130241394043, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.3407130241394043, 'eval_runtime': 4.1517, 'eval_samples_per_second': 75.15, 'eval_steps_per_second': 2.409, 'epoch': 2.0}
{'train@tur.pdtb.tdb_loss': 2.3746211528778076, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25132598939208484, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.018135150802231353, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.05437889466863138, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.04383318544809228, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.3746206760406494, 'train@tur.pdtb.tdb_runtime': 29.6454, 'train@tur.pdtb.tdb_samples_per_second': 82.677, 'train@tur.pdtb.tdb_steps_per_second': 2.597, 'epoch': 3.0}
{'loss': 2.4379, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.282322406768799, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.2823221683502197, 'eval_runtime': 4.1651, 'eval_samples_per_second': 74.908, 'eval_steps_per_second': 2.401, 'epoch': 3.0}
{'train@tur.pdtb.tdb_loss': 2.309035539627075, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.2766217870257038, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.03647015734233817, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.043250872597006815, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.056841263739598656, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.309035539627075, 'train@tur.pdtb.tdb_runtime': 29.62, 'train@tur.pdtb.tdb_samples_per_second': 82.748, 'train@tur.pdtb.tdb_steps_per_second': 2.6, 'epoch': 4.0}
{'loss': 2.3731, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.2429699897766113, 'eval_accuracy@tur.pdtb.tdb': 0.26282051282051283, 'eval_f1@tur.pdtb.tdb': 0.02449895948555466, 'eval_precision@tur.pdtb.tdb': 0.019958202716823404, 'eval_recall@tur.pdtb.tdb': 0.04727466013787771, 'eval_loss@tur.pdtb.tdb': 2.2429699897766113, 'eval_runtime': 4.1449, 'eval_samples_per_second': 75.273, 'eval_steps_per_second': 2.413, 'epoch': 4.0}
{'train@tur.pdtb.tdb_loss': 2.25992488861084, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3084455324357405, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.07688930527376928, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.09393444469225944, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.09035819869565402, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.259925127029419, 'train@tur.pdtb.tdb_runtime': 29.6763, 'train@tur.pdtb.tdb_samples_per_second': 82.591, 'train@tur.pdtb.tdb_steps_per_second': 2.595, 'epoch': 5.0}
{'loss': 2.3231, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.2070977687835693, 'eval_accuracy@tur.pdtb.tdb': 0.2916666666666667, 'eval_f1@tur.pdtb.tdb': 0.07458082564014767, 'eval_precision@tur.pdtb.tdb': 0.06910148579519118, 'eval_recall@tur.pdtb.tdb': 0.0977985023874464, 'eval_loss@tur.pdtb.tdb': 2.2070980072021484, 'eval_runtime': 4.1442, 'eval_samples_per_second': 75.285, 'eval_steps_per_second': 2.413, 'epoch': 5.0}
{'train@tur.pdtb.tdb_loss': 2.2158772945404053, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3223174214606283, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.08259884684448955, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.09089029076185969, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.10126480739794765, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.2158772945404053, 'train@tur.pdtb.tdb_runtime': 29.6434, 'train@tur.pdtb.tdb_samples_per_second': 82.683, 'train@tur.pdtb.tdb_steps_per_second': 2.598, 'epoch': 6.0}
{'loss': 2.2803, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.1803956031799316, 'eval_accuracy@tur.pdtb.tdb': 0.2916666666666667, 'eval_f1@tur.pdtb.tdb': 0.07192533550987389, 'eval_precision@tur.pdtb.tdb': 0.06266789863627809, 'eval_recall@tur.pdtb.tdb': 0.09945128106006916, 'eval_loss@tur.pdtb.tdb': 2.1803956031799316, 'eval_runtime': 4.1626, 'eval_samples_per_second': 74.953, 'eval_steps_per_second': 2.402, 'epoch': 6.0}
{'train@tur.pdtb.tdb_loss': 2.184549570083618, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3288453692370461, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.08633337543624538, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.08554367679155125, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.1083245679376229, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.184549331665039, 'train@tur.pdtb.tdb_runtime': 29.6211, 'train@tur.pdtb.tdb_samples_per_second': 82.745, 'train@tur.pdtb.tdb_steps_per_second': 2.6, 'epoch': 7.0}
{'loss': 2.2467, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.161267042160034, 'eval_accuracy@tur.pdtb.tdb': 0.3108974358974359, 'eval_f1@tur.pdtb.tdb': 0.07926792254111309, 'eval_precision@tur.pdtb.tdb': 0.08153669330139918, 'eval_recall@tur.pdtb.tdb': 0.10776462433474113, 'eval_loss@tur.pdtb.tdb': 2.1612672805786133, 'eval_runtime': 4.1603, 'eval_samples_per_second': 74.994, 'eval_steps_per_second': 2.404, 'epoch': 7.0}
{'train@tur.pdtb.tdb_loss': 2.171478509902954, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3325173398612811, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.08828739398892461, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.10128241796966711, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.10888317587206241, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.171478509902954, 'train@tur.pdtb.tdb_runtime': 29.6119, 'train@tur.pdtb.tdb_samples_per_second': 82.771, 'train@tur.pdtb.tdb_steps_per_second': 2.6, 'epoch': 8.0}
{'loss': 2.2227, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.152355909347534, 'eval_accuracy@tur.pdtb.tdb': 0.30128205128205127, 'eval_f1@tur.pdtb.tdb': 0.07563032000480943, 'eval_precision@tur.pdtb.tdb': 0.07529568645640074, 'eval_recall@tur.pdtb.tdb': 0.10390247341376667, 'eval_loss@tur.pdtb.tdb': 2.1523561477661133, 'eval_runtime': 4.1366, 'eval_samples_per_second': 75.425, 'eval_steps_per_second': 2.417, 'epoch': 8.0}
{'train@tur.pdtb.tdb_loss': 2.1464152336120605, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.33414932680538556, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.08762896894118832, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.10250326701803919, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11096220356157593, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1464154720306396, 'train@tur.pdtb.tdb_runtime': 29.5702, 'train@tur.pdtb.tdb_samples_per_second': 82.887, 'train@tur.pdtb.tdb_steps_per_second': 2.604, 'epoch': 9.0}
{'loss': 2.1997, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.134143352508545, 'eval_accuracy@tur.pdtb.tdb': 0.3141025641025641, 'eval_f1@tur.pdtb.tdb': 0.07886360945741323, 'eval_precision@tur.pdtb.tdb': 0.08068066306982563, 'eval_recall@tur.pdtb.tdb': 0.10932433912975002, 'eval_loss@tur.pdtb.tdb': 2.134143352508545, 'eval_runtime': 4.1216, 'eval_samples_per_second': 75.699, 'eval_steps_per_second': 2.426, 'epoch': 9.0}
{'train@tur.pdtb.tdb_loss': 2.1343774795532227, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3361893104855161, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.0881432587436312, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.10118436348757577, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11208424907243124, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1343774795532227, 'train@tur.pdtb.tdb_runtime': 29.5868, 'train@tur.pdtb.tdb_samples_per_second': 82.841, 'train@tur.pdtb.tdb_steps_per_second': 2.603, 'epoch': 10.0}
{'loss': 2.1855, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.125225067138672, 'eval_accuracy@tur.pdtb.tdb': 0.32051282051282054, 'eval_f1@tur.pdtb.tdb': 0.08262242264270205, 'eval_precision@tur.pdtb.tdb': 0.08414539390149146, 'eval_recall@tur.pdtb.tdb': 0.11177531952190689, 'eval_loss@tur.pdtb.tdb': 2.125225067138672, 'eval_runtime': 4.1516, 'eval_samples_per_second': 75.152, 'eval_steps_per_second': 2.409, 'epoch': 10.0}
{'train@tur.pdtb.tdb_loss': 2.1303625106811523, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3361893104855161, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.08923720349720884, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.10746316567749042, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11199179526609636, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1303627490997314, 'train@tur.pdtb.tdb_runtime': 29.6187, 'train@tur.pdtb.tdb_samples_per_second': 82.752, 'train@tur.pdtb.tdb_steps_per_second': 2.6, 'epoch': 11.0}
{'loss': 2.1815, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.123427629470825, 'eval_accuracy@tur.pdtb.tdb': 0.32051282051282054, 'eval_f1@tur.pdtb.tdb': 0.08173746932443324, 'eval_precision@tur.pdtb.tdb': 0.08346309716700472, 'eval_recall@tur.pdtb.tdb': 0.11106033838523902, 'eval_loss@tur.pdtb.tdb': 2.123427629470825, 'eval_runtime': 4.1639, 'eval_samples_per_second': 74.929, 'eval_steps_per_second': 2.402, 'epoch': 11.0}
{'train@tur.pdtb.tdb_loss': 2.127229690551758, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3382292941656467, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.08993414624741398, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.10336155690681478, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11280806866502313, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.127229690551758, 'train@tur.pdtb.tdb_runtime': 29.6377, 'train@tur.pdtb.tdb_samples_per_second': 82.699, 'train@tur.pdtb.tdb_steps_per_second': 2.598, 'epoch': 12.0}
{'loss': 2.1711, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.1208138465881348, 'eval_accuracy@tur.pdtb.tdb': 0.3173076923076923, 'eval_f1@tur.pdtb.tdb': 0.08247871760490057, 'eval_precision@tur.pdtb.tdb': 0.08046814296814296, 'eval_recall@tur.pdtb.tdb': 0.11122767439594852, 'eval_loss@tur.pdtb.tdb': 2.1208138465881348, 'eval_runtime': 4.1761, 'eval_samples_per_second': 74.71, 'eval_steps_per_second': 2.395, 'epoch': 12.0}
{'train_runtime': 1152.1018, 'train_samples_per_second': 25.529, 'train_steps_per_second': 0.802, 'train_loss': 2.3826207247647373, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.3826
  train_runtime            = 0:19:12.10
  train_samples_per_second =     25.529
  train_steps_per_second   =      0.802
{'train@eng.rst.gum_loss': 2.396225929260254, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.2909260991580917, 'train@eng.rst.gum_f1@eng.rst.gum': 0.0636228873235766, 'train@eng.rst.gum_precision@eng.rst.gum': 0.098241973894676, 'train@eng.rst.gum_recall@eng.rst.gum': 0.09098754332967904, 'train@eng.rst.gum_loss@eng.rst.gum': 2.396226167678833, 'train@eng.rst.gum_runtime': 163.3703, 'train@eng.rst.gum_samples_per_second': 85.064, 'train@eng.rst.gum_steps_per_second': 2.663, 'epoch': 1.0}
{'loss': 2.7323, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.470850706100464, 'eval_accuracy@eng.rst.gum': 0.2857142857142857, 'eval_f1@eng.rst.gum': 0.06586829117176797, 'eval_precision@eng.rst.gum': 0.09828396975370532, 'eval_recall@eng.rst.gum': 0.09539195192919914, 'eval_loss@eng.rst.gum': 2.4708504676818848, 'eval_runtime': 25.6624, 'eval_samples_per_second': 83.741, 'eval_steps_per_second': 2.65, 'epoch': 1.0}
{'train@eng.rst.gum_loss': 1.952039122581482, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.4336187666402821, 'train@eng.rst.gum_f1@eng.rst.gum': 0.2036443824125422, 'train@eng.rst.gum_precision@eng.rst.gum': 0.2831000818026593, 'train@eng.rst.gum_recall@eng.rst.gum': 0.214682884650712, 'train@eng.rst.gum_loss@eng.rst.gum': 1.952039361000061, 'train@eng.rst.gum_runtime': 163.3191, 'train@eng.rst.gum_samples_per_second': 85.091, 'train@eng.rst.gum_steps_per_second': 2.663, 'epoch': 2.0}
{'loss': 2.2163, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.0638175010681152, 'eval_accuracy@eng.rst.gum': 0.3922754769660307, 'eval_f1@eng.rst.gum': 0.19102010244433998, 'eval_precision@eng.rst.gum': 0.25893974679293946, 'eval_recall@eng.rst.gum': 0.2062737194536298, 'eval_loss@eng.rst.gum': 2.0638177394866943, 'eval_runtime': 25.6363, 'eval_samples_per_second': 83.827, 'eval_steps_per_second': 2.652, 'epoch': 2.0}
{'train@eng.rst.gum_loss': 1.7458651065826416, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.4915449377563503, 'train@eng.rst.gum_f1@eng.rst.gum': 0.2795246547110568, 'train@eng.rst.gum_precision@eng.rst.gum': 0.30384747354428837, 'train@eng.rst.gum_recall@eng.rst.gum': 0.299593768405461, 'train@eng.rst.gum_loss@eng.rst.gum': 1.7458652257919312, 'train@eng.rst.gum_runtime': 163.0038, 'train@eng.rst.gum_samples_per_second': 85.256, 'train@eng.rst.gum_steps_per_second': 2.669, 'epoch': 3.0}
{'loss': 1.9211, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.8852242231369019, 'eval_accuracy@eng.rst.gum': 0.45044206607724524, 'eval_f1@eng.rst.gum': 0.2660555768609248, 'eval_precision@eng.rst.gum': 0.2681406088467338, 'eval_recall@eng.rst.gum': 0.28965335064256886, 'eval_loss@eng.rst.gum': 1.8852243423461914, 'eval_runtime': 25.622, 'eval_samples_per_second': 83.873, 'eval_steps_per_second': 2.654, 'epoch': 3.0}
{'train@eng.rst.gum_loss': 1.6411352157592773, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.511981003094193, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3032027695577731, 'train@eng.rst.gum_precision@eng.rst.gum': 0.4453353363138702, 'train@eng.rst.gum_recall@eng.rst.gum': 0.32217420048799006, 'train@eng.rst.gum_loss@eng.rst.gum': 1.6411349773406982, 'train@eng.rst.gum_runtime': 163.4102, 'train@eng.rst.gum_samples_per_second': 85.044, 'train@eng.rst.gum_steps_per_second': 2.662, 'epoch': 4.0}
{'loss': 1.7721, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.805795431137085, 'eval_accuracy@eng.rst.gum': 0.47417403443462075, 'eval_f1@eng.rst.gum': 0.2948712684011644, 'eval_precision@eng.rst.gum': 0.34004957732829016, 'eval_recall@eng.rst.gum': 0.31659431660143666, 'eval_loss@eng.rst.gum': 1.8057951927185059, 'eval_runtime': 25.6822, 'eval_samples_per_second': 83.677, 'eval_steps_per_second': 2.648, 'epoch': 4.0}
{'train@eng.rst.gum_loss': 1.575709581375122, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5256530186371159, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3460062602981596, 'train@eng.rst.gum_precision@eng.rst.gum': 0.504709712348759, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3587267319616194, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5757098197937012, 'train@eng.rst.gum_runtime': 163.2622, 'train@eng.rst.gum_samples_per_second': 85.121, 'train@eng.rst.gum_steps_per_second': 2.664, 'epoch': 5.0}
{'loss': 1.6852, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.7577202320098877, 'eval_accuracy@eng.rst.gum': 0.48859934853420195, 'eval_f1@eng.rst.gum': 0.3386340466296446, 'eval_precision@eng.rst.gum': 0.40833838556068414, 'eval_recall@eng.rst.gum': 0.3550299262887023, 'eval_loss@eng.rst.gum': 1.7577199935913086, 'eval_runtime': 25.6172, 'eval_samples_per_second': 83.889, 'eval_steps_per_second': 2.654, 'epoch': 5.0}
{'train@eng.rst.gum_loss': 1.5290496349334717, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.54191552133554, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3781156869184864, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5204284015405704, 'train@eng.rst.gum_recall@eng.rst.gum': 0.38489231593967016, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5290496349334717, 'train@eng.rst.gum_runtime': 163.0513, 'train@eng.rst.gum_samples_per_second': 85.231, 'train@eng.rst.gum_steps_per_second': 2.668, 'epoch': 6.0}
{'loss': 1.6316, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.7241331338882446, 'eval_accuracy@eng.rst.gum': 0.49790600279199626, 'eval_f1@eng.rst.gum': 0.35851963290352556, 'eval_precision@eng.rst.gum': 0.4197751980678966, 'eval_recall@eng.rst.gum': 0.37099232655044867, 'eval_loss@eng.rst.gum': 1.7241332530975342, 'eval_runtime': 25.6029, 'eval_samples_per_second': 83.936, 'eval_steps_per_second': 2.656, 'epoch': 6.0}
{'train@eng.rst.gum_loss': 1.4956789016723633, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5503346045909189, 'train@eng.rst.gum_f1@eng.rst.gum': 0.39120826901737354, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5372539217629676, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3983005472796246, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4956790208816528, 'train@eng.rst.gum_runtime': 163.3785, 'train@eng.rst.gum_samples_per_second': 85.06, 'train@eng.rst.gum_steps_per_second': 2.663, 'epoch': 7.0}
{'loss': 1.5922, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.705857515335083, 'eval_accuracy@eng.rst.gum': 0.5034899953466728, 'eval_f1@eng.rst.gum': 0.3673194989049112, 'eval_precision@eng.rst.gum': 0.4262578503761307, 'eval_recall@eng.rst.gum': 0.38265722817448894, 'eval_loss@eng.rst.gum': 1.7058576345443726, 'eval_runtime': 25.6482, 'eval_samples_per_second': 83.787, 'eval_steps_per_second': 2.651, 'epoch': 7.0}
{'train@eng.rst.gum_loss': 1.4737414121627808, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5570266964092969, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4048981321108756, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5336717222652188, 'train@eng.rst.gum_recall@eng.rst.gum': 0.40784285611363424, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4737412929534912, 'train@eng.rst.gum_runtime': 163.2767, 'train@eng.rst.gum_samples_per_second': 85.113, 'train@eng.rst.gum_steps_per_second': 2.664, 'epoch': 8.0}
{'loss': 1.5621, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.691125512123108, 'eval_accuracy@eng.rst.gum': 0.5104699860400186, 'eval_f1@eng.rst.gum': 0.3737260029563803, 'eval_precision@eng.rst.gum': 0.4185256933493546, 'eval_recall@eng.rst.gum': 0.38914989809591377, 'eval_loss@eng.rst.gum': 1.6911256313323975, 'eval_runtime': 25.6219, 'eval_samples_per_second': 83.874, 'eval_steps_per_second': 2.654, 'epoch': 8.0}
{'train@eng.rst.gum_loss': 1.4566571712493896, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5628552925091747, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4169080859326645, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5255978501187569, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4203855858672602, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4566571712493896, 'train@eng.rst.gum_runtime': 163.1019, 'train@eng.rst.gum_samples_per_second': 85.204, 'train@eng.rst.gum_steps_per_second': 2.667, 'epoch': 9.0}
{'loss': 1.5397, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.6781363487243652, 'eval_accuracy@eng.rst.gum': 0.511400651465798, 'eval_f1@eng.rst.gum': 0.3829898086807483, 'eval_precision@eng.rst.gum': 0.423752020141882, 'eval_recall@eng.rst.gum': 0.39903334602052853, 'eval_loss@eng.rst.gum': 1.6781363487243652, 'eval_runtime': 25.6167, 'eval_samples_per_second': 83.891, 'eval_steps_per_second': 2.655, 'epoch': 9.0}
{'train@eng.rst.gum_loss': 1.4414173364639282, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5662373174066345, 'train@eng.rst.gum_f1@eng.rst.gum': 0.42367221948752326, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5276699263258068, 'train@eng.rst.gum_recall@eng.rst.gum': 0.42282450521327414, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4414173364639282, 'train@eng.rst.gum_runtime': 163.4662, 'train@eng.rst.gum_samples_per_second': 85.014, 'train@eng.rst.gum_steps_per_second': 2.661, 'epoch': 10.0}
{'loss': 1.5269, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.6688956022262573, 'eval_accuracy@eng.rst.gum': 0.5174499767333643, 'eval_f1@eng.rst.gum': 0.3884553116131439, 'eval_precision@eng.rst.gum': 0.43795300837962936, 'eval_recall@eng.rst.gum': 0.39967222137340136, 'eval_loss@eng.rst.gum': 1.6688956022262573, 'eval_runtime': 25.6954, 'eval_samples_per_second': 83.634, 'eval_steps_per_second': 2.646, 'epoch': 10.0}
{'train@eng.rst.gum_loss': 1.4353402853012085, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5683240987263438, 'train@eng.rst.gum_f1@eng.rst.gum': 0.42922862569495834, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5289957219802699, 'train@eng.rst.gum_recall@eng.rst.gum': 0.42988655131717324, 'train@eng.rst.gum_loss@eng.rst.gum': 1.435340166091919, 'train@eng.rst.gum_runtime': 163.4659, 'train@eng.rst.gum_samples_per_second': 85.015, 'train@eng.rst.gum_steps_per_second': 2.661, 'epoch': 11.0}
{'loss': 1.521, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.6634305715560913, 'eval_accuracy@eng.rst.gum': 0.5165193113075849, 'eval_f1@eng.rst.gum': 0.3890613566984693, 'eval_precision@eng.rst.gum': 0.4320590910899571, 'eval_recall@eng.rst.gum': 0.40259377096731974, 'eval_loss@eng.rst.gum': 1.6634308099746704, 'eval_runtime': 25.6313, 'eval_samples_per_second': 83.843, 'eval_steps_per_second': 2.653, 'epoch': 11.0}
{'train@eng.rst.gum_loss': 1.4321669340133667, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5691875944448442, 'train@eng.rst.gum_f1@eng.rst.gum': 0.43006961340653027, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5320197603129602, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4297393195306323, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4321668148040771, 'train@eng.rst.gum_runtime': 163.1048, 'train@eng.rst.gum_samples_per_second': 85.203, 'train@eng.rst.gum_steps_per_second': 2.667, 'epoch': 12.0}
{'loss': 1.5073, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.6618895530700684, 'eval_accuracy@eng.rst.gum': 0.5179153094462541, 'eval_f1@eng.rst.gum': 0.39174671990549387, 'eval_precision@eng.rst.gum': 0.4373547690251698, 'eval_recall@eng.rst.gum': 0.4043506505050552, 'eval_loss@eng.rst.gum': 1.661889672279358, 'eval_runtime': 25.6181, 'eval_samples_per_second': 83.886, 'eval_steps_per_second': 2.654, 'epoch': 12.0}
{'train_runtime': 6433.0097, 'train_samples_per_second': 25.923, 'train_steps_per_second': 0.811, 'train_loss': 1.767305688383022, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.3826
  train_runtime            = 0:19:12.10
  train_samples_per_second =     25.529
  train_steps_per_second   =      0.802
-------------------------------------------------------------------
Lang1:  zho.rst.sctb    Lang2:  eng.rst.gum
Saving run to:  runs/full_shot/FullShot=v4_zho.rst.sctb_eng.rst.gum_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 439 examples
read 94 examples
read 159 examples
read 13897 examples
read 2149 examples
read 2091 examples
Total prediction labels:  29
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (zh): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=29, bias=True)
    )
  )
)
{'train@zho.rst.sctb_loss': 3.1439881324768066, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.35079726651480636, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.025415023011176856, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.04724706817730073, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.041700404858299595, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.1439881324768066, 'train@zho.rst.sctb_runtime': 5.3834, 'train@zho.rst.sctb_samples_per_second': 81.547, 'train@zho.rst.sctb_steps_per_second': 2.601, 'epoch': 1.0}
{'loss': 3.2729, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.172538995742798, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.034797738147020446, 'eval_precision@zho.rst.sctb': 0.038202247191011236, 'eval_recall@zho.rst.sctb': 0.05388931888544892, 'eval_loss@zho.rst.sctb': 3.1725385189056396, 'eval_runtime': 1.3885, 'eval_samples_per_second': 67.699, 'eval_steps_per_second': 2.161, 'epoch': 1.0}
{'train@zho.rst.sctb_loss': 2.9494082927703857, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3348519362186788, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.020032051282051284, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.05128205128205128, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038866396761133605, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.9494080543518066, 'train@zho.rst.sctb_runtime': 5.3945, 'train@zho.rst.sctb_samples_per_second': 81.379, 'train@zho.rst.sctb_steps_per_second': 2.595, 'epoch': 2.0}
{'loss': 3.0592, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.9885451793670654, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.03185595567867036, 'eval_precision@zho.rst.sctb': 0.044050343249427915, 'eval_recall@zho.rst.sctb': 0.0540828173374613, 'eval_loss@zho.rst.sctb': 2.988546133041382, 'eval_runtime': 1.395, 'eval_samples_per_second': 67.384, 'eval_steps_per_second': 2.151, 'epoch': 2.0}
{'train@zho.rst.sctb_loss': 2.777190923690796, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3348519362186788, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.020032051282051284, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.05128205128205128, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038866396761133605, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.777191162109375, 'train@zho.rst.sctb_runtime': 5.4065, 'train@zho.rst.sctb_samples_per_second': 81.198, 'train@zho.rst.sctb_steps_per_second': 2.589, 'epoch': 3.0}
{'loss': 2.8946, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.8257880210876465, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 2.8257884979248047, 'eval_runtime': 1.3813, 'eval_samples_per_second': 68.051, 'eval_steps_per_second': 2.172, 'epoch': 3.0}
{'train@zho.rst.sctb_loss': 2.634800434112549, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3348519362186788, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.020032051282051284, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.05128205128205128, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038866396761133605, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.634800672531128, 'train@zho.rst.sctb_runtime': 5.4073, 'train@zho.rst.sctb_samples_per_second': 81.187, 'train@zho.rst.sctb_steps_per_second': 2.589, 'epoch': 4.0}
{'loss': 2.7378, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.693237066268921, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 2.6932373046875, 'eval_runtime': 1.3886, 'eval_samples_per_second': 67.696, 'eval_steps_per_second': 2.161, 'epoch': 4.0}
{'train@zho.rst.sctb_loss': 2.5234317779541016, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019230769230769232, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.01282051282051282, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.5234317779541016, 'train@zho.rst.sctb_runtime': 5.4348, 'train@zho.rst.sctb_samples_per_second': 80.776, 'train@zho.rst.sctb_steps_per_second': 2.576, 'epoch': 5.0}
{'loss': 2.6136, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.5921947956085205, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 2.5921947956085205, 'eval_runtime': 1.3846, 'eval_samples_per_second': 67.888, 'eval_steps_per_second': 2.167, 'epoch': 5.0}
{'train@zho.rst.sctb_loss': 2.438962936401367, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.33712984054669703, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.020866713061116596, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.038520348153375676, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.039271255060728746, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.438962697982788, 'train@zho.rst.sctb_runtime': 5.4032, 'train@zho.rst.sctb_samples_per_second': 81.248, 'train@zho.rst.sctb_steps_per_second': 2.591, 'epoch': 6.0}
{'loss': 2.5132, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.519016742706299, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 2.5190162658691406, 'eval_runtime': 1.4011, 'eval_samples_per_second': 67.09, 'eval_steps_per_second': 2.141, 'epoch': 6.0}
{'train@zho.rst.sctb_loss': 2.3803012371063232, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.33712984054669703, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.020866713061116596, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.038520348153375676, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.039271255060728746, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.3803012371063232, 'train@zho.rst.sctb_runtime': 5.4324, 'train@zho.rst.sctb_samples_per_second': 80.812, 'train@zho.rst.sctb_steps_per_second': 2.577, 'epoch': 7.0}
{'loss': 2.4428, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.4703493118286133, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 2.4703495502471924, 'eval_runtime': 1.3844, 'eval_samples_per_second': 67.901, 'eval_steps_per_second': 2.167, 'epoch': 7.0}
{'train@zho.rst.sctb_loss': 2.3437869548797607, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.33940774487471526, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.0216710875331565, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03601559730591988, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.03967611336032389, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.3437869548797607, 'train@zho.rst.sctb_runtime': 5.4159, 'train@zho.rst.sctb_samples_per_second': 81.058, 'train@zho.rst.sctb_steps_per_second': 2.585, 'epoch': 8.0}
{'loss': 2.391, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.4418673515319824, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 2.4418675899505615, 'eval_runtime': 1.3805, 'eval_samples_per_second': 68.091, 'eval_steps_per_second': 2.173, 'epoch': 8.0}
{'train@zho.rst.sctb_loss': 2.319101572036743, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.33940774487471526, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.02231390293674031, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.034887566137566134, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.03981753646497698, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.3191018104553223, 'train@zho.rst.sctb_runtime': 5.4587, 'train@zho.rst.sctb_samples_per_second': 80.422, 'train@zho.rst.sctb_steps_per_second': 2.565, 'epoch': 9.0}
{'loss': 2.3532, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.423173189163208, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.03185595567867036, 'eval_precision@zho.rst.sctb': 0.044050343249427915, 'eval_recall@zho.rst.sctb': 0.0540828173374613, 'eval_loss@zho.rst.sctb': 2.42317271232605, 'eval_runtime': 1.3858, 'eval_samples_per_second': 67.833, 'eval_steps_per_second': 2.165, 'epoch': 9.0}
{'train@zho.rst.sctb_loss': 2.303283929824829, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.35079726651480636, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.025909987448448985, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03971858766008279, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.041841827962952695, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.303283929824829, 'train@zho.rst.sctb_runtime': 5.4522, 'train@zho.rst.sctb_samples_per_second': 80.517, 'train@zho.rst.sctb_steps_per_second': 2.568, 'epoch': 10.0}
{'loss': 2.3423, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.4118411540985107, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.03093709884467266, 'eval_precision@zho.rst.sctb': 0.03489492963177174, 'eval_recall@zho.rst.sctb': 0.05243808049535604, 'eval_loss@zho.rst.sctb': 2.4118411540985107, 'eval_runtime': 1.4136, 'eval_samples_per_second': 66.496, 'eval_steps_per_second': 2.122, 'epoch': 10.0}
{'train@zho.rst.sctb_loss': 2.295116424560547, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3530751708428246, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.026561158140105512, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03879414610546686, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.042246686262547836, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.295116424560547, 'train@zho.rst.sctb_runtime': 5.434, 'train@zho.rst.sctb_samples_per_second': 80.788, 'train@zho.rst.sctb_steps_per_second': 2.576, 'epoch': 11.0}
{'loss': 2.3216, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.4058890342712402, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.034797738147020446, 'eval_precision@zho.rst.sctb': 0.038202247191011236, 'eval_recall@zho.rst.sctb': 0.05388931888544892, 'eval_loss@zho.rst.sctb': 2.4058895111083984, 'eval_runtime': 1.4058, 'eval_samples_per_second': 66.868, 'eval_steps_per_second': 2.134, 'epoch': 11.0}
{'train@zho.rst.sctb_loss': 2.2923052310943604, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.357630979498861, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.027878811329515554, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.04036477879522206, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04305640286173811, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.2923052310943604, 'train@zho.rst.sctb_runtime': 5.4655, 'train@zho.rst.sctb_samples_per_second': 80.323, 'train@zho.rst.sctb_steps_per_second': 2.562, 'epoch': 12.0}
{'loss': 2.3103, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.403958320617676, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.034797738147020446, 'eval_precision@zho.rst.sctb': 0.038202247191011236, 'eval_recall@zho.rst.sctb': 0.05388931888544892, 'eval_loss@zho.rst.sctb': 2.4039578437805176, 'eval_runtime': 1.4007, 'eval_samples_per_second': 67.111, 'eval_steps_per_second': 2.142, 'epoch': 12.0}
{'train_runtime': 212.7361, 'train_samples_per_second': 24.763, 'train_steps_per_second': 0.79, 'train_loss': 2.604363759358724, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.6044
  train_runtime            = 0:03:32.73
  train_samples_per_second =     24.763
  train_steps_per_second   =       0.79
{'train@eng.rst.gum_loss': 2.519317150115967, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.25566669065265885, 'train@eng.rst.gum_f1@eng.rst.gum': 0.04955330948179544, 'train@eng.rst.gum_precision@eng.rst.gum': 0.06108626128889571, 'train@eng.rst.gum_recall@eng.rst.gum': 0.06512431022813797, 'train@eng.rst.gum_loss@eng.rst.gum': 2.519317150115967, 'train@eng.rst.gum_runtime': 163.2465, 'train@eng.rst.gum_samples_per_second': 85.129, 'train@eng.rst.gum_steps_per_second': 2.665, 'epoch': 1.0}
{'loss': 2.7568, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.6001293659210205, 'eval_accuracy@eng.rst.gum': 0.25453699395067475, 'eval_f1@eng.rst.gum': 0.053169652195489706, 'eval_precision@eng.rst.gum': 0.06040072934232156, 'eval_recall@eng.rst.gum': 0.06983252100203463, 'eval_loss@eng.rst.gum': 2.6001293659210205, 'eval_runtime': 25.5453, 'eval_samples_per_second': 84.125, 'eval_steps_per_second': 2.662, 'epoch': 1.0}
{'train@eng.rst.gum_loss': 2.0450501441955566, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.4054112398359358, 'train@eng.rst.gum_f1@eng.rst.gum': 0.17717306278465075, 'train@eng.rst.gum_precision@eng.rst.gum': 0.23825420614490256, 'train@eng.rst.gum_recall@eng.rst.gum': 0.18349638848615443, 'train@eng.rst.gum_loss@eng.rst.gum': 2.0450501441955566, 'train@eng.rst.gum_runtime': 163.0432, 'train@eng.rst.gum_samples_per_second': 85.235, 'train@eng.rst.gum_steps_per_second': 2.668, 'epoch': 2.0}
{'loss': 2.3478, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.1432836055755615, 'eval_accuracy@eng.rst.gum': 0.3806421591437878, 'eval_f1@eng.rst.gum': 0.16931173956229886, 'eval_precision@eng.rst.gum': 0.2275271282499963, 'eval_recall@eng.rst.gum': 0.17964004860969654, 'eval_loss@eng.rst.gum': 2.1432838439941406, 'eval_runtime': 25.5334, 'eval_samples_per_second': 84.164, 'eval_steps_per_second': 2.663, 'epoch': 2.0}
{'train@eng.rst.gum_loss': 1.7851225137710571, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.48147082104051236, 'train@eng.rst.gum_f1@eng.rst.gum': 0.2734898219223672, 'train@eng.rst.gum_precision@eng.rst.gum': 0.4185440190690719, 'train@eng.rst.gum_recall@eng.rst.gum': 0.28876352814269174, 'train@eng.rst.gum_loss@eng.rst.gum': 1.7851223945617676, 'train@eng.rst.gum_runtime': 163.0796, 'train@eng.rst.gum_samples_per_second': 85.216, 'train@eng.rst.gum_steps_per_second': 2.667, 'epoch': 3.0}
{'loss': 1.9791, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.9169058799743652, 'eval_accuracy@eng.rst.gum': 0.44765006979990696, 'eval_f1@eng.rst.gum': 0.2650316298878111, 'eval_precision@eng.rst.gum': 0.2773911268622756, 'eval_recall@eng.rst.gum': 0.2810496910543375, 'eval_loss@eng.rst.gum': 1.9169058799743652, 'eval_runtime': 25.5017, 'eval_samples_per_second': 84.269, 'eval_steps_per_second': 2.666, 'epoch': 3.0}
{'train@eng.rst.gum_loss': 1.6643038988113403, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5065841548535656, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3079099449748869, 'train@eng.rst.gum_precision@eng.rst.gum': 0.4334946351851687, 'train@eng.rst.gum_recall@eng.rst.gum': 0.31969199897084755, 'train@eng.rst.gum_loss@eng.rst.gum': 1.6643037796020508, 'train@eng.rst.gum_runtime': 163.2722, 'train@eng.rst.gum_samples_per_second': 85.116, 'train@eng.rst.gum_steps_per_second': 2.664, 'epoch': 4.0}
{'loss': 1.7983, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.824739694595337, 'eval_accuracy@eng.rst.gum': 0.46905537459283386, 'eval_f1@eng.rst.gum': 0.2976798301657645, 'eval_precision@eng.rst.gum': 0.3342907929887096, 'eval_recall@eng.rst.gum': 0.3116907400781292, 'eval_loss@eng.rst.gum': 1.824739694595337, 'eval_runtime': 25.5784, 'eval_samples_per_second': 84.016, 'eval_steps_per_second': 2.658, 'epoch': 4.0}
{'train@eng.rst.gum_loss': 1.5904247760772705, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.52709217816795, 'train@eng.rst.gum_f1@eng.rst.gum': 0.36039563367894806, 'train@eng.rst.gum_precision@eng.rst.gum': 0.4980473055806428, 'train@eng.rst.gum_recall@eng.rst.gum': 0.36630236191890875, 'train@eng.rst.gum_loss@eng.rst.gum': 1.59042489528656, 'train@eng.rst.gum_runtime': 163.0775, 'train@eng.rst.gum_samples_per_second': 85.217, 'train@eng.rst.gum_steps_per_second': 2.667, 'epoch': 5.0}
{'loss': 1.6988, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.7669117450714111, 'eval_accuracy@eng.rst.gum': 0.48766868310842254, 'eval_f1@eng.rst.gum': 0.3420714314630368, 'eval_precision@eng.rst.gum': 0.3632351504615347, 'eval_recall@eng.rst.gum': 0.35541736894971715, 'eval_loss@eng.rst.gum': 1.7669117450714111, 'eval_runtime': 25.5165, 'eval_samples_per_second': 84.22, 'eval_steps_per_second': 2.665, 'epoch': 5.0}
{'train@eng.rst.gum_loss': 1.5395848751068115, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5401165719219976, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3867428296527087, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5003670085515767, 'train@eng.rst.gum_recall@eng.rst.gum': 0.390658935267692, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5395848751068115, 'train@eng.rst.gum_runtime': 163.3801, 'train@eng.rst.gum_samples_per_second': 85.059, 'train@eng.rst.gum_steps_per_second': 2.663, 'epoch': 6.0}
{'loss': 1.6385, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.7291406393051147, 'eval_accuracy@eng.rst.gum': 0.49697533736621685, 'eval_f1@eng.rst.gum': 0.36716807841051524, 'eval_precision@eng.rst.gum': 0.4603017706118588, 'eval_recall@eng.rst.gum': 0.3780589812385532, 'eval_loss@eng.rst.gum': 1.7291406393051147, 'eval_runtime': 25.5466, 'eval_samples_per_second': 84.121, 'eval_steps_per_second': 2.662, 'epoch': 6.0}
{'train@eng.rst.gum_loss': 1.5030533075332642, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5479599913650428, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3943544449627981, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5078246957783342, 'train@eng.rst.gum_recall@eng.rst.gum': 0.400286893340918, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5030533075332642, 'train@eng.rst.gum_runtime': 163.3539, 'train@eng.rst.gum_samples_per_second': 85.073, 'train@eng.rst.gum_steps_per_second': 2.663, 'epoch': 7.0}
{'loss': 1.5964, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.7079509496688843, 'eval_accuracy@eng.rst.gum': 0.504885993485342, 'eval_f1@eng.rst.gum': 0.3757639201273128, 'eval_precision@eng.rst.gum': 0.512936239422275, 'eval_recall@eng.rst.gum': 0.3895470437349015, 'eval_loss@eng.rst.gum': 1.7079509496688843, 'eval_runtime': 25.5822, 'eval_samples_per_second': 84.004, 'eval_steps_per_second': 2.658, 'epoch': 7.0}
{'train@eng.rst.gum_loss': 1.4796205759048462, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5562351586673383, 'train@eng.rst.gum_f1@eng.rst.gum': 0.40573623172894, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5221410029111856, 'train@eng.rst.gum_recall@eng.rst.gum': 0.40810327952769104, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4796205759048462, 'train@eng.rst.gum_runtime': 162.9134, 'train@eng.rst.gum_samples_per_second': 85.303, 'train@eng.rst.gum_steps_per_second': 2.67, 'epoch': 8.0}
{'loss': 1.5629, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.6925052404403687, 'eval_accuracy@eng.rst.gum': 0.5118659841786878, 'eval_f1@eng.rst.gum': 0.38516117804576255, 'eval_precision@eng.rst.gum': 0.5022848261130032, 'eval_recall@eng.rst.gum': 0.3957475723674173, 'eval_loss@eng.rst.gum': 1.6925052404403687, 'eval_runtime': 25.4785, 'eval_samples_per_second': 84.346, 'eval_steps_per_second': 2.669, 'epoch': 8.0}
{'train@eng.rst.gum_loss': 1.4591021537780762, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5655177376412175, 'train@eng.rst.gum_f1@eng.rst.gum': 0.42211405675774905, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5252143915716514, 'train@eng.rst.gum_recall@eng.rst.gum': 0.42463501142934074, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4591021537780762, 'train@eng.rst.gum_runtime': 163.3006, 'train@eng.rst.gum_samples_per_second': 85.101, 'train@eng.rst.gum_steps_per_second': 2.664, 'epoch': 9.0}
{'loss': 1.5395, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.6751031875610352, 'eval_accuracy@eng.rst.gum': 0.5118659841786878, 'eval_f1@eng.rst.gum': 0.39116276782275217, 'eval_precision@eng.rst.gum': 0.4748039310968461, 'eval_recall@eng.rst.gum': 0.40442415517434077, 'eval_loss@eng.rst.gum': 1.6751031875610352, 'eval_runtime': 25.5746, 'eval_samples_per_second': 84.029, 'eval_steps_per_second': 2.659, 'epoch': 9.0}
{'train@eng.rst.gum_loss': 1.444772481918335, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5681082247967187, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4284789011601859, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5340264289960137, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4257206853034566, 'train@eng.rst.gum_loss@eng.rst.gum': 1.444772720336914, 'train@eng.rst.gum_runtime': 163.2186, 'train@eng.rst.gum_samples_per_second': 85.143, 'train@eng.rst.gum_steps_per_second': 2.665, 'epoch': 10.0}
{'loss': 1.5223, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.6673011779785156, 'eval_accuracy@eng.rst.gum': 0.5155886458818055, 'eval_f1@eng.rst.gum': 0.3934159156847233, 'eval_precision@eng.rst.gum': 0.49338788232348135, 'eval_recall@eng.rst.gum': 0.40255159603945356, 'eval_loss@eng.rst.gum': 1.6673015356063843, 'eval_runtime': 25.5625, 'eval_samples_per_second': 84.068, 'eval_steps_per_second': 2.66, 'epoch': 10.0}
{'train@eng.rst.gum_loss': 1.4376121759414673, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5694034683744693, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4340778058368669, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5259175714514334, 'train@eng.rst.gum_recall@eng.rst.gum': 0.43361276093842205, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4376120567321777, 'train@eng.rst.gum_runtime': 163.0254, 'train@eng.rst.gum_samples_per_second': 85.244, 'train@eng.rst.gum_steps_per_second': 2.668, 'epoch': 11.0}
{'loss': 1.5198, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.6610397100448608, 'eval_accuracy@eng.rst.gum': 0.5127966496044672, 'eval_f1@eng.rst.gum': 0.3947907245638861, 'eval_precision@eng.rst.gum': 0.4733407023043618, 'eval_recall@eng.rst.gum': 0.40645204534621454, 'eval_loss@eng.rst.gum': 1.6610397100448608, 'eval_runtime': 25.5392, 'eval_samples_per_second': 84.145, 'eval_steps_per_second': 2.663, 'epoch': 11.0}
{'train@eng.rst.gum_loss': 1.4343931674957275, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5706987119522199, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4342160705407074, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5308794464042533, 'train@eng.rst.gum_recall@eng.rst.gum': 0.43262208576207684, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4343934059143066, 'train@eng.rst.gum_runtime': 163.1032, 'train@eng.rst.gum_samples_per_second': 85.204, 'train@eng.rst.gum_steps_per_second': 2.667, 'epoch': 12.0}
{'loss': 1.5023, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.659697413444519, 'eval_accuracy@eng.rst.gum': 0.5137273150302466, 'eval_f1@eng.rst.gum': 0.3946765865911502, 'eval_precision@eng.rst.gum': 0.4805821712453857, 'eval_recall@eng.rst.gum': 0.4054593509510738, 'eval_loss@eng.rst.gum': 1.659697413444519, 'eval_runtime': 25.5096, 'eval_samples_per_second': 84.243, 'eval_steps_per_second': 2.666, 'epoch': 12.0}
{'train_runtime': 6410.0158, 'train_samples_per_second': 26.016, 'train_steps_per_second': 0.814, 'train_loss': 1.7885586325692948, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.6044
  train_runtime            = 0:03:32.73
  train_samples_per_second =     24.763
  train_steps_per_second   =       0.79

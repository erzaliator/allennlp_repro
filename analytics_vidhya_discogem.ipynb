{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arg1</th>\n",
       "      <th>arg2</th>\n",
       "      <th>domconn_step1</th>\n",
       "      <th>majoritylabel_sampled</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It had been agreed that they should all meet i...</td>\n",
       "      <td>At one end of the big barn, on a sort of raise...</td>\n",
       "      <td>to set the scene</td>\n",
       "      <td>precedence</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Old Major (so he was always called, though the...</td>\n",
       "      <td>He was twelve years old and had lately grown r...</td>\n",
       "      <td>to provide background information</td>\n",
       "      <td>arg2-as-detail</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Before long the other animals began to arrive ...</td>\n",
       "      <td>The hens perched themselves on the window-sill...</td>\n",
       "      <td>in addition</td>\n",
       "      <td>conjunction</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The hens perched themselves on the window-sill...</td>\n",
       "      <td>Clover was a stout motherly mare approaching m...</td>\n",
       "      <td>for information</td>\n",
       "      <td>arg2-as-detail</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Clover was a stout motherly mare approaching m...</td>\n",
       "      <td>A white stripe down his nose gave him a somewh...</td>\n",
       "      <td>furthermore</td>\n",
       "      <td>conjunction</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                arg1  \\\n",
       "0  It had been agreed that they should all meet i...   \n",
       "1  Old Major (so he was always called, though the...   \n",
       "2  Before long the other animals began to arrive ...   \n",
       "3  The hens perched themselves on the window-sill...   \n",
       "4  Clover was a stout motherly mare approaching m...   \n",
       "\n",
       "                                                arg2  \\\n",
       "0  At one end of the big barn, on a sort of raise...   \n",
       "1  He was twelve years old and had lately grown r...   \n",
       "2  The hens perched themselves on the window-sill...   \n",
       "3  Clover was a stout motherly mare approaching m...   \n",
       "4  A white stripe down his nose gave him a somewh...   \n",
       "\n",
       "                       domconn_step1 majoritylabel_sampled  split  \n",
       "0                   to set the scene            precedence  train  \n",
       "1  to provide background information        arg2-as-detail  train  \n",
       "2                        in addition           conjunction  train  \n",
       "3                    for information        arg2-as-detail  train  \n",
       "4                        furthermore           conjunction  train  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('../DisCopy/DiscoGeM/DiscoGeM_corpus/DiscoGeMcorpus_annotations_wide_datasplit.csv')\n",
    "train_df = train_df[['arg1', 'arg2', 'domconn_step1', 'majoritylabel_sampled', 'split']]\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_len(sent):\n",
    "    return(len(sent))\n",
    "\n",
    "train_df.apply(lambda x: get_len(x['arg2']), axis=1).plot.hist(bins=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arg1</th>\n",
       "      <th>arg2</th>\n",
       "      <th>domconn_step1</th>\n",
       "      <th>majoritylabel_sampled</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It had been agreed that they should all meet i...</td>\n",
       "      <td>At one end of the big barn, on a sort of raise...</td>\n",
       "      <td>to set the scene</td>\n",
       "      <td>precedence</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Old Major (so he was always called, though the...</td>\n",
       "      <td>He was twelve years old and had lately grown r...</td>\n",
       "      <td>to provide background information</td>\n",
       "      <td>arg2-as-detail</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Before long the other animals began to arrive ...</td>\n",
       "      <td>The hens perched themselves on the window-sill...</td>\n",
       "      <td>in addition</td>\n",
       "      <td>conjunction</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The hens perched themselves on the window-sill...</td>\n",
       "      <td>Clover was a stout motherly mare approaching m...</td>\n",
       "      <td>for information</td>\n",
       "      <td>arg2-as-detail</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Clover was a stout motherly mare approaching m...</td>\n",
       "      <td>A white stripe down his nose gave him a somewh...</td>\n",
       "      <td>furthermore</td>\n",
       "      <td>conjunction</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                arg1  \\\n",
       "0  It had been agreed that they should all meet i...   \n",
       "1  Old Major (so he was always called, though the...   \n",
       "2  Before long the other animals began to arrive ...   \n",
       "3  The hens perched themselves on the window-sill...   \n",
       "4  Clover was a stout motherly mare approaching m...   \n",
       "\n",
       "                                                arg2  \\\n",
       "0  At one end of the big barn, on a sort of raise...   \n",
       "1  He was twelve years old and had lately grown r...   \n",
       "2  The hens perched themselves on the window-sill...   \n",
       "3  Clover was a stout motherly mare approaching m...   \n",
       "4  A white stripe down his nose gave him a somewh...   \n",
       "\n",
       "                       domconn_step1 majoritylabel_sampled  split  \n",
       "0                   to set the scene            precedence  train  \n",
       "1  to provide background information        arg2-as-detail  train  \n",
       "2                        in addition           conjunction  train  \n",
       "3                    for information        arg2-as-detail  train  \n",
       "4                        furthermore           conjunction  train  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['arg1'] = train_df.apply(lambda x: x['arg1'][:400], axis=1)\n",
    "train_df['arg2'] = train_df.apply(lambda x: x['arg2'][:400], axis=1)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1288\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('because', 194),\n",
       "             ('specifically', 27),\n",
       "             ('also', 25),\n",
       "             ('this is because', 19),\n",
       "             ('however', 14),\n",
       "             ('for the reason that', 12),\n",
       "             ('in addition', 7),\n",
       "             ('this was because', 7),\n",
       "             ('in more detail', 6),\n",
       "             ('and', 5),\n",
       "             ('for example', 4),\n",
       "             ('the reason is that', 4),\n",
       "             ('furthermore', 3),\n",
       "             ('incidentally', 3),\n",
       "             ('consequently', 3),\n",
       "             ('but', 3),\n",
       "             ('considering this', 3),\n",
       "             ('accordingly', 3),\n",
       "             ('the reason being', 3),\n",
       "             ('nothing', 2),\n",
       "             ('as a result', 2),\n",
       "             ('despite this', 2),\n",
       "             ('previously', 2),\n",
       "             ('indeed', 1),\n",
       "             ('ultimately', 1),\n",
       "             ('unfortunately', 1),\n",
       "             ('given', 1),\n",
       "             ('whilst', 1),\n",
       "             ('instead', 1),\n",
       "             ('in particular', 1),\n",
       "             ('for instance', 1),\n",
       "             ('importantly', 1),\n",
       "             ('by contrast', 1),\n",
       "             ('although', 1),\n",
       "             ('as', 1),\n",
       "             ('hitherto', 1),\n",
       "             ('so', 1),\n",
       "             ('to sum up', 1),\n",
       "             ('on the contrary', 1),\n",
       "             ('to provide further information', 1),\n",
       "             ('due to the fact that', 1),\n",
       "             ('more specifically', 1),\n",
       "             ('immediately', 1),\n",
       "             ('for this reason', 1),\n",
       "             ('additionally', 1),\n",
       "             ('it is beautiful up here', 1),\n",
       "             ('no connection', 1),\n",
       "             ('in contrast', 1),\n",
       "             ('to provide background information', 1),\n",
       "             ('moreover', 1),\n",
       "             ('more generally', 1),\n",
       "             ('in fact', 1),\n",
       "             ('even though', 1),\n",
       "             ('since', 1),\n",
       "             ('the reason was', 1),\n",
       "             ('therfore', 1),\n",
       "             ('essentially', 1)])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot values of implicit connectives\n",
    "\n",
    "# TODO: Move to Discopy image\n",
    "\n",
    "from collections import Counter, OrderedDict\n",
    "relation_distribution = {}\n",
    "relation_distribution_sorted = {}\n",
    "for index, row in train_df.iterrows():\n",
    "    conn = row['domconn_step1']\n",
    "    rel = row['majoritylabel_sampled']\n",
    "    if rel not in relation_distribution:\n",
    "        relation_distribution[rel] = Counter()\n",
    "    relation_distribution[rel].update({conn: 1})\n",
    "\n",
    "for rel in relation_distribution:\n",
    "    relation_distribution_sorted[rel] = OrderedDict(relation_distribution[rel].most_common())\n",
    "\n",
    "#sample value\n",
    "print(relation_distribution_sorted['reason'])\n",
    "\n",
    "for rel in relation_distribution_sorted:\n",
    "    w = relation_distribution_sorted[rel]\n",
    "    plt.bar(w.keys(), w.values())\n",
    "    plt.xticks(rotation = 90)\n",
    "    plt.title('Implicit connectives:'+rel)\n",
    "    plt.savefig('../DisCopy/graphs/connectives/implicit/'+rel+'.jpg')\n",
    "    plt.tight_layout()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26631/1560066367.py:103: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df.dropna(inplace=True)\n",
      "/tmp/ipykernel_26631/1560066367.py:104: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_df.dropna(inplace=True)\n",
      "/tmp/ipykernel_26631/1560066367.py:105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df.dropna(inplace=True)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/VD/kaveri/bert_categorical_tutorial/analytics_vidhya_discogem.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 110>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/analytics_vidhya_discogem.ipynb#ch0000000vscode-remote?line=107'>108</a>\u001b[0m train_df \u001b[39m=\u001b[39m train_df[[\u001b[39m'\u001b[39m\u001b[39marg1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39marg2\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdomconn_step1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmajoritylabel_sampled\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msplit\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m#already loaded train_df in top cells\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/analytics_vidhya_discogem.ipynb#ch0000000vscode-remote?line=108'>109</a>\u001b[0m train_df, val_df, test_df \u001b[39m=\u001b[39m get_splits(train_df)\n\u001b[0;32m--> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/analytics_vidhya_discogem.ipynb#ch0000000vscode-remote?line=109'>110</a>\u001b[0m mnli_dataset \u001b[39m=\u001b[39m MNLIDataBert(train_df, val_df, test_df)\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/analytics_vidhya_discogem.ipynb#ch0000000vscode-remote?line=110'>111</a>\u001b[0m train_loader, val_loader, test_loader \u001b[39m=\u001b[39m mnli_dataset\u001b[39m.\u001b[39mget_data_loaders()\n",
      "\u001b[1;32m/home/VD/kaveri/bert_categorical_tutorial/analytics_vidhya_discogem.ipynb Cell 7'\u001b[0m in \u001b[0;36mMNLIDataBert.__init__\u001b[0;34m(self, train_df, val_df, test_df)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/analytics_vidhya_discogem.ipynb#ch0000000vscode-remote?line=25'>26</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_data \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/analytics_vidhya_discogem.ipynb#ch0000000vscode-remote?line=26'>27</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_data \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/analytics_vidhya_discogem.ipynb#ch0000000vscode-remote?line=27'>28</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minit_data()\n",
      "\u001b[1;32m/home/VD/kaveri/bert_categorical_tutorial/analytics_vidhya_discogem.ipynb Cell 7'\u001b[0m in \u001b[0;36mMNLIDataBert.init_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/analytics_vidhya_discogem.ipynb#ch0000000vscode-remote?line=29'>30</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minit_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/analytics_vidhya_discogem.ipynb#ch0000000vscode-remote?line=30'>31</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_data(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_df)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/analytics_vidhya_discogem.ipynb#ch0000000vscode-remote?line=31'>32</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_data(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_df)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/analytics_vidhya_discogem.ipynb#ch0000000vscode-remote?line=32'>33</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_data(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_df)\n",
      "\u001b[1;32m/home/VD/kaveri/bert_categorical_tutorial/analytics_vidhya_discogem.ipynb Cell 7'\u001b[0m in \u001b[0;36mMNLIDataBert.load_data\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/analytics_vidhya_discogem.ipynb#ch0000000vscode-remote?line=43'>44</a>\u001b[0m label_list \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mmajoritylabel_sampled\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto_list()\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/analytics_vidhya_discogem.ipynb#ch0000000vscode-remote?line=45'>46</a>\u001b[0m \u001b[39mfor\u001b[39;00m (premise, hypothesis, label) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(premise_list, hypothesis_list, label_list):\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/analytics_vidhya_discogem.ipynb#ch0000000vscode-remote?line=46'>47</a>\u001b[0m   premise_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mencode(premise, add_special_tokens \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/analytics_vidhya_discogem.ipynb#ch0000000vscode-remote?line=47'>48</a>\u001b[0m   hypothesis_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mencode(hypothesis, add_special_tokens \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/analytics_vidhya_discogem.ipynb#ch0000000vscode-remote?line=48'>49</a>\u001b[0m   pair_token_ids \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mcls_token_id] \u001b[39m+\u001b[39m premise_id \u001b[39m+\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39msep_token_id] \u001b[39m+\u001b[39m hypothesis_id \u001b[39m+\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39msep_token_id]\n",
      "File \u001b[0;32m/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2218\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   2181\u001b[0m \u001b[39m@add_end_docstrings\u001b[39m(\n\u001b[1;32m   2182\u001b[0m     ENCODE_KWARGS_DOCSTRING,\n\u001b[1;32m   2183\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2201\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m   2202\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mint\u001b[39m]:\n\u001b[1;32m   2203\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2204\u001b[0m \u001b[39m    Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2216\u001b[0m \u001b[39m            method).\u001b[39;00m\n\u001b[1;32m   2217\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2218\u001b[0m     encoded_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode_plus(\n\u001b[1;32m   2219\u001b[0m         text,\n\u001b[1;32m   2220\u001b[0m         text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[1;32m   2221\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2222\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m   2223\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[1;32m   2224\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2225\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2226\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2227\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2228\u001b[0m     )\n\u001b[1;32m   2230\u001b[0m     \u001b[39mreturn\u001b[39;00m encoded_inputs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2548\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2538\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2539\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2540\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   2541\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2545\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2546\u001b[0m )\n\u001b[0;32m-> 2548\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encode_plus(\n\u001b[1;32m   2549\u001b[0m     text\u001b[39m=\u001b[39;49mtext,\n\u001b[1;32m   2550\u001b[0m     text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[1;32m   2551\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2552\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   2553\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   2554\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2555\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2556\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2557\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2558\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2559\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2560\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2561\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2562\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2563\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2564\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2565\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2566\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2567\u001b[0m )\n",
      "File \u001b[0;32m/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/tokenization_utils.py:646\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[39mif\u001b[39;00m return_offsets_mapping:\n\u001b[1;32m    638\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    639\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    640\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    644\u001b[0m     )\n\u001b[0;32m--> 646\u001b[0m first_ids \u001b[39m=\u001b[39m get_input_ids(text)\n\u001b[1;32m    647\u001b[0m second_ids \u001b[39m=\u001b[39m get_input_ids(text_pair) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    649\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_for_model(\n\u001b[1;32m    650\u001b[0m     first_ids,\n\u001b[1;32m    651\u001b[0m     pair_ids\u001b[39m=\u001b[39msecond_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    665\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[1;32m    666\u001b[0m )\n",
      "File \u001b[0;32m/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/tokenization_utils.py:615\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_input_ids\u001b[39m(text):\n\u001b[1;32m    614\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(text, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 615\u001b[0m         tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenize(text, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    616\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvert_tokens_to_ids(tokens)\n\u001b[1;32m    617\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(text, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(text) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(text[\u001b[39m0\u001b[39m], \u001b[39mstr\u001b[39m):\n",
      "File \u001b[0;32m/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/tokenization_utils.py:546\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    544\u001b[0m         tokenized_text\u001b[39m.\u001b[39mappend(token)\n\u001b[1;32m    545\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 546\u001b[0m         tokenized_text\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenize(token))\n\u001b[1;32m    547\u001b[0m \u001b[39m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[39;00m\n\u001b[1;32m    548\u001b[0m \u001b[39mreturn\u001b[39;00m tokenized_text\n",
      "File \u001b[0;32m/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/models/bert/tokenization_bert.py:224\u001b[0m, in \u001b[0;36mBertTokenizer._tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    222\u001b[0m split_tokens \u001b[39m=\u001b[39m []\n\u001b[1;32m    223\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_basic_tokenize:\n\u001b[0;32m--> 224\u001b[0m     \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbasic_tokenizer\u001b[39m.\u001b[39;49mtokenize(text, never_split\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mall_special_tokens):\n\u001b[1;32m    225\u001b[0m \n\u001b[1;32m    226\u001b[0m         \u001b[39m# If the token is part of the never_split set\u001b[39;00m\n\u001b[1;32m    227\u001b[0m         \u001b[39mif\u001b[39;00m token \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbasic_tokenizer\u001b[39m.\u001b[39mnever_split:\n\u001b[1;32m    228\u001b[0m             split_tokens\u001b[39m.\u001b[39mappend(token)\n",
      "File \u001b[0;32m/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/models/bert/tokenization_bert.py:391\u001b[0m, in \u001b[0;36mBasicTokenizer.tokenize\u001b[0;34m(self, text, never_split)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[39m# union() returns a new set by concatenating the two sets.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m never_split \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnever_split\u001b[39m.\u001b[39munion(\u001b[39mset\u001b[39m(never_split)) \u001b[39mif\u001b[39;00m never_split \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnever_split\n\u001b[0;32m--> 391\u001b[0m text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_clean_text(text)\n\u001b[1;32m    393\u001b[0m \u001b[39m# This was added on November 1st, 2018 for the multilingual and Chinese\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[39m# models. This is also applied to the English models now, but it doesn't\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[39m# matter since the English models were not trained on any Chinese data\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \u001b[39m# and generally don't have any Chinese data in them (there are Chinese\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[39m# characters in the vocabulary because Wikipedia does have some Chinese\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[39m# words in the English Wikipedia.).\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenize_chinese_chars:\n",
      "File \u001b[0;32m/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/models/bert/tokenization_bert.py:491\u001b[0m, in \u001b[0;36mBasicTokenizer._clean_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[39mfor\u001b[39;00m char \u001b[39min\u001b[39;00m text:\n\u001b[1;32m    490\u001b[0m     cp \u001b[39m=\u001b[39m \u001b[39mord\u001b[39m(char)\n\u001b[0;32m--> 491\u001b[0m     \u001b[39mif\u001b[39;00m cp \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m cp \u001b[39m==\u001b[39m \u001b[39m0xFFFD\u001b[39m \u001b[39mor\u001b[39;00m _is_control(char):\n\u001b[1;32m    492\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    493\u001b[0m     \u001b[39mif\u001b[39;00m _is_whitespace(char):\n",
      "File \u001b[0;32m/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/tokenization_utils.py:286\u001b[0m, in \u001b[0;36m_is_control\u001b[0;34m(char)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    285\u001b[0m cat \u001b[39m=\u001b[39m unicodedata\u001b[39m.\u001b[39mcategory(char)\n\u001b[0;32m--> 286\u001b[0m \u001b[39mif\u001b[39;00m cat\u001b[39m.\u001b[39;49mstartswith(\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m):\n\u001b[1;32m    287\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pickle\n",
    "import os\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "class MNLIDataBert(Dataset):\n",
    "\n",
    "  def __init__(self, train_df, val_df, test_df):\n",
    "    self.label_dict = {'synchronous':0, 'precedence':1, 'succession':2, 'reason':3, 'result':4, \n",
    "                      'arg1-as-cond':5, 'arg2-as-cond':6, 'arg1-as-negcond':7, 'arg2-as-negcond':8,\n",
    "                      'arg1-as-goal':9, 'arg2-as-goal':10, 'arg1-as-denier':11, 'arg2-as-denier':12, 'contrast':13, \n",
    "                      'similarity':14, 'conjunction':15, 'disjunction':16, 'arg1-as-instance':17, 'arg2-as-instance':18, \n",
    "                      'arg1-as-detail':19, 'arg2-as-detail':20, 'equivalence':21, 'arg1-as-manner':22, \n",
    "                      'arg2-as-manner':23, 'arg1-as-excpt':24, 'arg2-as-excpt':25, 'arg2-as-subst':26, 'differentcon':27, 'norel':28}\n",
    "\n",
    "    self.train_df = train_df\n",
    "    self.val_df = val_df\n",
    "    self.test_df = test_df\n",
    "\n",
    "    self.base_path = '/content/'\n",
    "    self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True) # Using a pre-trained BERT tokenizer to encode sentences\n",
    "    self.train_data = None\n",
    "    self.val_data = None\n",
    "    self.test_data = None\n",
    "    self.init_data()\n",
    "\n",
    "  def init_data(self):\n",
    "    self.train_data = self.load_data(self.train_df)\n",
    "    self.val_data = self.load_data(self.val_df)\n",
    "    self.test_data = self.load_data(self.test_df)\n",
    "\n",
    "  def load_data(self, df):\n",
    "    MAX_LEN = 512\n",
    "    token_ids = []\n",
    "    mask_ids = []\n",
    "    seg_ids = []\n",
    "    y = []\n",
    "\n",
    "    premise_list = df['arg1'].to_list()\n",
    "    hypothesis_list = df['arg2'].to_list()\n",
    "    label_list = df['majoritylabel_sampled'].to_list()\n",
    "\n",
    "    for (premise, hypothesis, label) in zip(premise_list, hypothesis_list, label_list):\n",
    "      premise_id = self.tokenizer.encode(premise, add_special_tokens = False)\n",
    "      hypothesis_id = self.tokenizer.encode(hypothesis, add_special_tokens = False)\n",
    "      pair_token_ids = [self.tokenizer.cls_token_id] + premise_id + [self.tokenizer.sep_token_id] + hypothesis_id + [self.tokenizer.sep_token_id]\n",
    "      premise_len = len(premise_id)\n",
    "      hypothesis_len = len(hypothesis_id)\n",
    "\n",
    "      segment_ids = torch.tensor([0] * (premise_len + 2) + [1] * (hypothesis_len + 1))  # sentence 0 and sentence 1\n",
    "      attention_mask_ids = torch.tensor([1] * (premise_len + hypothesis_len + 3))  # mask padded values\n",
    "\n",
    "      token_ids.append(torch.tensor(pair_token_ids))\n",
    "      seg_ids.append(segment_ids)\n",
    "      mask_ids.append(attention_mask_ids)\n",
    "      y.append(self.label_dict[label])\n",
    "    \n",
    "    token_ids = pad_sequence(token_ids, batch_first=True)\n",
    "    mask_ids = pad_sequence(mask_ids, batch_first=True)\n",
    "    seg_ids = pad_sequence(seg_ids, batch_first=True)\n",
    "    y = torch.tensor(y)\n",
    "    dataset = TensorDataset(token_ids, mask_ids, seg_ids, y)\n",
    "    print(len(dataset))\n",
    "    return dataset\n",
    "\n",
    "  def get_data_loaders(self, batch_size=32, shuffle=True):\n",
    "    train_loader = DataLoader(\n",
    "      self.train_data,\n",
    "      shuffle=shuffle,\n",
    "      batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "      self.val_data,\n",
    "      shuffle=shuffle,\n",
    "      batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "      self.test_data,\n",
    "      shuffle=shuffle,\n",
    "      batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def get_splits(train_df, split_function='csv_split'):\n",
    "  if split_function=='random':\n",
    "    #split into test and dev\n",
    "    val_df=train_df.sample(frac=0.1,random_state=200)\n",
    "    train_df=train_df.drop(val_df.index)\n",
    "\n",
    "    test_df=train_df.sample(frac=0.1,random_state=200)\n",
    "    train_df=train_df.drop(test_df.index)\n",
    "  elif split_function=='csv_split':\n",
    "    test_df = train_df[train_df['split']=='test']\n",
    "    val_df = train_df[train_df['split']=='dev']\n",
    "    train_df = train_df[train_df['split']=='train']\n",
    "\n",
    "  train_df.dropna(inplace=True)\n",
    "  val_df.dropna(inplace=True)\n",
    "  test_df.dropna(inplace=True)\n",
    "  return train_df, val_df, test_df\n",
    "\n",
    "train_df = train_df[['arg1', 'arg2', 'domconn_step1', 'majoritylabel_sampled', 'split']] #already loaded train_df in top cells\n",
    "train_df, val_df, test_df = get_splits(train_df)\n",
    "mnli_dataset = MNLIDataBert(train_df, val_df, test_df)\n",
    "train_loader, val_loader, test_loader = mnli_dataset.get_data_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: 'synchronous',\n",
       "  1: 'precedence',\n",
       "  2: 'succession',\n",
       "  3: 'reason',\n",
       "  4: 'result',\n",
       "  5: 'arg1-as-cond',\n",
       "  6: 'arg2-as-cond',\n",
       "  7: 'arg1-as-negcond',\n",
       "  8: 'arg2-as-negcond',\n",
       "  9: 'arg1-as-goal',\n",
       "  10: 'arg2-as-goal',\n",
       "  11: 'arg1-as-denier',\n",
       "  12: 'arg2-as-denier',\n",
       "  13: 'contrast',\n",
       "  14: 'similarity',\n",
       "  15: 'conjunction',\n",
       "  16: 'disjunction',\n",
       "  17: 'arg1-as-instance',\n",
       "  18: 'arg2-as-instance',\n",
       "  19: 'arg1-as-detail',\n",
       "  20: 'arg2-as-detail',\n",
       "  21: 'equivalence',\n",
       "  22: 'arg1-as-manner',\n",
       "  23: 'arg2-as-manner',\n",
       "  24: 'arg1-as-excpt',\n",
       "  25: 'arg2-as-excpt',\n",
       "  26: 'arg2-as-subst',\n",
       "  27: 'differentcon',\n",
       "  28: 'norel'},\n",
       " {'synchronous': 0,\n",
       "  'precedence': 1,\n",
       "  'succession': 2,\n",
       "  'reason': 3,\n",
       "  'result': 4,\n",
       "  'arg1-as-cond': 5,\n",
       "  'arg2-as-cond': 6,\n",
       "  'arg1-as-negcond': 7,\n",
       "  'arg2-as-negcond': 8,\n",
       "  'arg1-as-goal': 9,\n",
       "  'arg2-as-goal': 10,\n",
       "  'arg1-as-denier': 11,\n",
       "  'arg2-as-denier': 12,\n",
       "  'contrast': 13,\n",
       "  'similarity': 14,\n",
       "  'conjunction': 15,\n",
       "  'disjunction': 16,\n",
       "  'arg1-as-instance': 17,\n",
       "  'arg2-as-instance': 18,\n",
       "  'arg1-as-detail': 19,\n",
       "  'arg2-as-detail': 20,\n",
       "  'equivalence': 21,\n",
       "  'arg1-as-manner': 22,\n",
       "  'arg2-as-manner': 23,\n",
       "  'arg1-as-excpt': 24,\n",
       "  'arg2-as-excpt': 25,\n",
       "  'arg2-as-subst': 26,\n",
       "  'differentcon': 27,\n",
       "  'norel': 28})"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict = mnli_dataset.label_dict\n",
    "rev_label_dict = {label_dict[k]:k for k in label_dict.keys()}\n",
    "rev_label_dict, label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=29).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_acc(y_pred, y_test):\n",
    "  acc = (torch.log_softmax(y_pred, dim=1).argmax(dim=1) == y_test).sum().float() / float(y_test.size(0))\n",
    "  return acc\n",
    "\n",
    "import time\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer):  \n",
    "  total_step = len(train_loader)\n",
    "\n",
    "  for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_train_acc  = 0\n",
    "    for batch_idx, (pair_token_ids, mask_ids, seg_ids, y) in enumerate(train_loader):\n",
    "      optimizer.zero_grad()\n",
    "      pair_token_ids = pair_token_ids.to(device)\n",
    "      mask_ids = mask_ids.to(device)\n",
    "      seg_ids = seg_ids.to(device)\n",
    "      labels = y.to(device)\n",
    "\n",
    "      loss, prediction = model(pair_token_ids, \n",
    "                             token_type_ids=seg_ids, \n",
    "                             attention_mask=mask_ids, \n",
    "                             labels=labels).values()\n",
    "\n",
    "      acc = multi_acc(prediction, labels)\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      \n",
    "      total_train_loss += loss.item()\n",
    "      total_train_acc  += acc.item()\n",
    "\n",
    "    train_acc  = total_train_acc/len(train_loader)\n",
    "    train_loss = total_train_loss/len(train_loader)\n",
    "    model.eval()\n",
    "    total_val_acc  = 0\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "      for batch_idx, (pair_token_ids, mask_ids, seg_ids, y) in enumerate(val_loader):\n",
    "        optimizer.zero_grad()\n",
    "        pair_token_ids = pair_token_ids.to(device)\n",
    "        mask_ids = mask_ids.to(device)\n",
    "        seg_ids = seg_ids.to(device)\n",
    "        labels = y.to(device)\n",
    "        \n",
    "        loss, prediction = model(pair_token_ids, \n",
    "                             token_type_ids=seg_ids, \n",
    "                             attention_mask=mask_ids, \n",
    "                             labels=labels).values()\n",
    "        \n",
    "        acc = multi_acc(prediction, labels)\n",
    "\n",
    "        total_val_loss += loss.item()\n",
    "        total_val_acc  += acc.item()\n",
    "\n",
    "    val_acc  = total_val_acc/len(val_loader)\n",
    "    val_loss = total_val_loss/len(val_loader)\n",
    "    end = time.time()\n",
    "    hours, rem = divmod(end-start, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "    print(f'Epoch {epoch+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
    "    print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss: 0.1983 train_acc: 0.9465 | val_loss: 2.7948 val_acc: 0.4366\n",
      "00:01:04.71\n",
      "Epoch 2: train_loss: 0.1338 train_acc: 0.9674 | val_loss: 2.8571 val_acc: 0.4381\n",
      "00:01:05.22\n",
      "Epoch 3: train_loss: 0.0945 train_acc: 0.9773 | val_loss: 2.9481 val_acc: 0.4464\n",
      "00:01:05.35\n",
      "Epoch 4: train_loss: 0.0678 train_acc: 0.9855 | val_loss: 3.1161 val_acc: 0.4366\n",
      "00:01:05.46\n",
      "Epoch 5: train_loss: 0.0599 train_acc: 0.9868 | val_loss: 3.0695 val_acc: 0.4485\n",
      "00:01:05.13\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, val_loader, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'bert-discogem-nli.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_loss: 2.1201 test_acc: 0.4182\n",
      "00:00:02.07\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "  arg1-as-denier       0.25      0.11      0.15         9\n",
      "  arg1-as-detail       0.00      0.00      0.00         2\n",
      "    arg2-as-cond       0.00      0.00      0.00         0\n",
      "  arg2-as-denier       0.18      0.40      0.25        35\n",
      "  arg2-as-detail       0.57      0.46      0.51        94\n",
      "arg2-as-instance       0.24      0.22      0.23        23\n",
      "   arg2-as-subst       0.00      0.00      0.00         3\n",
      "     conjunction       0.51      0.57      0.54       164\n",
      "        contrast       0.27      0.21      0.24        19\n",
      "      precedence       0.54      0.41      0.46        37\n",
      "          reason       0.30      0.25      0.27        40\n",
      "          result       0.59      0.55      0.57       160\n",
      "      similarity       0.00      0.00      0.00         2\n",
      "      succession       0.00      0.00      0.00         2\n",
      "     synchronous       0.00      0.00      0.00         4\n",
      "\n",
      "        accuracy                           0.46       594\n",
      "       macro avg       0.23      0.21      0.21       594\n",
      "    weighted avg       0.47      0.46      0.46       594\n",
      "\n",
      "Test Loss: 2.120 |  Test Acc: 41.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate(model, iterator):\n",
    "  start = time.time()\n",
    "  model.eval()\n",
    "  total_val_acc  = 0\n",
    "  total_val_loss = 0\n",
    "\n",
    "  #for classification report\n",
    "  y_true = []\n",
    "  y_pred = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for batch_idx, (pair_token_ids, mask_ids, seg_ids, y) in enumerate(iterator):\n",
    "      optimizer.zero_grad()\n",
    "      pair_token_ids = pair_token_ids.to(device)\n",
    "      mask_ids = mask_ids.to(device)\n",
    "      seg_ids = seg_ids.to(device)\n",
    "      labels = y.to(device)\n",
    "      \n",
    "      loss, prediction = model(pair_token_ids, \n",
    "                            token_type_ids=seg_ids, \n",
    "                            attention_mask=mask_ids, \n",
    "                            labels=labels).values()\n",
    "      \n",
    "      acc = multi_acc(prediction, labels)\n",
    "\n",
    "      total_val_loss += loss.item()\n",
    "      total_val_acc  += acc.item()\n",
    "\n",
    "      argmax_predictions = torch.argmax(prediction,dim=1).tolist()\n",
    "      labels_list = labels.tolist()\n",
    "      assert(len(labels_list)==len(argmax_predictions))\n",
    "      for p in argmax_predictions: y_pred.append(rev_label_dict[int(p)])\n",
    "      for l in labels_list: y_true.append(rev_label_dict[l])\n",
    "\n",
    "  val_acc  = total_val_acc/len(val_loader)\n",
    "  val_loss = total_val_loss/len(val_loader)\n",
    "  end = time.time()\n",
    "  hours, rem = divmod(end-start, 3600)\n",
    "  minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "  print(f'Test_loss: {val_loss:.4f} test_acc: {val_acc:.4f}')\n",
    "  print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "\n",
    "  print(classification_report(y_true, y_pred))\n",
    "  return val_loss, val_acc, y_true, y_pred\n",
    "\n",
    "model.load_state_dict(torch.load('bert-discogem-nli.pt'))\n",
    "test_loss, test_acc, y_true, y_pred = evaluate(model, test_loader)\n",
    "print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make a README for this folder and delete unwanted files. Link snli and discogem notebooks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3409ea685db85227fbd9509d1b1ace14d085473eb2d57f3ba9dd0302d25f838"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

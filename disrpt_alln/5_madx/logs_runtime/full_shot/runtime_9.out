-------------------------------------------------------------------
Lang1:  deu.rst.pcc    Lang2:  rus.rst.rrt
Saving run to:  runs/full_shot/FullShot=v4_deu.rst.pcc_rus.rst.rrt_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2164 examples
read 241 examples
read 260 examples
read 28822 examples
read 2855 examples
read 2843 examples
Total prediction labels:  34
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (ru): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=34, bias=True)
    )
  )
)
{'train@deu.rst.pcc_loss': 3.176469326019287, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.09981515711645102, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.010602849507589997, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.04688987012689448, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.03967995434180541, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.176469564437866, 'train@deu.rst.pcc_runtime': 26.8352, 'train@deu.rst.pcc_samples_per_second': 80.64, 'train@deu.rst.pcc_steps_per_second': 2.534, 'epoch': 1.0}
{'loss': 3.3509, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.1892194747924805, 'eval_accuracy@deu.rst.pcc': 0.12448132780082988, 'eval_f1@deu.rst.pcc': 0.01917025102670652, 'eval_precision@deu.rst.pcc': 0.055817230273752017, 'eval_recall@deu.rst.pcc': 0.04807692307692307, 'eval_loss@deu.rst.pcc': 3.1892197132110596, 'eval_runtime': 3.1781, 'eval_samples_per_second': 75.832, 'eval_steps_per_second': 2.517, 'epoch': 1.0}
{'train@deu.rst.pcc_loss': 2.9647810459136963, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.11876155268022182, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.022089587938519166, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.03757879293061021, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.04847479560306756, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.9647810459136963, 'train@deu.rst.pcc_runtime': 25.9752, 'train@deu.rst.pcc_samples_per_second': 83.31, 'train@deu.rst.pcc_steps_per_second': 2.618, 'epoch': 2.0}
{'loss': 3.0724, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.001932382583618, 'eval_accuracy@deu.rst.pcc': 0.13692946058091288, 'eval_f1@deu.rst.pcc': 0.026317808838995278, 'eval_precision@deu.rst.pcc': 0.02678664948885537, 'eval_recall@deu.rst.pcc': 0.05495777370777371, 'eval_loss@deu.rst.pcc': 3.001932144165039, 'eval_runtime': 3.1965, 'eval_samples_per_second': 75.396, 'eval_steps_per_second': 2.503, 'epoch': 2.0}
{'train@deu.rst.pcc_loss': 2.8805887699127197, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.1400184842883549, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.03448250044635073, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.10204182011712551, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.05954579236774006, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.8805885314941406, 'train@deu.rst.pcc_runtime': 25.9879, 'train@deu.rst.pcc_samples_per_second': 83.27, 'train@deu.rst.pcc_steps_per_second': 2.617, 'epoch': 3.0}
{'loss': 2.9497, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.935312032699585, 'eval_accuracy@deu.rst.pcc': 0.13692946058091288, 'eval_f1@deu.rst.pcc': 0.025321404340936455, 'eval_precision@deu.rst.pcc': 0.018465330965330966, 'eval_recall@deu.rst.pcc': 0.05667480667480668, 'eval_loss@deu.rst.pcc': 2.935312271118164, 'eval_runtime': 3.6822, 'eval_samples_per_second': 65.45, 'eval_steps_per_second': 2.173, 'epoch': 3.0}
{'train@deu.rst.pcc_loss': 2.8213376998901367, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.19177449168207025, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.07492294583498588, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.1174109538767383, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.10384557488248992, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.821337938308716, 'train@deu.rst.pcc_runtime': 26.006, 'train@deu.rst.pcc_samples_per_second': 83.212, 'train@deu.rst.pcc_steps_per_second': 2.615, 'epoch': 4.0}
{'loss': 2.8839, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.89035964012146, 'eval_accuracy@deu.rst.pcc': 0.1825726141078838, 'eval_f1@deu.rst.pcc': 0.06919916382402384, 'eval_precision@deu.rst.pcc': 0.11022821133115251, 'eval_recall@deu.rst.pcc': 0.10595365282865284, 'eval_loss@deu.rst.pcc': 2.8903591632843018, 'eval_runtime': 3.1855, 'eval_samples_per_second': 75.654, 'eval_steps_per_second': 2.511, 'epoch': 4.0}
{'train@deu.rst.pcc_loss': 2.775073289871216, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.20609981515711645, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08146070311987777, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.10594993401113004, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.11995231972562179, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.775073289871216, 'train@deu.rst.pcc_runtime': 26.0207, 'train@deu.rst.pcc_samples_per_second': 83.164, 'train@deu.rst.pcc_steps_per_second': 2.613, 'epoch': 5.0}
{'loss': 2.8363, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.8531241416931152, 'eval_accuracy@deu.rst.pcc': 0.2033195020746888, 'eval_f1@deu.rst.pcc': 0.08312530336359224, 'eval_precision@deu.rst.pcc': 0.117458157694946, 'eval_recall@deu.rst.pcc': 0.1310185933347698, 'eval_loss@deu.rst.pcc': 2.8531243801116943, 'eval_runtime': 3.1773, 'eval_samples_per_second': 75.85, 'eval_steps_per_second': 2.518, 'epoch': 5.0}
{'train@deu.rst.pcc_loss': 2.7356748580932617, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2158040665434381, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08871314954487468, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.10469724936234757, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.12989998780538278, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7356748580932617, 'train@deu.rst.pcc_runtime': 26.025, 'train@deu.rst.pcc_samples_per_second': 83.151, 'train@deu.rst.pcc_steps_per_second': 2.613, 'epoch': 6.0}
{'loss': 2.7949, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.818760633468628, 'eval_accuracy@deu.rst.pcc': 0.1950207468879668, 'eval_f1@deu.rst.pcc': 0.07371362037664952, 'eval_precision@deu.rst.pcc': 0.12163414947338251, 'eval_recall@deu.rst.pcc': 0.12997303622303624, 'eval_loss@deu.rst.pcc': 2.8187599182128906, 'eval_runtime': 3.1782, 'eval_samples_per_second': 75.828, 'eval_steps_per_second': 2.517, 'epoch': 6.0}
{'train@deu.rst.pcc_loss': 2.7069833278656006, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2208872458410351, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09151093641076719, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.10416677772533219, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.13344144431336466, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7069830894470215, 'train@deu.rst.pcc_runtime': 26.0132, 'train@deu.rst.pcc_samples_per_second': 83.188, 'train@deu.rst.pcc_steps_per_second': 2.614, 'epoch': 7.0}
{'loss': 2.7624, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.794816493988037, 'eval_accuracy@deu.rst.pcc': 0.2074688796680498, 'eval_f1@deu.rst.pcc': 0.08585598017792839, 'eval_precision@deu.rst.pcc': 0.12898255398255398, 'eval_recall@deu.rst.pcc': 0.14083747815365463, 'eval_loss@deu.rst.pcc': 2.794816255569458, 'eval_runtime': 3.2016, 'eval_samples_per_second': 75.275, 'eval_steps_per_second': 2.499, 'epoch': 7.0}
{'train@deu.rst.pcc_loss': 2.6844115257263184, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.22273567467652494, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09270768861639467, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.10628732230901464, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.1356479235827054, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6844112873077393, 'train@deu.rst.pcc_runtime': 25.9743, 'train@deu.rst.pcc_samples_per_second': 83.313, 'train@deu.rst.pcc_steps_per_second': 2.618, 'epoch': 8.0}
{'loss': 2.7336, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.7760283946990967, 'eval_accuracy@deu.rst.pcc': 0.2074688796680498, 'eval_f1@deu.rst.pcc': 0.08474039659604411, 'eval_precision@deu.rst.pcc': 0.12962045646311926, 'eval_recall@deu.rst.pcc': 0.14083747815365463, 'eval_loss@deu.rst.pcc': 2.776028871536255, 'eval_runtime': 3.1949, 'eval_samples_per_second': 75.433, 'eval_steps_per_second': 2.504, 'epoch': 8.0}
{'train@deu.rst.pcc_loss': 2.6659226417541504, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.23059149722735675, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09706658426088577, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.10628164383172607, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.14084255034988438, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.665922164916992, 'train@deu.rst.pcc_runtime': 26.0045, 'train@deu.rst.pcc_samples_per_second': 83.216, 'train@deu.rst.pcc_steps_per_second': 2.615, 'epoch': 9.0}
{'loss': 2.7203, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.7629542350769043, 'eval_accuracy@deu.rst.pcc': 0.21161825726141079, 'eval_f1@deu.rst.pcc': 0.08698284682957196, 'eval_precision@deu.rst.pcc': 0.12850288563998244, 'eval_recall@deu.rst.pcc': 0.14404260635878283, 'eval_loss@deu.rst.pcc': 2.762953996658325, 'eval_runtime': 3.1834, 'eval_samples_per_second': 75.704, 'eval_steps_per_second': 2.513, 'epoch': 9.0}
{'train@deu.rst.pcc_loss': 2.654113292694092, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.23290203327171904, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.0986342304836112, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.10733725425502778, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.1428773916338657, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.654113531112671, 'train@deu.rst.pcc_runtime': 26.0178, 'train@deu.rst.pcc_samples_per_second': 83.174, 'train@deu.rst.pcc_steps_per_second': 2.614, 'epoch': 10.0}
{'loss': 2.7014, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.7510383129119873, 'eval_accuracy@deu.rst.pcc': 0.21161825726141079, 'eval_f1@deu.rst.pcc': 0.08667599675184118, 'eval_precision@deu.rst.pcc': 0.12866817849548112, 'eval_recall@deu.rst.pcc': 0.14404260635878283, 'eval_loss@deu.rst.pcc': 2.751038074493408, 'eval_runtime': 3.1832, 'eval_samples_per_second': 75.71, 'eval_steps_per_second': 2.513, 'epoch': 10.0}
{'train@deu.rst.pcc_loss': 2.6469907760620117, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2342883548983364, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.0991961004823134, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.10737849888637806, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.14415054072638867, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6469907760620117, 'train@deu.rst.pcc_runtime': 25.9855, 'train@deu.rst.pcc_samples_per_second': 83.277, 'train@deu.rst.pcc_steps_per_second': 2.617, 'epoch': 11.0}
{'loss': 2.69, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.7453761100769043, 'eval_accuracy@deu.rst.pcc': 0.21161825726141079, 'eval_f1@deu.rst.pcc': 0.08758987648468668, 'eval_precision@deu.rst.pcc': 0.12913277999484896, 'eval_recall@deu.rst.pcc': 0.14404260635878283, 'eval_loss@deu.rst.pcc': 2.7453765869140625, 'eval_runtime': 3.2143, 'eval_samples_per_second': 74.977, 'eval_steps_per_second': 2.489, 'epoch': 11.0}
{'train@deu.rst.pcc_loss': 2.6446151733398438, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.23382624768946395, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09897603085089658, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.10591722630770978, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.14395629773526056, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6446149349212646, 'train@deu.rst.pcc_runtime': 26.0063, 'train@deu.rst.pcc_samples_per_second': 83.21, 'train@deu.rst.pcc_steps_per_second': 2.615, 'epoch': 12.0}
{'loss': 2.6854, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.742946147918701, 'eval_accuracy@deu.rst.pcc': 0.21161825726141079, 'eval_f1@deu.rst.pcc': 0.08654846034869872, 'eval_precision@deu.rst.pcc': 0.12277707679595551, 'eval_recall@deu.rst.pcc': 0.14404260635878283, 'eval_loss@deu.rst.pcc': 2.7429468631744385, 'eval_runtime': 3.1997, 'eval_samples_per_second': 75.32, 'eval_steps_per_second': 2.5, 'epoch': 12.0}
{'train_runtime': 1008.2421, 'train_samples_per_second': 25.756, 'train_steps_per_second': 0.809, 'train_loss': 2.848426837547153, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.8484
  train_runtime            = 0:16:48.24
  train_samples_per_second =     25.756
  train_steps_per_second   =      0.809
{'train@rus.rst.rrt_loss': 1.6954904794692993, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5015960030532233, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.18429015651252326, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.22962883695446978, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.2001711707829557, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.6954905986785889, 'train@rus.rst.rrt_runtime': 342.9661, 'train@rus.rst.rrt_samples_per_second': 84.037, 'train@rus.rst.rrt_steps_per_second': 2.627, 'epoch': 1.0}
{'loss': 2.0795, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.7349188327789307, 'eval_accuracy@rus.rst.rrt': 0.482661996497373, 'eval_f1@rus.rst.rrt': 0.2057359065313239, 'eval_precision@rus.rst.rrt': 0.2395828632270947, 'eval_recall@rus.rst.rrt': 0.22262857318658516, 'eval_loss@rus.rst.rrt': 1.7349191904067993, 'eval_runtime': 34.3176, 'eval_samples_per_second': 83.193, 'eval_steps_per_second': 2.623, 'epoch': 1.0}
{'train@rus.rst.rrt_loss': 1.4913562536239624, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5447227812087988, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.228848600102135, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.34014266576345803, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.23300255928102223, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.491356372833252, 'train@rus.rst.rrt_runtime': 343.1822, 'train@rus.rst.rrt_samples_per_second': 83.985, 'train@rus.rst.rrt_steps_per_second': 2.625, 'epoch': 2.0}
{'loss': 1.6233, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.5528775453567505, 'eval_accuracy@rus.rst.rrt': 0.5239929947460595, 'eval_f1@rus.rst.rrt': 0.2514531394915867, 'eval_precision@rus.rst.rrt': 0.2978860620256259, 'eval_recall@rus.rst.rrt': 0.2588517154773012, 'eval_loss@rus.rst.rrt': 1.5528773069381714, 'eval_runtime': 34.3646, 'eval_samples_per_second': 83.08, 'eval_steps_per_second': 2.619, 'epoch': 2.0}
{'train@rus.rst.rrt_loss': 1.4171215295791626, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5669627367982791, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.2876780918134982, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4300223775245764, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.27824142015195913, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.4171215295791626, 'train@rus.rst.rrt_runtime': 343.1262, 'train@rus.rst.rrt_samples_per_second': 83.998, 'train@rus.rst.rrt_steps_per_second': 2.626, 'epoch': 3.0}
{'loss': 1.5025, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.4806153774261475, 'eval_accuracy@rus.rst.rrt': 0.5418563922942207, 'eval_f1@rus.rst.rrt': 0.31568720085090046, 'eval_precision@rus.rst.rrt': 0.5059248194105906, 'eval_recall@rus.rst.rrt': 0.30783843559400925, 'eval_loss@rus.rst.rrt': 1.4806153774261475, 'eval_runtime': 34.2478, 'eval_samples_per_second': 83.363, 'eval_steps_per_second': 2.628, 'epoch': 3.0}
{'train@rus.rst.rrt_loss': 1.3679980039596558, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5820206786482548, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.31896968249570695, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.44910998914546757, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.29925137411804187, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3679978847503662, 'train@rus.rst.rrt_runtime': 342.97, 'train@rus.rst.rrt_samples_per_second': 84.037, 'train@rus.rst.rrt_steps_per_second': 2.627, 'epoch': 4.0}
{'loss': 1.4435, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.4376665353775024, 'eval_accuracy@rus.rst.rrt': 0.554816112084063, 'eval_f1@rus.rst.rrt': 0.36086936817341553, 'eval_precision@rus.rst.rrt': 0.45169543291076436, 'eval_recall@rus.rst.rrt': 0.3423940516053335, 'eval_loss@rus.rst.rrt': 1.4376665353775024, 'eval_runtime': 34.3062, 'eval_samples_per_second': 83.221, 'eval_steps_per_second': 2.623, 'epoch': 4.0}
{'train@rus.rst.rrt_loss': 1.3357150554656982, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5907986954409826, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3344241674789037, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.45067407430207157, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.31399472080191426, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3357150554656982, 'train@rus.rst.rrt_runtime': 342.9297, 'train@rus.rst.rrt_samples_per_second': 84.046, 'train@rus.rst.rrt_steps_per_second': 2.627, 'epoch': 5.0}
{'loss': 1.4055, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.41378915309906, 'eval_accuracy@rus.rst.rrt': 0.5597197898423818, 'eval_f1@rus.rst.rrt': 0.3785210728420657, 'eval_precision@rus.rst.rrt': 0.4599410959959456, 'eval_recall@rus.rst.rrt': 0.361689106290571, 'eval_loss@rus.rst.rrt': 1.4137893915176392, 'eval_runtime': 34.3716, 'eval_samples_per_second': 83.063, 'eval_steps_per_second': 2.618, 'epoch': 5.0}
{'train@rus.rst.rrt_loss': 1.3144950866699219, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5974255776837138, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3446272119702227, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.45366947677784847, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3249878724217563, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3144950866699219, 'train@rus.rst.rrt_runtime': 342.9916, 'train@rus.rst.rrt_samples_per_second': 84.031, 'train@rus.rst.rrt_steps_per_second': 2.627, 'epoch': 6.0}
{'loss': 1.3791, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.3954479694366455, 'eval_accuracy@rus.rst.rrt': 0.5656742556917689, 'eval_f1@rus.rst.rrt': 0.3883959609889452, 'eval_precision@rus.rst.rrt': 0.46153594529765307, 'eval_recall@rus.rst.rrt': 0.37379249451376945, 'eval_loss@rus.rst.rrt': 1.3954479694366455, 'eval_runtime': 34.3304, 'eval_samples_per_second': 83.162, 'eval_steps_per_second': 2.622, 'epoch': 6.0}
{'train@rus.rst.rrt_loss': 1.2987028360366821, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6013461938796753, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.35313842304952925, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4381813729237019, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3339138642545352, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2987030744552612, 'train@rus.rst.rrt_runtime': 343.0973, 'train@rus.rst.rrt_samples_per_second': 84.005, 'train@rus.rst.rrt_steps_per_second': 2.626, 'epoch': 7.0}
{'loss': 1.359, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.3860960006713867, 'eval_accuracy@rus.rst.rrt': 0.568476357267951, 'eval_f1@rus.rst.rrt': 0.3928613002159202, 'eval_precision@rus.rst.rrt': 0.45799172271967087, 'eval_recall@rus.rst.rrt': 0.3796121611363108, 'eval_loss@rus.rst.rrt': 1.3860960006713867, 'eval_runtime': 34.341, 'eval_samples_per_second': 83.137, 'eval_steps_per_second': 2.621, 'epoch': 7.0}
{'train@rus.rst.rrt_loss': 1.2850977182388306, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6057525501353133, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3610947477174689, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4325258965247427, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34437043790590943, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2850978374481201, 'train@rus.rst.rrt_runtime': 342.8385, 'train@rus.rst.rrt_samples_per_second': 84.069, 'train@rus.rst.rrt_steps_per_second': 2.628, 'epoch': 8.0}
{'loss': 1.3443, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.3749717473983765, 'eval_accuracy@rus.rst.rrt': 0.5726795096322241, 'eval_f1@rus.rst.rrt': 0.40320609610232117, 'eval_precision@rus.rst.rrt': 0.4632254524470506, 'eval_recall@rus.rst.rrt': 0.3903161460766336, 'eval_loss@rus.rst.rrt': 1.374971628189087, 'eval_runtime': 34.3594, 'eval_samples_per_second': 83.092, 'eval_steps_per_second': 2.619, 'epoch': 8.0}
{'train@rus.rst.rrt_loss': 1.2741601467132568, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6066546388175699, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36371363875723445, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4340738045374291, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34632597304633045, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2741602659225464, 'train@rus.rst.rrt_runtime': 343.2996, 'train@rus.rst.rrt_samples_per_second': 83.956, 'train@rus.rst.rrt_steps_per_second': 2.625, 'epoch': 9.0}
{'loss': 1.3334, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.368895411491394, 'eval_accuracy@rus.rst.rrt': 0.574430823117338, 'eval_f1@rus.rst.rrt': 0.40566978205628657, 'eval_precision@rus.rst.rrt': 0.46271136848670114, 'eval_recall@rus.rst.rrt': 0.3927940555657927, 'eval_loss@rus.rst.rrt': 1.3688952922821045, 'eval_runtime': 34.3248, 'eval_samples_per_second': 83.176, 'eval_steps_per_second': 2.622, 'epoch': 9.0}
{'train@rus.rst.rrt_loss': 1.2677007913589478, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6091527305530497, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3643804916124202, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4390430891375022, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3448456002592936, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2677006721496582, 'train@rus.rst.rrt_runtime': 343.4094, 'train@rus.rst.rrt_samples_per_second': 83.929, 'train@rus.rst.rrt_steps_per_second': 2.624, 'epoch': 10.0}
{'loss': 1.3258, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.3657439947128296, 'eval_accuracy@rus.rst.rrt': 0.5740805604203152, 'eval_f1@rus.rst.rrt': 0.40412094355716577, 'eval_precision@rus.rst.rrt': 0.46188010678299757, 'eval_recall@rus.rst.rrt': 0.3908341842979526, 'eval_loss@rus.rst.rrt': 1.36574387550354, 'eval_runtime': 34.3676, 'eval_samples_per_second': 83.072, 'eval_steps_per_second': 2.619, 'epoch': 10.0}
{'train@rus.rst.rrt_loss': 1.2639987468719482, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6095343834570814, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3673821076617392, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.43684034062272004, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3494868626948708, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2639986276626587, 'train@rus.rst.rrt_runtime': 342.8273, 'train@rus.rst.rrt_samples_per_second': 84.071, 'train@rus.rst.rrt_steps_per_second': 2.628, 'epoch': 11.0}
{'loss': 1.3194, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.3654158115386963, 'eval_accuracy@rus.rst.rrt': 0.5730297723292469, 'eval_f1@rus.rst.rrt': 0.40597976766542804, 'eval_precision@rus.rst.rrt': 0.46006958766778744, 'eval_recall@rus.rst.rrt': 0.3953069750327182, 'eval_loss@rus.rst.rrt': 1.3654159307479858, 'eval_runtime': 34.358, 'eval_samples_per_second': 83.096, 'eval_steps_per_second': 2.619, 'epoch': 11.0}
{'train@rus.rst.rrt_loss': 1.2625291347503662, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.609916036361113, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36748729020172816, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.43535494389461854, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34983619843712, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2625291347503662, 'train@rus.rst.rrt_runtime': 343.2169, 'train@rus.rst.rrt_samples_per_second': 83.976, 'train@rus.rst.rrt_steps_per_second': 2.625, 'epoch': 12.0}
{'loss': 1.3119, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.3643120527267456, 'eval_accuracy@rus.rst.rrt': 0.5737302977232924, 'eval_f1@rus.rst.rrt': 0.40799274350671366, 'eval_precision@rus.rst.rrt': 0.461099351161713, 'eval_recall@rus.rst.rrt': 0.39747278849936546, 'eval_loss@rus.rst.rrt': 1.3643120527267456, 'eval_runtime': 34.3092, 'eval_samples_per_second': 83.214, 'eval_steps_per_second': 2.623, 'epoch': 12.0}
{'train_runtime': 13277.6304, 'train_samples_per_second': 26.049, 'train_steps_per_second': 0.814, 'train_loss': 1.4522566320805297, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.8484
  train_runtime            = 0:16:48.24
  train_samples_per_second =     25.756
  train_steps_per_second   =      0.809
-------------------------------------------------------------------
Lang1:  eng.pdtb.pdtb    Lang2:  rus.rst.rrt
Saving run to:  runs/full_shot/FullShot=v4_eng.pdtb.pdtb_rus.rst.rrt_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 43920 examples
read 1674 examples
read 2257 examples
read 28822 examples
read 2855 examples
read 2843 examples
Total prediction labels:  45
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (ru): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=45, bias=True)
    )
  )
)
{'train@eng.pdtb.pdtb_loss': 1.2810450792312622, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.5920081967213114, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.25244812424034435, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.36534343916020595, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.25049002374043033, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.2810451984405518, 'train@eng.pdtb.pdtb_runtime': 513.2436, 'train@eng.pdtb.pdtb_samples_per_second': 85.573, 'train@eng.pdtb.pdtb_steps_per_second': 2.675, 'epoch': 1.0}
{'loss': 1.8718, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.1984972953796387, 'eval_accuracy@eng.pdtb.pdtb': 0.6242532855436081, 'eval_f1@eng.pdtb.pdtb': 0.29742295648758477, 'eval_precision@eng.pdtb.pdtb': 0.3313869624375966, 'eval_recall@eng.pdtb.pdtb': 0.2973458648534337, 'eval_loss@eng.pdtb.pdtb': 1.1984972953796387, 'eval_runtime': 19.9452, 'eval_samples_per_second': 83.93, 'eval_steps_per_second': 2.657, 'epoch': 1.0}
{'train@eng.pdtb.pdtb_loss': 1.1041978597640991, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6350637522768671, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.3385799292357671, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.47686758998161766, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.33335202647286005, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.1041979789733887, 'train@eng.pdtb.pdtb_runtime': 513.7305, 'train@eng.pdtb.pdtb_samples_per_second': 85.492, 'train@eng.pdtb.pdtb_steps_per_second': 2.673, 'epoch': 2.0}
{'loss': 1.232, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.029449224472046, 'eval_accuracy@eng.pdtb.pdtb': 0.6666666666666666, 'eval_f1@eng.pdtb.pdtb': 0.39455639034388723, 'eval_precision@eng.pdtb.pdtb': 0.4423274419429962, 'eval_recall@eng.pdtb.pdtb': 0.3891279871932934, 'eval_loss@eng.pdtb.pdtb': 1.0294491052627563, 'eval_runtime': 20.008, 'eval_samples_per_second': 83.667, 'eval_steps_per_second': 2.649, 'epoch': 2.0}
{'train@eng.pdtb.pdtb_loss': 1.049961805343628, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.650591985428051, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4310945766212476, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.47046222514275315, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4148462266440603, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.049961805343628, 'train@eng.pdtb.pdtb_runtime': 513.3653, 'train@eng.pdtb.pdtb_samples_per_second': 85.553, 'train@eng.pdtb.pdtb_steps_per_second': 2.675, 'epoch': 3.0}
{'loss': 1.1275, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 0.9881629943847656, 'eval_accuracy@eng.pdtb.pdtb': 0.6780167264038232, 'eval_f1@eng.pdtb.pdtb': 0.4823870478707609, 'eval_precision@eng.pdtb.pdtb': 0.5700633341414659, 'eval_recall@eng.pdtb.pdtb': 0.4571709574569122, 'eval_loss@eng.pdtb.pdtb': 0.9881628751754761, 'eval_runtime': 19.9954, 'eval_samples_per_second': 83.719, 'eval_steps_per_second': 2.651, 'epoch': 3.0}
{'train@eng.pdtb.pdtb_loss': 1.0056841373443604, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6639571948998179, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.44651175018187006, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5157840852170748, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4401785812951296, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.00568425655365, 'train@eng.pdtb.pdtb_runtime': 513.5389, 'train@eng.pdtb.pdtb_samples_per_second': 85.524, 'train@eng.pdtb.pdtb_steps_per_second': 2.674, 'epoch': 4.0}
{'loss': 1.078, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 0.9529651403427124, 'eval_accuracy@eng.pdtb.pdtb': 0.6863799283154122, 'eval_f1@eng.pdtb.pdtb': 0.5044489697735086, 'eval_precision@eng.pdtb.pdtb': 0.5482645689660155, 'eval_recall@eng.pdtb.pdtb': 0.4920652520605409, 'eval_loss@eng.pdtb.pdtb': 0.9529650807380676, 'eval_runtime': 19.9818, 'eval_samples_per_second': 83.776, 'eval_steps_per_second': 2.652, 'epoch': 4.0}
{'train@eng.pdtb.pdtb_loss': 0.9805963039398193, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6714025500910746, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4542311488649982, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5199927594039444, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.44985499319694966, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9805963039398193, 'train@eng.pdtb.pdtb_runtime': 513.5024, 'train@eng.pdtb.pdtb_samples_per_second': 85.53, 'train@eng.pdtb.pdtb_steps_per_second': 2.674, 'epoch': 5.0}
{'loss': 1.0472, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 0.9332332015037537, 'eval_accuracy@eng.pdtb.pdtb': 0.6905615292712067, 'eval_f1@eng.pdtb.pdtb': 0.5240880306143786, 'eval_precision@eng.pdtb.pdtb': 0.5502782863872219, 'eval_recall@eng.pdtb.pdtb': 0.5190559291729776, 'eval_loss@eng.pdtb.pdtb': 0.9332332015037537, 'eval_runtime': 19.9705, 'eval_samples_per_second': 83.824, 'eval_steps_per_second': 2.654, 'epoch': 5.0}
{'train@eng.pdtb.pdtb_loss': 0.9618003964424133, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6776867030965391, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4617321262597524, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5239669329019027, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.457677481549568, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9618003368377686, 'train@eng.pdtb.pdtb_runtime': 513.3942, 'train@eng.pdtb.pdtb_samples_per_second': 85.548, 'train@eng.pdtb.pdtb_steps_per_second': 2.674, 'epoch': 6.0}
{'loss': 1.0253, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 0.9260389804840088, 'eval_accuracy@eng.pdtb.pdtb': 0.6875746714456392, 'eval_f1@eng.pdtb.pdtb': 0.5282248584349349, 'eval_precision@eng.pdtb.pdtb': 0.5521014117358457, 'eval_recall@eng.pdtb.pdtb': 0.5213592866128205, 'eval_loss@eng.pdtb.pdtb': 0.9260388612747192, 'eval_runtime': 20.0247, 'eval_samples_per_second': 83.597, 'eval_steps_per_second': 2.647, 'epoch': 6.0}
{'train@eng.pdtb.pdtb_loss': 0.950163722038269, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6774362477231329, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4640504917999393, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5307359223251382, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.45629587202232236, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.950163722038269, 'train@eng.pdtb.pdtb_runtime': 513.479, 'train@eng.pdtb.pdtb_samples_per_second': 85.534, 'train@eng.pdtb.pdtb_steps_per_second': 2.674, 'epoch': 7.0}
{'loss': 1.01, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 0.9129180908203125, 'eval_accuracy@eng.pdtb.pdtb': 0.6941457586618877, 'eval_f1@eng.pdtb.pdtb': 0.5338006184930191, 'eval_precision@eng.pdtb.pdtb': 0.5755381820447966, 'eval_recall@eng.pdtb.pdtb': 0.5201908571688915, 'eval_loss@eng.pdtb.pdtb': 0.912918210029602, 'eval_runtime': 19.9663, 'eval_samples_per_second': 83.841, 'eval_steps_per_second': 2.654, 'epoch': 7.0}
{'train@eng.pdtb.pdtb_loss': 0.9381495118141174, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6829918032786885, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.47093162017688805, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5285369039351469, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4683305134285593, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9381494522094727, 'train@eng.pdtb.pdtb_runtime': 513.5411, 'train@eng.pdtb.pdtb_samples_per_second': 85.524, 'train@eng.pdtb.pdtb_steps_per_second': 2.674, 'epoch': 8.0}
{'loss': 0.9985, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 0.9081308841705322, 'eval_accuracy@eng.pdtb.pdtb': 0.6929510155316607, 'eval_f1@eng.pdtb.pdtb': 0.5575293659300334, 'eval_precision@eng.pdtb.pdtb': 0.6244854728817026, 'eval_recall@eng.pdtb.pdtb': 0.5430408103891383, 'eval_loss@eng.pdtb.pdtb': 0.9081308841705322, 'eval_runtime': 22.9143, 'eval_samples_per_second': 73.055, 'eval_steps_per_second': 2.313, 'epoch': 8.0}
{'train@eng.pdtb.pdtb_loss': 0.92962247133255, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6856102003642988, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4737736336821186, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5323378127904127, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4696462164605462, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9296225905418396, 'train@eng.pdtb.pdtb_runtime': 512.9688, 'train@eng.pdtb.pdtb_samples_per_second': 85.619, 'train@eng.pdtb.pdtb_steps_per_second': 2.677, 'epoch': 9.0}
{'loss': 0.9862, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 0.9056497812271118, 'eval_accuracy@eng.pdtb.pdtb': 0.6947431302270012, 'eval_f1@eng.pdtb.pdtb': 0.5635562729332281, 'eval_precision@eng.pdtb.pdtb': 0.6308535220429149, 'eval_recall@eng.pdtb.pdtb': 0.5468464778253059, 'eval_loss@eng.pdtb.pdtb': 0.9056497812271118, 'eval_runtime': 19.9988, 'eval_samples_per_second': 83.705, 'eval_steps_per_second': 2.65, 'epoch': 9.0}
{'train@eng.pdtb.pdtb_loss': 0.9263473749160767, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6861794171220401, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.47608807762861277, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5783914224848007, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.470143986067671, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9263473749160767, 'train@eng.pdtb.pdtb_runtime': 513.219, 'train@eng.pdtb.pdtb_samples_per_second': 85.578, 'train@eng.pdtb.pdtb_steps_per_second': 2.675, 'epoch': 10.0}
{'loss': 0.9809, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 0.8997315764427185, 'eval_accuracy@eng.pdtb.pdtb': 0.6965352449223416, 'eval_f1@eng.pdtb.pdtb': 0.5596627797346279, 'eval_precision@eng.pdtb.pdtb': 0.6295359321811032, 'eval_recall@eng.pdtb.pdtb': 0.5420507741587453, 'eval_loss@eng.pdtb.pdtb': 0.8997315764427185, 'eval_runtime': 19.9656, 'eval_samples_per_second': 83.844, 'eval_steps_per_second': 2.655, 'epoch': 10.0}
{'train@eng.pdtb.pdtb_loss': 0.9219785928726196, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6876138433515483, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4775270080901467, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5383245477629102, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.47355591741891995, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9219785928726196, 'train@eng.pdtb.pdtb_runtime': 513.1617, 'train@eng.pdtb.pdtb_samples_per_second': 85.587, 'train@eng.pdtb.pdtb_steps_per_second': 2.676, 'epoch': 11.0}
{'loss': 0.9754, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.8996318578720093, 'eval_accuracy@eng.pdtb.pdtb': 0.6929510155316607, 'eval_f1@eng.pdtb.pdtb': 0.5649049440696439, 'eval_precision@eng.pdtb.pdtb': 0.5994025206361139, 'eval_recall@eng.pdtb.pdtb': 0.553778284944108, 'eval_loss@eng.pdtb.pdtb': 0.8996319770812988, 'eval_runtime': 19.9939, 'eval_samples_per_second': 83.726, 'eval_steps_per_second': 2.651, 'epoch': 11.0}
{'train@eng.pdtb.pdtb_loss': 0.9205985069274902, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6885018214936248, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.49522548946386413, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5668418066633684, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4834577655511926, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9205984473228455, 'train@eng.pdtb.pdtb_runtime': 513.2131, 'train@eng.pdtb.pdtb_samples_per_second': 85.578, 'train@eng.pdtb.pdtb_steps_per_second': 2.675, 'epoch': 12.0}
{'loss': 0.9718, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.8977541327476501, 'eval_accuracy@eng.pdtb.pdtb': 0.6977299880525687, 'eval_f1@eng.pdtb.pdtb': 0.5628913513275606, 'eval_precision@eng.pdtb.pdtb': 0.6291774372077898, 'eval_recall@eng.pdtb.pdtb': 0.5464855858668694, 'eval_loss@eng.pdtb.pdtb': 0.8977540731430054, 'eval_runtime': 19.971, 'eval_samples_per_second': 83.822, 'eval_steps_per_second': 2.654, 'epoch': 12.0}
{'train_runtime': 19477.6959, 'train_samples_per_second': 27.059, 'train_steps_per_second': 0.846, 'train_loss': 1.1087162604057605, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.1087
  train_runtime            = 5:24:37.69
  train_samples_per_second =     27.059
  train_steps_per_second   =      0.846
{'train@rus.rst.rrt_loss': 1.5304481983184814, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5424675595031573, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.24009168404747622, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.3340308873345598, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.24241089305125402, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.5304481983184814, 'train@rus.rst.rrt_runtime': 343.1146, 'train@rus.rst.rrt_samples_per_second': 84.001, 'train@rus.rst.rrt_steps_per_second': 2.626, 'epoch': 1.0}
{'loss': 1.9114, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.5911983251571655, 'eval_accuracy@rus.rst.rrt': 0.5215411558669002, 'eval_f1@rus.rst.rrt': 0.27691182151715993, 'eval_precision@rus.rst.rrt': 0.3314831460298194, 'eval_recall@rus.rst.rrt': 0.27908120230811523, 'eval_loss@rus.rst.rrt': 1.5911979675292969, 'eval_runtime': 34.4637, 'eval_samples_per_second': 82.841, 'eval_steps_per_second': 2.611, 'epoch': 1.0}
{'train@rus.rst.rrt_loss': 1.40943443775177, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.568524044132954, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.2897487378306589, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.41960494203928467, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.27971011526020173, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.40943443775177, 'train@rus.rst.rrt_runtime': 343.0497, 'train@rus.rst.rrt_samples_per_second': 84.017, 'train@rus.rst.rrt_steps_per_second': 2.626, 'epoch': 2.0}
{'loss': 1.5248, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.4872184991836548, 'eval_accuracy@rus.rst.rrt': 0.5380035026269703, 'eval_f1@rus.rst.rrt': 0.31357635368428166, 'eval_precision@rus.rst.rrt': 0.3840811886156809, 'eval_recall@rus.rst.rrt': 0.3072259063911271, 'eval_loss@rus.rst.rrt': 1.4872184991836548, 'eval_runtime': 34.3837, 'eval_samples_per_second': 83.034, 'eval_steps_per_second': 2.618, 'epoch': 2.0}
{'train@rus.rst.rrt_loss': 1.3597846031188965, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5808063284990632, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.32395828510512725, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4321447365511337, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.31001400434187965, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3597846031188965, 'train@rus.rst.rrt_runtime': 343.0796, 'train@rus.rst.rrt_samples_per_second': 84.01, 'train@rus.rst.rrt_steps_per_second': 2.626, 'epoch': 3.0}
{'loss': 1.4423, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.4424185752868652, 'eval_accuracy@rus.rst.rrt': 0.5513134851138354, 'eval_f1@rus.rst.rrt': 0.36399580602272946, 'eval_precision@rus.rst.rrt': 0.49253290594248567, 'eval_recall@rus.rst.rrt': 0.35128409332854543, 'eval_loss@rus.rst.rrt': 1.4424183368682861, 'eval_runtime': 34.3956, 'eval_samples_per_second': 83.005, 'eval_steps_per_second': 2.617, 'epoch': 3.0}
{'train@rus.rst.rrt_loss': 1.319549322128296, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5954826174450073, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3478827463665617, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4524025737201315, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3257800063452107, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3195494413375854, 'train@rus.rst.rrt_runtime': 342.7885, 'train@rus.rst.rrt_samples_per_second': 84.081, 'train@rus.rst.rrt_steps_per_second': 2.628, 'epoch': 4.0}
{'loss': 1.3947, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.402165412902832, 'eval_accuracy@rus.rst.rrt': 0.568476357267951, 'eval_f1@rus.rst.rrt': 0.40209903859321156, 'eval_precision@rus.rst.rrt': 0.48094770661225716, 'eval_recall@rus.rst.rrt': 0.3853305776094973, 'eval_loss@rus.rst.rrt': 1.4021656513214111, 'eval_runtime': 34.3781, 'eval_samples_per_second': 83.047, 'eval_steps_per_second': 2.618, 'epoch': 4.0}
{'train@rus.rst.rrt_loss': 1.2939952611923218, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6004441051974186, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3563795271063752, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.444521380709881, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.33544942182256965, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2939952611923218, 'train@rus.rst.rrt_runtime': 343.1403, 'train@rus.rst.rrt_samples_per_second': 83.995, 'train@rus.rst.rrt_steps_per_second': 2.626, 'epoch': 5.0}
{'loss': 1.3665, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.38558828830719, 'eval_accuracy@rus.rst.rrt': 0.5688266199649737, 'eval_f1@rus.rst.rrt': 0.40855200844748657, 'eval_precision@rus.rst.rrt': 0.5117471165237872, 'eval_recall@rus.rst.rrt': 0.39235686410144127, 'eval_loss@rus.rst.rrt': 1.3855880498886108, 'eval_runtime': 34.4037, 'eval_samples_per_second': 82.985, 'eval_steps_per_second': 2.616, 'epoch': 5.0}
{'train@rus.rst.rrt_loss': 1.2766121625900269, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.60540559294983, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36474729085240815, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.46290759977177987, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34237785236906987, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2766120433807373, 'train@rus.rst.rrt_runtime': 343.0677, 'train@rus.rst.rrt_samples_per_second': 84.013, 'train@rus.rst.rrt_steps_per_second': 2.626, 'epoch': 6.0}
{'loss': 1.3439, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.3746743202209473, 'eval_accuracy@rus.rst.rrt': 0.5723292469352014, 'eval_f1@rus.rst.rrt': 0.41625325171611877, 'eval_precision@rus.rst.rrt': 0.5165858942906427, 'eval_recall@rus.rst.rrt': 0.3982774827024546, 'eval_loss@rus.rst.rrt': 1.3746742010116577, 'eval_runtime': 34.4169, 'eval_samples_per_second': 82.953, 'eval_steps_per_second': 2.615, 'epoch': 6.0}
{'train@rus.rst.rrt_loss': 1.2619361877441406, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6086669904933731, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.37037343161769026, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4483167946978762, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34904353412222827, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2619359493255615, 'train@rus.rst.rrt_runtime': 343.0587, 'train@rus.rst.rrt_samples_per_second': 84.015, 'train@rus.rst.rrt_steps_per_second': 2.626, 'epoch': 7.0}
{'loss': 1.3274, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.3651872873306274, 'eval_accuracy@rus.rst.rrt': 0.5740805604203152, 'eval_f1@rus.rst.rrt': 0.4222820302365218, 'eval_precision@rus.rst.rrt': 0.5061319522943273, 'eval_recall@rus.rst.rrt': 0.40354199759871456, 'eval_loss@rus.rst.rrt': 1.365187406539917, 'eval_runtime': 34.4678, 'eval_samples_per_second': 82.831, 'eval_steps_per_second': 2.611, 'epoch': 7.0}
{'train@rus.rst.rrt_loss': 1.252681851387024, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6108528207619179, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3757294447580263, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4427241600278455, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.35604331252848714, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.252681851387024, 'train@rus.rst.rrt_runtime': 343.1147, 'train@rus.rst.rrt_samples_per_second': 84.001, 'train@rus.rst.rrt_steps_per_second': 2.626, 'epoch': 8.0}
{'loss': 1.3131, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.357919692993164, 'eval_accuracy@rus.rst.rrt': 0.5814360770577933, 'eval_f1@rus.rst.rrt': 0.42862486585129306, 'eval_precision@rus.rst.rrt': 0.49179651699403226, 'eval_recall@rus.rst.rrt': 0.4125051462542243, 'eval_loss@rus.rst.rrt': 1.357919692993164, 'eval_runtime': 34.4169, 'eval_samples_per_second': 82.953, 'eval_steps_per_second': 2.615, 'epoch': 8.0}
{'train@rus.rst.rrt_loss': 1.2406527996063232, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6141489140240094, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3797521915518716, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4523942342794422, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.35848854675790814, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2406529188156128, 'train@rus.rst.rrt_runtime': 343.0569, 'train@rus.rst.rrt_samples_per_second': 84.015, 'train@rus.rst.rrt_steps_per_second': 2.626, 'epoch': 9.0}
{'loss': 1.3054, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.3521099090576172, 'eval_accuracy@rus.rst.rrt': 0.5779334500875657, 'eval_f1@rus.rst.rrt': 0.4291015860631448, 'eval_precision@rus.rst.rrt': 0.5048989968308354, 'eval_recall@rus.rst.rrt': 0.41017677963941496, 'eval_loss@rus.rst.rrt': 1.3521099090576172, 'eval_runtime': 34.366, 'eval_samples_per_second': 83.076, 'eval_steps_per_second': 2.619, 'epoch': 9.0}
{'train@rus.rst.rrt_loss': 1.236193299293518, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6161612656998127, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3793962433913659, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4529056486241021, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.35652410386076183, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2361934185028076, 'train@rus.rst.rrt_runtime': 343.0839, 'train@rus.rst.rrt_samples_per_second': 84.009, 'train@rus.rst.rrt_steps_per_second': 2.626, 'epoch': 10.0}
{'loss': 1.2965, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.3486922979354858, 'eval_accuracy@rus.rst.rrt': 0.580385288966725, 'eval_f1@rus.rst.rrt': 0.4298600398575105, 'eval_precision@rus.rst.rrt': 0.5063915397297485, 'eval_recall@rus.rst.rrt': 0.40909739873805806, 'eval_loss@rus.rst.rrt': 1.3486922979354858, 'eval_runtime': 34.3636, 'eval_samples_per_second': 83.082, 'eval_steps_per_second': 2.619, 'epoch': 10.0}
{'train@rus.rst.rrt_loss': 1.2323092222213745, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.615883699951426, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.38138633408853084, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.45055002286785845, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.36096260135144914, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2323092222213745, 'train@rus.rst.rrt_runtime': 343.1739, 'train@rus.rst.rrt_samples_per_second': 83.987, 'train@rus.rst.rrt_steps_per_second': 2.625, 'epoch': 11.0}
{'loss': 1.2919, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.3474925756454468, 'eval_accuracy@rus.rst.rrt': 0.5810858143607706, 'eval_f1@rus.rst.rrt': 0.4329859888886453, 'eval_precision@rus.rst.rrt': 0.5058662442887784, 'eval_recall@rus.rst.rrt': 0.41449648827258045, 'eval_loss@rus.rst.rrt': 1.3474925756454468, 'eval_runtime': 34.3899, 'eval_samples_per_second': 83.019, 'eval_steps_per_second': 2.617, 'epoch': 11.0}
{'train@rus.rst.rrt_loss': 1.231374740600586, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6163694400111026, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3813226584276277, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.45057677384777983, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3607290415556321, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2313746213912964, 'train@rus.rst.rrt_runtime': 343.0, 'train@rus.rst.rrt_samples_per_second': 84.029, 'train@rus.rst.rrt_steps_per_second': 2.627, 'epoch': 12.0}
{'loss': 1.2867, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.3468456268310547, 'eval_accuracy@rus.rst.rrt': 0.5796847635726795, 'eval_f1@rus.rst.rrt': 0.4297285790314723, 'eval_precision@rus.rst.rrt': 0.4927625320567113, 'eval_recall@rus.rst.rrt': 0.4134204356400913, 'eval_loss@rus.rst.rrt': 1.3468456268310547, 'eval_runtime': 34.4101, 'eval_samples_per_second': 82.97, 'eval_steps_per_second': 2.616, 'epoch': 12.0}
{'train_runtime': 13279.6945, 'train_samples_per_second': 26.045, 'train_steps_per_second': 0.814, 'train_loss': 1.4003788831275434, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.1087
  train_runtime            = 5:24:37.69
  train_samples_per_second =     27.059
  train_steps_per_second   =      0.846
-------------------------------------------------------------------
Lang1:  eng.rst.gum    Lang2:  rus.rst.rrt
Saving run to:  runs/full_shot/FullShot=v4_eng.rst.gum_rus.rst.rrt_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 13897 examples
read 2149 examples
read 2091 examples
read 28822 examples
read 2855 examples
read 2843 examples
Total prediction labels:  28
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (ru): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=28, bias=True)
    )
  )
)
{'train@eng.rst.gum_loss': 2.500237226486206, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.25048571634165645, 'train@eng.rst.gum_f1@eng.rst.gum': 0.04457662096440799, 'train@eng.rst.gum_precision@eng.rst.gum': 0.06285484155664901, 'train@eng.rst.gum_recall@eng.rst.gum': 0.06090128997067786, 'train@eng.rst.gum_loss@eng.rst.gum': 2.500237464904785, 'train@eng.rst.gum_runtime': 162.4396, 'train@eng.rst.gum_samples_per_second': 85.552, 'train@eng.rst.gum_steps_per_second': 2.678, 'epoch': 1.0}
{'loss': 2.7405, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.582864284515381, 'eval_accuracy@eng.rst.gum': 0.2517449976733364, 'eval_f1@eng.rst.gum': 0.048222020352099076, 'eval_precision@eng.rst.gum': 0.05633870056841499, 'eval_recall@eng.rst.gum': 0.06626671104351313, 'eval_loss@eng.rst.gum': 2.582864284515381, 'eval_runtime': 25.408, 'eval_samples_per_second': 84.58, 'eval_steps_per_second': 2.676, 'epoch': 1.0}
{'train@eng.rst.gum_loss': 2.0673201084136963, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.39871914801755776, 'train@eng.rst.gum_f1@eng.rst.gum': 0.1646678432913323, 'train@eng.rst.gum_precision@eng.rst.gum': 0.2577847336315465, 'train@eng.rst.gum_recall@eng.rst.gum': 0.17225185284001635, 'train@eng.rst.gum_loss@eng.rst.gum': 2.0673201084136963, 'train@eng.rst.gum_runtime': 162.4831, 'train@eng.rst.gum_samples_per_second': 85.529, 'train@eng.rst.gum_steps_per_second': 2.677, 'epoch': 2.0}
{'loss': 2.35, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.1699252128601074, 'eval_accuracy@eng.rst.gum': 0.37412750116333177, 'eval_f1@eng.rst.gum': 0.163265419285719, 'eval_precision@eng.rst.gum': 0.2625639830055164, 'eval_recall@eng.rst.gum': 0.17280373552050357, 'eval_loss@eng.rst.gum': 2.1699254512786865, 'eval_runtime': 25.4284, 'eval_samples_per_second': 84.512, 'eval_steps_per_second': 2.674, 'epoch': 2.0}
{'train@eng.rst.gum_loss': 1.7975918054580688, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.47988774555659497, 'train@eng.rst.gum_f1@eng.rst.gum': 0.2656562383694845, 'train@eng.rst.gum_precision@eng.rst.gum': 0.4123721921291013, 'train@eng.rst.gum_recall@eng.rst.gum': 0.27946399130924443, 'train@eng.rst.gum_loss@eng.rst.gum': 1.797592043876648, 'train@eng.rst.gum_runtime': 162.4546, 'train@eng.rst.gum_samples_per_second': 85.544, 'train@eng.rst.gum_steps_per_second': 2.678, 'epoch': 3.0}
{'loss': 1.9985, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.9306020736694336, 'eval_accuracy@eng.rst.gum': 0.4523033969288041, 'eval_f1@eng.rst.gum': 0.25410096362468887, 'eval_precision@eng.rst.gum': 0.3026338991837655, 'eval_recall@eng.rst.gum': 0.2722244370813974, 'eval_loss@eng.rst.gum': 1.9306020736694336, 'eval_runtime': 25.3737, 'eval_samples_per_second': 84.694, 'eval_steps_per_second': 2.68, 'epoch': 3.0}
{'train@eng.rst.gum_loss': 1.6685949563980103, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5102540116571922, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3089406110698922, 'train@eng.rst.gum_precision@eng.rst.gum': 0.4219481385608518, 'train@eng.rst.gum_recall@eng.rst.gum': 0.31523104401918706, 'train@eng.rst.gum_loss@eng.rst.gum': 1.6685949563980103, 'train@eng.rst.gum_runtime': 162.4599, 'train@eng.rst.gum_samples_per_second': 85.541, 'train@eng.rst.gum_steps_per_second': 2.678, 'epoch': 4.0}
{'loss': 1.808, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.8356224298477173, 'eval_accuracy@eng.rst.gum': 0.47324336900884134, 'eval_f1@eng.rst.gum': 0.2936190365771363, 'eval_precision@eng.rst.gum': 0.34044560618316005, 'eval_recall@eng.rst.gum': 0.30447173358895924, 'eval_loss@eng.rst.gum': 1.8356221914291382, 'eval_runtime': 25.3571, 'eval_samples_per_second': 84.749, 'eval_steps_per_second': 2.682, 'epoch': 4.0}
{'train@eng.rst.gum_loss': 1.5900437831878662, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5252212707778657, 'train@eng.rst.gum_f1@eng.rst.gum': 0.34109706782610316, 'train@eng.rst.gum_precision@eng.rst.gum': 0.4552379640232679, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3497846815261568, 'train@eng.rst.gum_loss@eng.rst.gum': 1.590043544769287, 'train@eng.rst.gum_runtime': 162.5162, 'train@eng.rst.gum_samples_per_second': 85.511, 'train@eng.rst.gum_steps_per_second': 2.677, 'epoch': 5.0}
{'loss': 1.7047, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.775879979133606, 'eval_accuracy@eng.rst.gum': 0.4881340158213122, 'eval_f1@eng.rst.gum': 0.32012706558865256, 'eval_precision@eng.rst.gum': 0.3732553531802632, 'eval_recall@eng.rst.gum': 0.3356431924366772, 'eval_loss@eng.rst.gum': 1.7758798599243164, 'eval_runtime': 25.3854, 'eval_samples_per_second': 84.655, 'eval_steps_per_second': 2.679, 'epoch': 5.0}
{'train@eng.rst.gum_loss': 1.5390952825546265, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.538821328344247, 'train@eng.rst.gum_f1@eng.rst.gum': 0.37002318520277355, 'train@eng.rst.gum_precision@eng.rst.gum': 0.4975910147358266, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3725526293186773, 'train@eng.rst.gum_loss@eng.rst.gum': 1.539095401763916, 'train@eng.rst.gum_runtime': 162.3921, 'train@eng.rst.gum_samples_per_second': 85.577, 'train@eng.rst.gum_steps_per_second': 2.679, 'epoch': 6.0}
{'loss': 1.6426, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.7385308742523193, 'eval_accuracy@eng.rst.gum': 0.495114006514658, 'eval_f1@eng.rst.gum': 0.33412573322946804, 'eval_precision@eng.rst.gum': 0.40316730702429365, 'eval_recall@eng.rst.gum': 0.3472077161701337, 'eval_loss@eng.rst.gum': 1.7385308742523193, 'eval_runtime': 25.394, 'eval_samples_per_second': 84.626, 'eval_steps_per_second': 2.678, 'epoch': 6.0}
{'train@eng.rst.gum_loss': 1.5027732849121094, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.546232999928042, 'train@eng.rst.gum_f1@eng.rst.gum': 0.38855312857207436, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5375652384668492, 'train@eng.rst.gum_recall@eng.rst.gum': 0.39009435436099105, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5027732849121094, 'train@eng.rst.gum_runtime': 162.2634, 'train@eng.rst.gum_samples_per_second': 85.645, 'train@eng.rst.gum_steps_per_second': 2.681, 'epoch': 7.0}
{'loss': 1.5978, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.7174142599105835, 'eval_accuracy@eng.rst.gum': 0.504885993485342, 'eval_f1@eng.rst.gum': 0.3644261436024717, 'eval_precision@eng.rst.gum': 0.4578725225319814, 'eval_recall@eng.rst.gum': 0.37477212325067255, 'eval_loss@eng.rst.gum': 1.7174140214920044, 'eval_runtime': 25.4759, 'eval_samples_per_second': 84.354, 'eval_steps_per_second': 2.669, 'epoch': 7.0}
{'train@eng.rst.gum_loss': 1.4780129194259644, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5556594948550047, 'train@eng.rst.gum_f1@eng.rst.gum': 0.40530555285099285, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5372641758368288, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4061183221434999, 'train@eng.rst.gum_loss@eng.rst.gum': 1.478013038635254, 'train@eng.rst.gum_runtime': 162.4426, 'train@eng.rst.gum_samples_per_second': 85.55, 'train@eng.rst.gum_steps_per_second': 2.678, 'epoch': 8.0}
{'loss': 1.5647, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.7002575397491455, 'eval_accuracy@eng.rst.gum': 0.5090739879013495, 'eval_f1@eng.rst.gum': 0.37736978302961965, 'eval_precision@eng.rst.gum': 0.4828855386282927, 'eval_recall@eng.rst.gum': 0.3869459991911393, 'eval_loss@eng.rst.gum': 1.7002575397491455, 'eval_runtime': 25.4544, 'eval_samples_per_second': 84.425, 'eval_steps_per_second': 2.671, 'epoch': 8.0}
{'train@eng.rst.gum_loss': 1.456945776939392, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5638627041807585, 'train@eng.rst.gum_f1@eng.rst.gum': 0.42139468606300545, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5466809680879551, 'train@eng.rst.gum_recall@eng.rst.gum': 0.41954874304202683, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4569458961486816, 'train@eng.rst.gum_runtime': 162.4434, 'train@eng.rst.gum_samples_per_second': 85.55, 'train@eng.rst.gum_steps_per_second': 2.678, 'epoch': 9.0}
{'loss': 1.5406, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.682391881942749, 'eval_accuracy@eng.rst.gum': 0.5062819916240112, 'eval_f1@eng.rst.gum': 0.38259306677771926, 'eval_precision@eng.rst.gum': 0.45698877221389467, 'eval_recall@eng.rst.gum': 0.3907573444828865, 'eval_loss@eng.rst.gum': 1.682391881942749, 'eval_runtime': 25.4164, 'eval_samples_per_second': 84.552, 'eval_steps_per_second': 2.675, 'epoch': 9.0}
{'train@eng.rst.gum_loss': 1.442173719406128, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5668129812189682, 'train@eng.rst.gum_f1@eng.rst.gum': 0.42525979334976727, 'train@eng.rst.gum_precision@eng.rst.gum': 0.53607187633961, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4211464975165524, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4421736001968384, 'train@eng.rst.gum_runtime': 162.4215, 'train@eng.rst.gum_samples_per_second': 85.561, 'train@eng.rst.gum_steps_per_second': 2.678, 'epoch': 10.0}
{'loss': 1.525, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.6746349334716797, 'eval_accuracy@eng.rst.gum': 0.5127966496044672, 'eval_f1@eng.rst.gum': 0.3886373007517701, 'eval_precision@eng.rst.gum': 0.4575654149525487, 'eval_recall@eng.rst.gum': 0.3933036823798982, 'eval_loss@eng.rst.gum': 1.6746350526809692, 'eval_runtime': 25.4593, 'eval_samples_per_second': 84.409, 'eval_steps_per_second': 2.671, 'epoch': 10.0}
{'train@eng.rst.gum_loss': 1.435594081878662, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5681801827732604, 'train@eng.rst.gum_f1@eng.rst.gum': 0.42858049011135596, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5333035577264333, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4261873044795972, 'train@eng.rst.gum_loss@eng.rst.gum': 1.435594081878662, 'train@eng.rst.gum_runtime': 162.3118, 'train@eng.rst.gum_samples_per_second': 85.619, 'train@eng.rst.gum_steps_per_second': 2.68, 'epoch': 11.0}
{'loss': 1.5199, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.668849229812622, 'eval_accuracy@eng.rst.gum': 0.511400651465798, 'eval_f1@eng.rst.gum': 0.38998568988166993, 'eval_precision@eng.rst.gum': 0.45297240983401504, 'eval_recall@eng.rst.gum': 0.39891902600981494, 'eval_loss@eng.rst.gum': 1.6688493490219116, 'eval_runtime': 25.3905, 'eval_samples_per_second': 84.638, 'eval_steps_per_second': 2.678, 'epoch': 11.0}
{'train@eng.rst.gum_loss': 1.4325593709945679, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5683240987263438, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4289826884905391, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5307306343400678, 'train@eng.rst.gum_recall@eng.rst.gum': 0.42597379025234583, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4325592517852783, 'train@eng.rst.gum_runtime': 162.6893, 'train@eng.rst.gum_samples_per_second': 85.421, 'train@eng.rst.gum_steps_per_second': 2.674, 'epoch': 12.0}
{'loss': 1.5071, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.6678321361541748, 'eval_accuracy@eng.rst.gum': 0.5146579804560261, 'eval_f1@eng.rst.gum': 0.39073557507740864, 'eval_precision@eng.rst.gum': 0.45373652720568697, 'eval_recall@eng.rst.gum': 0.3992416142678035, 'eval_loss@eng.rst.gum': 1.667832374572754, 'eval_runtime': 25.4338, 'eval_samples_per_second': 84.494, 'eval_steps_per_second': 2.674, 'epoch': 12.0}
{'train_runtime': 6395.0585, 'train_samples_per_second': 26.077, 'train_steps_per_second': 0.816, 'train_loss': 1.791613629220546, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.7916
  train_runtime            = 1:46:35.05
  train_samples_per_second =     26.077
  train_steps_per_second   =      0.816
{'train@rus.rst.rrt_loss': 1.5277879238128662, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5426757338144473, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.24424894152100537, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.3177127596039934, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.24842627302428644, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.5277880430221558, 'train@rus.rst.rrt_runtime': 343.0717, 'train@rus.rst.rrt_samples_per_second': 84.012, 'train@rus.rst.rrt_steps_per_second': 2.626, 'epoch': 1.0}
{'loss': 1.881, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.5900455713272095, 'eval_accuracy@rus.rst.rrt': 0.5078809106830122, 'eval_f1@rus.rst.rrt': 0.2633745112616885, 'eval_precision@rus.rst.rrt': 0.35407128973699686, 'eval_recall@rus.rst.rrt': 0.26671589316112865, 'eval_loss@rus.rst.rrt': 1.5900452136993408, 'eval_runtime': 34.2745, 'eval_samples_per_second': 83.298, 'eval_steps_per_second': 2.626, 'epoch': 1.0}
{'train@rus.rst.rrt_loss': 1.4178014993667603, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5699465685934356, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.307693422208796, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4576575451872682, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.29474464231153696, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.4178016185760498, 'train@rus.rst.rrt_runtime': 343.1186, 'train@rus.rst.rrt_samples_per_second': 84.0, 'train@rus.rst.rrt_steps_per_second': 2.626, 'epoch': 2.0}
{'loss': 1.5221, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.489925503730774, 'eval_accuracy@rus.rst.rrt': 0.5450087565674255, 'eval_f1@rus.rst.rrt': 0.3547312580069226, 'eval_precision@rus.rst.rrt': 0.46392602058054133, 'eval_recall@rus.rst.rrt': 0.3425463835485823, 'eval_loss@rus.rst.rrt': 1.4899253845214844, 'eval_runtime': 34.3256, 'eval_samples_per_second': 83.174, 'eval_steps_per_second': 2.622, 'epoch': 2.0}
{'train@rus.rst.rrt_loss': 1.368248462677002, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5829574630490597, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3343712607352398, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.41665891110972697, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3211327605586993, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3682481050491333, 'train@rus.rst.rrt_runtime': 343.3074, 'train@rus.rst.rrt_samples_per_second': 83.954, 'train@rus.rst.rrt_steps_per_second': 2.624, 'epoch': 3.0}
{'loss': 1.4439, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.4425901174545288, 'eval_accuracy@rus.rst.rrt': 0.5537653239929947, 'eval_f1@rus.rst.rrt': 0.37587855207106047, 'eval_precision@rus.rst.rrt': 0.45074266486925296, 'eval_recall@rus.rst.rrt': 0.36783261325993505, 'eval_loss@rus.rst.rrt': 1.4425904750823975, 'eval_runtime': 34.3366, 'eval_samples_per_second': 83.147, 'eval_steps_per_second': 2.621, 'epoch': 3.0}
{'train@rus.rst.rrt_loss': 1.3276604413986206, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5945458330442024, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3459730515929411, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.42958776856496333, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3286846627996216, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3276605606079102, 'train@rus.rst.rrt_runtime': 342.9096, 'train@rus.rst.rrt_samples_per_second': 84.051, 'train@rus.rst.rrt_steps_per_second': 2.628, 'epoch': 4.0}
{'loss': 1.403, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.4043195247650146, 'eval_accuracy@rus.rst.rrt': 0.565323992994746, 'eval_f1@rus.rst.rrt': 0.3878085624005932, 'eval_precision@rus.rst.rrt': 0.4626391042067808, 'eval_recall@rus.rst.rrt': 0.3791755751507314, 'eval_loss@rus.rst.rrt': 1.404319405555725, 'eval_runtime': 34.3085, 'eval_samples_per_second': 83.215, 'eval_steps_per_second': 2.623, 'epoch': 4.0}
{'train@rus.rst.rrt_loss': 1.3021070957183838, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5998195822635487, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.35262568785430165, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4267919337425194, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.33679870602562584, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.302107334136963, 'train@rus.rst.rrt_runtime': 342.9597, 'train@rus.rst.rrt_samples_per_second': 84.039, 'train@rus.rst.rrt_steps_per_second': 2.627, 'epoch': 5.0}
{'loss': 1.3725, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.3857587575912476, 'eval_accuracy@rus.rst.rrt': 0.5709281961471103, 'eval_f1@rus.rst.rrt': 0.3975067513378759, 'eval_precision@rus.rst.rrt': 0.48454952785911193, 'eval_recall@rus.rst.rrt': 0.39002492998463917, 'eval_loss@rus.rst.rrt': 1.3857587575912476, 'eval_runtime': 34.2276, 'eval_samples_per_second': 83.412, 'eval_steps_per_second': 2.629, 'epoch': 5.0}
{'train@rus.rst.rrt_loss': 1.2844270467758179, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6028034140587052, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.357344502745741, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.48288848713867893, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3406982638114791, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.284427285194397, 'train@rus.rst.rrt_runtime': 342.928, 'train@rus.rst.rrt_samples_per_second': 84.047, 'train@rus.rst.rrt_steps_per_second': 2.627, 'epoch': 6.0}
{'loss': 1.3507, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.373175859451294, 'eval_accuracy@rus.rst.rrt': 0.5709281961471103, 'eval_f1@rus.rst.rrt': 0.402954309578191, 'eval_precision@rus.rst.rrt': 0.4785511776163835, 'eval_recall@rus.rst.rrt': 0.39375105493720725, 'eval_loss@rus.rst.rrt': 1.373176097869873, 'eval_runtime': 34.2457, 'eval_samples_per_second': 83.368, 'eval_steps_per_second': 2.628, 'epoch': 6.0}
{'train@rus.rst.rrt_loss': 1.2708990573883057, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6071056831586982, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3649540619180815, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.48270223109574545, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34742343468475423, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2708992958068848, 'train@rus.rst.rrt_runtime': 343.0049, 'train@rus.rst.rrt_samples_per_second': 84.028, 'train@rus.rst.rrt_steps_per_second': 2.627, 'epoch': 7.0}
{'loss': 1.3359, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.3659448623657227, 'eval_accuracy@rus.rst.rrt': 0.5747810858143608, 'eval_f1@rus.rst.rrt': 0.41078246144757125, 'eval_precision@rus.rst.rrt': 0.47483955688080554, 'eval_recall@rus.rst.rrt': 0.40119322912390665, 'eval_loss@rus.rst.rrt': 1.365944743156433, 'eval_runtime': 34.2601, 'eval_samples_per_second': 83.333, 'eval_steps_per_second': 2.627, 'epoch': 7.0}
{'train@rus.rst.rrt_loss': 1.2602955102920532, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.610540559294983, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3720159628613976, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4564192590764847, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.35570301779943, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2602956295013428, 'train@rus.rst.rrt_runtime': 343.1537, 'train@rus.rst.rrt_samples_per_second': 83.992, 'train@rus.rst.rrt_steps_per_second': 2.626, 'epoch': 8.0}
{'loss': 1.3204, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.3559974431991577, 'eval_accuracy@rus.rst.rrt': 0.580385288966725, 'eval_f1@rus.rst.rrt': 0.4213338500777189, 'eval_precision@rus.rst.rrt': 0.48170573750636747, 'eval_recall@rus.rst.rrt': 0.41129475970771934, 'eval_loss@rus.rst.rrt': 1.3559973239898682, 'eval_runtime': 34.2721, 'eval_samples_per_second': 83.304, 'eval_steps_per_second': 2.626, 'epoch': 8.0}
{'train@rus.rst.rrt_loss': 1.2506181001663208, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6133162167788495, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3752944923871749, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.45117114697839167, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.35816512179928217, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2506184577941895, 'train@rus.rst.rrt_runtime': 343.2785, 'train@rus.rst.rrt_samples_per_second': 83.961, 'train@rus.rst.rrt_steps_per_second': 2.625, 'epoch': 9.0}
{'loss': 1.3106, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.35128653049469, 'eval_accuracy@rus.rst.rrt': 0.5775831873905429, 'eval_f1@rus.rst.rrt': 0.41798506542802516, 'eval_precision@rus.rst.rrt': 0.47972005795353295, 'eval_recall@rus.rst.rrt': 0.4069107385697349, 'eval_loss@rus.rst.rrt': 1.3512864112854004, 'eval_runtime': 34.2506, 'eval_samples_per_second': 83.356, 'eval_steps_per_second': 2.628, 'epoch': 9.0}
{'train@rus.rst.rrt_loss': 1.2446807622909546, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.615051002706266, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.37702451072094206, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.46795848525257494, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3573114036371355, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2446805238723755, 'train@rus.rst.rrt_runtime': 342.8704, 'train@rus.rst.rrt_samples_per_second': 84.061, 'train@rus.rst.rrt_steps_per_second': 2.628, 'epoch': 10.0}
{'loss': 1.3033, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.3472914695739746, 'eval_accuracy@rus.rst.rrt': 0.5824868651488616, 'eval_f1@rus.rst.rrt': 0.42254049311973657, 'eval_precision@rus.rst.rrt': 0.49112186803768215, 'eval_recall@rus.rst.rrt': 0.40793130675590006, 'eval_loss@rus.rst.rrt': 1.3472912311553955, 'eval_runtime': 34.3106, 'eval_samples_per_second': 83.21, 'eval_steps_per_second': 2.623, 'epoch': 10.0}
{'train@rus.rst.rrt_loss': 1.24173104763031, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6160224828256193, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3804902943751676, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.45710599453365397, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3624502503250599, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2417309284210205, 'train@rus.rst.rrt_runtime': 343.0466, 'train@rus.rst.rrt_samples_per_second': 84.018, 'train@rus.rst.rrt_steps_per_second': 2.626, 'epoch': 11.0}
{'loss': 1.2971, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.348604440689087, 'eval_accuracy@rus.rst.rrt': 0.5824868651488616, 'eval_f1@rus.rst.rrt': 0.4306307698133763, 'eval_precision@rus.rst.rrt': 0.49398405627514197, 'eval_recall@rus.rst.rrt': 0.4183525680168274, 'eval_loss@rus.rst.rrt': 1.3486045598983765, 'eval_runtime': 34.2674, 'eval_samples_per_second': 83.315, 'eval_steps_per_second': 2.626, 'epoch': 11.0}
{'train@rus.rst.rrt_loss': 1.2403392791748047, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6163347442925543, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3800765306167554, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4600960943505807, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.36237169163806277, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2403391599655151, 'train@rus.rst.rrt_runtime': 343.3577, 'train@rus.rst.rrt_samples_per_second': 83.942, 'train@rus.rst.rrt_steps_per_second': 2.624, 'epoch': 12.0}
{'loss': 1.2934, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.3464974164962769, 'eval_accuracy@rus.rst.rrt': 0.5817863397548161, 'eval_f1@rus.rst.rrt': 0.4288447227837909, 'eval_precision@rus.rst.rrt': 0.4926000856598942, 'eval_recall@rus.rst.rrt': 0.41726745971573437, 'eval_loss@rus.rst.rrt': 1.3464971780776978, 'eval_runtime': 34.313, 'eval_samples_per_second': 83.205, 'eval_steps_per_second': 2.623, 'epoch': 12.0}
{'train_runtime': 13282.7456, 'train_samples_per_second': 26.039, 'train_steps_per_second': 0.814, 'train_loss': 1.402820481311997, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.7916
  train_runtime            = 1:46:35.05
  train_samples_per_second =     26.077
  train_steps_per_second   =      0.816
-------------------------------------------------------------------
Lang1:  eng.rst.rstdt    Lang2:  rus.rst.rrt
Saving run to:  runs/full_shot/FullShot=v4_eng.rst.rstdt_rus.rst.rrt_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 16002 examples
read 1621 examples
read 2155 examples
read 28822 examples
read 2855 examples
read 2843 examples
Total prediction labels:  30
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (ru): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=30, bias=True)
    )
  )
)
{'train@eng.rst.rstdt_loss': 1.7269352674484253, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.5153730783652043, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.08898381319331856, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.15155567051644822, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.11448353831825073, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.7269353866577148, 'train@eng.rst.rstdt_runtime': 186.8999, 'train@eng.rst.rstdt_samples_per_second': 85.618, 'train@eng.rst.rstdt_steps_per_second': 2.681, 'epoch': 1.0}
{'loss': 2.1467, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.708458423614502, 'eval_accuracy@eng.rst.rstdt': 0.5206662553979026, 'eval_f1@eng.rst.rstdt': 0.08969353107680983, 'eval_precision@eng.rst.rstdt': 0.09229458509156258, 'eval_recall@eng.rst.rstdt': 0.11290775741561705, 'eval_loss@eng.rst.rstdt': 1.7084585428237915, 'eval_runtime': 19.2667, 'eval_samples_per_second': 84.135, 'eval_steps_per_second': 2.647, 'epoch': 1.0}
{'train@eng.rst.rstdt_loss': 1.4192103147506714, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6022372203474565, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.20337930841348467, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.3051076364806665, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.21391991078498918, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.419210433959961, 'train@eng.rst.rstdt_runtime': 186.9145, 'train@eng.rst.rstdt_samples_per_second': 85.611, 'train@eng.rst.rstdt_steps_per_second': 2.68, 'epoch': 2.0}
{'loss': 1.597, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.4214997291564941, 'eval_accuracy@eng.rst.rstdt': 0.6125848241826033, 'eval_f1@eng.rst.rstdt': 0.2031475333043123, 'eval_precision@eng.rst.rstdt': 0.24589544370580363, 'eval_recall@eng.rst.rstdt': 0.21337317449186707, 'eval_loss@eng.rst.rstdt': 1.4214998483657837, 'eval_runtime': 19.2562, 'eval_samples_per_second': 84.181, 'eval_steps_per_second': 2.648, 'epoch': 2.0}
{'train@eng.rst.rstdt_loss': 1.3135125637054443, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.63167104111986, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.2810111749270407, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.43327604400981834, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.27036786371901234, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.3135126829147339, 'train@eng.rst.rstdt_runtime': 187.4256, 'train@eng.rst.rstdt_samples_per_second': 85.378, 'train@eng.rst.rstdt_steps_per_second': 2.673, 'epoch': 3.0}
{'loss': 1.4236, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.3390125036239624, 'eval_accuracy@eng.rst.rstdt': 0.6384947563232573, 'eval_f1@eng.rst.rstdt': 0.27387925180061706, 'eval_precision@eng.rst.rstdt': 0.367464124018071, 'eval_recall@eng.rst.rstdt': 0.2682166719783954, 'eval_loss@eng.rst.rstdt': 1.3390125036239624, 'eval_runtime': 19.2875, 'eval_samples_per_second': 84.044, 'eval_steps_per_second': 2.644, 'epoch': 3.0}
{'train@eng.rst.rstdt_loss': 1.248795747756958, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6454193225846769, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3343599223183144, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.4608536472738075, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.31923784003583805, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.2487956285476685, 'train@eng.rst.rstdt_runtime': 186.8392, 'train@eng.rst.rstdt_samples_per_second': 85.646, 'train@eng.rst.rstdt_steps_per_second': 2.681, 'epoch': 4.0}
{'loss': 1.3327, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.3010919094085693, 'eval_accuracy@eng.rst.rstdt': 0.6391116594694632, 'eval_f1@eng.rst.rstdt': 0.3188179853255293, 'eval_precision@eng.rst.rstdt': 0.38369412621387206, 'eval_recall@eng.rst.rstdt': 0.31890628550145844, 'eval_loss@eng.rst.rstdt': 1.3010919094085693, 'eval_runtime': 19.2624, 'eval_samples_per_second': 84.153, 'eval_steps_per_second': 2.648, 'epoch': 4.0}
{'train@eng.rst.rstdt_loss': 1.1981537342071533, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6572303462067242, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3569252654475957, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5321201123811062, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3322135846503914, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1981537342071533, 'train@eng.rst.rstdt_runtime': 187.203, 'train@eng.rst.rstdt_samples_per_second': 85.479, 'train@eng.rst.rstdt_steps_per_second': 2.676, 'epoch': 5.0}
{'loss': 1.278, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.2566754817962646, 'eval_accuracy@eng.rst.rstdt': 0.6452806909315237, 'eval_f1@eng.rst.rstdt': 0.33287863447628313, 'eval_precision@eng.rst.rstdt': 0.43615695127860155, 'eval_recall@eng.rst.rstdt': 0.32501266727701256, 'eval_loss@eng.rst.rstdt': 1.256675362586975, 'eval_runtime': 19.2846, 'eval_samples_per_second': 84.057, 'eval_steps_per_second': 2.645, 'epoch': 5.0}
{'train@eng.rst.rstdt_loss': 1.165116786956787, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6626671666041745, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.37804581300826023, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5225010097997926, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3472472694662083, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1651169061660767, 'train@eng.rst.rstdt_runtime': 186.7222, 'train@eng.rst.rstdt_samples_per_second': 85.699, 'train@eng.rst.rstdt_steps_per_second': 2.683, 'epoch': 6.0}
{'loss': 1.2327, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.2320923805236816, 'eval_accuracy@eng.rst.rstdt': 0.64898210980876, 'eval_f1@eng.rst.rstdt': 0.35815331894814906, 'eval_precision@eng.rst.rstdt': 0.4889690316849735, 'eval_recall@eng.rst.rstdt': 0.34303256901436907, 'eval_loss@eng.rst.rstdt': 1.2320923805236816, 'eval_runtime': 19.2851, 'eval_samples_per_second': 84.055, 'eval_steps_per_second': 2.645, 'epoch': 6.0}
{'train@eng.rst.rstdt_loss': 1.1448373794555664, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6661667291588551, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.386922589024624, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5245630252908964, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3534369757807438, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1448373794555664, 'train@eng.rst.rstdt_runtime': 187.0445, 'train@eng.rst.rstdt_samples_per_second': 85.552, 'train@eng.rst.rstdt_steps_per_second': 2.679, 'epoch': 7.0}
{'loss': 1.2074, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.2143630981445312, 'eval_accuracy@eng.rst.rstdt': 0.6483652066625539, 'eval_f1@eng.rst.rstdt': 0.3591663724158296, 'eval_precision@eng.rst.rstdt': 0.47937790223632604, 'eval_recall@eng.rst.rstdt': 0.3409320534575573, 'eval_loss@eng.rst.rstdt': 1.2143630981445312, 'eval_runtime': 19.2759, 'eval_samples_per_second': 84.095, 'eval_steps_per_second': 2.646, 'epoch': 7.0}
{'train@eng.rst.rstdt_loss': 1.1280490159988403, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6689163854518185, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3962582462707138, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5747272061659348, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3644057837373825, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1280488967895508, 'train@eng.rst.rstdt_runtime': 186.7817, 'train@eng.rst.rstdt_samples_per_second': 85.672, 'train@eng.rst.rstdt_steps_per_second': 2.682, 'epoch': 8.0}
{'loss': 1.1902, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.2079683542251587, 'eval_accuracy@eng.rst.rstdt': 0.6471314003701419, 'eval_f1@eng.rst.rstdt': 0.35942995306415115, 'eval_precision@eng.rst.rstdt': 0.4767451743001645, 'eval_recall@eng.rst.rstdt': 0.3458530270302012, 'eval_loss@eng.rst.rstdt': 1.2079682350158691, 'eval_runtime': 19.2565, 'eval_samples_per_second': 84.179, 'eval_steps_per_second': 2.648, 'epoch': 8.0}
{'train@eng.rst.rstdt_loss': 1.1171009540557861, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.671541057367829, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4035002219392138, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.571029328094615, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3688618926295849, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1171009540557861, 'train@eng.rst.rstdt_runtime': 187.1332, 'train@eng.rst.rstdt_samples_per_second': 85.511, 'train@eng.rst.rstdt_steps_per_second': 2.677, 'epoch': 9.0}
{'loss': 1.1735, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.198913812637329, 'eval_accuracy@eng.rst.rstdt': 0.6483652066625539, 'eval_f1@eng.rst.rstdt': 0.3569816700640372, 'eval_precision@eng.rst.rstdt': 0.47305409631963957, 'eval_recall@eng.rst.rstdt': 0.34389751975238053, 'eval_loss@eng.rst.rstdt': 1.198913812637329, 'eval_runtime': 19.2626, 'eval_samples_per_second': 84.153, 'eval_steps_per_second': 2.648, 'epoch': 9.0}
{'train@eng.rst.rstdt_loss': 1.112205982208252, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6690413698287714, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4094717242894453, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5583368433149187, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.37741537728901015, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.112205982208252, 'train@eng.rst.rstdt_runtime': 186.8072, 'train@eng.rst.rstdt_samples_per_second': 85.661, 'train@eng.rst.rstdt_steps_per_second': 2.682, 'epoch': 10.0}
{'loss': 1.1637, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.2037960290908813, 'eval_accuracy@eng.rst.rstdt': 0.6434299814929056, 'eval_f1@eng.rst.rstdt': 0.3696898503327953, 'eval_precision@eng.rst.rstdt': 0.4954667215453951, 'eval_recall@eng.rst.rstdt': 0.3536981508757991, 'eval_loss@eng.rst.rstdt': 1.203796148300171, 'eval_runtime': 19.2517, 'eval_samples_per_second': 84.2, 'eval_steps_per_second': 2.649, 'epoch': 10.0}
{'train@eng.rst.rstdt_loss': 1.105078935623169, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.670978627671541, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.40786345539341323, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.567565237206291, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.37369689664120864, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.105078935623169, 'train@eng.rst.rstdt_runtime': 186.7949, 'train@eng.rst.rstdt_samples_per_second': 85.666, 'train@eng.rst.rstdt_steps_per_second': 2.682, 'epoch': 11.0}
{'loss': 1.1577, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.1950854063034058, 'eval_accuracy@eng.rst.rstdt': 0.64898210980876, 'eval_f1@eng.rst.rstdt': 0.37230571050144534, 'eval_precision@eng.rst.rstdt': 0.532091926692396, 'eval_recall@eng.rst.rstdt': 0.3537255557467129, 'eval_loss@eng.rst.rstdt': 1.1950854063034058, 'eval_runtime': 19.2544, 'eval_samples_per_second': 84.188, 'eval_steps_per_second': 2.649, 'epoch': 11.0}
{'train@eng.rst.rstdt_loss': 1.103601098060608, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6712910886139233, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.40735956827053726, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5688571048291492, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.37315567230602703, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1036012172698975, 'train@eng.rst.rstdt_runtime': 186.9559, 'train@eng.rst.rstdt_samples_per_second': 85.592, 'train@eng.rst.rstdt_steps_per_second': 2.68, 'epoch': 12.0}
{'loss': 1.1543, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.1931471824645996, 'eval_accuracy@eng.rst.rstdt': 0.6502159161011721, 'eval_f1@eng.rst.rstdt': 0.37236331191495137, 'eval_precision@eng.rst.rstdt': 0.5331463239943441, 'eval_recall@eng.rst.rstdt': 0.3533607742307612, 'eval_loss@eng.rst.rstdt': 1.19314706325531, 'eval_runtime': 19.2069, 'eval_samples_per_second': 84.397, 'eval_steps_per_second': 2.655, 'epoch': 12.0}
{'train_runtime': 7236.4863, 'train_samples_per_second': 26.536, 'train_steps_per_second': 0.831, 'train_loss': 1.3381284103342794, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.3381
  train_runtime            = 2:00:36.48
  train_samples_per_second =     26.536
  train_steps_per_second   =      0.831
{'train@rus.rst.rrt_loss': 1.5505869388580322, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5348345014225244, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.22186166978140767, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.26246462721608405, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.23066355686104242, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.5505869388580322, 'train@rus.rst.rrt_runtime': 342.799, 'train@rus.rst.rrt_samples_per_second': 84.078, 'train@rus.rst.rrt_steps_per_second': 2.628, 'epoch': 1.0}
{'loss': 1.9257, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.611080527305603, 'eval_accuracy@rus.rst.rrt': 0.5106830122591944, 'eval_f1@rus.rst.rrt': 0.2433701548283632, 'eval_precision@rus.rst.rrt': 0.29506213779250046, 'eval_recall@rus.rst.rrt': 0.2541707450553389, 'eval_loss@rus.rst.rrt': 1.6110806465148926, 'eval_runtime': 34.2639, 'eval_samples_per_second': 83.324, 'eval_steps_per_second': 2.627, 'epoch': 1.0}
{'train@rus.rst.rrt_loss': 1.4363319873809814, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5632502949136077, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.27715764302461776, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.3918177831662092, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.268762539916772, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.4363319873809814, 'train@rus.rst.rrt_runtime': 342.8532, 'train@rus.rst.rrt_samples_per_second': 84.065, 'train@rus.rst.rrt_steps_per_second': 2.628, 'epoch': 2.0}
{'loss': 1.5455, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.5135778188705444, 'eval_accuracy@rus.rst.rrt': 0.5373029772329246, 'eval_f1@rus.rst.rrt': 0.3164192968836516, 'eval_precision@rus.rst.rrt': 0.4649421251557738, 'eval_recall@rus.rst.rrt': 0.3040278807345235, 'eval_loss@rus.rst.rrt': 1.5135778188705444, 'eval_runtime': 34.2639, 'eval_samples_per_second': 83.324, 'eval_steps_per_second': 2.627, 'epoch': 2.0}
{'train@rus.rst.rrt_loss': 1.382760763168335, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5782041496079384, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.31810544435437244, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.41564780933228945, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3062385034443229, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3827608823776245, 'train@rus.rst.rrt_runtime': 343.1078, 'train@rus.rst.rrt_samples_per_second': 84.003, 'train@rus.rst.rrt_steps_per_second': 2.626, 'epoch': 3.0}
{'loss': 1.4632, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.4572134017944336, 'eval_accuracy@rus.rst.rrt': 0.5506129597197899, 'eval_f1@rus.rst.rrt': 0.3645658917904066, 'eval_precision@rus.rst.rrt': 0.43250690953844445, 'eval_recall@rus.rst.rrt': 0.3541013762342667, 'eval_loss@rus.rst.rrt': 1.4572134017944336, 'eval_runtime': 34.3278, 'eval_samples_per_second': 83.169, 'eval_steps_per_second': 2.622, 'epoch': 3.0}
{'train@rus.rst.rrt_loss': 1.3429330587387085, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5877454722087294, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3285798754284229, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.44246471795738723, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3117449929786108, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3429327011108398, 'train@rus.rst.rrt_runtime': 342.9301, 'train@rus.rst.rrt_samples_per_second': 84.046, 'train@rus.rst.rrt_steps_per_second': 2.627, 'epoch': 4.0}
{'loss': 1.4188, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.4224590063095093, 'eval_accuracy@rus.rst.rrt': 0.5576182136602452, 'eval_f1@rus.rst.rrt': 0.3680000622654865, 'eval_precision@rus.rst.rrt': 0.4439583553976803, 'eval_recall@rus.rst.rrt': 0.35439374655470635, 'eval_loss@rus.rst.rrt': 1.4224590063095093, 'eval_runtime': 34.2856, 'eval_samples_per_second': 83.271, 'eval_steps_per_second': 2.625, 'epoch': 4.0}
{'train@rus.rst.rrt_loss': 1.3151609897613525, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.595135660259524, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.33927529454991767, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.44467480894076666, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3218165539289013, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3151609897613525, 'train@rus.rst.rrt_runtime': 342.9817, 'train@rus.rst.rrt_samples_per_second': 84.034, 'train@rus.rst.rrt_steps_per_second': 2.627, 'epoch': 5.0}
{'loss': 1.3876, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.4036890268325806, 'eval_accuracy@rus.rst.rrt': 0.5625218914185639, 'eval_f1@rus.rst.rrt': 0.37325038805759475, 'eval_precision@rus.rst.rrt': 0.4289091081196454, 'eval_recall@rus.rst.rrt': 0.3666034585246208, 'eval_loss@rus.rst.rrt': 1.4036891460418701, 'eval_runtime': 34.2748, 'eval_samples_per_second': 83.297, 'eval_steps_per_second': 2.626, 'epoch': 5.0}
{'train@rus.rst.rrt_loss': 1.297359824180603, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.600478800915967, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.34708569568455194, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.45097302986396826, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.32838338622928365, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.297359824180603, 'train@rus.rst.rrt_runtime': 342.9669, 'train@rus.rst.rrt_samples_per_second': 84.037, 'train@rus.rst.rrt_steps_per_second': 2.627, 'epoch': 6.0}
{'loss': 1.3644, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.3937941789627075, 'eval_accuracy@rus.rst.rrt': 0.5674255691768827, 'eval_f1@rus.rst.rrt': 0.38418903247733216, 'eval_precision@rus.rst.rrt': 0.4585453151256063, 'eval_recall@rus.rst.rrt': 0.37506228679431564, 'eval_loss@rus.rst.rrt': 1.3937941789627075, 'eval_runtime': 34.3101, 'eval_samples_per_second': 83.212, 'eval_steps_per_second': 2.623, 'epoch': 6.0}
{'train@rus.rst.rrt_loss': 1.28125, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6047116785788633, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3607032872361422, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4351444256533107, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34162167075999467, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2812501192092896, 'train@rus.rst.rrt_runtime': 342.8342, 'train@rus.rst.rrt_samples_per_second': 84.07, 'train@rus.rst.rrt_steps_per_second': 2.628, 'epoch': 7.0}
{'loss': 1.3459, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.3785805702209473, 'eval_accuracy@rus.rst.rrt': 0.5730297723292469, 'eval_f1@rus.rst.rrt': 0.39859670302689776, 'eval_precision@rus.rst.rrt': 0.4585781642349837, 'eval_recall@rus.rst.rrt': 0.38688902568352784, 'eval_loss@rus.rst.rrt': 1.3785805702209473, 'eval_runtime': 34.3128, 'eval_samples_per_second': 83.205, 'eval_steps_per_second': 2.623, 'epoch': 7.0}
{'train@rus.rst.rrt_loss': 1.269950270652771, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6076261189369232, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3660137893454538, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.43514876651255857, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3472496102975918, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2699503898620605, 'train@rus.rst.rrt_runtime': 342.8344, 'train@rus.rst.rrt_samples_per_second': 84.07, 'train@rus.rst.rrt_steps_per_second': 2.628, 'epoch': 8.0}
{'loss': 1.3297, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.3690755367279053, 'eval_accuracy@rus.rst.rrt': 0.5730297723292469, 'eval_f1@rus.rst.rrt': 0.40311877266061985, 'eval_precision@rus.rst.rrt': 0.4486903428445244, 'eval_recall@rus.rst.rrt': 0.3923661106605822, 'eval_loss@rus.rst.rrt': 1.3690754175186157, 'eval_runtime': 34.2933, 'eval_samples_per_second': 83.252, 'eval_steps_per_second': 2.624, 'epoch': 8.0}
{'train@rus.rst.rrt_loss': 1.2596687078475952, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6104711678578864, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36975878613104185, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4857477975694344, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3506594166187992, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2596687078475952, 'train@rus.rst.rrt_runtime': 342.6661, 'train@rus.rst.rrt_samples_per_second': 84.111, 'train@rus.rst.rrt_steps_per_second': 2.629, 'epoch': 9.0}
{'loss': 1.3241, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.3642449378967285, 'eval_accuracy@rus.rst.rrt': 0.5758318739054291, 'eval_f1@rus.rst.rrt': 0.4060261705357365, 'eval_precision@rus.rst.rrt': 0.46381783972663726, 'eval_recall@rus.rst.rrt': 0.39408491044840277, 'eval_loss@rus.rst.rrt': 1.3642449378967285, 'eval_runtime': 34.2829, 'eval_samples_per_second': 83.278, 'eval_steps_per_second': 2.625, 'epoch': 9.0}
{'train@rus.rst.rrt_loss': 1.2539132833480835, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6120324751925612, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3696271969438362, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.48721753790304995, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3486167046377108, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2539132833480835, 'train@rus.rst.rrt_runtime': 342.5956, 'train@rus.rst.rrt_samples_per_second': 84.128, 'train@rus.rst.rrt_steps_per_second': 2.63, 'epoch': 10.0}
{'loss': 1.3133, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.360886573791504, 'eval_accuracy@rus.rst.rrt': 0.5765323992994746, 'eval_f1@rus.rst.rrt': 0.4109022573105867, 'eval_precision@rus.rst.rrt': 0.47281994678845113, 'eval_recall@rus.rst.rrt': 0.39609355042083744, 'eval_loss@rus.rst.rrt': 1.360886573791504, 'eval_runtime': 34.2737, 'eval_samples_per_second': 83.3, 'eval_steps_per_second': 2.626, 'epoch': 10.0}
{'train@rus.rst.rrt_loss': 1.2502496242523193, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6131427381861079, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.37300005916239504, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4612823473491797, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3540750657664737, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2502497434616089, 'train@rus.rst.rrt_runtime': 343.3252, 'train@rus.rst.rrt_samples_per_second': 83.95, 'train@rus.rst.rrt_steps_per_second': 2.624, 'epoch': 11.0}
{'loss': 1.3063, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.3599408864974976, 'eval_accuracy@rus.rst.rrt': 0.5775831873905429, 'eval_f1@rus.rst.rrt': 0.4140157964177676, 'eval_precision@rus.rst.rrt': 0.47019712881894, 'eval_recall@rus.rst.rrt': 0.401447961967174, 'eval_loss@rus.rst.rrt': 1.3599408864974976, 'eval_runtime': 34.3154, 'eval_samples_per_second': 83.199, 'eval_steps_per_second': 2.623, 'epoch': 11.0}
{'train@rus.rst.rrt_loss': 1.2488993406295776, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6131427381861079, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3730365047224426, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.46047952545633924, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.35447258258185116, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2488993406295776, 'train@rus.rst.rrt_runtime': 343.4128, 'train@rus.rst.rrt_samples_per_second': 83.928, 'train@rus.rst.rrt_steps_per_second': 2.624, 'epoch': 12.0}
{'loss': 1.3022, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.3587584495544434, 'eval_accuracy@rus.rst.rrt': 0.5779334500875657, 'eval_f1@rus.rst.rrt': 0.41529249591311873, 'eval_precision@rus.rst.rrt': 0.47272942917876987, 'eval_recall@rus.rst.rrt': 0.403188305733236, 'eval_loss@rus.rst.rrt': 1.3587583303451538, 'eval_runtime': 34.2678, 'eval_samples_per_second': 83.314, 'eval_steps_per_second': 2.626, 'epoch': 12.0}
{'train_runtime': 13274.4286, 'train_samples_per_second': 26.055, 'train_steps_per_second': 0.814, 'train_loss': 1.4188838934748427, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.3381
  train_runtime            = 2:00:36.48
  train_samples_per_second =     26.536
  train_steps_per_second   =      0.831
-------------------------------------------------------------------
Lang1:  eng.sdrt.stac    Lang2:  rus.rst.rrt
Saving run to:  runs/full_shot/FullShot=v4_eng.sdrt.stac_rus.rst.rrt_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 9580 examples
read 1145 examples
read 1510 examples
read 28822 examples
read 2855 examples
read 2843 examples
Total prediction labels:  38
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (ru): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=38, bias=True)
    )
  )
)
{'train@eng.sdrt.stac_loss': 2.086280345916748, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.3515657620041754, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.06952475376773494, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.099487860891174, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.11109273751448343, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 2.086280584335327, 'train@eng.sdrt.stac_runtime': 111.9985, 'train@eng.sdrt.stac_samples_per_second': 85.537, 'train@eng.sdrt.stac_steps_per_second': 2.679, 'epoch': 1.0}
{'loss': 2.5337, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.0396478176116943, 'eval_accuracy@eng.sdrt.stac': 0.36331877729257644, 'eval_f1@eng.sdrt.stac': 0.07322063710297563, 'eval_precision@eng.sdrt.stac': 0.09384293945673935, 'eval_recall@eng.sdrt.stac': 0.11459486627416823, 'eval_loss@eng.sdrt.stac': 2.0396480560302734, 'eval_runtime': 13.723, 'eval_samples_per_second': 83.437, 'eval_steps_per_second': 2.623, 'epoch': 1.0}
{'train@eng.sdrt.stac_loss': 1.8731828927993774, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4332985386221294, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.1360274768350156, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.14880094215484874, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.1798403420756059, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.8731828927993774, 'train@eng.sdrt.stac_runtime': 111.9828, 'train@eng.sdrt.stac_samples_per_second': 85.549, 'train@eng.sdrt.stac_steps_per_second': 2.679, 'epoch': 2.0}
{'loss': 2.0157, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.8211278915405273, 'eval_accuracy@eng.sdrt.stac': 0.4366812227074236, 'eval_f1@eng.sdrt.stac': 0.13101005305401536, 'eval_precision@eng.sdrt.stac': 0.12471550037226611, 'eval_recall@eng.sdrt.stac': 0.17819464568499455, 'eval_loss@eng.sdrt.stac': 1.8211276531219482, 'eval_runtime': 13.7323, 'eval_samples_per_second': 83.38, 'eval_steps_per_second': 2.622, 'epoch': 2.0}
{'train@eng.sdrt.stac_loss': 1.7727861404418945, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.45, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.15455676333533086, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.13971486825869467, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.19320798497816888, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.7727861404418945, 'train@eng.sdrt.stac_runtime': 112.049, 'train@eng.sdrt.stac_samples_per_second': 85.498, 'train@eng.sdrt.stac_steps_per_second': 2.677, 'epoch': 3.0}
{'loss': 1.861, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.7243601083755493, 'eval_accuracy@eng.sdrt.stac': 0.4480349344978166, 'eval_f1@eng.sdrt.stac': 0.14507308714795408, 'eval_precision@eng.sdrt.stac': 0.1298535308123088, 'eval_recall@eng.sdrt.stac': 0.1871237078697266, 'eval_loss@eng.sdrt.stac': 1.7243601083755493, 'eval_runtime': 13.7063, 'eval_samples_per_second': 83.538, 'eval_steps_per_second': 2.627, 'epoch': 3.0}
{'train@eng.sdrt.stac_loss': 1.703682780265808, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4708768267223382, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.19120325988914755, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.24655737210699424, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.21900680275793039, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.703682541847229, 'train@eng.sdrt.stac_runtime': 112.0338, 'train@eng.sdrt.stac_samples_per_second': 85.51, 'train@eng.sdrt.stac_steps_per_second': 2.678, 'epoch': 4.0}
{'loss': 1.7811, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.6535366773605347, 'eval_accuracy@eng.sdrt.stac': 0.47510917030567684, 'eval_f1@eng.sdrt.stac': 0.1801354610134188, 'eval_precision@eng.sdrt.stac': 0.17982551357611748, 'eval_recall@eng.sdrt.stac': 0.21121443630417588, 'eval_loss@eng.sdrt.stac': 1.6535366773605347, 'eval_runtime': 13.6895, 'eval_samples_per_second': 83.641, 'eval_steps_per_second': 2.63, 'epoch': 4.0}
{'train@eng.sdrt.stac_loss': 1.6561065912246704, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4878914405010438, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.21259096810224082, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.21766857798628725, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2374874847958325, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.6561065912246704, 'train@eng.sdrt.stac_runtime': 112.0785, 'train@eng.sdrt.stac_samples_per_second': 85.476, 'train@eng.sdrt.stac_steps_per_second': 2.677, 'epoch': 5.0}
{'loss': 1.7279, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.610446572303772, 'eval_accuracy@eng.sdrt.stac': 0.49519650655021835, 'eval_f1@eng.sdrt.stac': 0.20348444779991237, 'eval_precision@eng.sdrt.stac': 0.2143093593898855, 'eval_recall@eng.sdrt.stac': 0.22805184655961627, 'eval_loss@eng.sdrt.stac': 1.610446572303772, 'eval_runtime': 13.81, 'eval_samples_per_second': 82.911, 'eval_steps_per_second': 2.607, 'epoch': 5.0}
{'train@eng.sdrt.stac_loss': 1.6149178743362427, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.49123173277661797, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2144600367127791, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.23135561518118522, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.23686351328409183, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.6149179935455322, 'train@eng.sdrt.stac_runtime': 112.0413, 'train@eng.sdrt.stac_samples_per_second': 85.504, 'train@eng.sdrt.stac_steps_per_second': 2.678, 'epoch': 6.0}
{'loss': 1.6791, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.5839277505874634, 'eval_accuracy@eng.sdrt.stac': 0.48995633187772925, 'eval_f1@eng.sdrt.stac': 0.20564513702389392, 'eval_precision@eng.sdrt.stac': 0.21618572081603846, 'eval_recall@eng.sdrt.stac': 0.22830423408324113, 'eval_loss@eng.sdrt.stac': 1.5839277505874634, 'eval_runtime': 13.7214, 'eval_samples_per_second': 83.446, 'eval_steps_per_second': 2.624, 'epoch': 6.0}
{'train@eng.sdrt.stac_loss': 1.592308521270752, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.501670146137787, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2323002492559525, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.2916583054896279, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.25367944311132234, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.592308759689331, 'train@eng.sdrt.stac_runtime': 112.0082, 'train@eng.sdrt.stac_samples_per_second': 85.529, 'train@eng.sdrt.stac_steps_per_second': 2.678, 'epoch': 7.0}
{'loss': 1.6472, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.5653618574142456, 'eval_accuracy@eng.sdrt.stac': 0.49170305676855897, 'eval_f1@eng.sdrt.stac': 0.21966828412988526, 'eval_precision@eng.sdrt.stac': 0.274872708828371, 'eval_recall@eng.sdrt.stac': 0.23548443362667995, 'eval_loss@eng.sdrt.stac': 1.565361738204956, 'eval_runtime': 13.7526, 'eval_samples_per_second': 83.257, 'eval_steps_per_second': 2.618, 'epoch': 7.0}
{'train@eng.sdrt.stac_loss': 1.5592045783996582, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5109603340292276, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2514453796233145, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.2767292515132802, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2703542394021327, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5592046976089478, 'train@eng.sdrt.stac_runtime': 111.9791, 'train@eng.sdrt.stac_samples_per_second': 85.552, 'train@eng.sdrt.stac_steps_per_second': 2.679, 'epoch': 8.0}
{'loss': 1.6221, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.5301274061203003, 'eval_accuracy@eng.sdrt.stac': 0.5117903930131005, 'eval_f1@eng.sdrt.stac': 0.24475629718521516, 'eval_precision@eng.sdrt.stac': 0.274005335026812, 'eval_recall@eng.sdrt.stac': 0.26158127159586647, 'eval_loss@eng.sdrt.stac': 1.5301274061203003, 'eval_runtime': 13.7213, 'eval_samples_per_second': 83.447, 'eval_steps_per_second': 2.624, 'epoch': 8.0}
{'train@eng.sdrt.stac_loss': 1.5416786670684814, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5179540709812108, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2723875471466618, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.37966521111191953, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.29058295829819014, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5416784286499023, 'train@eng.sdrt.stac_runtime': 111.9148, 'train@eng.sdrt.stac_samples_per_second': 85.601, 'train@eng.sdrt.stac_steps_per_second': 2.681, 'epoch': 9.0}
{'loss': 1.5951, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.5182870626449585, 'eval_accuracy@eng.sdrt.stac': 0.509170305676856, 'eval_f1@eng.sdrt.stac': 0.24369458103031622, 'eval_precision@eng.sdrt.stac': 0.2662014364484898, 'eval_recall@eng.sdrt.stac': 0.2631928155854798, 'eval_loss@eng.sdrt.stac': 1.5182870626449585, 'eval_runtime': 13.6971, 'eval_samples_per_second': 83.594, 'eval_steps_per_second': 2.628, 'epoch': 9.0}
{'train@eng.sdrt.stac_loss': 1.525254249572754, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.522651356993737, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2786663453739951, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.3680967717416926, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.30042472620677785, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.525254249572754, 'train@eng.sdrt.stac_runtime': 111.9293, 'train@eng.sdrt.stac_samples_per_second': 85.59, 'train@eng.sdrt.stac_steps_per_second': 2.68, 'epoch': 10.0}
{'loss': 1.5839, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.5051681995391846, 'eval_accuracy@eng.sdrt.stac': 0.5152838427947598, 'eval_f1@eng.sdrt.stac': 0.2479486722363195, 'eval_precision@eng.sdrt.stac': 0.25152995014168944, 'eval_recall@eng.sdrt.stac': 0.2689211165498016, 'eval_loss@eng.sdrt.stac': 1.5051681995391846, 'eval_runtime': 13.7285, 'eval_samples_per_second': 83.403, 'eval_steps_per_second': 2.622, 'epoch': 10.0}
{'train@eng.sdrt.stac_loss': 1.5170875787734985, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5231732776617954, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2819002974857462, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.3558303387270212, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3039236738133208, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.517087459564209, 'train@eng.sdrt.stac_runtime': 111.9294, 'train@eng.sdrt.stac_samples_per_second': 85.59, 'train@eng.sdrt.stac_steps_per_second': 2.68, 'epoch': 11.0}
{'loss': 1.5692, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.5007119178771973, 'eval_accuracy@eng.sdrt.stac': 0.5117903930131005, 'eval_f1@eng.sdrt.stac': 0.25114383256155787, 'eval_precision@eng.sdrt.stac': 0.25315710394154173, 'eval_recall@eng.sdrt.stac': 0.27278363144204487, 'eval_loss@eng.sdrt.stac': 1.5007120370864868, 'eval_runtime': 13.7214, 'eval_samples_per_second': 83.446, 'eval_steps_per_second': 2.624, 'epoch': 11.0}
{'train@eng.sdrt.stac_loss': 1.5155730247497559, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5254697286012526, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2834055119198162, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.36200043746285754, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.30521520309794303, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5155729055404663, 'train@eng.sdrt.stac_runtime': 112.1891, 'train@eng.sdrt.stac_samples_per_second': 85.392, 'train@eng.sdrt.stac_steps_per_second': 2.674, 'epoch': 12.0}
{'loss': 1.5655, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.4987075328826904, 'eval_accuracy@eng.sdrt.stac': 0.5179039301310043, 'eval_f1@eng.sdrt.stac': 0.2533870592341274, 'eval_precision@eng.sdrt.stac': 0.25548818530358014, 'eval_recall@eng.sdrt.stac': 0.2750563243104663, 'eval_loss@eng.sdrt.stac': 1.4987075328826904, 'eval_runtime': 13.747, 'eval_samples_per_second': 83.291, 'eval_steps_per_second': 2.619, 'epoch': 12.0}
{'train_runtime': 4361.784, 'train_samples_per_second': 26.356, 'train_steps_per_second': 0.825, 'train_loss': 1.7651289961073133, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.7651
  train_runtime            = 1:12:41.78
  train_samples_per_second =     26.356
  train_steps_per_second   =      0.825
{'train@rus.rst.rrt_loss': 1.7078664302825928, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.4981958226354868, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.19188337801150207, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.2678612204687882, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.20611395950567915, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.7078665494918823, 'train@rus.rst.rrt_runtime': 343.4152, 'train@rus.rst.rrt_samples_per_second': 83.928, 'train@rus.rst.rrt_steps_per_second': 2.624, 'epoch': 1.0}
{'loss': 2.1657, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.7458375692367554, 'eval_accuracy@rus.rst.rrt': 0.476707530647986, 'eval_f1@rus.rst.rrt': 0.21071307692267843, 'eval_precision@rus.rst.rrt': 0.21800015190683977, 'eval_recall@rus.rst.rrt': 0.2264748327322274, 'eval_loss@rus.rst.rrt': 1.7458375692367554, 'eval_runtime': 34.4055, 'eval_samples_per_second': 82.981, 'eval_steps_per_second': 2.616, 'epoch': 1.0}
{'train@rus.rst.rrt_loss': 1.507839560508728, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5406633821386441, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.24190854919351842, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4207737292896044, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.2387704363477119, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.507839560508728, 'train@rus.rst.rrt_runtime': 343.1671, 'train@rus.rst.rrt_samples_per_second': 83.988, 'train@rus.rst.rrt_steps_per_second': 2.626, 'epoch': 2.0}
{'loss': 1.6498, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.5638712644577026, 'eval_accuracy@rus.rst.rrt': 0.5204903677758319, 'eval_f1@rus.rst.rrt': 0.2773226381714419, 'eval_precision@rus.rst.rrt': 0.4101580740077744, 'eval_recall@rus.rst.rrt': 0.27068320754706005, 'eval_loss@rus.rst.rrt': 1.563871145248413, 'eval_runtime': 34.3874, 'eval_samples_per_second': 83.025, 'eval_steps_per_second': 2.617, 'epoch': 2.0}
{'train@rus.rst.rrt_loss': 1.4234586954116821, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5716466588023038, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3178059652849257, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.44749534978505595, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3021822982371864, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.4234585762023926, 'train@rus.rst.rrt_runtime': 343.6893, 'train@rus.rst.rrt_samples_per_second': 83.861, 'train@rus.rst.rrt_steps_per_second': 2.622, 'epoch': 3.0}
{'loss': 1.5197, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.4826501607894897, 'eval_accuracy@rus.rst.rrt': 0.5446584938704028, 'eval_f1@rus.rst.rrt': 0.3631217107661214, 'eval_precision@rus.rst.rrt': 0.5018921061623443, 'eval_recall@rus.rst.rrt': 0.3445544467414042, 'eval_loss@rus.rst.rrt': 1.4826503992080688, 'eval_runtime': 34.362, 'eval_samples_per_second': 83.086, 'eval_steps_per_second': 2.619, 'epoch': 3.0}
{'train@rus.rst.rrt_loss': 1.3743408918380737, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.583928943168413, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3328957009249202, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4620607564782613, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.31073648273299226, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3743408918380737, 'train@rus.rst.rrt_runtime': 342.9443, 'train@rus.rst.rrt_samples_per_second': 84.043, 'train@rus.rst.rrt_steps_per_second': 2.627, 'epoch': 4.0}
{'loss': 1.4531, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.4421117305755615, 'eval_accuracy@rus.rst.rrt': 0.5600700525394046, 'eval_f1@rus.rst.rrt': 0.3860430180403299, 'eval_precision@rus.rst.rrt': 0.5185383104373927, 'eval_recall@rus.rst.rrt': 0.3646752617937587, 'eval_loss@rus.rst.rrt': 1.442111849784851, 'eval_runtime': 37.0036, 'eval_samples_per_second': 77.155, 'eval_steps_per_second': 2.432, 'epoch': 4.0}
{'train@rus.rst.rrt_loss': 1.336097002029419, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5918395669974326, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.34404736157852533, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4498168977505733, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.32540666924608525, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.336097002029419, 'train@rus.rst.rrt_runtime': 342.8343, 'train@rus.rst.rrt_samples_per_second': 84.07, 'train@rus.rst.rrt_steps_per_second': 2.628, 'epoch': 5.0}
{'loss': 1.4113, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.4109855890274048, 'eval_accuracy@rus.rst.rrt': 0.5681260945709282, 'eval_f1@rus.rst.rrt': 0.3996359710754614, 'eval_precision@rus.rst.rrt': 0.5092437012995006, 'eval_recall@rus.rst.rrt': 0.3823583048659733, 'eval_loss@rus.rst.rrt': 1.4109855890274048, 'eval_runtime': 34.3653, 'eval_samples_per_second': 83.078, 'eval_steps_per_second': 2.619, 'epoch': 5.0}
{'train@rus.rst.rrt_loss': 1.3130110502243042, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5974602734022622, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3497809967289349, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.45510040649158284, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.33131506680290274, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3130109310150146, 'train@rus.rst.rrt_runtime': 343.5057, 'train@rus.rst.rrt_samples_per_second': 83.905, 'train@rus.rst.rrt_steps_per_second': 2.623, 'epoch': 6.0}
{'loss': 1.3806, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.396630048751831, 'eval_accuracy@rus.rst.rrt': 0.5726795096322241, 'eval_f1@rus.rst.rrt': 0.4069866761251981, 'eval_precision@rus.rst.rrt': 0.5059609778552521, 'eval_recall@rus.rst.rrt': 0.38949195735690934, 'eval_loss@rus.rst.rrt': 1.396630048751831, 'eval_runtime': 34.4089, 'eval_samples_per_second': 82.973, 'eval_steps_per_second': 2.616, 'epoch': 6.0}
{'train@rus.rst.rrt_loss': 1.2979216575622559, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6022482825619319, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.358231497767243, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4450114933615492, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.33960389994087364, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2979215383529663, 'train@rus.rst.rrt_runtime': 342.9502, 'train@rus.rst.rrt_samples_per_second': 84.041, 'train@rus.rst.rrt_steps_per_second': 2.627, 'epoch': 7.0}
{'loss': 1.3614, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.3867454528808594, 'eval_accuracy@rus.rst.rrt': 0.5747810858143608, 'eval_f1@rus.rst.rrt': 0.41351504463222466, 'eval_precision@rus.rst.rrt': 0.5090315314627681, 'eval_recall@rus.rst.rrt': 0.39609771162344065, 'eval_loss@rus.rst.rrt': 1.3867453336715698, 'eval_runtime': 34.3798, 'eval_samples_per_second': 83.043, 'eval_steps_per_second': 2.618, 'epoch': 7.0}
{'train@rus.rst.rrt_loss': 1.28390371799469, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6040177642078968, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3642528853521192, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4544397639850107, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.347140181489557, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.28390371799469, 'train@rus.rst.rrt_runtime': 342.8326, 'train@rus.rst.rrt_samples_per_second': 84.07, 'train@rus.rst.rrt_steps_per_second': 2.628, 'epoch': 8.0}
{'loss': 1.3429, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.3774588108062744, 'eval_accuracy@rus.rst.rrt': 0.5747810858143608, 'eval_f1@rus.rst.rrt': 0.4166957549062927, 'eval_precision@rus.rst.rrt': 0.4943304200291674, 'eval_recall@rus.rst.rrt': 0.402638587703849, 'eval_loss@rus.rst.rrt': 1.377458930015564, 'eval_runtime': 34.3802, 'eval_samples_per_second': 83.042, 'eval_steps_per_second': 2.618, 'epoch': 8.0}
{'train@rus.rst.rrt_loss': 1.2718737125396729, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6068281174103116, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3666107878483013, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4440185828036963, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3478678288537504, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2718738317489624, 'train@rus.rst.rrt_runtime': 343.6634, 'train@rus.rst.rrt_samples_per_second': 83.867, 'train@rus.rst.rrt_steps_per_second': 2.622, 'epoch': 9.0}
{'loss': 1.3346, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.3720006942749023, 'eval_accuracy@rus.rst.rrt': 0.5796847635726795, 'eval_f1@rus.rst.rrt': 0.42246372595573467, 'eval_precision@rus.rst.rrt': 0.5133805481173777, 'eval_recall@rus.rst.rrt': 0.40322395452679877, 'eval_loss@rus.rst.rrt': 1.372000813484192, 'eval_runtime': 34.3436, 'eval_samples_per_second': 83.13, 'eval_steps_per_second': 2.621, 'epoch': 9.0}
{'train@rus.rst.rrt_loss': 1.26496160030365, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6100548192353064, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.37008764284387097, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4706136835294092, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3478609897769301, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2649613618850708, 'train@rus.rst.rrt_runtime': 343.6497, 'train@rus.rst.rrt_samples_per_second': 83.87, 'train@rus.rst.rrt_steps_per_second': 2.622, 'epoch': 10.0}
{'loss': 1.3221, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.3694454431533813, 'eval_accuracy@rus.rst.rrt': 0.5821366024518388, 'eval_f1@rus.rst.rrt': 0.4233982083799882, 'eval_precision@rus.rst.rrt': 0.5187210063313813, 'eval_recall@rus.rst.rrt': 0.40213362754626547, 'eval_loss@rus.rst.rrt': 1.3694453239440918, 'eval_runtime': 34.3747, 'eval_samples_per_second': 83.055, 'eval_steps_per_second': 2.618, 'epoch': 10.0}
{'train@rus.rst.rrt_loss': 1.2603328227996826, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.610020123516758, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3713685794871722, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4607662913735469, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3516174069162342, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.260332703590393, 'train@rus.rst.rrt_runtime': 343.2781, 'train@rus.rst.rrt_samples_per_second': 83.961, 'train@rus.rst.rrt_steps_per_second': 2.625, 'epoch': 11.0}
{'loss': 1.3165, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.3677242994308472, 'eval_accuracy@rus.rst.rrt': 0.5786339754816112, 'eval_f1@rus.rst.rrt': 0.4222123960094301, 'eval_precision@rus.rst.rrt': 0.5064293180043877, 'eval_recall@rus.rst.rrt': 0.40453596148812726, 'eval_loss@rus.rst.rrt': 1.3677244186401367, 'eval_runtime': 34.3405, 'eval_samples_per_second': 83.138, 'eval_steps_per_second': 2.621, 'epoch': 11.0}
{'train@rus.rst.rrt_loss': 1.2598828077316284, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6099507320796613, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3717618986203374, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4610470520836174, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.35255778854964037, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2598828077316284, 'train@rus.rst.rrt_runtime': 343.5524, 'train@rus.rst.rrt_samples_per_second': 83.894, 'train@rus.rst.rrt_steps_per_second': 2.623, 'epoch': 12.0}
{'loss': 1.3122, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.3663157224655151, 'eval_accuracy@rus.rst.rrt': 0.580385288966725, 'eval_f1@rus.rst.rrt': 0.4225264250214823, 'eval_precision@rus.rst.rrt': 0.5083047462830198, 'eval_recall@rus.rst.rrt': 0.4045461582223899, 'eval_loss@rus.rst.rrt': 1.3663157224655151, 'eval_runtime': 34.3903, 'eval_samples_per_second': 83.018, 'eval_steps_per_second': 2.617, 'epoch': 12.0}
{'train_runtime': 13290.3615, 'train_samples_per_second': 26.024, 'train_steps_per_second': 0.814, 'train_loss': 1.4641727132793714, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.7651
  train_runtime            = 1:12:41.78
  train_samples_per_second =     26.356
  train_steps_per_second   =      0.825
-------------------------------------------------------------------
Lang1:  fas.rst.prstc    Lang2:  rus.rst.rrt
Saving run to:  runs/full_shot/FullShot=v4_fas.rst.prstc_rus.rst.rrt_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 4100 examples
read 499 examples
read 592 examples
read 28822 examples
read 2855 examples
read 2843 examples
Total prediction labels:  30
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (ar): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (ru): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=30, bias=True)
    )
  )
)
{'train@fas.rst.prstc_loss': 2.400078058242798, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.23780487804878048, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.02260214430599826, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.013988522238163558, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.058823529411764705, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.4000778198242188, 'train@fas.rst.prstc_runtime': 48.0297, 'train@fas.rst.prstc_samples_per_second': 85.364, 'train@fas.rst.prstc_steps_per_second': 2.686, 'epoch': 1.0}
{'loss': 2.7258, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.3251311779022217, 'eval_accuracy@fas.rst.prstc': 0.24448897795591182, 'eval_f1@fas.rst.prstc': 0.026194310252281264, 'eval_precision@fas.rst.prstc': 0.016299265197060788, 'eval_recall@fas.rst.prstc': 0.06666666666666667, 'eval_loss@fas.rst.prstc': 2.3251311779022217, 'eval_runtime': 6.1103, 'eval_samples_per_second': 81.665, 'eval_steps_per_second': 2.619, 'epoch': 1.0}
{'train@fas.rst.prstc_loss': 2.3524723052978516, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2890243902439024, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04542846896476687, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03333719275867145, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07486024196912808, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3524723052978516, 'train@fas.rst.prstc_runtime': 48.0455, 'train@fas.rst.prstc_samples_per_second': 85.336, 'train@fas.rst.prstc_steps_per_second': 2.685, 'epoch': 2.0}
{'loss': 2.3958, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.2663917541503906, 'eval_accuracy@fas.rst.prstc': 0.3106212424849699, 'eval_f1@fas.rst.prstc': 0.05466116519114957, 'eval_precision@fas.rst.prstc': 0.042026106335561926, 'eval_recall@fas.rst.prstc': 0.08632929436920884, 'eval_loss@fas.rst.prstc': 2.2663915157318115, 'eval_runtime': 6.1158, 'eval_samples_per_second': 81.592, 'eval_steps_per_second': 2.616, 'epoch': 2.0}
{'train@fas.rst.prstc_loss': 2.3307878971099854, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.25634146341463415, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.0346050700819827, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03203025134446218, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.06449429308002524, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3307876586914062, 'train@fas.rst.prstc_runtime': 47.9782, 'train@fas.rst.prstc_samples_per_second': 85.456, 'train@fas.rst.prstc_steps_per_second': 2.689, 'epoch': 3.0}
{'loss': 2.3538, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.2435495853424072, 'eval_accuracy@fas.rst.prstc': 0.280561122244489, 'eval_f1@fas.rst.prstc': 0.04498550724637681, 'eval_precision@fas.rst.prstc': 0.047947659724221776, 'eval_recall@fas.rst.prstc': 0.07720123544784983, 'eval_loss@fas.rst.prstc': 2.2435498237609863, 'eval_runtime': 6.1279, 'eval_samples_per_second': 81.431, 'eval_steps_per_second': 2.611, 'epoch': 3.0}
{'train@fas.rst.prstc_loss': 2.2677841186523438, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.294390243902439, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.046623736979302256, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03379574932283144, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07653712439694918, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.267784357070923, 'train@fas.rst.prstc_runtime': 47.9878, 'train@fas.rst.prstc_samples_per_second': 85.438, 'train@fas.rst.prstc_steps_per_second': 2.688, 'epoch': 4.0}
{'loss': 2.3304, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.1763880252838135, 'eval_accuracy@fas.rst.prstc': 0.3046092184368738, 'eval_f1@fas.rst.prstc': 0.05400068763191115, 'eval_precision@fas.rst.prstc': 0.03992510362147326, 'eval_recall@fas.rst.prstc': 0.08488952245188881, 'eval_loss@fas.rst.prstc': 2.1763880252838135, 'eval_runtime': 6.089, 'eval_samples_per_second': 81.951, 'eval_steps_per_second': 2.628, 'epoch': 4.0}
{'train@fas.rst.prstc_loss': 2.2054522037506104, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3253658536585366, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.0559917195266884, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.04502568472861334, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08708791972872074, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.2054526805877686, 'train@fas.rst.prstc_runtime': 48.1025, 'train@fas.rst.prstc_samples_per_second': 85.235, 'train@fas.rst.prstc_steps_per_second': 2.682, 'epoch': 5.0}
{'loss': 2.2621, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.095393419265747, 'eval_accuracy@fas.rst.prstc': 0.3527054108216433, 'eval_f1@fas.rst.prstc': 0.06730011984249273, 'eval_precision@fas.rst.prstc': 0.055203114109493565, 'eval_recall@fas.rst.prstc': 0.09940128296507483, 'eval_loss@fas.rst.prstc': 2.095393657684326, 'eval_runtime': 6.1399, 'eval_samples_per_second': 81.271, 'eval_steps_per_second': 2.606, 'epoch': 5.0}
{'train@fas.rst.prstc_loss': 2.185521125793457, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3360975609756098, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.057529724390528114, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.0459287218272934, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.09000010697132099, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.185520887374878, 'train@fas.rst.prstc_runtime': 48.0503, 'train@fas.rst.prstc_samples_per_second': 85.327, 'train@fas.rst.prstc_steps_per_second': 2.685, 'epoch': 6.0}
{'loss': 2.2209, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.0779380798339844, 'eval_accuracy@fas.rst.prstc': 0.3687374749498998, 'eval_f1@fas.rst.prstc': 0.07048989690499124, 'eval_precision@fas.rst.prstc': 0.05838475084351602, 'eval_recall@fas.rst.prstc': 0.10397244000950344, 'eval_loss@fas.rst.prstc': 2.0779383182525635, 'eval_runtime': 6.1152, 'eval_samples_per_second': 81.6, 'eval_steps_per_second': 2.616, 'epoch': 6.0}
{'train@fas.rst.prstc_loss': 2.172528028488159, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3426829268292683, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.058824731168472394, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.047244396571708765, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.09178545831862477, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.17252779006958, 'train@fas.rst.prstc_runtime': 48.1033, 'train@fas.rst.prstc_samples_per_second': 85.233, 'train@fas.rst.prstc_steps_per_second': 2.682, 'epoch': 7.0}
{'loss': 2.2019, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.058499813079834, 'eval_accuracy@fas.rst.prstc': 0.37675350701402804, 'eval_f1@fas.rst.prstc': 0.07239583333333334, 'eval_precision@fas.rst.prstc': 0.06047161453008928, 'eval_recall@fas.rst.prstc': 0.1062247564742219, 'eval_loss@fas.rst.prstc': 2.058499813079834, 'eval_runtime': 6.1576, 'eval_samples_per_second': 81.037, 'eval_steps_per_second': 2.598, 'epoch': 7.0}
{'train@fas.rst.prstc_loss': 2.1504178047180176, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3497560975609756, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.06055765698736571, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.10619009757284976, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.09396116751927537, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1504178047180176, 'train@fas.rst.prstc_runtime': 48.1029, 'train@fas.rst.prstc_samples_per_second': 85.234, 'train@fas.rst.prstc_steps_per_second': 2.682, 'epoch': 8.0}
{'loss': 2.1838, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.043858289718628, 'eval_accuracy@fas.rst.prstc': 0.3787575150300601, 'eval_f1@fas.rst.prstc': 0.07272139625080802, 'eval_precision@fas.rst.prstc': 0.0602687114429858, 'eval_recall@fas.rst.prstc': 0.10673794250415775, 'eval_loss@fas.rst.prstc': 2.043858289718628, 'eval_runtime': 6.1339, 'eval_samples_per_second': 81.351, 'eval_steps_per_second': 2.608, 'epoch': 8.0}
{'train@fas.rst.prstc_loss': 2.1376142501831055, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.35536585365853657, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.06839448382110863, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.0896525003869976, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.09803557275134316, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1376142501831055, 'train@fas.rst.prstc_runtime': 48.1261, 'train@fas.rst.prstc_samples_per_second': 85.193, 'train@fas.rst.prstc_steps_per_second': 2.68, 'epoch': 9.0}
{'loss': 2.1666, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.0324923992156982, 'eval_accuracy@fas.rst.prstc': 0.3787575150300601, 'eval_f1@fas.rst.prstc': 0.07504157137062088, 'eval_precision@fas.rst.prstc': 0.12462965455792517, 'eval_recall@fas.rst.prstc': 0.10787935626664666, 'eval_loss@fas.rst.prstc': 2.0324923992156982, 'eval_runtime': 6.1226, 'eval_samples_per_second': 81.502, 'eval_steps_per_second': 2.613, 'epoch': 9.0}
{'train@fas.rst.prstc_loss': 2.1215786933898926, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.37585365853658537, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.08659858029156584, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.09384074204147896, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.11155353031067841, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1215786933898926, 'train@fas.rst.prstc_runtime': 48.0965, 'train@fas.rst.prstc_samples_per_second': 85.245, 'train@fas.rst.prstc_steps_per_second': 2.682, 'epoch': 10.0}
{'loss': 2.1618, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.018948554992676, 'eval_accuracy@fas.rst.prstc': 0.3967935871743487, 'eval_f1@fas.rst.prstc': 0.0986097008795537, 'eval_precision@fas.rst.prstc': 0.12695238095238096, 'eval_recall@fas.rst.prstc': 0.12252741618836828, 'eval_loss@fas.rst.prstc': 2.018948793411255, 'eval_runtime': 6.1348, 'eval_samples_per_second': 81.339, 'eval_steps_per_second': 2.608, 'epoch': 10.0}
{'train@fas.rst.prstc_loss': 2.1125051975250244, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3826829268292683, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.09080549872619315, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.0935183293065123, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.1158622902039742, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1125049591064453, 'train@fas.rst.prstc_runtime': 48.0744, 'train@fas.rst.prstc_samples_per_second': 85.285, 'train@fas.rst.prstc_steps_per_second': 2.683, 'epoch': 11.0}
{'loss': 2.1497, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.0117058753967285, 'eval_accuracy@fas.rst.prstc': 0.39879759519038077, 'eval_f1@fas.rst.prstc': 0.10213542590099703, 'eval_precision@fas.rst.prstc': 0.1207509988122233, 'eval_recall@fas.rst.prstc': 0.12542321591576946, 'eval_loss@fas.rst.prstc': 2.0117058753967285, 'eval_runtime': 6.1043, 'eval_samples_per_second': 81.746, 'eval_steps_per_second': 2.621, 'epoch': 11.0}
{'train@fas.rst.prstc_loss': 2.1105239391326904, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.38414634146341464, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.09148692426057924, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.09326835041647126, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.11667990451538947, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1105239391326904, 'train@fas.rst.prstc_runtime': 48.0785, 'train@fas.rst.prstc_samples_per_second': 85.277, 'train@fas.rst.prstc_steps_per_second': 2.683, 'epoch': 12.0}
{'loss': 2.1416, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.0110013484954834, 'eval_accuracy@fas.rst.prstc': 0.4028056112224449, 'eval_f1@fas.rst.prstc': 0.10608980662078006, 'eval_precision@fas.rst.prstc': 0.12107416870028147, 'eval_recall@fas.rst.prstc': 0.1288987257880982, 'eval_loss@fas.rst.prstc': 2.0110013484954834, 'eval_runtime': 6.1229, 'eval_samples_per_second': 81.498, 'eval_steps_per_second': 2.613, 'epoch': 12.0}
{'train_runtime': 1872.0621, 'train_samples_per_second': 26.281, 'train_steps_per_second': 0.827, 'train_loss': 2.2745199868845387, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.2745
  train_runtime            = 0:31:12.06
  train_samples_per_second =     26.281
  train_steps_per_second   =      0.827
{'train@rus.rst.rrt_loss': 1.6921226978302002, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.504406356255638, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.18979764777540173, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.2343310741993135, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.20557637778962753, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.6921229362487793, 'train@rus.rst.rrt_runtime': 343.1071, 'train@rus.rst.rrt_samples_per_second': 84.003, 'train@rus.rst.rrt_steps_per_second': 2.626, 'epoch': 1.0}
{'loss': 2.1248, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.727810263633728, 'eval_accuracy@rus.rst.rrt': 0.4896672504378284, 'eval_f1@rus.rst.rrt': 0.21344256215047028, 'eval_precision@rus.rst.rrt': 0.2263290996223209, 'eval_recall@rus.rst.rrt': 0.2285301387022666, 'eval_loss@rus.rst.rrt': 1.727810263633728, 'eval_runtime': 34.3342, 'eval_samples_per_second': 83.153, 'eval_steps_per_second': 2.621, 'epoch': 1.0}
{'train@rus.rst.rrt_loss': 1.494505763053894, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5450697383942822, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.23331313980231974, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.31888117955863887, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.2360382904565233, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.4945056438446045, 'train@rus.rst.rrt_runtime': 343.0174, 'train@rus.rst.rrt_samples_per_second': 84.025, 'train@rus.rst.rrt_steps_per_second': 2.627, 'epoch': 2.0}
{'loss': 1.626, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.5532636642456055, 'eval_accuracy@rus.rst.rrt': 0.5236427320490368, 'eval_f1@rus.rst.rrt': 0.2645966625944567, 'eval_precision@rus.rst.rrt': 0.368568742544013, 'eval_recall@rus.rst.rrt': 0.2650526994504906, 'eval_loss@rus.rst.rrt': 1.5532636642456055, 'eval_runtime': 34.3249, 'eval_samples_per_second': 83.176, 'eval_steps_per_second': 2.622, 'epoch': 2.0}
{'train@rus.rst.rrt_loss': 1.4163669347763062, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5701200471861773, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.2916110476166587, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4508273072134129, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.2830967562953171, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.4163668155670166, 'train@rus.rst.rrt_runtime': 343.8569, 'train@rus.rst.rrt_samples_per_second': 83.82, 'train@rus.rst.rrt_steps_per_second': 2.62, 'epoch': 3.0}
{'loss': 1.506, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.4808900356292725, 'eval_accuracy@rus.rst.rrt': 0.5446584938704028, 'eval_f1@rus.rst.rrt': 0.31609297371887884, 'eval_precision@rus.rst.rrt': 0.4252632765406737, 'eval_recall@rus.rst.rrt': 0.31212440962002713, 'eval_loss@rus.rst.rrt': 1.4808900356292725, 'eval_runtime': 34.3429, 'eval_samples_per_second': 83.132, 'eval_steps_per_second': 2.621, 'epoch': 3.0}
{'train@rus.rst.rrt_loss': 1.3675081729888916, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5820206786482548, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.31361681790506646, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.45357694929745834, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.2970202076373431, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3675081729888916, 'train@rus.rst.rrt_runtime': 343.687, 'train@rus.rst.rrt_samples_per_second': 83.861, 'train@rus.rst.rrt_steps_per_second': 2.622, 'epoch': 4.0}
{'loss': 1.4453, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.438252329826355, 'eval_accuracy@rus.rst.rrt': 0.5600700525394046, 'eval_f1@rus.rst.rrt': 0.3621391473375363, 'eval_precision@rus.rst.rrt': 0.4590184236201204, 'eval_recall@rus.rst.rrt': 0.3456062866356148, 'eval_loss@rus.rst.rrt': 1.438252568244934, 'eval_runtime': 34.3969, 'eval_samples_per_second': 83.002, 'eval_steps_per_second': 2.617, 'epoch': 4.0}
{'train@rus.rst.rrt_loss': 1.3339861631393433, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5897578238845327, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3300333194978192, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4584734912952219, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3117287182711109, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3339862823486328, 'train@rus.rst.rrt_runtime': 343.6027, 'train@rus.rst.rrt_samples_per_second': 83.882, 'train@rus.rst.rrt_steps_per_second': 2.622, 'epoch': 5.0}
{'loss': 1.407, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.413017749786377, 'eval_accuracy@rus.rst.rrt': 0.5660245183887916, 'eval_f1@rus.rst.rrt': 0.38113950508338934, 'eval_precision@rus.rst.rrt': 0.4953111921674897, 'eval_recall@rus.rst.rrt': 0.3640310831931316, 'eval_loss@rus.rst.rrt': 1.413017749786377, 'eval_runtime': 34.3811, 'eval_samples_per_second': 83.04, 'eval_steps_per_second': 2.618, 'epoch': 5.0}
{'train@rus.rst.rrt_loss': 1.310115098953247, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5964540975643605, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.34222254128093693, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4583315302926954, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.32220092171547027, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.310115098953247, 'train@rus.rst.rrt_runtime': 343.6067, 'train@rus.rst.rrt_samples_per_second': 83.881, 'train@rus.rst.rrt_steps_per_second': 2.622, 'epoch': 6.0}
{'loss': 1.377, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.3966732025146484, 'eval_accuracy@rus.rst.rrt': 0.569877408056042, 'eval_f1@rus.rst.rrt': 0.3851661937902313, 'eval_precision@rus.rst.rrt': 0.4898206091865205, 'eval_recall@rus.rst.rrt': 0.37033081034945087, 'eval_loss@rus.rst.rrt': 1.3966730833053589, 'eval_runtime': 34.3244, 'eval_samples_per_second': 83.177, 'eval_steps_per_second': 2.622, 'epoch': 6.0}
{'train@rus.rst.rrt_loss': 1.2931841611862183, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6006175837901603, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.35462975027562055, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.43897414359900877, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3339648574129312, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2931841611862183, 'train@rus.rst.rrt_runtime': 343.6186, 'train@rus.rst.rrt_samples_per_second': 83.878, 'train@rus.rst.rrt_steps_per_second': 2.622, 'epoch': 7.0}
{'loss': 1.3571, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.3850828409194946, 'eval_accuracy@rus.rst.rrt': 0.5751313485113836, 'eval_f1@rus.rst.rrt': 0.4009937598462827, 'eval_precision@rus.rst.rrt': 0.47596637521312235, 'eval_recall@rus.rst.rrt': 0.3845918939083568, 'eval_loss@rus.rst.rrt': 1.3850829601287842, 'eval_runtime': 34.3354, 'eval_samples_per_second': 83.15, 'eval_steps_per_second': 2.621, 'epoch': 7.0}
{'train@rus.rst.rrt_loss': 1.2808045148849487, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6031850669627368, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3610957904359049, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4534704848177565, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3425914555256918, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2808045148849487, 'train@rus.rst.rrt_runtime': 343.0178, 'train@rus.rst.rrt_samples_per_second': 84.025, 'train@rus.rst.rrt_steps_per_second': 2.627, 'epoch': 8.0}
{'loss': 1.3401, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.3768291473388672, 'eval_accuracy@rus.rst.rrt': 0.5779334500875657, 'eval_f1@rus.rst.rrt': 0.4085625459823846, 'eval_precision@rus.rst.rrt': 0.469809147651893, 'eval_recall@rus.rst.rrt': 0.3936970394377973, 'eval_loss@rus.rst.rrt': 1.3768291473388672, 'eval_runtime': 34.2913, 'eval_samples_per_second': 83.257, 'eval_steps_per_second': 2.625, 'epoch': 8.0}
{'train@rus.rst.rrt_loss': 1.2686636447906494, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6071750745957949, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3661659802085324, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.46567232324646035, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3455953664354872, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2686636447906494, 'train@rus.rst.rrt_runtime': 342.9647, 'train@rus.rst.rrt_samples_per_second': 84.038, 'train@rus.rst.rrt_steps_per_second': 2.627, 'epoch': 9.0}
{'loss': 1.33, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.3700342178344727, 'eval_accuracy@rus.rst.rrt': 0.5765323992994746, 'eval_f1@rus.rst.rrt': 0.409239671896684, 'eval_precision@rus.rst.rrt': 0.47323307756980987, 'eval_recall@rus.rst.rrt': 0.3922045793156051, 'eval_loss@rus.rst.rrt': 1.3700344562530518, 'eval_runtime': 34.3201, 'eval_samples_per_second': 83.187, 'eval_steps_per_second': 2.622, 'epoch': 9.0}
{'train@rus.rst.rrt_loss': 1.2638914585113525, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6087016862119214, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3675938478005314, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.45826224887540934, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34519915988974054, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.263891339302063, 'train@rus.rst.rrt_runtime': 343.2194, 'train@rus.rst.rrt_samples_per_second': 83.975, 'train@rus.rst.rrt_steps_per_second': 2.625, 'epoch': 10.0}
{'loss': 1.3202, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.368241548538208, 'eval_accuracy@rus.rst.rrt': 0.5779334500875657, 'eval_f1@rus.rst.rrt': 0.4148311275628671, 'eval_precision@rus.rst.rrt': 0.5266630860408209, 'eval_recall@rus.rst.rrt': 0.3952824756370625, 'eval_loss@rus.rst.rrt': 1.3682414293289185, 'eval_runtime': 34.313, 'eval_samples_per_second': 83.205, 'eval_steps_per_second': 2.623, 'epoch': 10.0}
{'train@rus.rst.rrt_loss': 1.2591421604156494, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6104711678578864, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.37064638876330575, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.45062267252935706, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3508960761921973, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2591421604156494, 'train@rus.rst.rrt_runtime': 342.8398, 'train@rus.rst.rrt_samples_per_second': 84.068, 'train@rus.rst.rrt_steps_per_second': 2.628, 'epoch': 11.0}
{'loss': 1.3132, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.3674464225769043, 'eval_accuracy@rus.rst.rrt': 0.5775831873905429, 'eval_f1@rus.rst.rrt': 0.41707074604783334, 'eval_precision@rus.rst.rrt': 0.5236668846127733, 'eval_recall@rus.rst.rrt': 0.4017343347307501, 'eval_loss@rus.rst.rrt': 1.3674463033676147, 'eval_runtime': 34.2979, 'eval_samples_per_second': 83.241, 'eval_steps_per_second': 2.624, 'epoch': 11.0}
{'train@rus.rst.rrt_loss': 1.257535696029663, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6097425577683714, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3704261628243271, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.45397084990392533, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.35043701040883657, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.257535696029663, 'train@rus.rst.rrt_runtime': 342.8464, 'train@rus.rst.rrt_samples_per_second': 84.067, 'train@rus.rst.rrt_steps_per_second': 2.628, 'epoch': 12.0}
{'loss': 1.3094, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.3652880191802979, 'eval_accuracy@rus.rst.rrt': 0.5765323992994746, 'eval_f1@rus.rst.rrt': 0.42033275794600644, 'eval_precision@rus.rst.rrt': 0.5256465754449712, 'eval_recall@rus.rst.rrt': 0.40328411811401355, 'eval_loss@rus.rst.rrt': 1.3652877807617188, 'eval_runtime': 34.3153, 'eval_samples_per_second': 83.199, 'eval_steps_per_second': 2.623, 'epoch': 12.0}
{'train_runtime': 13292.3431, 'train_samples_per_second': 26.02, 'train_steps_per_second': 0.813, 'train_loss': 1.4546673513984751, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.2745
  train_runtime            = 0:31:12.06
  train_samples_per_second =     26.281
  train_steps_per_second   =      0.827
-------------------------------------------------------------------
Lang1:  fra.sdrt.annodis    Lang2:  rus.rst.rrt
Saving run to:  runs/full_shot/FullShot=v4_fra.sdrt.annodis_rus.rst.rrt_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2185 examples
read 528 examples
read 625 examples
read 28822 examples
read 2855 examples
read 2843 examples
Total prediction labels:  36
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (fr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (ru): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=36, bias=True)
    )
  )
)
{'train@fra.sdrt.annodis_loss': 2.6469244956970215, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.22151029748283751, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.037880586579924366, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.045803402397021346, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.06204655822932251, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.6469244956970215, 'train@fra.sdrt.annodis_runtime': 26.192, 'train@fra.sdrt.annodis_samples_per_second': 83.423, 'train@fra.sdrt.annodis_steps_per_second': 2.634, 'epoch': 1.0}
{'loss': 3.1415, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.661527633666992, 'eval_accuracy@fra.sdrt.annodis': 0.2178030303030303, 'eval_f1@fra.sdrt.annodis': 0.03158101959401097, 'eval_precision@fra.sdrt.annodis': 0.03712297094322073, 'eval_recall@fra.sdrt.annodis': 0.05996598883410928, 'eval_loss@fra.sdrt.annodis': 2.6615281105041504, 'eval_runtime': 6.5834, 'eval_samples_per_second': 80.202, 'eval_steps_per_second': 2.582, 'epoch': 1.0}
{'train@fra.sdrt.annodis_loss': 2.392247438430786, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.2755148741418764, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.059104250409327305, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.04584338564866114, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.08444905087688176, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.392247438430786, 'train@fra.sdrt.annodis_runtime': 26.2833, 'train@fra.sdrt.annodis_samples_per_second': 83.133, 'train@fra.sdrt.annodis_steps_per_second': 2.625, 'epoch': 2.0}
{'loss': 2.5126, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.4039385318756104, 'eval_accuracy@fra.sdrt.annodis': 0.26136363636363635, 'eval_f1@fra.sdrt.annodis': 0.05533796042270619, 'eval_precision@fra.sdrt.annodis': 0.04339353015383469, 'eval_recall@fra.sdrt.annodis': 0.07741767381781227, 'eval_loss@fra.sdrt.annodis': 2.4039390087127686, 'eval_runtime': 6.5953, 'eval_samples_per_second': 80.057, 'eval_steps_per_second': 2.578, 'epoch': 2.0}
{'train@fra.sdrt.annodis_loss': 2.3148512840270996, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.28787185354691075, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.06268839836236424, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.07891972679649974, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.08573472837207724, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.3148510456085205, 'train@fra.sdrt.annodis_runtime': 26.2657, 'train@fra.sdrt.annodis_samples_per_second': 83.188, 'train@fra.sdrt.annodis_steps_per_second': 2.627, 'epoch': 3.0}
{'loss': 2.3825, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.3300225734710693, 'eval_accuracy@fra.sdrt.annodis': 0.2784090909090909, 'eval_f1@fra.sdrt.annodis': 0.05882021906823634, 'eval_precision@fra.sdrt.annodis': 0.04821963449989862, 'eval_recall@fra.sdrt.annodis': 0.08120460370373836, 'eval_loss@fra.sdrt.annodis': 2.3300228118896484, 'eval_runtime': 6.6319, 'eval_samples_per_second': 79.616, 'eval_steps_per_second': 2.563, 'epoch': 3.0}
{'train@fra.sdrt.annodis_loss': 2.262439012527466, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.3034324942791762, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.07071745126999995, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.07994505375846347, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.09161336679611791, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.262439250946045, 'train@fra.sdrt.annodis_runtime': 26.307, 'train@fra.sdrt.annodis_samples_per_second': 83.058, 'train@fra.sdrt.annodis_steps_per_second': 2.623, 'epoch': 4.0}
{'loss': 2.3268, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.282320499420166, 'eval_accuracy@fra.sdrt.annodis': 0.2840909090909091, 'eval_f1@fra.sdrt.annodis': 0.06109593199757134, 'eval_precision@fra.sdrt.annodis': 0.04977783230210415, 'eval_recall@fra.sdrt.annodis': 0.08276050366739186, 'eval_loss@fra.sdrt.annodis': 2.282320499420166, 'eval_runtime': 6.9017, 'eval_samples_per_second': 76.503, 'eval_steps_per_second': 2.463, 'epoch': 4.0}
{'train@fra.sdrt.annodis_loss': 2.2186028957366943, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.32356979405034325, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.07399732011054244, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.08681215630812973, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.10071172907856678, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.2186026573181152, 'train@fra.sdrt.annodis_runtime': 26.258, 'train@fra.sdrt.annodis_samples_per_second': 83.213, 'train@fra.sdrt.annodis_steps_per_second': 2.628, 'epoch': 5.0}
{'loss': 2.2717, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.247215986251831, 'eval_accuracy@fra.sdrt.annodis': 0.30113636363636365, 'eval_f1@fra.sdrt.annodis': 0.06419594163282928, 'eval_precision@fra.sdrt.annodis': 0.052962482672271154, 'eval_recall@fra.sdrt.annodis': 0.09143497945851702, 'eval_loss@fra.sdrt.annodis': 2.24721622467041, 'eval_runtime': 6.5937, 'eval_samples_per_second': 80.076, 'eval_steps_per_second': 2.578, 'epoch': 5.0}
{'train@fra.sdrt.annodis_loss': 2.1773183345794678, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.3377574370709382, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.08394054542279164, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.14270504956152022, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.10799250934292043, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.1773183345794678, 'train@fra.sdrt.annodis_runtime': 26.2394, 'train@fra.sdrt.annodis_samples_per_second': 83.272, 'train@fra.sdrt.annodis_steps_per_second': 2.63, 'epoch': 6.0}
{'loss': 2.2305, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.2139086723327637, 'eval_accuracy@fra.sdrt.annodis': 0.30492424242424243, 'eval_f1@fra.sdrt.annodis': 0.06513805173900661, 'eval_precision@fra.sdrt.annodis': 0.05342800233643168, 'eval_recall@fra.sdrt.annodis': 0.09276930097283159, 'eval_loss@fra.sdrt.annodis': 2.2139086723327637, 'eval_runtime': 6.6274, 'eval_samples_per_second': 79.669, 'eval_steps_per_second': 2.565, 'epoch': 6.0}
{'train@fra.sdrt.annodis_loss': 2.1387765407562256, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.36064073226544624, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.09735999382213624, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.11998926433103144, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.11849178359555819, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.1387765407562256, 'train@fra.sdrt.annodis_runtime': 26.2839, 'train@fra.sdrt.annodis_samples_per_second': 83.131, 'train@fra.sdrt.annodis_steps_per_second': 2.625, 'epoch': 7.0}
{'loss': 2.1953, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.182987928390503, 'eval_accuracy@fra.sdrt.annodis': 0.3106060606060606, 'eval_f1@fra.sdrt.annodis': 0.06831266593532999, 'eval_precision@fra.sdrt.annodis': 0.0617481140046634, 'eval_recall@fra.sdrt.annodis': 0.09456986973278153, 'eval_loss@fra.sdrt.annodis': 2.182987689971924, 'eval_runtime': 6.6274, 'eval_samples_per_second': 79.669, 'eval_steps_per_second': 2.565, 'epoch': 7.0}
{'train@fra.sdrt.annodis_loss': 2.104923963546753, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.37299771167048057, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.1085971795287957, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12212062141842013, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.12717516309460197, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.104923725128174, 'train@fra.sdrt.annodis_runtime': 26.2701, 'train@fra.sdrt.annodis_samples_per_second': 83.174, 'train@fra.sdrt.annodis_steps_per_second': 2.627, 'epoch': 8.0}
{'loss': 2.1645, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.1567142009735107, 'eval_accuracy@fra.sdrt.annodis': 0.3143939393939394, 'eval_f1@fra.sdrt.annodis': 0.07596238915247953, 'eval_precision@fra.sdrt.annodis': 0.10128069195548108, 'eval_recall@fra.sdrt.annodis': 0.09900936516399171, 'eval_loss@fra.sdrt.annodis': 2.1567142009735107, 'eval_runtime': 6.6617, 'eval_samples_per_second': 79.259, 'eval_steps_per_second': 2.552, 'epoch': 8.0}
{'train@fra.sdrt.annodis_loss': 2.0791945457458496, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.38443935926773454, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.11850102219092179, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12426235240147661, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.13620199945563874, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.0791945457458496, 'train@fra.sdrt.annodis_runtime': 26.2928, 'train@fra.sdrt.annodis_samples_per_second': 83.102, 'train@fra.sdrt.annodis_steps_per_second': 2.624, 'epoch': 9.0}
{'loss': 2.1396, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.136611223220825, 'eval_accuracy@fra.sdrt.annodis': 0.32575757575757575, 'eval_f1@fra.sdrt.annodis': 0.08455331565775843, 'eval_precision@fra.sdrt.annodis': 0.09963173641599621, 'eval_recall@fra.sdrt.annodis': 0.10530229809040254, 'eval_loss@fra.sdrt.annodis': 2.136611223220825, 'eval_runtime': 6.6321, 'eval_samples_per_second': 79.613, 'eval_steps_per_second': 2.563, 'epoch': 9.0}
{'train@fra.sdrt.annodis_loss': 2.0602595806121826, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.391304347826087, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.12184848332316929, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12245008479502825, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1402064574078593, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.0602595806121826, 'train@fra.sdrt.annodis_runtime': 26.333, 'train@fra.sdrt.annodis_samples_per_second': 82.976, 'train@fra.sdrt.annodis_steps_per_second': 2.62, 'epoch': 10.0}
{'loss': 2.1187, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.1227617263793945, 'eval_accuracy@fra.sdrt.annodis': 0.32575757575757575, 'eval_f1@fra.sdrt.annodis': 0.08452920782972383, 'eval_precision@fra.sdrt.annodis': 0.09583941330084388, 'eval_recall@fra.sdrt.annodis': 0.10532100838948667, 'eval_loss@fra.sdrt.annodis': 2.1227622032165527, 'eval_runtime': 6.6325, 'eval_samples_per_second': 79.608, 'eval_steps_per_second': 2.563, 'epoch': 10.0}
{'train@fra.sdrt.annodis_loss': 2.048475980758667, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.3977116704805492, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.12592220038845564, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12267613178849307, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1453247243708843, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.048475980758667, 'train@fra.sdrt.annodis_runtime': 26.3419, 'train@fra.sdrt.annodis_samples_per_second': 82.948, 'train@fra.sdrt.annodis_steps_per_second': 2.619, 'epoch': 11.0}
{'loss': 2.0894, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.112818956375122, 'eval_accuracy@fra.sdrt.annodis': 0.32765151515151514, 'eval_f1@fra.sdrt.annodis': 0.08489302350230342, 'eval_precision@fra.sdrt.annodis': 0.09291261728343232, 'eval_recall@fra.sdrt.annodis': 0.10600687944572809, 'eval_loss@fra.sdrt.annodis': 2.112818717956543, 'eval_runtime': 6.6479, 'eval_samples_per_second': 79.424, 'eval_steps_per_second': 2.557, 'epoch': 11.0}
{'train@fra.sdrt.annodis_loss': 2.0444161891937256, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4022883295194508, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.12727577741444807, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12311647788323264, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.14707594771648708, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.0444161891937256, 'train@fra.sdrt.annodis_runtime': 26.2733, 'train@fra.sdrt.annodis_samples_per_second': 83.164, 'train@fra.sdrt.annodis_steps_per_second': 2.626, 'epoch': 12.0}
{'loss': 2.0924, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.1098077297210693, 'eval_accuracy@fra.sdrt.annodis': 0.32765151515151514, 'eval_f1@fra.sdrt.annodis': 0.08471711067369919, 'eval_precision@fra.sdrt.annodis': 0.09229301252661463, 'eval_recall@fra.sdrt.annodis': 0.10617353970238491, 'eval_loss@fra.sdrt.annodis': 2.1098077297210693, 'eval_runtime': 6.6154, 'eval_samples_per_second': 79.813, 'eval_steps_per_second': 2.57, 'epoch': 12.0}
{'train_runtime': 1058.9004, 'train_samples_per_second': 24.762, 'train_steps_per_second': 0.782, 'train_loss': 2.3054695313679425, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.3055
  train_runtime            = 0:17:38.90
  train_samples_per_second =     24.762
  train_steps_per_second   =      0.782
{'train@rus.rst.rrt_loss': 1.6849640607833862, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5072167094580529, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.1903590542302706, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.2206939707716497, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.20377852111225597, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.6849638223648071, 'train@rus.rst.rrt_runtime': 343.397, 'train@rus.rst.rrt_samples_per_second': 83.932, 'train@rus.rst.rrt_steps_per_second': 2.624, 'epoch': 1.0}
{'loss': 2.1307, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.723931074142456, 'eval_accuracy@rus.rst.rrt': 0.487215411558669, 'eval_f1@rus.rst.rrt': 0.2093900103750367, 'eval_precision@rus.rst.rrt': 0.2388193734273934, 'eval_recall@rus.rst.rrt': 0.2242806321877829, 'eval_loss@rus.rst.rrt': 1.7239313125610352, 'eval_runtime': 34.3375, 'eval_samples_per_second': 83.145, 'eval_steps_per_second': 2.621, 'epoch': 1.0}
{'train@rus.rst.rrt_loss': 1.4860377311706543, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.547463742974117, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.23115389418977283, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.3079154647427368, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.2364333320574178, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.4860378503799438, 'train@rus.rst.rrt_runtime': 342.9212, 'train@rus.rst.rrt_samples_per_second': 84.048, 'train@rus.rst.rrt_steps_per_second': 2.627, 'epoch': 2.0}
{'loss': 1.6188, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.5479716062545776, 'eval_accuracy@rus.rst.rrt': 0.521891418563923, 'eval_f1@rus.rst.rrt': 0.256211596342153, 'eval_precision@rus.rst.rrt': 0.355097700449686, 'eval_recall@rus.rst.rrt': 0.26160019130839984, 'eval_loss@rus.rst.rrt': 1.547971487045288, 'eval_runtime': 36.7985, 'eval_samples_per_second': 77.585, 'eval_steps_per_second': 2.446, 'epoch': 2.0}
{'train@rus.rst.rrt_loss': 1.4113339185714722, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5717507459579488, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.29323777136419493, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4469308293855501, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.28408110307463413, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.411333680152893, 'train@rus.rst.rrt_runtime': 343.0126, 'train@rus.rst.rrt_samples_per_second': 84.026, 'train@rus.rst.rrt_steps_per_second': 2.627, 'epoch': 3.0}
{'loss': 1.4998, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.478515625, 'eval_accuracy@rus.rst.rrt': 0.5467600700525395, 'eval_f1@rus.rst.rrt': 0.3338668358584935, 'eval_precision@rus.rst.rrt': 0.47537148738950424, 'eval_recall@rus.rst.rrt': 0.3227943702220464, 'eval_loss@rus.rst.rrt': 1.478515625, 'eval_runtime': 34.3482, 'eval_samples_per_second': 83.119, 'eval_steps_per_second': 2.62, 'epoch': 3.0}
{'train@rus.rst.rrt_loss': 1.363513469696045, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5837901602942197, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.31946613869913154, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.45090622262441826, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3005026008219139, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3635132312774658, 'train@rus.rst.rrt_runtime': 343.2786, 'train@rus.rst.rrt_samples_per_second': 83.961, 'train@rus.rst.rrt_steps_per_second': 2.625, 'epoch': 4.0}
{'loss': 1.4401, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.4333603382110596, 'eval_accuracy@rus.rst.rrt': 0.563922942206655, 'eval_f1@rus.rst.rrt': 0.36923052828080816, 'eval_precision@rus.rst.rrt': 0.4934291495479417, 'eval_recall@rus.rst.rrt': 0.3520084076709247, 'eval_loss@rus.rst.rrt': 1.4333604574203491, 'eval_runtime': 34.3819, 'eval_samples_per_second': 83.038, 'eval_steps_per_second': 2.618, 'epoch': 4.0}
{'train@rus.rst.rrt_loss': 1.3304684162139893, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5911803483450142, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3337741853859064, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.43453264429329325, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.31597012099937927, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3304684162139893, 'train@rus.rst.rrt_runtime': 343.0656, 'train@rus.rst.rrt_samples_per_second': 84.013, 'train@rus.rst.rrt_steps_per_second': 2.626, 'epoch': 5.0}
{'loss': 1.4005, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.4101141691207886, 'eval_accuracy@rus.rst.rrt': 0.5677758318739055, 'eval_f1@rus.rst.rrt': 0.3761331547389877, 'eval_precision@rus.rst.rrt': 0.5012891869065113, 'eval_recall@rus.rst.rrt': 0.3606121103382517, 'eval_loss@rus.rst.rrt': 1.4101141691207886, 'eval_runtime': 34.3213, 'eval_samples_per_second': 83.185, 'eval_steps_per_second': 2.622, 'epoch': 5.0}
{'train@rus.rst.rrt_loss': 1.3075402975082397, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5977378391506488, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3441098640078001, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.44010736909728915, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3246902200092924, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3075402975082397, 'train@rus.rst.rrt_runtime': 342.8563, 'train@rus.rst.rrt_samples_per_second': 84.064, 'train@rus.rst.rrt_steps_per_second': 2.628, 'epoch': 6.0}
{'loss': 1.3753, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.392543077468872, 'eval_accuracy@rus.rst.rrt': 0.5723292469352014, 'eval_f1@rus.rst.rrt': 0.38939950212143454, 'eval_precision@rus.rst.rrt': 0.4855676321698991, 'eval_recall@rus.rst.rrt': 0.37090463569105875, 'eval_loss@rus.rst.rrt': 1.392543077468872, 'eval_runtime': 34.3763, 'eval_samples_per_second': 83.051, 'eval_steps_per_second': 2.618, 'epoch': 6.0}
{'train@rus.rst.rrt_loss': 1.2914143800735474, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6023870654361252, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3550203695694442, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.43694253830121643, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.33611093234258416, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2914142608642578, 'train@rus.rst.rrt_runtime': 343.2162, 'train@rus.rst.rrt_samples_per_second': 83.976, 'train@rus.rst.rrt_steps_per_second': 2.625, 'epoch': 7.0}
{'loss': 1.355, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.384746789932251, 'eval_accuracy@rus.rst.rrt': 0.5712784588441331, 'eval_f1@rus.rst.rrt': 0.3994231670412331, 'eval_precision@rus.rst.rrt': 0.4654166328132606, 'eval_recall@rus.rst.rrt': 0.3838454569466738, 'eval_loss@rus.rst.rrt': 1.3847469091415405, 'eval_runtime': 34.3275, 'eval_samples_per_second': 83.17, 'eval_steps_per_second': 2.622, 'epoch': 7.0}
{'train@rus.rst.rrt_loss': 1.2797563076019287, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6051280272014433, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36210980499328604, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.432177833144096, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3453004853171903, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2797563076019287, 'train@rus.rst.rrt_runtime': 343.1074, 'train@rus.rst.rrt_samples_per_second': 84.003, 'train@rus.rst.rrt_steps_per_second': 2.626, 'epoch': 8.0}
{'loss': 1.3361, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.3738806247711182, 'eval_accuracy@rus.rst.rrt': 0.5775831873905429, 'eval_f1@rus.rst.rrt': 0.4036541020373226, 'eval_precision@rus.rst.rrt': 0.4602841308805198, 'eval_recall@rus.rst.rrt': 0.39279177160814355, 'eval_loss@rus.rst.rrt': 1.3738807439804077, 'eval_runtime': 34.4112, 'eval_samples_per_second': 82.967, 'eval_steps_per_second': 2.615, 'epoch': 8.0}
{'train@rus.rst.rrt_loss': 1.2680543661117554, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6075914232183749, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3644156999324732, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.44007818046479225, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34605360027153526, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2680542469024658, 'train@rus.rst.rrt_runtime': 343.3189, 'train@rus.rst.rrt_samples_per_second': 83.951, 'train@rus.rst.rrt_steps_per_second': 2.624, 'epoch': 9.0}
{'loss': 1.3291, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.3684355020523071, 'eval_accuracy@rus.rst.rrt': 0.5737302977232924, 'eval_f1@rus.rst.rrt': 0.4055849071506989, 'eval_precision@rus.rst.rrt': 0.4676118492840432, 'eval_recall@rus.rst.rrt': 0.392205619627955, 'eval_loss@rus.rst.rrt': 1.368435263633728, 'eval_runtime': 34.3417, 'eval_samples_per_second': 83.135, 'eval_steps_per_second': 2.621, 'epoch': 9.0}
{'train@rus.rst.rrt_loss': 1.2619543075561523, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6091180348345014, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3666305770420337, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.44370481690369695, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.345964104629256, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2619543075561523, 'train@rus.rst.rrt_runtime': 343.2712, 'train@rus.rst.rrt_samples_per_second': 83.963, 'train@rus.rst.rrt_steps_per_second': 2.625, 'epoch': 10.0}
{'loss': 1.3184, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.3652971982955933, 'eval_accuracy@rus.rst.rrt': 0.578984238178634, 'eval_f1@rus.rst.rrt': 0.41030742530255954, 'eval_precision@rus.rst.rrt': 0.4788485065779792, 'eval_recall@rus.rst.rrt': 0.3938649121589376, 'eval_loss@rus.rst.rrt': 1.3652974367141724, 'eval_runtime': 34.3429, 'eval_samples_per_second': 83.132, 'eval_steps_per_second': 2.621, 'epoch': 10.0}
{'train@rus.rst.rrt_loss': 1.2579200267791748, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.610020123516758, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36837408837945185, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4387381926850064, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3503122977956355, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2579200267791748, 'train@rus.rst.rrt_runtime': 343.1231, 'train@rus.rst.rrt_samples_per_second': 83.999, 'train@rus.rst.rrt_steps_per_second': 2.626, 'epoch': 11.0}
{'loss': 1.3124, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.3653316497802734, 'eval_accuracy@rus.rst.rrt': 0.5754816112084064, 'eval_f1@rus.rst.rrt': 0.4103109758747797, 'eval_precision@rus.rst.rrt': 0.4847404880746577, 'eval_recall@rus.rst.rrt': 0.39548800329691447, 'eval_loss@rus.rst.rrt': 1.3653315305709839, 'eval_runtime': 34.3177, 'eval_samples_per_second': 83.193, 'eval_steps_per_second': 2.623, 'epoch': 11.0}
{'train@rus.rst.rrt_loss': 1.2564666271209717, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.610748733606273, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3684333088316579, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4405244421430284, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3498450875025984, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2564668655395508, 'train@rus.rst.rrt_runtime': 342.914, 'train@rus.rst.rrt_samples_per_second': 84.05, 'train@rus.rst.rrt_steps_per_second': 2.627, 'epoch': 12.0}
{'loss': 1.3073, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.3630998134613037, 'eval_accuracy@rus.rst.rrt': 0.5765323992994746, 'eval_f1@rus.rst.rrt': 0.4103074084080185, 'eval_precision@rus.rst.rrt': 0.4825762731554955, 'eval_recall@rus.rst.rrt': 0.39652542523517537, 'eval_loss@rus.rst.rrt': 1.3630996942520142, 'eval_runtime': 34.3361, 'eval_samples_per_second': 83.149, 'eval_steps_per_second': 2.621, 'epoch': 12.0}
{'train_runtime': 13283.7726, 'train_samples_per_second': 26.037, 'train_steps_per_second': 0.814, 'train_loss': 1.451964823969461, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.3055
  train_runtime            = 0:17:38.90
  train_samples_per_second =     24.762
  train_steps_per_second   =      0.782
-------------------------------------------------------------------
Lang1:  nld.rst.nldt    Lang2:  rus.rst.rrt
Saving run to:  runs/full_shot/FullShot=v4_nld.rst.nldt_rus.rst.rrt_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 1608 examples
read 331 examples
read 326 examples
read 28822 examples
read 2855 examples
read 2843 examples
Total prediction labels:  40
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (ru): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=40, bias=True)
    )
  )
)
{'train@nld.rst.nldt_loss': 3.226160764694214, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26430348258706465, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01306566650270536, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.00825948383084577, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03125, 'train@nld.rst.nldt_loss@nld.rst.nldt': 3.226160764694214, 'train@nld.rst.nldt_runtime': 19.3297, 'train@nld.rst.nldt_samples_per_second': 83.188, 'train@nld.rst.nldt_steps_per_second': 2.638, 'epoch': 1.0}
{'loss': 3.5036, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.194896936416626, 'eval_accuracy@nld.rst.nldt': 0.27794561933534745, 'eval_f1@nld.rst.nldt': 0.016110673321075214, 'eval_precision@nld.rst.nldt': 0.010294282197605462, 'eval_recall@nld.rst.nldt': 0.037037037037037035, 'eval_loss@nld.rst.nldt': 3.194896936416626, 'eval_runtime': 4.2871, 'eval_samples_per_second': 77.209, 'eval_steps_per_second': 2.566, 'epoch': 1.0}
{'train@nld.rst.nldt_loss': 2.896277904510498, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26430348258706465, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01306566650270536, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.00825948383084577, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03125, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.896277666091919, 'train@nld.rst.nldt_runtime': 19.3927, 'train@nld.rst.nldt_samples_per_second': 82.918, 'train@nld.rst.nldt_steps_per_second': 2.63, 'epoch': 2.0}
{'loss': 3.0461, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.8234703540802, 'eval_accuracy@nld.rst.nldt': 0.27794561933534745, 'eval_f1@nld.rst.nldt': 0.016110673321075214, 'eval_precision@nld.rst.nldt': 0.010294282197605462, 'eval_recall@nld.rst.nldt': 0.037037037037037035, 'eval_loss@nld.rst.nldt': 2.8234703540802, 'eval_runtime': 4.3017, 'eval_samples_per_second': 76.947, 'eval_steps_per_second': 2.557, 'epoch': 2.0}
{'train@nld.rst.nldt_loss': 2.799072265625, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.27238805970149255, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.019786516188007162, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.033093079128884755, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03499883286647993, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.799072504043579, 'train@nld.rst.nldt_runtime': 19.4365, 'train@nld.rst.nldt_samples_per_second': 82.731, 'train@nld.rst.nldt_steps_per_second': 2.624, 'epoch': 3.0}
{'loss': 2.8732, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.737579345703125, 'eval_accuracy@nld.rst.nldt': 0.28700906344410876, 'eval_f1@nld.rst.nldt': 0.02355543598481451, 'eval_precision@nld.rst.nldt': 0.03354266594141983, 'eval_recall@nld.rst.nldt': 0.04121105232216343, 'eval_loss@nld.rst.nldt': 2.737579584121704, 'eval_runtime': 4.3209, 'eval_samples_per_second': 76.605, 'eval_steps_per_second': 2.546, 'epoch': 3.0}
{'train@nld.rst.nldt_loss': 2.749847173690796, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.27922885572139305, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.025153596403596403, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.02665132783882784, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03929913632119514, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.749847173690796, 'train@nld.rst.nldt_runtime': 19.4791, 'train@nld.rst.nldt_samples_per_second': 82.55, 'train@nld.rst.nldt_steps_per_second': 2.618, 'epoch': 4.0}
{'loss': 2.7768, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.6949126720428467, 'eval_accuracy@nld.rst.nldt': 0.3021148036253776, 'eval_f1@nld.rst.nldt': 0.03670917729432358, 'eval_precision@nld.rst.nldt': 0.043244611170656186, 'eval_recall@nld.rst.nldt': 0.05215474273445287, 'eval_loss@nld.rst.nldt': 2.694912910461426, 'eval_runtime': 4.3129, 'eval_samples_per_second': 76.747, 'eval_steps_per_second': 2.55, 'epoch': 4.0}
{'train@nld.rst.nldt_loss': 2.7139785289764404, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2879353233830846, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.029510166145730626, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.026655916846996995, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.04440184407096172, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.7139785289764404, 'train@nld.rst.nldt_runtime': 19.4081, 'train@nld.rst.nldt_samples_per_second': 82.852, 'train@nld.rst.nldt_steps_per_second': 2.628, 'epoch': 5.0}
{'loss': 2.7506, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.662404775619507, 'eval_accuracy@nld.rst.nldt': 0.30513595166163143, 'eval_f1@nld.rst.nldt': 0.035996943951363686, 'eval_precision@nld.rst.nldt': 0.03494347802793335, 'eval_recall@nld.rst.nldt': 0.054212355903177155, 'eval_loss@nld.rst.nldt': 2.662404775619507, 'eval_runtime': 4.3, 'eval_samples_per_second': 76.977, 'eval_steps_per_second': 2.558, 'epoch': 5.0}
{'train@nld.rst.nldt_loss': 2.680293560028076, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2941542288557214, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03228933370660694, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.02688945053775952, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.04891981792717087, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6802937984466553, 'train@nld.rst.nldt_runtime': 19.4748, 'train@nld.rst.nldt_samples_per_second': 82.568, 'train@nld.rst.nldt_steps_per_second': 2.619, 'epoch': 6.0}
{'loss': 2.7207, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.6352217197418213, 'eval_accuracy@nld.rst.nldt': 0.31419939577039274, 'eval_f1@nld.rst.nldt': 0.03893298059964727, 'eval_precision@nld.rst.nldt': 0.03511180177846845, 'eval_recall@nld.rst.nldt': 0.060041407867494824, 'eval_loss@nld.rst.nldt': 2.6352219581604004, 'eval_runtime': 4.311, 'eval_samples_per_second': 76.781, 'eval_steps_per_second': 2.552, 'epoch': 6.0}
{'train@nld.rst.nldt_loss': 2.6519951820373535, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30286069651741293, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03455405885238193, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.0271500002915996, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.053265639589168995, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6519949436187744, 'train@nld.rst.nldt_runtime': 19.4524, 'train@nld.rst.nldt_samples_per_second': 82.663, 'train@nld.rst.nldt_steps_per_second': 2.622, 'epoch': 7.0}
{'loss': 2.6845, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.612539291381836, 'eval_accuracy@nld.rst.nldt': 0.3202416918429003, 'eval_f1@nld.rst.nldt': 0.04017001269476939, 'eval_precision@nld.rst.nldt': 0.03466743920013074, 'eval_recall@nld.rst.nldt': 0.06315722209442017, 'eval_loss@nld.rst.nldt': 2.612539291381836, 'eval_runtime': 4.3091, 'eval_samples_per_second': 76.814, 'eval_steps_per_second': 2.553, 'epoch': 7.0}
{'train@nld.rst.nldt_loss': 2.629514217376709, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30783582089552236, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.0359330819460122, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.027106176023159623, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.057171451914098975, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.629514217376709, 'train@nld.rst.nldt_runtime': 19.4597, 'train@nld.rst.nldt_samples_per_second': 82.632, 'train@nld.rst.nldt_steps_per_second': 2.621, 'epoch': 8.0}
{'loss': 2.6691, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.595667839050293, 'eval_accuracy@nld.rst.nldt': 0.3323262839879154, 'eval_f1@nld.rst.nldt': 0.04707713080698149, 'eval_precision@nld.rst.nldt': 0.07453344817685903, 'eval_recall@nld.rst.nldt': 0.07074994248907292, 'eval_loss@nld.rst.nldt': 2.595668077468872, 'eval_runtime': 4.2917, 'eval_samples_per_second': 77.125, 'eval_steps_per_second': 2.563, 'epoch': 8.0}
{'train@nld.rst.nldt_loss': 2.616117000579834, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30907960199004975, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03649909692956051, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.027467973470704055, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05784197012138188, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.616117000579834, 'train@nld.rst.nldt_runtime': 19.5017, 'train@nld.rst.nldt_samples_per_second': 82.455, 'train@nld.rst.nldt_steps_per_second': 2.615, 'epoch': 9.0}
{'loss': 2.6468, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.584118127822876, 'eval_accuracy@nld.rst.nldt': 0.33534743202416917, 'eval_f1@nld.rst.nldt': 0.04767454819569942, 'eval_precision@nld.rst.nldt': 0.07508353398279954, 'eval_recall@nld.rst.nldt': 0.07115251897860594, 'eval_loss@nld.rst.nldt': 2.584118366241455, 'eval_runtime': 4.3149, 'eval_samples_per_second': 76.71, 'eval_steps_per_second': 2.549, 'epoch': 9.0}
{'train@nld.rst.nldt_loss': 2.606750249862671, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30970149253731344, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03675969243078015, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.02781688889819802, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05791549953314659, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.606750249862671, 'train@nld.rst.nldt_runtime': 19.4617, 'train@nld.rst.nldt_samples_per_second': 82.624, 'train@nld.rst.nldt_steps_per_second': 2.621, 'epoch': 10.0}
{'loss': 2.6343, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.575843095779419, 'eval_accuracy@nld.rst.nldt': 0.3323262839879154, 'eval_f1@nld.rst.nldt': 0.04660615169283487, 'eval_precision@nld.rst.nldt': 0.07438271604938272, 'eval_recall@nld.rst.nldt': 0.06909490580988166, 'eval_loss@nld.rst.nldt': 2.575843095779419, 'eval_runtime': 4.3031, 'eval_samples_per_second': 76.922, 'eval_steps_per_second': 2.556, 'epoch': 10.0}
{'train@nld.rst.nldt_loss': 2.5998504161834717, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31156716417910446, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03713147351703259, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.027882074658166667, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05883403361344537, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.59985089302063, 'train@nld.rst.nldt_runtime': 19.4855, 'train@nld.rst.nldt_samples_per_second': 82.523, 'train@nld.rst.nldt_steps_per_second': 2.617, 'epoch': 11.0}
{'loss': 2.6272, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.570558547973633, 'eval_accuracy@nld.rst.nldt': 0.33534743202416917, 'eval_f1@nld.rst.nldt': 0.04757945458019149, 'eval_precision@nld.rst.nldt': 0.05661125267969098, 'eval_recall@nld.rst.nldt': 0.07115251897860594, 'eval_loss@nld.rst.nldt': 2.5705580711364746, 'eval_runtime': 4.3227, 'eval_samples_per_second': 76.573, 'eval_steps_per_second': 2.545, 'epoch': 11.0}
{'train@nld.rst.nldt_loss': 2.596985101699829, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31281094527363185, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03740098992031231, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.028028377323949097, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05938842203548085, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.596985101699829, 'train@nld.rst.nldt_runtime': 19.4293, 'train@nld.rst.nldt_samples_per_second': 82.761, 'train@nld.rst.nldt_steps_per_second': 2.625, 'epoch': 12.0}
{'loss': 2.6275, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.568855047225952, 'eval_accuracy@nld.rst.nldt': 0.3323262839879154, 'eval_f1@nld.rst.nldt': 0.04698203479665391, 'eval_precision@nld.rst.nldt': 0.056061341985454594, 'eval_recall@nld.rst.nldt': 0.07074994248907292, 'eval_loss@nld.rst.nldt': 2.5688552856445312, 'eval_runtime': 4.3218, 'eval_samples_per_second': 76.588, 'eval_steps_per_second': 2.545, 'epoch': 12.0}
{'train_runtime': 773.4334, 'train_samples_per_second': 24.948, 'train_steps_per_second': 0.791, 'train_loss': 2.796691844665926, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.7967
  train_runtime            = 0:12:53.43
  train_samples_per_second =     24.948
  train_steps_per_second   =      0.791
{'train@rus.rst.rrt_loss': 1.6866142749786377, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5027756574838665, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.18557653971253538, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.23117460795802874, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.19962014212336587, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.6866141557693481, 'train@rus.rst.rrt_runtime': 342.8799, 'train@rus.rst.rrt_samples_per_second': 84.059, 'train@rus.rst.rrt_steps_per_second': 2.628, 'epoch': 1.0}
{'loss': 2.1259, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.7314950227737427, 'eval_accuracy@rus.rst.rrt': 0.48056042031523644, 'eval_f1@rus.rst.rrt': 0.2019475456485548, 'eval_precision@rus.rst.rrt': 0.2025673931415329, 'eval_recall@rus.rst.rrt': 0.2179111962298046, 'eval_loss@rus.rst.rrt': 1.7314950227737427, 'eval_runtime': 34.3597, 'eval_samples_per_second': 83.091, 'eval_steps_per_second': 2.619, 'epoch': 1.0}
{'train@rus.rst.rrt_loss': 1.4862090349197388, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5480535701894387, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.23587093596035694, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.3172226203550674, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.23921300742502705, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.4862089157104492, 'train@rus.rst.rrt_runtime': 342.6544, 'train@rus.rst.rrt_samples_per_second': 84.114, 'train@rus.rst.rrt_steps_per_second': 2.629, 'epoch': 2.0}
{'loss': 1.6211, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.5554465055465698, 'eval_accuracy@rus.rst.rrt': 0.5260945709281961, 'eval_f1@rus.rst.rrt': 0.2627136093268297, 'eval_precision@rus.rst.rrt': 0.3611998366323361, 'eval_recall@rus.rst.rrt': 0.26607942217318536, 'eval_loss@rus.rst.rrt': 1.5554465055465698, 'eval_runtime': 34.3368, 'eval_samples_per_second': 83.147, 'eval_steps_per_second': 2.621, 'epoch': 2.0}
{'train@rus.rst.rrt_loss': 1.4142566919326782, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5681423912289224, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.2838597528468742, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.41372635063625457, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.27777234942030804, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.4142565727233887, 'train@rus.rst.rrt_runtime': 342.9191, 'train@rus.rst.rrt_samples_per_second': 84.049, 'train@rus.rst.rrt_steps_per_second': 2.627, 'epoch': 3.0}
{'loss': 1.4991, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.4871165752410889, 'eval_accuracy@rus.rst.rrt': 0.5418563922942207, 'eval_f1@rus.rst.rrt': 0.3114899537239946, 'eval_precision@rus.rst.rrt': 0.4331354224565695, 'eval_recall@rus.rst.rrt': 0.3057344874902484, 'eval_loss@rus.rst.rrt': 1.4871166944503784, 'eval_runtime': 34.3326, 'eval_samples_per_second': 83.157, 'eval_steps_per_second': 2.621, 'epoch': 3.0}
{'train@rus.rst.rrt_loss': 1.364701509475708, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5832003330788981, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.31785879598287775, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.43891800217853644, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.2996739992310784, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3647016286849976, 'train@rus.rst.rrt_runtime': 343.147, 'train@rus.rst.rrt_samples_per_second': 83.993, 'train@rus.rst.rrt_steps_per_second': 2.626, 'epoch': 4.0}
{'loss': 1.4401, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.4414868354797363, 'eval_accuracy@rus.rst.rrt': 0.5611208406304729, 'eval_f1@rus.rst.rrt': 0.36402717025646814, 'eval_precision@rus.rst.rrt': 0.4826184176576913, 'eval_recall@rus.rst.rrt': 0.3476934979857, 'eval_loss@rus.rst.rrt': 1.4414868354797363, 'eval_runtime': 34.3589, 'eval_samples_per_second': 83.093, 'eval_steps_per_second': 2.619, 'epoch': 4.0}
{'train@rus.rst.rrt_loss': 1.332513689994812, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5911803483450142, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3331762195581013, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4415542401455896, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3148732031562594, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.332513689994812, 'train@rus.rst.rrt_runtime': 342.9727, 'train@rus.rst.rrt_samples_per_second': 84.036, 'train@rus.rst.rrt_steps_per_second': 2.627, 'epoch': 5.0}
{'loss': 1.4016, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.417763352394104, 'eval_accuracy@rus.rst.rrt': 0.5607705779334501, 'eval_f1@rus.rst.rrt': 0.3768973259797699, 'eval_precision@rus.rst.rrt': 0.475982486578496, 'eval_recall@rus.rst.rrt': 0.3623933568937714, 'eval_loss@rus.rst.rrt': 1.417763113975525, 'eval_runtime': 34.3112, 'eval_samples_per_second': 83.209, 'eval_steps_per_second': 2.623, 'epoch': 5.0}
{'train@rus.rst.rrt_loss': 1.3098995685577393, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5959336617861356, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3438840989281071, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4880213757238281, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3244667431869683, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3098995685577393, 'train@rus.rst.rrt_runtime': 342.8911, 'train@rus.rst.rrt_samples_per_second': 84.056, 'train@rus.rst.rrt_steps_per_second': 2.628, 'epoch': 6.0}
{'loss': 1.3763, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.3980885744094849, 'eval_accuracy@rus.rst.rrt': 0.5709281961471103, 'eval_f1@rus.rst.rrt': 0.38818649735760646, 'eval_precision@rus.rst.rrt': 0.47275227376661566, 'eval_recall@rus.rst.rrt': 0.37380527924136897, 'eval_loss@rus.rst.rrt': 1.3980885744094849, 'eval_runtime': 34.302, 'eval_samples_per_second': 83.231, 'eval_steps_per_second': 2.624, 'epoch': 6.0}
{'train@rus.rst.rrt_loss': 1.29524564743042, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6014502810353203, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.35389185376197024, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.45488058237572754, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3351720463709764, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2952455282211304, 'train@rus.rst.rrt_runtime': 347.2676, 'train@rus.rst.rrt_samples_per_second': 82.997, 'train@rus.rst.rrt_steps_per_second': 2.595, 'epoch': 7.0}
{'loss': 1.3548, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.3890576362609863, 'eval_accuracy@rus.rst.rrt': 0.5740805604203152, 'eval_f1@rus.rst.rrt': 0.40224883201138356, 'eval_precision@rus.rst.rrt': 0.489674191069881, 'eval_recall@rus.rst.rrt': 0.38832351129502696, 'eval_loss@rus.rst.rrt': 1.3890576362609863, 'eval_runtime': 34.5539, 'eval_samples_per_second': 82.624, 'eval_steps_per_second': 2.605, 'epoch': 7.0}
{'train@rus.rst.rrt_loss': 1.2830891609191895, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6038442856151551, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3604301260636773, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4374360230443713, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34382789137549363, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2830891609191895, 'train@rus.rst.rrt_runtime': 342.3993, 'train@rus.rst.rrt_samples_per_second': 84.177, 'train@rus.rst.rrt_steps_per_second': 2.631, 'epoch': 8.0}
{'loss': 1.3429, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.3772058486938477, 'eval_accuracy@rus.rst.rrt': 0.5772329246935202, 'eval_f1@rus.rst.rrt': 0.4080602026078082, 'eval_precision@rus.rst.rrt': 0.47631335793994173, 'eval_recall@rus.rst.rrt': 0.399339015655813, 'eval_loss@rus.rst.rrt': 1.3772058486938477, 'eval_runtime': 34.338, 'eval_samples_per_second': 83.144, 'eval_steps_per_second': 2.621, 'epoch': 8.0}
{'train@rus.rst.rrt_loss': 1.2707722187042236, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.605926028728055, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36361060228617453, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4468123173062482, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34502091881943203, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2707722187042236, 'train@rus.rst.rrt_runtime': 342.4212, 'train@rus.rst.rrt_samples_per_second': 84.171, 'train@rus.rst.rrt_steps_per_second': 2.631, 'epoch': 9.0}
{'loss': 1.331, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.3703557252883911, 'eval_accuracy@rus.rst.rrt': 0.5761821366024519, 'eval_f1@rus.rst.rrt': 0.4111292597799741, 'eval_precision@rus.rst.rrt': 0.4845111494019162, 'eval_recall@rus.rst.rrt': 0.39852788646931536, 'eval_loss@rus.rst.rrt': 1.3703557252883911, 'eval_runtime': 34.3212, 'eval_samples_per_second': 83.185, 'eval_steps_per_second': 2.622, 'epoch': 9.0}
{'train@rus.rst.rrt_loss': 1.2651032209396362, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6076261189369232, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36534166663186096, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.45152204710298066, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34459346623326853, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2651032209396362, 'train@rus.rst.rrt_runtime': 342.426, 'train@rus.rst.rrt_samples_per_second': 84.17, 'train@rus.rst.rrt_steps_per_second': 2.631, 'epoch': 10.0}
{'loss': 1.3212, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.368199348449707, 'eval_accuracy@rus.rst.rrt': 0.578984238178634, 'eval_f1@rus.rst.rrt': 0.4130110091534533, 'eval_precision@rus.rst.rrt': 0.4898717387886469, 'eval_recall@rus.rst.rrt': 0.39797716342428835, 'eval_loss@rus.rst.rrt': 1.368199348449707, 'eval_runtime': 34.3069, 'eval_samples_per_second': 83.219, 'eval_steps_per_second': 2.623, 'epoch': 10.0}
{'train@rus.rst.rrt_loss': 1.2611809968948364, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6080424675595032, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36709141232860726, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4456229969442828, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34833529860760987, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2611808776855469, 'train@rus.rst.rrt_runtime': 342.6, 'train@rus.rst.rrt_samples_per_second': 84.127, 'train@rus.rst.rrt_steps_per_second': 2.63, 'epoch': 11.0}
{'loss': 1.3153, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.3671447038650513, 'eval_accuracy@rus.rst.rrt': 0.5779334500875657, 'eval_f1@rus.rst.rrt': 0.4169155401086842, 'eval_precision@rus.rst.rrt': 0.48686781286369446, 'eval_recall@rus.rst.rrt': 0.40346354763391185, 'eval_loss@rus.rst.rrt': 1.3671447038650513, 'eval_runtime': 34.3107, 'eval_samples_per_second': 83.21, 'eval_steps_per_second': 2.623, 'epoch': 11.0}
{'train@rus.rst.rrt_loss': 1.2598227262496948, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6083894247449865, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36783920025096567, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.44764330273548913, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34924988691198017, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2598227262496948, 'train@rus.rst.rrt_runtime': 342.7422, 'train@rus.rst.rrt_samples_per_second': 84.092, 'train@rus.rst.rrt_steps_per_second': 2.629, 'epoch': 12.0}
{'loss': 1.3113, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.3655518293380737, 'eval_accuracy@rus.rst.rrt': 0.5796847635726795, 'eval_f1@rus.rst.rrt': 0.41604862202375203, 'eval_precision@rus.rst.rrt': 0.4878918040802948, 'eval_recall@rus.rst.rrt': 0.4033536299882603, 'eval_loss@rus.rst.rrt': 1.3655515909194946, 'eval_runtime': 34.3259, 'eval_samples_per_second': 83.173, 'eval_steps_per_second': 2.622, 'epoch': 12.0}
{'train_runtime': 13278.0044, 'train_samples_per_second': 26.048, 'train_steps_per_second': 0.814, 'train_loss': 1.453383671192165, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.7967
  train_runtime            = 0:12:53.43
  train_samples_per_second =     24.948
  train_steps_per_second   =      0.791
-------------------------------------------------------------------
Lang1:  por.rst.cstn    Lang2:  rus.rst.rrt
Saving run to:  runs/full_shot/FullShot=v4_por.rst.cstn_rus.rst.rrt_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 4148 examples
read 573 examples
read 272 examples
read 28822 examples
read 2855 examples
read 2843 examples
Total prediction labels:  37
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (pt): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (ru): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (pt): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=37, bias=True)
    )
  )
)
{'train@por.rst.cstn_loss': 2.4717140197753906, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.2774831243972999, 'train@por.rst.cstn_f1@por.rst.cstn': 0.013575674655595393, 'train@por.rst.cstn_precision@por.rst.cstn': 0.008671347637415621, 'train@por.rst.cstn_recall@por.rst.cstn': 0.03125, 'train@por.rst.cstn_loss@por.rst.cstn': 2.4717140197753906, 'train@por.rst.cstn_runtime': 49.5029, 'train@por.rst.cstn_samples_per_second': 83.793, 'train@por.rst.cstn_steps_per_second': 2.626, 'epoch': 1.0}
{'loss': 2.9157, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.5961949825286865, 'eval_accuracy@por.rst.cstn': 0.28097731239092494, 'eval_f1@por.rst.cstn': 0.019940549913301957, 'eval_precision@por.rst.cstn': 0.012771696017769315, 'eval_recall@por.rst.cstn': 0.045454545454545456, 'eval_loss@por.rst.cstn': 2.5961952209472656, 'eval_runtime': 7.1571, 'eval_samples_per_second': 80.06, 'eval_steps_per_second': 2.515, 'epoch': 1.0}
{'train@por.rst.cstn_loss': 2.178529739379883, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.3907907425265188, 'train@por.rst.cstn_f1@por.rst.cstn': 0.05905217148665774, 'train@por.rst.cstn_precision@por.rst.cstn': 0.06897785553857552, 'train@por.rst.cstn_recall@por.rst.cstn': 0.06808284549643347, 'train@por.rst.cstn_loss@por.rst.cstn': 2.1785295009613037, 'train@por.rst.cstn_runtime': 49.5872, 'train@por.rst.cstn_samples_per_second': 83.651, 'train@por.rst.cstn_steps_per_second': 2.622, 'epoch': 2.0}
{'loss': 2.3544, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.3066959381103516, 'eval_accuracy@por.rst.cstn': 0.33856893542757416, 'eval_f1@por.rst.cstn': 0.0805266877481932, 'eval_precision@por.rst.cstn': 0.08205407823728433, 'eval_recall@por.rst.cstn': 0.09739190322194274, 'eval_loss@por.rst.cstn': 2.3066959381103516, 'eval_runtime': 7.1413, 'eval_samples_per_second': 80.237, 'eval_steps_per_second': 2.521, 'epoch': 2.0}
{'train@por.rst.cstn_loss': 1.9409822225570679, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.4710703953712633, 'train@por.rst.cstn_f1@por.rst.cstn': 0.07483629584466484, 'train@por.rst.cstn_precision@por.rst.cstn': 0.10109313306720158, 'train@por.rst.cstn_recall@por.rst.cstn': 0.09112104922943472, 'train@por.rst.cstn_loss@por.rst.cstn': 1.940982460975647, 'train@por.rst.cstn_runtime': 49.5847, 'train@por.rst.cstn_samples_per_second': 83.655, 'train@por.rst.cstn_steps_per_second': 2.622, 'epoch': 3.0}
{'loss': 2.1139, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.078976631164551, 'eval_accuracy@por.rst.cstn': 0.38394415357766143, 'eval_f1@por.rst.cstn': 0.09338648115412372, 'eval_precision@por.rst.cstn': 0.08809306824809941, 'eval_recall@por.rst.cstn': 0.1285895749074447, 'eval_loss@por.rst.cstn': 2.07897686958313, 'eval_runtime': 7.1694, 'eval_samples_per_second': 79.922, 'eval_steps_per_second': 2.511, 'epoch': 3.0}
{'train@por.rst.cstn_loss': 1.797484040260315, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5320636451301832, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1072276237100636, 'train@por.rst.cstn_precision@por.rst.cstn': 0.16162702050327016, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1197534548748561, 'train@por.rst.cstn_loss@por.rst.cstn': 1.7974839210510254, 'train@por.rst.cstn_runtime': 49.5178, 'train@por.rst.cstn_samples_per_second': 83.768, 'train@por.rst.cstn_steps_per_second': 2.625, 'epoch': 4.0}
{'loss': 1.9314, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.9456367492675781, 'eval_accuracy@por.rst.cstn': 0.4432809773123909, 'eval_f1@por.rst.cstn': 0.14068918463659774, 'eval_precision@por.rst.cstn': 0.12769919667178914, 'eval_recall@por.rst.cstn': 0.16964407665468617, 'eval_loss@por.rst.cstn': 1.9456369876861572, 'eval_runtime': 7.1569, 'eval_samples_per_second': 80.063, 'eval_steps_per_second': 2.515, 'epoch': 4.0}
{'train@por.rst.cstn_loss': 1.7036932706832886, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5426711668273867, 'train@por.rst.cstn_f1@por.rst.cstn': 0.11420843758111839, 'train@por.rst.cstn_precision@por.rst.cstn': 0.12838253441465847, 'train@por.rst.cstn_recall@por.rst.cstn': 0.129233271375537, 'train@por.rst.cstn_loss@por.rst.cstn': 1.703693151473999, 'train@por.rst.cstn_runtime': 49.4981, 'train@por.rst.cstn_samples_per_second': 83.801, 'train@por.rst.cstn_steps_per_second': 2.626, 'epoch': 5.0}
{'loss': 1.813, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.8488675355911255, 'eval_accuracy@por.rst.cstn': 0.46596858638743455, 'eval_f1@por.rst.cstn': 0.16222815902553755, 'eval_precision@por.rst.cstn': 0.17686455254497396, 'eval_recall@por.rst.cstn': 0.18455047368090846, 'eval_loss@por.rst.cstn': 1.848867416381836, 'eval_runtime': 7.1389, 'eval_samples_per_second': 80.264, 'eval_steps_per_second': 2.521, 'epoch': 5.0}
{'train@por.rst.cstn_loss': 1.645878791809082, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.562198649951784, 'train@por.rst.cstn_f1@por.rst.cstn': 0.12327913842340235, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13200578002891245, 'train@por.rst.cstn_recall@por.rst.cstn': 0.13735760540362363, 'train@por.rst.cstn_loss@por.rst.cstn': 1.6458786725997925, 'train@por.rst.cstn_runtime': 49.4883, 'train@por.rst.cstn_samples_per_second': 83.818, 'train@por.rst.cstn_steps_per_second': 2.627, 'epoch': 6.0}
{'loss': 1.7318, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.7866886854171753, 'eval_accuracy@por.rst.cstn': 0.47469458987783597, 'eval_f1@por.rst.cstn': 0.1675135439038104, 'eval_precision@por.rst.cstn': 0.16988952605835725, 'eval_recall@por.rst.cstn': 0.18988413061639609, 'eval_loss@por.rst.cstn': 1.7866885662078857, 'eval_runtime': 7.1343, 'eval_samples_per_second': 80.317, 'eval_steps_per_second': 2.523, 'epoch': 6.0}
{'train@por.rst.cstn_loss': 1.6031767129898071, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5759402121504339, 'train@por.rst.cstn_f1@por.rst.cstn': 0.13248857373899534, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13338498629879542, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14517424935590323, 'train@por.rst.cstn_loss@por.rst.cstn': 1.6031765937805176, 'train@por.rst.cstn_runtime': 49.6254, 'train@por.rst.cstn_samples_per_second': 83.586, 'train@por.rst.cstn_steps_per_second': 2.62, 'epoch': 7.0}
{'loss': 1.6829, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.7445260286331177, 'eval_accuracy@por.rst.cstn': 0.493891797556719, 'eval_f1@por.rst.cstn': 0.18027541390731006, 'eval_precision@por.rst.cstn': 0.17993960452094424, 'eval_recall@por.rst.cstn': 0.20215638959658097, 'eval_loss@por.rst.cstn': 1.7445257902145386, 'eval_runtime': 7.1486, 'eval_samples_per_second': 80.156, 'eval_steps_per_second': 2.518, 'epoch': 7.0}
{'train@por.rst.cstn_loss': 1.5762971639633179, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.583413693346191, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1364765209471672, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13573907482404263, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14843156760032555, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5762970447540283, 'train@por.rst.cstn_runtime': 49.6042, 'train@por.rst.cstn_samples_per_second': 83.622, 'train@por.rst.cstn_steps_per_second': 2.621, 'epoch': 8.0}
{'loss': 1.6483, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.719840168952942, 'eval_accuracy@por.rst.cstn': 0.49912739965095987, 'eval_f1@por.rst.cstn': 0.18129122918123197, 'eval_precision@por.rst.cstn': 0.18078820692227612, 'eval_recall@por.rst.cstn': 0.20249147392376016, 'eval_loss@por.rst.cstn': 1.719840168952942, 'eval_runtime': 7.1486, 'eval_samples_per_second': 80.156, 'eval_steps_per_second': 2.518, 'epoch': 8.0}
{'train@por.rst.cstn_loss': 1.5573501586914062, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5906460945033751, 'train@por.rst.cstn_f1@por.rst.cstn': 0.14123887291002057, 'train@por.rst.cstn_precision@por.rst.cstn': 0.1390571235423562, 'train@por.rst.cstn_recall@por.rst.cstn': 0.15216262693048432, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5573502779006958, 'train@por.rst.cstn_runtime': 49.6254, 'train@por.rst.cstn_samples_per_second': 83.586, 'train@por.rst.cstn_steps_per_second': 2.62, 'epoch': 9.0}
{'loss': 1.623, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.700939655303955, 'eval_accuracy@por.rst.cstn': 0.5026178010471204, 'eval_f1@por.rst.cstn': 0.18470254275785544, 'eval_precision@por.rst.cstn': 0.18317792301648766, 'eval_recall@por.rst.cstn': 0.20375873433133676, 'eval_loss@por.rst.cstn': 1.7009395360946655, 'eval_runtime': 7.1453, 'eval_samples_per_second': 80.193, 'eval_steps_per_second': 2.519, 'epoch': 9.0}
{'train@por.rst.cstn_loss': 1.5404701232910156, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.589440694310511, 'train@por.rst.cstn_f1@por.rst.cstn': 0.14054757664511922, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13442750002860895, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1535122292366795, 'train@por.rst.cstn_loss@por.rst.cstn': 1.540470004081726, 'train@por.rst.cstn_runtime': 49.6621, 'train@por.rst.cstn_samples_per_second': 83.524, 'train@por.rst.cstn_steps_per_second': 2.618, 'epoch': 10.0}
{'loss': 1.5985, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.6882216930389404, 'eval_accuracy@por.rst.cstn': 0.5130890052356021, 'eval_f1@por.rst.cstn': 0.1893579272298086, 'eval_precision@por.rst.cstn': 0.1828891072348634, 'eval_recall@por.rst.cstn': 0.2097226875128893, 'eval_loss@por.rst.cstn': 1.6882215738296509, 'eval_runtime': 7.1758, 'eval_samples_per_second': 79.851, 'eval_steps_per_second': 2.508, 'epoch': 10.0}
{'train@por.rst.cstn_loss': 1.5318454504013062, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5947444551591128, 'train@por.rst.cstn_f1@por.rst.cstn': 0.142925824670657, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13639762652341747, 'train@por.rst.cstn_recall@por.rst.cstn': 0.15573651874696937, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5318453311920166, 'train@por.rst.cstn_runtime': 49.5666, 'train@por.rst.cstn_samples_per_second': 83.685, 'train@por.rst.cstn_steps_per_second': 2.623, 'epoch': 11.0}
{'loss': 1.5948, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.6829718351364136, 'eval_accuracy@por.rst.cstn': 0.5113438045375218, 'eval_f1@por.rst.cstn': 0.19563336036881357, 'eval_precision@por.rst.cstn': 0.22645837985731634, 'eval_recall@por.rst.cstn': 0.21274977103196369, 'eval_loss@por.rst.cstn': 1.6829719543457031, 'eval_runtime': 7.1456, 'eval_samples_per_second': 80.189, 'eval_steps_per_second': 2.519, 'epoch': 11.0}
{'train@por.rst.cstn_loss': 1.5294235944747925, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5947444551591128, 'train@por.rst.cstn_f1@por.rst.cstn': 0.14312005936025807, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13640158642491781, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1557423017801723, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5294235944747925, 'train@por.rst.cstn_runtime': 49.4374, 'train@por.rst.cstn_samples_per_second': 83.904, 'train@por.rst.cstn_steps_per_second': 2.63, 'epoch': 12.0}
{'loss': 1.5857, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.6792411804199219, 'eval_accuracy@por.rst.cstn': 0.5165794066317626, 'eval_f1@por.rst.cstn': 0.197352586466216, 'eval_precision@por.rst.cstn': 0.22812876665031334, 'eval_recall@por.rst.cstn': 0.2142612977827249, 'eval_loss@por.rst.cstn': 1.679241418838501, 'eval_runtime': 7.1373, 'eval_samples_per_second': 80.283, 'eval_steps_per_second': 2.522, 'epoch': 12.0}
{'train_runtime': 1944.7628, 'train_samples_per_second': 25.595, 'train_steps_per_second': 0.802, 'train_loss': 1.8827964489276592, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.8828
  train_runtime            = 0:32:24.76
  train_samples_per_second =     25.595
  train_steps_per_second   =      0.802
{'train@rus.rst.rrt_loss': 1.6263445615768433, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5179376864894872, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.20188704445043149, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.2419594501326058, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.21647327326870547, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.6263446807861328, 'train@rus.rst.rrt_runtime': 342.6043, 'train@rus.rst.rrt_samples_per_second': 84.126, 'train@rus.rst.rrt_steps_per_second': 2.63, 'epoch': 1.0}
{'loss': 2.0047, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.6782406568527222, 'eval_accuracy@rus.rst.rrt': 0.4949211908931699, 'eval_f1@rus.rst.rrt': 0.22236178295618056, 'eval_precision@rus.rst.rrt': 0.2499190787012168, 'eval_recall@rus.rst.rrt': 0.23955293720616755, 'eval_loss@rus.rst.rrt': 1.678240418434143, 'eval_runtime': 34.2313, 'eval_samples_per_second': 83.403, 'eval_steps_per_second': 2.629, 'epoch': 1.0}
{'train@rus.rst.rrt_loss': 1.4728840589523315, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5504822704878218, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.2409733813996014, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.3513319439220985, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.2455809712662175, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.4728840589523315, 'train@rus.rst.rrt_runtime': 342.4873, 'train@rus.rst.rrt_samples_per_second': 84.155, 'train@rus.rst.rrt_steps_per_second': 2.631, 'epoch': 2.0}
{'loss': 1.5956, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.5490684509277344, 'eval_accuracy@rus.rst.rrt': 0.5274956217162872, 'eval_f1@rus.rst.rrt': 0.2598315113155511, 'eval_precision@rus.rst.rrt': 0.2979932976281835, 'eval_recall@rus.rst.rrt': 0.2674394235280281, 'eval_loss@rus.rst.rrt': 1.549068570137024, 'eval_runtime': 34.2105, 'eval_samples_per_second': 83.454, 'eval_steps_per_second': 2.631, 'epoch': 2.0}
{'train@rus.rst.rrt_loss': 1.4078762531280518, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5719242245506905, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.30270916737299336, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.43661786952759535, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.29104609082956145, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.4078762531280518, 'train@rus.rst.rrt_runtime': 342.4913, 'train@rus.rst.rrt_samples_per_second': 84.154, 'train@rus.rst.rrt_steps_per_second': 2.631, 'epoch': 3.0}
{'loss': 1.4925, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.4863404035568237, 'eval_accuracy@rus.rst.rrt': 0.5418563922942207, 'eval_f1@rus.rst.rrt': 0.3442292462311832, 'eval_precision@rus.rst.rrt': 0.4315323032054185, 'eval_recall@rus.rst.rrt': 0.33284614092709247, 'eval_loss@rus.rst.rrt': 1.4863406419754028, 'eval_runtime': 34.2196, 'eval_samples_per_second': 83.432, 'eval_steps_per_second': 2.63, 'epoch': 3.0}
{'train@rus.rst.rrt_loss': 1.3593029975891113, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5866352092151829, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3252293965088599, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.43336629941859767, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.30662817936719744, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3593029975891113, 'train@rus.rst.rrt_runtime': 342.3783, 'train@rus.rst.rrt_samples_per_second': 84.182, 'train@rus.rst.rrt_steps_per_second': 2.632, 'epoch': 4.0}
{'loss': 1.4398, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.4432355165481567, 'eval_accuracy@rus.rst.rrt': 0.5558669001751313, 'eval_f1@rus.rst.rrt': 0.36724193384331516, 'eval_precision@rus.rst.rrt': 0.4440345151974524, 'eval_recall@rus.rst.rrt': 0.35093324016896504, 'eval_loss@rus.rst.rrt': 1.4432357549667358, 'eval_runtime': 34.2292, 'eval_samples_per_second': 83.408, 'eval_steps_per_second': 2.629, 'epoch': 4.0}
{'train@rus.rst.rrt_loss': 1.3280456066131592, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5927763513982375, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3377000128479391, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4355219506145671, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.31934715541155756, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3280456066131592, 'train@rus.rst.rrt_runtime': 342.6273, 'train@rus.rst.rrt_samples_per_second': 84.121, 'train@rus.rst.rrt_steps_per_second': 2.63, 'epoch': 5.0}
{'loss': 1.4034, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.4208890199661255, 'eval_accuracy@rus.rst.rrt': 0.5600700525394046, 'eval_f1@rus.rst.rrt': 0.37828847541486166, 'eval_precision@rus.rst.rrt': 0.4592767147920517, 'eval_recall@rus.rst.rrt': 0.3624155723700734, 'eval_loss@rus.rst.rrt': 1.420888900756836, 'eval_runtime': 36.6408, 'eval_samples_per_second': 77.919, 'eval_steps_per_second': 2.456, 'epoch': 5.0}
{'train@rus.rst.rrt_loss': 1.3079689741134644, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5987440149885505, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3475205656886336, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4485446707022428, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3272538800659191, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3079688549041748, 'train@rus.rst.rrt_runtime': 342.8119, 'train@rus.rst.rrt_samples_per_second': 84.075, 'train@rus.rst.rrt_steps_per_second': 2.628, 'epoch': 6.0}
{'loss': 1.3736, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.405308723449707, 'eval_accuracy@rus.rst.rrt': 0.5677758318739055, 'eval_f1@rus.rst.rrt': 0.3920015875581365, 'eval_precision@rus.rst.rrt': 0.4691206953288686, 'eval_recall@rus.rst.rrt': 0.37596506182420014, 'eval_loss@rus.rst.rrt': 1.4053089618682861, 'eval_runtime': 34.338, 'eval_samples_per_second': 83.144, 'eval_steps_per_second': 2.621, 'epoch': 6.0}
{'train@rus.rst.rrt_loss': 1.2922310829162598, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6038789813337034, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.35831439639178525, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.44309721159540183, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3388816174296584, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2922312021255493, 'train@rus.rst.rrt_runtime': 342.627, 'train@rus.rst.rrt_samples_per_second': 84.121, 'train@rus.rst.rrt_steps_per_second': 2.63, 'epoch': 7.0}
{'loss': 1.3578, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.3964576721191406, 'eval_accuracy@rus.rst.rrt': 0.5642732049036777, 'eval_f1@rus.rst.rrt': 0.3982749029145035, 'eval_precision@rus.rst.rrt': 0.4637566415529921, 'eval_recall@rus.rst.rrt': 0.3836831143261243, 'eval_loss@rus.rst.rrt': 1.3964577913284302, 'eval_runtime': 34.3373, 'eval_samples_per_second': 83.146, 'eval_steps_per_second': 2.621, 'epoch': 7.0}
{'train@rus.rst.rrt_loss': 1.2786798477172852, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.605717854416765, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36576321421651536, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4360465457584121, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34760326048772655, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2786799669265747, 'train@rus.rst.rrt_runtime': 342.5202, 'train@rus.rst.rrt_samples_per_second': 84.147, 'train@rus.rst.rrt_steps_per_second': 2.631, 'epoch': 8.0}
{'loss': 1.3421, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.3840779066085815, 'eval_accuracy@rus.rst.rrt': 0.5712784588441331, 'eval_f1@rus.rst.rrt': 0.4108370824510706, 'eval_precision@rus.rst.rrt': 0.4624038551388534, 'eval_recall@rus.rst.rrt': 0.3983461971138546, 'eval_loss@rus.rst.rrt': 1.3840779066085815, 'eval_runtime': 34.2775, 'eval_samples_per_second': 83.291, 'eval_steps_per_second': 2.626, 'epoch': 8.0}
{'train@rus.rst.rrt_loss': 1.268437147140503, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6090486433974047, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36806728483077733, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4389484920152347, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3497206137290411, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2684370279312134, 'train@rus.rst.rrt_runtime': 342.5642, 'train@rus.rst.rrt_samples_per_second': 84.136, 'train@rus.rst.rrt_steps_per_second': 2.63, 'epoch': 9.0}
{'loss': 1.3309, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.3790901899337769, 'eval_accuracy@rus.rst.rrt': 0.5705779334500876, 'eval_f1@rus.rst.rrt': 0.41232503840135193, 'eval_precision@rus.rst.rrt': 0.4654989302162205, 'eval_recall@rus.rst.rrt': 0.3981027368996417, 'eval_loss@rus.rst.rrt': 1.3790903091430664, 'eval_runtime': 34.2908, 'eval_samples_per_second': 83.259, 'eval_steps_per_second': 2.625, 'epoch': 9.0}
{'train@rus.rst.rrt_loss': 1.2619779109954834, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6107140378877246, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3700607028631111, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.44510502945536995, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3490810957660484, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2619779109954834, 'train@rus.rst.rrt_runtime': 343.2242, 'train@rus.rst.rrt_samples_per_second': 83.974, 'train@rus.rst.rrt_steps_per_second': 2.625, 'epoch': 10.0}
{'loss': 1.3226, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.375709891319275, 'eval_accuracy@rus.rst.rrt': 0.5765323992994746, 'eval_f1@rus.rst.rrt': 0.4167398794040022, 'eval_precision@rus.rst.rrt': 0.474835215835957, 'eval_recall@rus.rst.rrt': 0.400114816719871, 'eval_loss@rus.rst.rrt': 1.3757100105285645, 'eval_runtime': 34.2878, 'eval_samples_per_second': 83.266, 'eval_steps_per_second': 2.625, 'epoch': 10.0}
{'train@rus.rst.rrt_loss': 1.2584251165390015, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6118589965998196, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.37218928547122376, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4418156988921649, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.35335729325121545, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2584251165390015, 'train@rus.rst.rrt_runtime': 342.3706, 'train@rus.rst.rrt_samples_per_second': 84.184, 'train@rus.rst.rrt_steps_per_second': 2.632, 'epoch': 11.0}
{'loss': 1.3146, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.3755170106887817, 'eval_accuracy@rus.rst.rrt': 0.5730297723292469, 'eval_f1@rus.rst.rrt': 0.41501378609812517, 'eval_precision@rus.rst.rrt': 0.46803264970336933, 'eval_recall@rus.rst.rrt': 0.4013349761390848, 'eval_loss@rus.rst.rrt': 1.3755171298980713, 'eval_runtime': 34.271, 'eval_samples_per_second': 83.307, 'eval_steps_per_second': 2.626, 'epoch': 11.0}
{'train@rus.rst.rrt_loss': 1.2569315433502197, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6118243008812713, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.37166541960598176, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.44162064755254843, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.35277317676520414, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2569313049316406, 'train@rus.rst.rrt_runtime': 343.158, 'train@rus.rst.rrt_samples_per_second': 83.99, 'train@rus.rst.rrt_steps_per_second': 2.626, 'epoch': 12.0}
{'loss': 1.3102, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.3738008737564087, 'eval_accuracy@rus.rst.rrt': 0.5733800350262697, 'eval_f1@rus.rst.rrt': 0.4158286383757543, 'eval_precision@rus.rst.rrt': 0.4688841501149159, 'eval_recall@rus.rst.rrt': 0.40163057495507676, 'eval_loss@rus.rst.rrt': 1.3738008737564087, 'eval_runtime': 34.3092, 'eval_samples_per_second': 83.214, 'eval_steps_per_second': 2.623, 'epoch': 12.0}
{'train_runtime': 13307.1318, 'train_samples_per_second': 25.991, 'train_steps_per_second': 0.812, 'train_loss': 1.4406537653294662, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.8828
  train_runtime            = 0:32:24.76
  train_samples_per_second =     25.595
  train_steps_per_second   =      0.802
-------------------------------------------------------------------
Lang1:  spa.rst.rststb    Lang2:  rus.rst.rrt
Saving run to:  runs/full_shot/FullShot=v4_spa.rst.rststb_rus.rst.rrt_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2240 examples
read 383 examples
read 426 examples
read 28822 examples
read 2855 examples
read 2843 examples
Total prediction labels:  35
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (ru): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=35, bias=True)
    )
  )
)
{'train@spa.rst.rststb_loss': 2.7561824321746826, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.2080357142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.01535457155358266, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.03932686078476004, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.03732805294535851, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.7561824321746826, 'train@spa.rst.rststb_runtime': 26.8982, 'train@spa.rst.rststb_samples_per_second': 83.277, 'train@spa.rst.rststb_steps_per_second': 2.602, 'epoch': 1.0}
{'loss': 3.1531, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.796491861343384, 'eval_accuracy@spa.rst.rststb': 0.206266318537859, 'eval_f1@spa.rst.rststb': 0.014966373022639005, 'eval_precision@spa.rst.rststb': 0.009038901601830664, 'eval_recall@spa.rst.rststb': 0.043478260869565216, 'eval_loss@spa.rst.rststb': 2.7964916229248047, 'eval_runtime': 4.865, 'eval_samples_per_second': 78.725, 'eval_steps_per_second': 2.467, 'epoch': 1.0}
{'train@spa.rst.rststb_loss': 2.5481197834014893, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.26875, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.03829818412340581, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.049591378278892116, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.05341845168803804, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.5481197834014893, 'train@spa.rst.rststb_runtime': 26.944, 'train@spa.rst.rststb_samples_per_second': 83.135, 'train@spa.rst.rststb_steps_per_second': 2.598, 'epoch': 2.0}
{'loss': 2.6717, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.656358003616333, 'eval_accuracy@spa.rst.rststb': 0.23237597911227154, 'eval_f1@spa.rst.rststb': 0.030633832912293626, 'eval_precision@spa.rst.rststb': 0.06115764445891418, 'eval_recall@spa.rst.rststb': 0.05186076173393303, 'eval_loss@spa.rst.rststb': 2.656358003616333, 'eval_runtime': 4.8844, 'eval_samples_per_second': 78.414, 'eval_steps_per_second': 2.457, 'epoch': 2.0}
{'train@spa.rst.rststb_loss': 2.4343066215515137, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.32767857142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.05125464602276025, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.0775100114416476, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.07119094438836267, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.4343066215515137, 'train@spa.rst.rststb_runtime': 26.9143, 'train@spa.rst.rststb_samples_per_second': 83.227, 'train@spa.rst.rststb_steps_per_second': 2.601, 'epoch': 3.0}
{'loss': 2.529, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.5784130096435547, 'eval_accuracy@spa.rst.rststb': 0.2793733681462141, 'eval_f1@spa.rst.rststb': 0.05103043563664647, 'eval_precision@spa.rst.rststb': 0.04609193077036924, 'eval_recall@spa.rst.rststb': 0.07101574978960205, 'eval_loss@spa.rst.rststb': 2.5784130096435547, 'eval_runtime': 4.8888, 'eval_samples_per_second': 78.342, 'eval_steps_per_second': 2.455, 'epoch': 3.0}
{'train@spa.rst.rststb_loss': 2.340941905975342, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.34330357142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.06527001163369124, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.07960937149097481, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.0831428123919824, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.340941905975342, 'train@spa.rst.rststb_runtime': 26.8928, 'train@spa.rst.rststb_samples_per_second': 83.294, 'train@spa.rst.rststb_steps_per_second': 2.603, 'epoch': 4.0}
{'loss': 2.4361, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.511411666870117, 'eval_accuracy@spa.rst.rststb': 0.31592689295039167, 'eval_f1@spa.rst.rststb': 0.06865032778891025, 'eval_precision@spa.rst.rststb': 0.06665980806466079, 'eval_recall@spa.rst.rststb': 0.08775789551054668, 'eval_loss@spa.rst.rststb': 2.511411666870117, 'eval_runtime': 4.864, 'eval_samples_per_second': 78.742, 'eval_steps_per_second': 2.467, 'epoch': 4.0}
{'train@spa.rst.rststb_loss': 2.2599871158599854, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.37589285714285714, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.0834115930784678, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.08385727006269732, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.10459876002969484, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.2599868774414062, 'train@spa.rst.rststb_runtime': 26.9113, 'train@spa.rst.rststb_samples_per_second': 83.236, 'train@spa.rst.rststb_steps_per_second': 2.601, 'epoch': 5.0}
{'loss': 2.3462, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.4553334712982178, 'eval_accuracy@spa.rst.rststb': 0.34986945169712796, 'eval_f1@spa.rst.rststb': 0.10193552954628703, 'eval_precision@spa.rst.rststb': 0.11449192035953314, 'eval_recall@spa.rst.rststb': 0.1199996815627386, 'eval_loss@spa.rst.rststb': 2.4553334712982178, 'eval_runtime': 4.873, 'eval_samples_per_second': 78.596, 'eval_steps_per_second': 2.463, 'epoch': 5.0}
{'train@spa.rst.rststb_loss': 2.193037748336792, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.39107142857142857, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.09235466743293544, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.12159849140024805, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.11514898102323172, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.193037748336792, 'train@spa.rst.rststb_runtime': 26.946, 'train@spa.rst.rststb_samples_per_second': 83.129, 'train@spa.rst.rststb_steps_per_second': 2.598, 'epoch': 6.0}
{'loss': 2.2748, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.405360698699951, 'eval_accuracy@spa.rst.rststb': 0.3629242819843342, 'eval_f1@spa.rst.rststb': 0.10250565966345108, 'eval_precision@spa.rst.rststb': 0.10495856501375414, 'eval_recall@spa.rst.rststb': 0.1277505618524109, 'eval_loss@spa.rst.rststb': 2.4053609371185303, 'eval_runtime': 4.8877, 'eval_samples_per_second': 78.36, 'eval_steps_per_second': 2.455, 'epoch': 6.0}
{'train@spa.rst.rststb_loss': 2.1348187923431396, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.40714285714285714, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.09697785605786001, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.12006576822211515, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.12300067977366995, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.1348187923431396, 'train@spa.rst.rststb_runtime': 26.9313, 'train@spa.rst.rststb_samples_per_second': 83.175, 'train@spa.rst.rststb_steps_per_second': 2.599, 'epoch': 7.0}
{'loss': 2.2149, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.3632354736328125, 'eval_accuracy@spa.rst.rststb': 0.37597911227154046, 'eval_f1@spa.rst.rststb': 0.10603573313893254, 'eval_precision@spa.rst.rststb': 0.10030315372439393, 'eval_recall@spa.rst.rststb': 0.13726478180966883, 'eval_loss@spa.rst.rststb': 2.3632352352142334, 'eval_runtime': 4.8765, 'eval_samples_per_second': 78.541, 'eval_steps_per_second': 2.461, 'epoch': 7.0}
{'train@spa.rst.rststb_loss': 2.0904877185821533, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4133928571428571, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.09996020210254407, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.11256559392374929, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.12648579975536517, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0904877185821533, 'train@spa.rst.rststb_runtime': 26.9376, 'train@spa.rst.rststb_samples_per_second': 83.155, 'train@spa.rst.rststb_steps_per_second': 2.599, 'epoch': 8.0}
{'loss': 2.1655, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.329244375228882, 'eval_accuracy@spa.rst.rststb': 0.3785900783289817, 'eval_f1@spa.rst.rststb': 0.10797664655629842, 'eval_precision@spa.rst.rststb': 0.10132258591279951, 'eval_recall@spa.rst.rststb': 0.13835149563634272, 'eval_loss@spa.rst.rststb': 2.3292441368103027, 'eval_runtime': 4.887, 'eval_samples_per_second': 78.37, 'eval_steps_per_second': 2.455, 'epoch': 8.0}
{'train@spa.rst.rststb_loss': 2.0573463439941406, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.41696428571428573, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10132548338531153, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.11297613753006278, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.1299238549183411, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0573463439941406, 'train@spa.rst.rststb_runtime': 26.9054, 'train@spa.rst.rststb_samples_per_second': 83.255, 'train@spa.rst.rststb_steps_per_second': 2.602, 'epoch': 9.0}
{'loss': 2.1233, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.3024580478668213, 'eval_accuracy@spa.rst.rststb': 0.37597911227154046, 'eval_f1@spa.rst.rststb': 0.10707407375785404, 'eval_precision@spa.rst.rststb': 0.09355116559367375, 'eval_recall@spa.rst.rststb': 0.14298127738253746, 'eval_loss@spa.rst.rststb': 2.3024580478668213, 'eval_runtime': 4.8782, 'eval_samples_per_second': 78.513, 'eval_steps_per_second': 2.46, 'epoch': 9.0}
{'train@spa.rst.rststb_loss': 2.033235788345337, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.425, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.1050695012069354, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.12199774997705529, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13329618287453354, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.033235788345337, 'train@spa.rst.rststb_runtime': 26.9238, 'train@spa.rst.rststb_samples_per_second': 83.198, 'train@spa.rst.rststb_steps_per_second': 2.6, 'epoch': 10.0}
{'loss': 2.0952, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.2842893600463867, 'eval_accuracy@spa.rst.rststb': 0.3838120104438642, 'eval_f1@spa.rst.rststb': 0.11015999880368149, 'eval_precision@spa.rst.rststb': 0.09733835005574135, 'eval_recall@spa.rst.rststb': 0.14482949364848346, 'eval_loss@spa.rst.rststb': 2.284289598464966, 'eval_runtime': 4.8605, 'eval_samples_per_second': 78.799, 'eval_steps_per_second': 2.469, 'epoch': 10.0}
{'train@spa.rst.rststb_loss': 2.019286870956421, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.42544642857142856, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10643567192547618, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.15428876481343345, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.1341991717472571, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.019286632537842, 'train@spa.rst.rststb_runtime': 26.9371, 'train@spa.rst.rststb_samples_per_second': 83.157, 'train@spa.rst.rststb_steps_per_second': 2.599, 'epoch': 11.0}
{'loss': 2.0771, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.271925210952759, 'eval_accuracy@spa.rst.rststb': 0.38642297650130547, 'eval_f1@spa.rst.rststb': 0.11035810943186107, 'eval_precision@spa.rst.rststb': 0.0968489666593622, 'eval_recall@spa.rst.rststb': 0.1454784229151934, 'eval_loss@spa.rst.rststb': 2.271925210952759, 'eval_runtime': 4.8955, 'eval_samples_per_second': 78.236, 'eval_steps_per_second': 2.451, 'epoch': 11.0}
{'train@spa.rst.rststb_loss': 2.0147361755371094, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.42723214285714284, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10778543315011994, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.15822279597962613, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.1349895250433165, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0147361755371094, 'train@spa.rst.rststb_runtime': 26.9087, 'train@spa.rst.rststb_samples_per_second': 83.244, 'train@spa.rst.rststb_steps_per_second': 2.601, 'epoch': 12.0}
{'loss': 2.0705, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.2679450511932373, 'eval_accuracy@spa.rst.rststb': 0.38903394255874674, 'eval_f1@spa.rst.rststb': 0.11150815466373466, 'eval_precision@spa.rst.rststb': 0.09862466853770385, 'eval_recall@spa.rst.rststb': 0.14602878064771954, 'eval_loss@spa.rst.rststb': 2.2679450511932373, 'eval_runtime': 4.8711, 'eval_samples_per_second': 78.627, 'eval_steps_per_second': 2.463, 'epoch': 12.0}
{'train_runtime': 1061.2928, 'train_samples_per_second': 25.328, 'train_steps_per_second': 0.791, 'train_loss': 2.3464542933872767, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.3465
  train_runtime            = 0:17:41.29
  train_samples_per_second =     25.328
  train_steps_per_second   =      0.791
{'train@rus.rst.rrt_loss': 1.6374800205230713, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5170702935257789, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.20398881300847876, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.25287400991992476, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.21437309166270124, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.6374799013137817, 'train@rus.rst.rrt_runtime': 343.1827, 'train@rus.rst.rrt_samples_per_second': 83.984, 'train@rus.rst.rrt_steps_per_second': 2.625, 'epoch': 1.0}
{'loss': 2.0547, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.6844667196273804, 'eval_accuracy@rus.rst.rrt': 0.500875656742557, 'eval_f1@rus.rst.rrt': 0.22349546564378006, 'eval_precision@rus.rst.rrt': 0.24418698097839953, 'eval_recall@rus.rst.rrt': 0.23609324714771146, 'eval_loss@rus.rst.rrt': 1.6844664812088013, 'eval_runtime': 34.3178, 'eval_samples_per_second': 83.193, 'eval_steps_per_second': 2.623, 'epoch': 1.0}
{'train@rus.rst.rrt_loss': 1.4700478315353394, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5515925334813684, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.2417977008518246, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.3129153773829632, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.24376212798038982, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.4700477123260498, 'train@rus.rst.rrt_runtime': 343.0685, 'train@rus.rst.rrt_samples_per_second': 84.012, 'train@rus.rst.rrt_steps_per_second': 2.626, 'epoch': 2.0}
{'loss': 1.5908, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.5430908203125, 'eval_accuracy@rus.rst.rrt': 0.5225919439579685, 'eval_f1@rus.rst.rrt': 0.26080509217676395, 'eval_precision@rus.rst.rrt': 0.34334959453590913, 'eval_recall@rus.rst.rrt': 0.26282162644761525, 'eval_loss@rus.rst.rrt': 1.543091058731079, 'eval_runtime': 34.3332, 'eval_samples_per_second': 83.156, 'eval_steps_per_second': 2.621, 'epoch': 2.0}
{'train@rus.rst.rrt_loss': 1.4050545692443848, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5687322184442439, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.2857809231588246, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4026790071705906, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.2811622432305445, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.4050545692443848, 'train@rus.rst.rrt_runtime': 343.107, 'train@rus.rst.rrt_samples_per_second': 84.003, 'train@rus.rst.rrt_steps_per_second': 2.626, 'epoch': 3.0}
{'loss': 1.4855, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.479828119277954, 'eval_accuracy@rus.rst.rrt': 0.5345008756567425, 'eval_f1@rus.rst.rrt': 0.3014385728097413, 'eval_precision@rus.rst.rrt': 0.3684448674172458, 'eval_recall@rus.rst.rrt': 0.2984317389898119, 'eval_loss@rus.rst.rrt': 1.4798282384872437, 'eval_runtime': 34.3243, 'eval_samples_per_second': 83.177, 'eval_steps_per_second': 2.622, 'epoch': 3.0}
{'train@rus.rst.rrt_loss': 1.3577346801757812, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5844146832280896, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.31741198742049437, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4284440197372673, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.300110997165333, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3577347993850708, 'train@rus.rst.rrt_runtime': 343.0623, 'train@rus.rst.rrt_samples_per_second': 84.014, 'train@rus.rst.rrt_steps_per_second': 2.626, 'epoch': 4.0}
{'loss': 1.4315, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.435928463935852, 'eval_accuracy@rus.rst.rrt': 0.5590192644483363, 'eval_f1@rus.rst.rrt': 0.3549753831977962, 'eval_precision@rus.rst.rrt': 0.4654072347336003, 'eval_recall@rus.rst.rrt': 0.33792132950692616, 'eval_loss@rus.rst.rrt': 1.435928463935852, 'eval_runtime': 34.2916, 'eval_samples_per_second': 83.256, 'eval_steps_per_second': 2.625, 'epoch': 4.0}
{'train@rus.rst.rrt_loss': 1.3271814584732056, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5909027825966275, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3314280321252529, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.43733723278853015, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.31248749364345957, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3271815776824951, 'train@rus.rst.rrt_runtime': 342.9659, 'train@rus.rst.rrt_samples_per_second': 84.038, 'train@rus.rst.rrt_steps_per_second': 2.627, 'epoch': 5.0}
{'loss': 1.3959, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.4136388301849365, 'eval_accuracy@rus.rst.rrt': 0.5625218914185639, 'eval_f1@rus.rst.rrt': 0.3785293186013242, 'eval_precision@rus.rst.rrt': 0.49937560406798515, 'eval_recall@rus.rst.rrt': 0.35922397151012303, 'eval_loss@rus.rst.rrt': 1.4136388301849365, 'eval_runtime': 34.2549, 'eval_samples_per_second': 83.346, 'eval_steps_per_second': 2.627, 'epoch': 5.0}
{'train@rus.rst.rrt_loss': 1.3061410188674927, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5973908819651655, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3454774717280348, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.43502801983589784, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3259324418237405, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3061408996582031, 'train@rus.rst.rrt_runtime': 343.0497, 'train@rus.rst.rrt_samples_per_second': 84.017, 'train@rus.rst.rrt_steps_per_second': 2.626, 'epoch': 6.0}
{'loss': 1.3718, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.3948888778686523, 'eval_accuracy@rus.rst.rrt': 0.5719789842381786, 'eval_f1@rus.rst.rrt': 0.39118714074760497, 'eval_precision@rus.rst.rrt': 0.4770904597247309, 'eval_recall@rus.rst.rrt': 0.37374307913895155, 'eval_loss@rus.rst.rrt': 1.3948887586593628, 'eval_runtime': 34.3418, 'eval_samples_per_second': 83.135, 'eval_steps_per_second': 2.621, 'epoch': 6.0}
{'train@rus.rst.rrt_loss': 1.2910248041152954, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.600999236694192, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3548354384031875, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4297947176885455, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3362670835195789, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2910246849060059, 'train@rus.rst.rrt_runtime': 343.1947, 'train@rus.rst.rrt_samples_per_second': 83.981, 'train@rus.rst.rrt_steps_per_second': 2.625, 'epoch': 7.0}
{'loss': 1.3519, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.3863705396652222, 'eval_accuracy@rus.rst.rrt': 0.5695271453590193, 'eval_f1@rus.rst.rrt': 0.39465220867991746, 'eval_precision@rus.rst.rrt': 0.46347553103985034, 'eval_recall@rus.rst.rrt': 0.37784064456299477, 'eval_loss@rus.rst.rrt': 1.3863706588745117, 'eval_runtime': 34.3123, 'eval_samples_per_second': 83.206, 'eval_steps_per_second': 2.623, 'epoch': 7.0}
{'train@rus.rst.rrt_loss': 1.2775582075119019, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6049198528901534, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3624588438538385, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4225639431221891, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34561756687599465, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2775582075119019, 'train@rus.rst.rrt_runtime': 343.019, 'train@rus.rst.rrt_samples_per_second': 84.024, 'train@rus.rst.rrt_steps_per_second': 2.627, 'epoch': 8.0}
{'loss': 1.3357, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.3728779554367065, 'eval_accuracy@rus.rst.rrt': 0.5761821366024519, 'eval_f1@rus.rst.rrt': 0.4048708664725586, 'eval_precision@rus.rst.rrt': 0.46038560492228464, 'eval_recall@rus.rst.rrt': 0.39120475219934997, 'eval_loss@rus.rst.rrt': 1.3728779554367065, 'eval_runtime': 34.3031, 'eval_samples_per_second': 83.229, 'eval_steps_per_second': 2.624, 'epoch': 8.0}
{'train@rus.rst.rrt_loss': 1.2658873796463013, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6079383804038582, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36657450575746664, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4554530057715699, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34718370391243913, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2658873796463013, 'train@rus.rst.rrt_runtime': 343.1498, 'train@rus.rst.rrt_samples_per_second': 83.992, 'train@rus.rst.rrt_steps_per_second': 2.626, 'epoch': 9.0}
{'loss': 1.3264, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.3666417598724365, 'eval_accuracy@rus.rst.rrt': 0.5733800350262697, 'eval_f1@rus.rst.rrt': 0.4091197824571905, 'eval_precision@rus.rst.rrt': 0.4755836148945128, 'eval_recall@rus.rst.rrt': 0.3928830969434258, 'eval_loss@rus.rst.rrt': 1.366641640663147, 'eval_runtime': 34.2879, 'eval_samples_per_second': 83.266, 'eval_steps_per_second': 2.625, 'epoch': 9.0}
{'train@rus.rst.rrt_loss': 1.2608027458190918, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6086669904933731, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3674348134685416, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.46165411375794924, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34625844584714766, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2608028650283813, 'train@rus.rst.rrt_runtime': 343.0982, 'train@rus.rst.rrt_samples_per_second': 84.005, 'train@rus.rst.rrt_steps_per_second': 2.626, 'epoch': 10.0}
{'loss': 1.3166, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.3650084733963013, 'eval_accuracy@rus.rst.rrt': 0.5768826619964974, 'eval_f1@rus.rst.rrt': 0.40988852200492043, 'eval_precision@rus.rst.rrt': 0.47923317393937176, 'eval_recall@rus.rst.rrt': 0.3919031090663444, 'eval_loss@rus.rst.rrt': 1.3650082349777222, 'eval_runtime': 34.3419, 'eval_samples_per_second': 83.135, 'eval_steps_per_second': 2.621, 'epoch': 10.0}
{'train@rus.rst.rrt_loss': 1.2572124004364014, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6090833391159531, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36849539226725536, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4344917713846097, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3495123334517205, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2572124004364014, 'train@rus.rst.rrt_runtime': 342.958, 'train@rus.rst.rrt_samples_per_second': 84.039, 'train@rus.rst.rrt_steps_per_second': 2.627, 'epoch': 11.0}
{'loss': 1.3095, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.3652009963989258, 'eval_accuracy@rus.rst.rrt': 0.5733800350262697, 'eval_f1@rus.rst.rrt': 0.4096404576247991, 'eval_precision@rus.rst.rrt': 0.4667897167823416, 'eval_recall@rus.rst.rrt': 0.3956006965914031, 'eval_loss@rus.rst.rrt': 1.3652009963989258, 'eval_runtime': 34.2848, 'eval_samples_per_second': 83.273, 'eval_steps_per_second': 2.625, 'epoch': 11.0}
{'train@rus.rst.rrt_loss': 1.2554951906204224, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6094302963014364, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3687535490515326, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.43429267989405834, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34990465839735335, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.255495309829712, 'train@rus.rst.rrt_runtime': 343.1935, 'train@rus.rst.rrt_samples_per_second': 83.982, 'train@rus.rst.rrt_steps_per_second': 2.625, 'epoch': 12.0}
{'loss': 1.3059, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.363107442855835, 'eval_accuracy@rus.rst.rrt': 0.5761821366024519, 'eval_f1@rus.rst.rrt': 0.41003378242124766, 'eval_precision@rus.rst.rrt': 0.4729064165104119, 'eval_recall@rus.rst.rrt': 0.39565062769914167, 'eval_loss@rus.rst.rrt': 1.3631073236465454, 'eval_runtime': 34.2926, 'eval_samples_per_second': 83.254, 'eval_steps_per_second': 2.624, 'epoch': 12.0}
{'train_runtime': 13275.0111, 'train_samples_per_second': 26.054, 'train_steps_per_second': 0.814, 'train_loss': 1.4396786028219513, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.3465
  train_runtime            = 0:17:41.29
  train_samples_per_second =     25.328
  train_steps_per_second   =      0.791
-------------------------------------------------------------------
Lang1:  spa.rst.sctb    Lang2:  rus.rst.rrt
Saving run to:  runs/full_shot/FullShot=v4_spa.rst.sctb_rus.rst.rrt_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 439 examples
read 94 examples
read 159 examples
read 28822 examples
read 2855 examples
read 2843 examples
Total prediction labels:  32
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (ru): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=32, bias=True)
    )
  )
)
{'train@spa.rst.sctb_loss': 3.204655170440674, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3348519362186788, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.020452173913043478, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.013835294117647058, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.0392, 'train@spa.rst.sctb_loss@spa.rst.sctb': 3.2046546936035156, 'train@spa.rst.sctb_runtime': 5.4862, 'train@spa.rst.sctb_samples_per_second': 80.019, 'train@spa.rst.sctb_steps_per_second': 2.552, 'epoch': 1.0}
{'loss': 3.3479, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.196976900100708, 'eval_accuracy@spa.rst.sctb': 0.35106382978723405, 'eval_f1@spa.rst.sctb': 0.0320855614973262, 'eval_precision@spa.rst.sctb': 0.022058823529411766, 'eval_recall@spa.rst.sctb': 0.058823529411764705, 'eval_loss@spa.rst.sctb': 3.19697642326355, 'eval_runtime': 1.4009, 'eval_samples_per_second': 67.1, 'eval_steps_per_second': 2.142, 'epoch': 1.0}
{'train@spa.rst.sctb_loss': 2.958763599395752, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3416856492027335, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.021258503401360544, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.014269406392694063, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.041666666666666664, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.9587631225585938, 'train@spa.rst.sctb_runtime': 5.5232, 'train@spa.rst.sctb_samples_per_second': 79.482, 'train@spa.rst.sctb_steps_per_second': 2.535, 'epoch': 2.0}
{'loss': 3.1054, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.949554681777954, 'eval_accuracy@spa.rst.sctb': 0.35106382978723405, 'eval_f1@spa.rst.sctb': 0.03105882352941176, 'eval_precision@spa.rst.sctb': 0.021099744245524295, 'eval_recall@spa.rst.sctb': 0.058823529411764705, 'eval_loss@spa.rst.sctb': 2.949554204940796, 'eval_runtime': 1.411, 'eval_samples_per_second': 66.618, 'eval_steps_per_second': 2.126, 'epoch': 2.0}
{'train@spa.rst.sctb_loss': 2.7144453525543213, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3416856492027335, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.021258503401360544, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.014269406392694063, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.041666666666666664, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.7144455909729004, 'train@spa.rst.sctb_runtime': 5.5513, 'train@spa.rst.sctb_samples_per_second': 79.08, 'train@spa.rst.sctb_steps_per_second': 2.522, 'epoch': 3.0}
{'loss': 2.8803, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.7115566730499268, 'eval_accuracy@spa.rst.sctb': 0.35106382978723405, 'eval_f1@spa.rst.sctb': 0.03081232492997199, 'eval_precision@spa.rst.sctb': 0.020872865275142316, 'eval_recall@spa.rst.sctb': 0.058823529411764705, 'eval_loss@spa.rst.sctb': 2.711556911468506, 'eval_runtime': 1.4117, 'eval_samples_per_second': 66.588, 'eval_steps_per_second': 2.125, 'epoch': 3.0}
{'train@spa.rst.sctb_loss': 2.515563726425171, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3416856492027335, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02129471890971039, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.014302059496567507, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.041666666666666664, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.515563726425171, 'train@spa.rst.sctb_runtime': 5.6027, 'train@spa.rst.sctb_samples_per_second': 78.355, 'train@spa.rst.sctb_steps_per_second': 2.499, 'epoch': 4.0}
{'loss': 2.6473, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.5241894721984863, 'eval_accuracy@spa.rst.sctb': 0.35106382978723405, 'eval_f1@spa.rst.sctb': 0.03081232492997199, 'eval_precision@spa.rst.sctb': 0.020872865275142316, 'eval_recall@spa.rst.sctb': 0.058823529411764705, 'eval_loss@spa.rst.sctb': 2.5241894721984863, 'eval_runtime': 1.4241, 'eval_samples_per_second': 66.005, 'eval_steps_per_second': 2.107, 'epoch': 4.0}
{'train@spa.rst.sctb_loss': 2.3837411403656006, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3439635535307517, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.022226627896730994, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.02478448275862069, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.042114695340501794, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.3837406635284424, 'train@spa.rst.sctb_runtime': 5.5265, 'train@spa.rst.sctb_samples_per_second': 79.435, 'train@spa.rst.sctb_steps_per_second': 2.533, 'epoch': 5.0}
{'loss': 2.4882, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.4007270336151123, 'eval_accuracy@spa.rst.sctb': 0.35106382978723405, 'eval_f1@spa.rst.sctb': 0.03081232492997199, 'eval_precision@spa.rst.sctb': 0.020872865275142316, 'eval_recall@spa.rst.sctb': 0.058823529411764705, 'eval_loss@spa.rst.sctb': 2.400726079940796, 'eval_runtime': 1.4086, 'eval_samples_per_second': 66.735, 'eval_steps_per_second': 2.13, 'epoch': 5.0}
{'train@spa.rst.sctb_loss': 2.3072235584259033, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3462414578587699, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.023104789861149944, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.031067588325652845, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04256272401433692, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.3072235584259033, 'train@spa.rst.sctb_runtime': 5.5212, 'train@spa.rst.sctb_samples_per_second': 79.512, 'train@spa.rst.sctb_steps_per_second': 2.536, 'epoch': 6.0}
{'loss': 2.3823, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.3316285610198975, 'eval_accuracy@spa.rst.sctb': 0.3723404255319149, 'eval_f1@spa.rst.sctb': 0.041794024078348195, 'eval_precision@spa.rst.sctb': 0.050980392156862744, 'eval_recall@spa.rst.sctb': 0.06501547987616098, 'eval_loss@spa.rst.sctb': 2.3316285610198975, 'eval_runtime': 1.4091, 'eval_samples_per_second': 66.707, 'eval_steps_per_second': 2.129, 'epoch': 6.0}
{'train@spa.rst.sctb_loss': 2.261284112930298, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.35079726651480636, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.024854395232290397, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.02661064425770308, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04345878136200717, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.2612838745117188, 'train@spa.rst.sctb_runtime': 5.5174, 'train@spa.rst.sctb_samples_per_second': 79.567, 'train@spa.rst.sctb_steps_per_second': 2.537, 'epoch': 7.0}
{'loss': 2.3221, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.291865587234497, 'eval_accuracy@spa.rst.sctb': 0.3829787234042553, 'eval_f1@spa.rst.sctb': 0.04652844744455159, 'eval_precision@spa.rst.sctb': 0.05710508922670192, 'eval_recall@spa.rst.sctb': 0.06811145510835914, 'eval_loss@spa.rst.sctb': 2.291865348815918, 'eval_runtime': 1.4243, 'eval_samples_per_second': 65.998, 'eval_steps_per_second': 2.106, 'epoch': 7.0}
{'train@spa.rst.sctb_loss': 2.234682321548462, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3621867881548975, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.028534150215566143, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.030685240963855425, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04569892473118279, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.234682083129883, 'train@spa.rst.sctb_runtime': 5.543, 'train@spa.rst.sctb_samples_per_second': 79.198, 'train@spa.rst.sctb_steps_per_second': 2.526, 'epoch': 8.0}
{'loss': 2.2698, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.2701354026794434, 'eval_accuracy@spa.rst.sctb': 0.3723404255319149, 'eval_f1@spa.rst.sctb': 0.04523091881380651, 'eval_precision@spa.rst.sctb': 0.05080213903743316, 'eval_recall@spa.rst.sctb': 0.06632892391406324, 'eval_loss@spa.rst.sctb': 2.2701351642608643, 'eval_runtime': 1.4385, 'eval_samples_per_second': 65.344, 'eval_steps_per_second': 2.085, 'epoch': 8.0}
{'train@spa.rst.sctb_loss': 2.2160048484802246, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3690205011389522, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03090255609940649, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03126059065601549, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.047213261648745515, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.2160048484802246, 'train@spa.rst.sctb_runtime': 5.5235, 'train@spa.rst.sctb_samples_per_second': 79.478, 'train@spa.rst.sctb_steps_per_second': 2.535, 'epoch': 9.0}
{'loss': 2.2698, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.2547833919525146, 'eval_accuracy@spa.rst.sctb': 0.39361702127659576, 'eval_f1@spa.rst.sctb': 0.052912690499928784, 'eval_precision@spa.rst.sctb': 0.05482506728181469, 'eval_recall@spa.rst.sctb': 0.07252087437845951, 'eval_loss@spa.rst.sctb': 2.2547833919525146, 'eval_runtime': 1.4304, 'eval_samples_per_second': 65.716, 'eval_steps_per_second': 2.097, 'epoch': 9.0}
{'train@spa.rst.sctb_loss': 2.203416347503662, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.37813211845102507, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.033504687962337305, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03199561160895379, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04917562724014337, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.203416347503662, 'train@spa.rst.sctb_runtime': 5.5199, 'train@spa.rst.sctb_samples_per_second': 79.53, 'train@spa.rst.sctb_steps_per_second': 2.536, 'epoch': 10.0}
{'loss': 2.2444, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.2455244064331055, 'eval_accuracy@spa.rst.sctb': 0.40425531914893614, 'eval_f1@spa.rst.sctb': 0.05550697137199901, 'eval_precision@spa.rst.sctb': 0.05236728837876614, 'eval_recall@spa.rst.sctb': 0.07561684961065766, 'eval_loss@spa.rst.sctb': 2.2455248832702637, 'eval_runtime': 1.4224, 'eval_samples_per_second': 66.086, 'eval_steps_per_second': 2.109, 'epoch': 10.0}
{'train@spa.rst.sctb_loss': 2.196658134460449, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3712984054669704, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03290756862185434, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.030531276778063412, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04834229390681003, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.196658134460449, 'train@spa.rst.sctb_runtime': 5.5425, 'train@spa.rst.sctb_samples_per_second': 79.206, 'train@spa.rst.sctb_steps_per_second': 2.526, 'epoch': 11.0}
{'loss': 2.2393, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.240105390548706, 'eval_accuracy@spa.rst.sctb': 0.40425531914893614, 'eval_f1@spa.rst.sctb': 0.05550697137199901, 'eval_precision@spa.rst.sctb': 0.05236728837876614, 'eval_recall@spa.rst.sctb': 0.07561684961065766, 'eval_loss@spa.rst.sctb': 2.240104913711548, 'eval_runtime': 1.4229, 'eval_samples_per_second': 66.062, 'eval_steps_per_second': 2.108, 'epoch': 11.0}
{'train@spa.rst.sctb_loss': 2.194357395172119, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3712984054669704, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03290756862185434, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.030531276778063412, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04834229390681003, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.19435715675354, 'train@spa.rst.sctb_runtime': 5.5338, 'train@spa.rst.sctb_samples_per_second': 79.33, 'train@spa.rst.sctb_steps_per_second': 2.53, 'epoch': 12.0}
{'loss': 2.2341, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.238560676574707, 'eval_accuracy@spa.rst.sctb': 0.40425531914893614, 'eval_f1@spa.rst.sctb': 0.05550697137199901, 'eval_precision@spa.rst.sctb': 0.05236728837876614, 'eval_recall@spa.rst.sctb': 0.07561684961065766, 'eval_loss@spa.rst.sctb': 2.238560914993286, 'eval_runtime': 1.4225, 'eval_samples_per_second': 66.083, 'eval_steps_per_second': 2.109, 'epoch': 12.0}
{'train_runtime': 216.8884, 'train_samples_per_second': 24.289, 'train_steps_per_second': 0.775, 'train_loss': 2.5359187807355608, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.5359
  train_runtime            = 0:03:36.88
  train_samples_per_second =     24.289
  train_steps_per_second   =      0.775
{'train@rus.rst.rrt_loss': 1.7202659845352173, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.4964263409895219, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.18149889770218486, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.23036267805239075, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.1968442554611318, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.7202662229537964, 'train@rus.rst.rrt_runtime': 343.6578, 'train@rus.rst.rrt_samples_per_second': 83.868, 'train@rus.rst.rrt_steps_per_second': 2.622, 'epoch': 1.0}
{'loss': 2.1575, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.7570476531982422, 'eval_accuracy@rus.rst.rrt': 0.4725043782837128, 'eval_f1@rus.rst.rrt': 0.1995659101043559, 'eval_precision@rus.rst.rrt': 0.20480852236241084, 'eval_recall@rus.rst.rrt': 0.21570441691797948, 'eval_loss@rus.rst.rrt': 1.7570478916168213, 'eval_runtime': 34.303, 'eval_samples_per_second': 83.229, 'eval_steps_per_second': 2.624, 'epoch': 1.0}
{'train@rus.rst.rrt_loss': 1.502946138381958, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5424675595031573, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.22643634301376886, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.31644966177173184, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.23198866611013463, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.5029460191726685, 'train@rus.rst.rrt_runtime': 343.0896, 'train@rus.rst.rrt_samples_per_second': 84.007, 'train@rus.rst.rrt_steps_per_second': 2.626, 'epoch': 2.0}
{'loss': 1.6439, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.5657457113265991, 'eval_accuracy@rus.rst.rrt': 0.5155866900175131, 'eval_f1@rus.rst.rrt': 0.2491627249788776, 'eval_precision@rus.rst.rrt': 0.34221831717307394, 'eval_recall@rus.rst.rrt': 0.25525376458592997, 'eval_loss@rus.rst.rrt': 1.5657457113265991, 'eval_runtime': 34.2955, 'eval_samples_per_second': 83.247, 'eval_steps_per_second': 2.624, 'epoch': 2.0}
{'train@rus.rst.rrt_loss': 1.4236719608306885, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5645340364998959, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.2797725355107477, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4448816814463937, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.2744936222649177, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.4236723184585571, 'train@rus.rst.rrt_runtime': 343.015, 'train@rus.rst.rrt_samples_per_second': 84.025, 'train@rus.rst.rrt_steps_per_second': 2.627, 'epoch': 3.0}
{'loss': 1.5122, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.4935023784637451, 'eval_accuracy@rus.rst.rrt': 0.5415061295971979, 'eval_f1@rus.rst.rrt': 0.31402724855010866, 'eval_precision@rus.rst.rrt': 0.3926164384867607, 'eval_recall@rus.rst.rrt': 0.3102353239503875, 'eval_loss@rus.rst.rrt': 1.4935023784637451, 'eval_runtime': 34.2834, 'eval_samples_per_second': 83.276, 'eval_steps_per_second': 2.625, 'epoch': 3.0}
{'train@rus.rst.rrt_loss': 1.3703709840774536, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5798348483797099, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3123540116477706, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4690317272529928, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.29520960233008964, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3703709840774536, 'train@rus.rst.rrt_runtime': 343.0059, 'train@rus.rst.rrt_samples_per_second': 84.028, 'train@rus.rst.rrt_steps_per_second': 2.627, 'epoch': 4.0}
{'loss': 1.448, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.445052146911621, 'eval_accuracy@rus.rst.rrt': 0.5565674255691769, 'eval_f1@rus.rst.rrt': 0.3533090787869561, 'eval_precision@rus.rst.rrt': 0.4531567848782535, 'eval_recall@rus.rst.rrt': 0.34009585449357943, 'eval_loss@rus.rst.rrt': 1.4450523853302002, 'eval_runtime': 34.2708, 'eval_samples_per_second': 83.307, 'eval_steps_per_second': 2.626, 'epoch': 4.0}
{'train@rus.rst.rrt_loss': 1.3367284536361694, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5899313024772743, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.32894220582595635, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4980334193956642, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.310909093807435, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3367284536361694, 'train@rus.rst.rrt_runtime': 343.1493, 'train@rus.rst.rrt_samples_per_second': 83.993, 'train@rus.rst.rrt_steps_per_second': 2.626, 'epoch': 5.0}
{'loss': 1.4079, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.4204024076461792, 'eval_accuracy@rus.rst.rrt': 0.563922942206655, 'eval_f1@rus.rst.rrt': 0.37398568276837324, 'eval_precision@rus.rst.rrt': 0.4806180868170814, 'eval_recall@rus.rst.rrt': 0.3579532898950358, 'eval_loss@rus.rst.rrt': 1.4204025268554688, 'eval_runtime': 34.3038, 'eval_samples_per_second': 83.227, 'eval_steps_per_second': 2.624, 'epoch': 5.0}
{'train@rus.rst.rrt_loss': 1.3134433031082153, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5957948789119423, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3417226301279722, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4940463163587447, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3222952406051815, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3134434223175049, 'train@rus.rst.rrt_runtime': 342.9566, 'train@rus.rst.rrt_samples_per_second': 84.04, 'train@rus.rst.rrt_steps_per_second': 2.627, 'epoch': 6.0}
{'loss': 1.3813, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.4003450870513916, 'eval_accuracy@rus.rst.rrt': 0.5681260945709282, 'eval_f1@rus.rst.rrt': 0.38507250671307414, 'eval_precision@rus.rst.rrt': 0.4764917283485972, 'eval_recall@rus.rst.rrt': 0.36915096400913894, 'eval_loss@rus.rst.rrt': 1.4003450870513916, 'eval_runtime': 34.2474, 'eval_samples_per_second': 83.364, 'eval_steps_per_second': 2.628, 'epoch': 6.0}
{'train@rus.rst.rrt_loss': 1.2975481748580933, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6006522795087086, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3526949293810938, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.462354932754498, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.33325475528566134, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2975480556488037, 'train@rus.rst.rrt_runtime': 342.9108, 'train@rus.rst.rrt_samples_per_second': 84.051, 'train@rus.rst.rrt_steps_per_second': 2.628, 'epoch': 7.0}
{'loss': 1.3592, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.391935110092163, 'eval_accuracy@rus.rst.rrt': 0.5674255691768827, 'eval_f1@rus.rst.rrt': 0.38824891047388643, 'eval_precision@rus.rst.rrt': 0.46848191782747967, 'eval_recall@rus.rst.rrt': 0.3747173569863414, 'eval_loss@rus.rst.rrt': 1.3919352293014526, 'eval_runtime': 34.2941, 'eval_samples_per_second': 83.251, 'eval_steps_per_second': 2.624, 'epoch': 7.0}
{'train@rus.rst.rrt_loss': 1.2851513624191284, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6025605440288668, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3592699409397197, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4566505477669434, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34267498013425945, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2851511240005493, 'train@rus.rst.rrt_runtime': 343.1054, 'train@rus.rst.rrt_samples_per_second': 84.003, 'train@rus.rst.rrt_steps_per_second': 2.626, 'epoch': 8.0}
{'loss': 1.3426, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.38017737865448, 'eval_accuracy@rus.rst.rrt': 0.5737302977232924, 'eval_f1@rus.rst.rrt': 0.40059878367450924, 'eval_precision@rus.rst.rrt': 0.45793493768085985, 'eval_recall@rus.rst.rrt': 0.39082642210065, 'eval_loss@rus.rst.rrt': 1.3801772594451904, 'eval_runtime': 34.2718, 'eval_samples_per_second': 83.305, 'eval_steps_per_second': 2.626, 'epoch': 8.0}
{'train@rus.rst.rrt_loss': 1.2729227542877197, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6059954201651516, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3621727959657672, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.46619624244203345, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34377970505900496, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2729226350784302, 'train@rus.rst.rrt_runtime': 343.1468, 'train@rus.rst.rrt_samples_per_second': 83.993, 'train@rus.rst.rrt_steps_per_second': 2.626, 'epoch': 9.0}
{'loss': 1.3334, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.374130129814148, 'eval_accuracy@rus.rst.rrt': 0.5712784588441331, 'eval_f1@rus.rst.rrt': 0.3979494582113667, 'eval_precision@rus.rst.rrt': 0.4609702361677958, 'eval_recall@rus.rst.rrt': 0.38575959508823954, 'eval_loss@rus.rst.rrt': 1.374130129814148, 'eval_runtime': 34.2671, 'eval_samples_per_second': 83.316, 'eval_steps_per_second': 2.626, 'epoch': 9.0}
{'train@rus.rst.rrt_loss': 1.2670027017593384, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6071750745957949, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36424096772488074, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4651922105644486, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3428645276072318, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2670025825500488, 'train@rus.rst.rrt_runtime': 343.1358, 'train@rus.rst.rrt_samples_per_second': 83.996, 'train@rus.rst.rrt_steps_per_second': 2.626, 'epoch': 10.0}
{'loss': 1.3236, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.3716498613357544, 'eval_accuracy@rus.rst.rrt': 0.5772329246935202, 'eval_f1@rus.rst.rrt': 0.4045738371525645, 'eval_precision@rus.rst.rrt': 0.46937504439305183, 'eval_recall@rus.rst.rrt': 0.39050980711351446, 'eval_loss@rus.rst.rrt': 1.3716498613357544, 'eval_runtime': 34.2478, 'eval_samples_per_second': 83.363, 'eval_steps_per_second': 2.628, 'epoch': 10.0}
{'train@rus.rst.rrt_loss': 1.2625658512115479, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6074179446256331, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36507592387085913, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4509026569940677, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3468570062868826, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2625658512115479, 'train@rus.rst.rrt_runtime': 342.9693, 'train@rus.rst.rrt_samples_per_second': 84.037, 'train@rus.rst.rrt_steps_per_second': 2.627, 'epoch': 11.0}
{'loss': 1.3176, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.3708198070526123, 'eval_accuracy@rus.rst.rrt': 0.5807355516637478, 'eval_f1@rus.rst.rrt': 0.41707770762397994, 'eval_precision@rus.rst.rrt': 0.5165403506448453, 'eval_recall@rus.rst.rrt': 0.40065160313908305, 'eval_loss@rus.rst.rrt': 1.3708196878433228, 'eval_runtime': 34.3205, 'eval_samples_per_second': 83.186, 'eval_steps_per_second': 2.622, 'epoch': 11.0}
{'train@rus.rst.rrt_loss': 1.2611860036849976, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6088057733675665, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36601736299566684, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4527253603914611, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34749033927591877, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2611860036849976, 'train@rus.rst.rrt_runtime': 343.0407, 'train@rus.rst.rrt_samples_per_second': 84.019, 'train@rus.rst.rrt_steps_per_second': 2.627, 'epoch': 12.0}
{'loss': 1.3126, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.368605613708496, 'eval_accuracy@rus.rst.rrt': 0.5786339754816112, 'eval_f1@rus.rst.rrt': 0.4138954109533796, 'eval_precision@rus.rst.rrt': 0.511379884447361, 'eval_recall@rus.rst.rrt': 0.39857512683851654, 'eval_loss@rus.rst.rrt': 1.368605613708496, 'eval_runtime': 34.2508, 'eval_samples_per_second': 83.356, 'eval_steps_per_second': 2.628, 'epoch': 12.0}
{'train_runtime': 13274.7079, 'train_samples_per_second': 26.054, 'train_steps_per_second': 0.814, 'train_loss': 1.4616615557908748, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.5359
  train_runtime            = 0:03:36.88
  train_samples_per_second =     24.289
  train_steps_per_second   =      0.775
-------------------------------------------------------------------
Lang1:  tur.pdtb.tdb    Lang2:  rus.rst.rrt
Saving run to:  runs/full_shot/FullShot=v4_tur.pdtb.tdb_rus.rst.rrt_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2451 examples
read 312 examples
read 422 examples
read 28822 examples
read 2855 examples
read 2843 examples
Total prediction labels:  45
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (tr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (ru): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (tr): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=45, bias=True)
    )
  )
)
{'train@tur.pdtb.tdb_loss': 2.9053173065185547, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01741967515426626, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010891738953044898, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.905317544937134, 'train@tur.pdtb.tdb_runtime': 29.5147, 'train@tur.pdtb.tdb_samples_per_second': 83.043, 'train@tur.pdtb.tdb_steps_per_second': 2.609, 'epoch': 1.0}
{'loss': 3.3434, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.8366730213165283, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.836672782897949, 'eval_runtime': 4.1139, 'eval_samples_per_second': 75.84, 'eval_steps_per_second': 2.431, 'epoch': 1.0}
{'train@tur.pdtb.tdb_loss': 2.463733196258545, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01741967515426626, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010891738953044898, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.463733196258545, 'train@tur.pdtb.tdb_runtime': 29.521, 'train@tur.pdtb.tdb_samples_per_second': 83.026, 'train@tur.pdtb.tdb_steps_per_second': 2.608, 'epoch': 2.0}
{'loss': 2.6408, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.3467206954956055, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.3467204570770264, 'eval_runtime': 4.1221, 'eval_samples_per_second': 75.69, 'eval_steps_per_second': 2.426, 'epoch': 2.0}
{'train@tur.pdtb.tdb_loss': 2.3866260051727295, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01741967515426626, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010891738953044898, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.3866260051727295, 'train@tur.pdtb.tdb_runtime': 29.5087, 'train@tur.pdtb.tdb_samples_per_second': 83.06, 'train@tur.pdtb.tdb_steps_per_second': 2.609, 'epoch': 3.0}
{'loss': 2.4438, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.299083948135376, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.299084186553955, 'eval_runtime': 4.1249, 'eval_samples_per_second': 75.638, 'eval_steps_per_second': 2.424, 'epoch': 3.0}
{'train@tur.pdtb.tdb_loss': 2.324892044067383, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.2586699306405549, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.024824669077919404, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.057800909432510754, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.0473831248970343, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.3248918056488037, 'train@tur.pdtb.tdb_runtime': 29.5103, 'train@tur.pdtb.tdb_samples_per_second': 83.056, 'train@tur.pdtb.tdb_steps_per_second': 2.609, 'epoch': 4.0}
{'loss': 2.3831, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.263359546661377, 'eval_accuracy@tur.pdtb.tdb': 0.26282051282051283, 'eval_f1@tur.pdtb.tdb': 0.018968309044644922, 'eval_precision@tur.pdtb.tdb': 0.012023460410557185, 'eval_recall@tur.pdtb.tdb': 0.044906900328587074, 'eval_loss@tur.pdtb.tdb': 2.263359546661377, 'eval_runtime': 4.114, 'eval_samples_per_second': 75.839, 'eval_steps_per_second': 2.431, 'epoch': 4.0}
{'train@tur.pdtb.tdb_loss': 2.276393175125122, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.2974296205630355, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.06377929081378894, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.10449248685193138, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.07449339448314769, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.276393175125122, 'train@tur.pdtb.tdb_runtime': 29.5518, 'train@tur.pdtb.tdb_samples_per_second': 82.939, 'train@tur.pdtb.tdb_steps_per_second': 2.606, 'epoch': 5.0}
{'loss': 2.3354, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.2287487983703613, 'eval_accuracy@tur.pdtb.tdb': 0.28205128205128205, 'eval_f1@tur.pdtb.tdb': 0.054460189429507894, 'eval_precision@tur.pdtb.tdb': 0.07646713313097014, 'eval_recall@tur.pdtb.tdb': 0.0673281360737066, 'eval_loss@tur.pdtb.tdb': 2.2287485599517822, 'eval_runtime': 4.1324, 'eval_samples_per_second': 75.501, 'eval_steps_per_second': 2.42, 'epoch': 5.0}
{'train@tur.pdtb.tdb_loss': 2.2328097820281982, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3198694410444716, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.08503475372713366, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.09630373882447034, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.09919917709842331, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.2328097820281982, 'train@tur.pdtb.tdb_runtime': 29.5404, 'train@tur.pdtb.tdb_samples_per_second': 82.971, 'train@tur.pdtb.tdb_steps_per_second': 2.607, 'epoch': 6.0}
{'loss': 2.2948, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.2024292945861816, 'eval_accuracy@tur.pdtb.tdb': 0.2948717948717949, 'eval_f1@tur.pdtb.tdb': 0.07412668231874663, 'eval_precision@tur.pdtb.tdb': 0.08673872806623549, 'eval_recall@tur.pdtb.tdb': 0.10036515121946361, 'eval_loss@tur.pdtb.tdb': 2.2024295330047607, 'eval_runtime': 4.1356, 'eval_samples_per_second': 75.443, 'eval_steps_per_second': 2.418, 'epoch': 6.0}
{'train@tur.pdtb.tdb_loss': 2.2004354000091553, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3333333333333333, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.08980837266654355, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.08809450040584625, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.10950195821623403, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.2004354000091553, 'train@tur.pdtb.tdb_runtime': 29.5354, 'train@tur.pdtb.tdb_samples_per_second': 82.985, 'train@tur.pdtb.tdb_steps_per_second': 2.607, 'epoch': 7.0}
{'loss': 2.2644, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.18169903755188, 'eval_accuracy@tur.pdtb.tdb': 0.30128205128205127, 'eval_f1@tur.pdtb.tdb': 0.07668944485293225, 'eval_precision@tur.pdtb.tdb': 0.07813510019392372, 'eval_recall@tur.pdtb.tdb': 0.10283873617386698, 'eval_loss@tur.pdtb.tdb': 2.181698799133301, 'eval_runtime': 4.1189, 'eval_samples_per_second': 75.748, 'eval_steps_per_second': 2.428, 'epoch': 7.0}
{'train@tur.pdtb.tdb_loss': 2.18524169921875, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3333333333333333, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09042401788481903, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.0882456390918149, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.10918738985860023, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.185241222381592, 'train@tur.pdtb.tdb_runtime': 29.4907, 'train@tur.pdtb.tdb_samples_per_second': 83.111, 'train@tur.pdtb.tdb_steps_per_second': 2.611, 'epoch': 8.0}
{'loss': 2.2398, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.1700003147125244, 'eval_accuracy@tur.pdtb.tdb': 0.2980769230769231, 'eval_f1@tur.pdtb.tdb': 0.07578648695669972, 'eval_precision@tur.pdtb.tdb': 0.07472538675230012, 'eval_recall@tur.pdtb.tdb': 0.10150183777814505, 'eval_loss@tur.pdtb.tdb': 2.1700005531311035, 'eval_runtime': 4.1113, 'eval_samples_per_second': 75.888, 'eval_steps_per_second': 2.432, 'epoch': 8.0}
{'train@tur.pdtb.tdb_loss': 2.160712718963623, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.33741330069359443, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09001569285935256, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.0894899022760812, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11238142302273416, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.160712718963623, 'train@tur.pdtb.tdb_runtime': 29.5131, 'train@tur.pdtb.tdb_samples_per_second': 83.048, 'train@tur.pdtb.tdb_steps_per_second': 2.609, 'epoch': 9.0}
{'loss': 2.2185, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.152876615524292, 'eval_accuracy@tur.pdtb.tdb': 0.3141025641025641, 'eval_f1@tur.pdtb.tdb': 0.07874925563604808, 'eval_precision@tur.pdtb.tdb': 0.07865992932791546, 'eval_recall@tur.pdtb.tdb': 0.10826060188985034, 'eval_loss@tur.pdtb.tdb': 2.152876615524292, 'eval_runtime': 4.1274, 'eval_samples_per_second': 75.592, 'eval_steps_per_second': 2.423, 'epoch': 9.0}
{'train@tur.pdtb.tdb_loss': 2.147613048553467, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3353733170134639, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09020190639939385, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.08525950915052087, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11199986938460654, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.147613048553467, 'train@tur.pdtb.tdb_runtime': 29.5328, 'train@tur.pdtb.tdb_samples_per_second': 82.993, 'train@tur.pdtb.tdb_steps_per_second': 2.607, 'epoch': 10.0}
{'loss': 2.1977, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.142408609390259, 'eval_accuracy@tur.pdtb.tdb': 0.3141025641025641, 'eval_f1@tur.pdtb.tdb': 0.07915432039928516, 'eval_precision@tur.pdtb.tdb': 0.07452823455851497, 'eval_recall@tur.pdtb.tdb': 0.1088270387603269, 'eval_loss@tur.pdtb.tdb': 2.142408609390259, 'eval_runtime': 4.1193, 'eval_samples_per_second': 75.742, 'eval_steps_per_second': 2.428, 'epoch': 10.0}
{'train@tur.pdtb.tdb_loss': 2.14261794090271, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3378212974296206, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09051297683276467, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.08731973788833602, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11224952754924013, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.142618179321289, 'train@tur.pdtb.tdb_runtime': 29.5377, 'train@tur.pdtb.tdb_samples_per_second': 82.979, 'train@tur.pdtb.tdb_steps_per_second': 2.607, 'epoch': 11.0}
{'loss': 2.1981, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.139749765396118, 'eval_accuracy@tur.pdtb.tdb': 0.3173076923076923, 'eval_f1@tur.pdtb.tdb': 0.0797540283041674, 'eval_precision@tur.pdtb.tdb': 0.07833499910923765, 'eval_recall@tur.pdtb.tdb': 0.10937468388628528, 'eval_loss@tur.pdtb.tdb': 2.1397500038146973, 'eval_runtime': 4.1271, 'eval_samples_per_second': 75.598, 'eval_steps_per_second': 2.423, 'epoch': 11.0}
{'train@tur.pdtb.tdb_loss': 2.1397440433502197, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3382292941656467, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09085385244130728, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.08670681930287938, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.112618112136513, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1397440433502197, 'train@tur.pdtb.tdb_runtime': 29.5303, 'train@tur.pdtb.tdb_samples_per_second': 83.0, 'train@tur.pdtb.tdb_steps_per_second': 2.607, 'epoch': 12.0}
{'loss': 2.1798, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.137444496154785, 'eval_accuracy@tur.pdtb.tdb': 0.3141025641025641, 'eval_f1@tur.pdtb.tdb': 0.07925655900399517, 'eval_precision@tur.pdtb.tdb': 0.07479121668844989, 'eval_recall@tur.pdtb.tdb': 0.1088270387603269, 'eval_loss@tur.pdtb.tdb': 2.1374447345733643, 'eval_runtime': 4.1191, 'eval_samples_per_second': 75.744, 'eval_steps_per_second': 2.428, 'epoch': 12.0}
{'train_runtime': 1149.6966, 'train_samples_per_second': 25.582, 'train_steps_per_second': 0.804, 'train_loss': 2.3949654484208014, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =      2.395
  train_runtime            = 0:19:09.69
  train_samples_per_second =     25.582
  train_steps_per_second   =      0.804
{'train@rus.rst.rrt_loss': 1.7266353368759155, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.4930955520088821, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.1792736995185456, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.21807877476219184, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.1972428972507127, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.726635456085205, 'train@rus.rst.rrt_runtime': 343.3456, 'train@rus.rst.rrt_samples_per_second': 83.945, 'train@rus.rst.rrt_steps_per_second': 2.624, 'epoch': 1.0}
{'loss': 2.1368, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.763777256011963, 'eval_accuracy@rus.rst.rrt': 0.4749562171628722, 'eval_f1@rus.rst.rrt': 0.1984270029245681, 'eval_precision@rus.rst.rrt': 0.2421285677966033, 'eval_recall@rus.rst.rrt': 0.21824859110361677, 'eval_loss@rus.rst.rrt': 1.763777256011963, 'eval_runtime': 34.3956, 'eval_samples_per_second': 83.005, 'eval_steps_per_second': 2.617, 'epoch': 1.0}
{'train@rus.rst.rrt_loss': 1.5084044933319092, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.541287905072514, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.22715049344117635, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.33592139093141243, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.23372603307448428, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.5084046125411987, 'train@rus.rst.rrt_runtime': 343.1393, 'train@rus.rst.rrt_samples_per_second': 83.995, 'train@rus.rst.rrt_steps_per_second': 2.626, 'epoch': 2.0}
{'loss': 1.6527, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.571976900100708, 'eval_accuracy@rus.rst.rrt': 0.5180385288966725, 'eval_f1@rus.rst.rrt': 0.25491128863446777, 'eval_precision@rus.rst.rrt': 0.3354863588920703, 'eval_recall@rus.rst.rrt': 0.26105086358515733, 'eval_loss@rus.rst.rrt': 1.571976900100708, 'eval_runtime': 34.3576, 'eval_samples_per_second': 83.097, 'eval_steps_per_second': 2.62, 'epoch': 2.0}
{'train@rus.rst.rrt_loss': 1.4351463317871094, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5638748178474776, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.2913336574458848, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.3985767772491131, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.2814397361592059, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.4351462125778198, 'train@rus.rst.rrt_runtime': 343.2291, 'train@rus.rst.rrt_samples_per_second': 83.973, 'train@rus.rst.rrt_steps_per_second': 2.625, 'epoch': 3.0}
{'loss': 1.5224, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.5091592073440552, 'eval_accuracy@rus.rst.rrt': 0.5453590192644483, 'eval_f1@rus.rst.rrt': 0.3443470515241131, 'eval_precision@rus.rst.rrt': 0.4344487147387531, 'eval_recall@rus.rst.rrt': 0.33292114874348816, 'eval_loss@rus.rst.rrt': 1.5091592073440552, 'eval_runtime': 34.3663, 'eval_samples_per_second': 83.075, 'eval_steps_per_second': 2.619, 'epoch': 3.0}
{'train@rus.rst.rrt_loss': 1.3767492771148682, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5800430226909999, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.31780797490942997, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4274558384397044, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3007642400679095, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3767492771148682, 'train@rus.rst.rrt_runtime': 343.2772, 'train@rus.rst.rrt_samples_per_second': 83.961, 'train@rus.rst.rrt_steps_per_second': 2.625, 'epoch': 4.0}
{'loss': 1.4583, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.4540762901306152, 'eval_accuracy@rus.rst.rrt': 0.5558669001751313, 'eval_f1@rus.rst.rrt': 0.3638164014538705, 'eval_precision@rus.rst.rrt': 0.45760152737669857, 'eval_recall@rus.rst.rrt': 0.3465866043543763, 'eval_loss@rus.rst.rrt': 1.4540762901306152, 'eval_runtime': 34.381, 'eval_samples_per_second': 83.04, 'eval_steps_per_second': 2.618, 'epoch': 4.0}
{'train@rus.rst.rrt_loss': 1.3457196950912476, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5867739920893762, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.32859427745671926, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.43490697365648145, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.31238896579812425, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.345719814300537, 'train@rus.rst.rrt_runtime': 343.4174, 'train@rus.rst.rrt_samples_per_second': 83.927, 'train@rus.rst.rrt_steps_per_second': 2.624, 'epoch': 5.0}
{'loss': 1.4202, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.4317601919174194, 'eval_accuracy@rus.rst.rrt': 0.5614711033274956, 'eval_f1@rus.rst.rrt': 0.37986423015476145, 'eval_precision@rus.rst.rrt': 0.48801143750244363, 'eval_recall@rus.rst.rrt': 0.3636961087807088, 'eval_loss@rus.rst.rrt': 1.431760311126709, 'eval_runtime': 34.4218, 'eval_samples_per_second': 82.942, 'eval_steps_per_second': 2.615, 'epoch': 5.0}
{'train@rus.rst.rrt_loss': 1.3227542638778687, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5939907015474291, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.34093029726687124, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.43770241058569215, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.32374472382488323, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3227542638778687, 'train@rus.rst.rrt_runtime': 343.354, 'train@rus.rst.rrt_samples_per_second': 83.943, 'train@rus.rst.rrt_steps_per_second': 2.624, 'epoch': 6.0}
{'loss': 1.3923, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.4113917350769043, 'eval_accuracy@rus.rst.rrt': 0.5674255691768827, 'eval_f1@rus.rst.rrt': 0.3898856409943157, 'eval_precision@rus.rst.rrt': 0.4805743869862732, 'eval_recall@rus.rst.rrt': 0.37512492276057785, 'eval_loss@rus.rst.rrt': 1.4113917350769043, 'eval_runtime': 34.3713, 'eval_samples_per_second': 83.063, 'eval_steps_per_second': 2.618, 'epoch': 6.0}
{'train@rus.rst.rrt_loss': 1.306113839149475, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5986052321143571, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.34875395055190467, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4805105283974976, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.33057013996479084, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3061137199401855, 'train@rus.rst.rrt_runtime': 343.1939, 'train@rus.rst.rrt_samples_per_second': 83.982, 'train@rus.rst.rrt_steps_per_second': 2.625, 'epoch': 7.0}
{'loss': 1.3683, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.4012058973312378, 'eval_accuracy@rus.rst.rrt': 0.5688266199649737, 'eval_f1@rus.rst.rrt': 0.39388082909081595, 'eval_precision@rus.rst.rrt': 0.4599638358904994, 'eval_recall@rus.rst.rrt': 0.3804625306943231, 'eval_loss@rus.rst.rrt': 1.4012060165405273, 'eval_runtime': 34.4099, 'eval_samples_per_second': 82.97, 'eval_steps_per_second': 2.616, 'epoch': 7.0}
{'train@rus.rst.rrt_loss': 1.2932251691818237, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6012768024425786, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.35736151852550796, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4741836422136952, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3411740249193469, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2932251691818237, 'train@rus.rst.rrt_runtime': 343.1535, 'train@rus.rst.rrt_samples_per_second': 83.992, 'train@rus.rst.rrt_steps_per_second': 2.626, 'epoch': 8.0}
{'loss': 1.3537, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.3901852369308472, 'eval_accuracy@rus.rst.rrt': 0.5733800350262697, 'eval_f1@rus.rst.rrt': 0.4056508597052067, 'eval_precision@rus.rst.rrt': 0.46024248351422625, 'eval_recall@rus.rst.rrt': 0.3931079251431181, 'eval_loss@rus.rst.rrt': 1.3901852369308472, 'eval_runtime': 34.3787, 'eval_samples_per_second': 83.046, 'eval_steps_per_second': 2.618, 'epoch': 8.0}
{'train@rus.rst.rrt_loss': 1.2809205055236816, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6051280272014433, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36053704375561313, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.47593579451242435, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34329544699468884, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2809206247329712, 'train@rus.rst.rrt_runtime': 343.284, 'train@rus.rst.rrt_samples_per_second': 83.96, 'train@rus.rst.rrt_steps_per_second': 2.625, 'epoch': 9.0}
{'loss': 1.3434, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.38344407081604, 'eval_accuracy@rus.rst.rrt': 0.5726795096322241, 'eval_f1@rus.rst.rrt': 0.4095569442513215, 'eval_precision@rus.rst.rrt': 0.47602592394979426, 'eval_recall@rus.rst.rrt': 0.39581521384615526, 'eval_loss@rus.rst.rrt': 1.38344407081604, 'eval_runtime': 34.3902, 'eval_samples_per_second': 83.018, 'eval_steps_per_second': 2.617, 'epoch': 9.0}
{'train@rus.rst.rrt_loss': 1.2734779119491577, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6069322045659565, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36339859564694527, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.46190815887390324, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3437061203273125, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2734780311584473, 'train@rus.rst.rrt_runtime': 344.8753, 'train@rus.rst.rrt_samples_per_second': 83.572, 'train@rus.rst.rrt_steps_per_second': 2.613, 'epoch': 10.0}
{'loss': 1.3326, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.377124309539795, 'eval_accuracy@rus.rst.rrt': 0.5800350262697023, 'eval_f1@rus.rst.rrt': 0.4158327284000388, 'eval_precision@rus.rst.rrt': 0.48463389338995944, 'eval_recall@rus.rst.rrt': 0.3994433490226008, 'eval_loss@rus.rst.rrt': 1.377124309539795, 'eval_runtime': 34.5645, 'eval_samples_per_second': 82.599, 'eval_steps_per_second': 2.604, 'epoch': 10.0}
{'train@rus.rst.rrt_loss': 1.270267128944397, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6071750745957949, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.364490830886559, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4569105311149139, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34643655899629516, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.270267367362976, 'train@rus.rst.rrt_runtime': 345.0023, 'train@rus.rst.rrt_samples_per_second': 83.541, 'train@rus.rst.rrt_steps_per_second': 2.612, 'epoch': 11.0}
{'loss': 1.3246, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.3776260614395142, 'eval_accuracy@rus.rst.rrt': 0.5747810858143608, 'eval_f1@rus.rst.rrt': 0.4124377218364218, 'eval_precision@rus.rst.rrt': 0.4745714014142639, 'eval_recall@rus.rst.rrt': 0.39864354600769086, 'eval_loss@rus.rst.rrt': 1.3776259422302246, 'eval_runtime': 34.5265, 'eval_samples_per_second': 82.69, 'eval_steps_per_second': 2.607, 'epoch': 11.0}
{'train@rus.rst.rrt_loss': 1.2691596746444702, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6078689889667614, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3649413721766573, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.45717665573752164, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3469544541237419, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2691596746444702, 'train@rus.rst.rrt_runtime': 344.9201, 'train@rus.rst.rrt_samples_per_second': 83.561, 'train@rus.rst.rrt_steps_per_second': 2.612, 'epoch': 12.0}
{'loss': 1.3205, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.3766913414001465, 'eval_accuracy@rus.rst.rrt': 0.5779334500875657, 'eval_f1@rus.rst.rrt': 0.4156130320711479, 'eval_precision@rus.rst.rrt': 0.4782851374325804, 'eval_recall@rus.rst.rrt': 0.4010575408468015, 'eval_loss@rus.rst.rrt': 1.3766913414001465, 'eval_runtime': 34.5654, 'eval_samples_per_second': 82.597, 'eval_steps_per_second': 2.604, 'epoch': 12.0}
{'train_runtime': 13336.9454, 'train_samples_per_second': 25.933, 'train_steps_per_second': 0.811, 'train_loss': 1.4688101093547679, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =      2.395
  train_runtime            = 0:19:09.69
  train_samples_per_second =     25.582
  train_steps_per_second   =      0.804
-------------------------------------------------------------------
Lang1:  zho.rst.sctb    Lang2:  rus.rst.rrt
Saving run to:  runs/full_shot/FullShot=v4_zho.rst.sctb_rus.rst.rrt_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 439 examples
read 94 examples
read 159 examples
read 28822 examples
read 2855 examples
read 2843 examples
Total prediction labels:  32
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (zh): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (ru): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=32, bias=True)
    )
  )
)
{'train@zho.rst.sctb_loss': 3.2106432914733887, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3348519362186788, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.02069545574700214, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03211009174311927, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.03918722786647315, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.2106432914733887, 'train@zho.rst.sctb_runtime': 5.4215, 'train@zho.rst.sctb_samples_per_second': 80.974, 'train@zho.rst.sctb_steps_per_second': 2.582, 'epoch': 1.0}
{'loss': 3.3647, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.2365405559539795, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 3.236539840698242, 'eval_runtime': 1.3809, 'eval_samples_per_second': 68.074, 'eval_steps_per_second': 2.173, 'epoch': 1.0}
{'train@zho.rst.sctb_loss': 3.0006017684936523, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.0006017684936523, 'train@zho.rst.sctb_runtime': 5.4462, 'train@zho.rst.sctb_samples_per_second': 80.607, 'train@zho.rst.sctb_steps_per_second': 2.571, 'epoch': 2.0}
{'loss': 3.1282, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.0351452827453613, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 3.0351462364196777, 'eval_runtime': 1.4105, 'eval_samples_per_second': 66.643, 'eval_steps_per_second': 2.127, 'epoch': 2.0}
{'train@zho.rst.sctb_loss': 2.8181264400482178, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.8181264400482178, 'train@zho.rst.sctb_runtime': 5.4527, 'train@zho.rst.sctb_samples_per_second': 80.511, 'train@zho.rst.sctb_steps_per_second': 2.568, 'epoch': 3.0}
{'loss': 2.9391, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.860560894012451, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.860560655593872, 'eval_runtime': 1.4031, 'eval_samples_per_second': 66.994, 'eval_steps_per_second': 2.138, 'epoch': 3.0}
{'train@zho.rst.sctb_loss': 2.6683714389801025, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.6683716773986816, 'train@zho.rst.sctb_runtime': 5.453, 'train@zho.rst.sctb_samples_per_second': 80.506, 'train@zho.rst.sctb_steps_per_second': 2.567, 'epoch': 4.0}
{'loss': 2.7777, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.7198262214660645, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.719825506210327, 'eval_runtime': 1.4099, 'eval_samples_per_second': 66.673, 'eval_steps_per_second': 2.128, 'epoch': 4.0}
{'train@zho.rst.sctb_loss': 2.551145315170288, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.551145076751709, 'train@zho.rst.sctb_runtime': 5.4684, 'train@zho.rst.sctb_samples_per_second': 80.279, 'train@zho.rst.sctb_steps_per_second': 2.56, 'epoch': 5.0}
{'loss': 2.6392, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.613093376159668, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.613093137741089, 'eval_runtime': 1.4209, 'eval_samples_per_second': 66.157, 'eval_steps_per_second': 2.111, 'epoch': 5.0}
{'train@zho.rst.sctb_loss': 2.4594485759735107, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3348519362186788, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.02005677652438983, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03208061960922373, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038866396761133605, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.4594480991363525, 'train@zho.rst.sctb_runtime': 5.4675, 'train@zho.rst.sctb_samples_per_second': 80.292, 'train@zho.rst.sctb_steps_per_second': 2.561, 'epoch': 6.0}
{'loss': 2.5394, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.533808708190918, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 2.533808708190918, 'eval_runtime': 1.4202, 'eval_samples_per_second': 66.19, 'eval_steps_per_second': 2.112, 'epoch': 6.0}
{'train@zho.rst.sctb_loss': 2.392529249191284, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.33712984054669703, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.020884069076840164, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.0321396993810787, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.039271255060728746, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.3925294876098633, 'train@zho.rst.sctb_runtime': 5.4727, 'train@zho.rst.sctb_samples_per_second': 80.217, 'train@zho.rst.sctb_steps_per_second': 2.558, 'epoch': 7.0}
{'loss': 2.4596, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.47770619392395, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 2.4777069091796875, 'eval_runtime': 1.3989, 'eval_samples_per_second': 67.195, 'eval_steps_per_second': 2.145, 'epoch': 7.0}
{'train@zho.rst.sctb_loss': 2.349257707595825, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3439635535307517, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.023198201404861226, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03706719614492236, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04048582995951417, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.3492579460144043, 'train@zho.rst.sctb_runtime': 5.4477, 'train@zho.rst.sctb_samples_per_second': 80.584, 'train@zho.rst.sctb_steps_per_second': 2.57, 'epoch': 8.0}
{'loss': 2.4107, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.4437289237976074, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.03185595567867036, 'eval_precision@zho.rst.sctb': 0.044050343249427915, 'eval_recall@zho.rst.sctb': 0.0540828173374613, 'eval_loss@zho.rst.sctb': 2.443728446960449, 'eval_runtime': 1.3948, 'eval_samples_per_second': 67.394, 'eval_steps_per_second': 2.151, 'epoch': 8.0}
{'train@zho.rst.sctb_loss': 2.320253372192383, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3530751708428246, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.02599681020733652, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03632075471698113, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04210526315789474, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.320253372192383, 'train@zho.rst.sctb_runtime': 5.4784, 'train@zho.rst.sctb_samples_per_second': 80.133, 'train@zho.rst.sctb_steps_per_second': 2.556, 'epoch': 9.0}
{'loss': 2.3607, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.421437978744507, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.035909445745511324, 'eval_precision@zho.rst.sctb': 0.043859649122807015, 'eval_recall@zho.rst.sctb': 0.05553405572755418, 'eval_loss@zho.rst.sctb': 2.421438455581665, 'eval_runtime': 1.4087, 'eval_samples_per_second': 66.73, 'eval_steps_per_second': 2.13, 'epoch': 9.0}
{'train@zho.rst.sctb_loss': 2.301981210708618, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3690205011389522, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.030815018315018313, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.039624674842066145, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04508069435971382, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.301981210708618, 'train@zho.rst.sctb_runtime': 5.4448, 'train@zho.rst.sctb_samples_per_second': 80.628, 'train@zho.rst.sctb_steps_per_second': 2.571, 'epoch': 10.0}
{'loss': 2.344, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.4081153869628906, 'eval_accuracy@zho.rst.sctb': 0.35106382978723405, 'eval_f1@zho.rst.sctb': 0.04004576659038902, 'eval_precision@zho.rst.sctb': 0.04425837320574162, 'eval_recall@zho.rst.sctb': 0.058630030959752326, 'eval_loss@zho.rst.sctb': 2.408115863800049, 'eval_runtime': 1.4116, 'eval_samples_per_second': 66.593, 'eval_steps_per_second': 2.125, 'epoch': 10.0}
{'train@zho.rst.sctb_loss': 2.292677164077759, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3690205011389522, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.030760587599097926, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03865137470091838, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04508069435971382, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.2926769256591797, 'train@zho.rst.sctb_runtime': 5.4661, 'train@zho.rst.sctb_samples_per_second': 80.313, 'train@zho.rst.sctb_steps_per_second': 2.561, 'epoch': 11.0}
{'loss': 2.3248, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.401366949081421, 'eval_accuracy@zho.rst.sctb': 0.35106382978723405, 'eval_f1@zho.rst.sctb': 0.04004576659038902, 'eval_precision@zho.rst.sctb': 0.04425837320574162, 'eval_recall@zho.rst.sctb': 0.058630030959752326, 'eval_loss@zho.rst.sctb': 2.401366710662842, 'eval_runtime': 1.4114, 'eval_samples_per_second': 66.598, 'eval_steps_per_second': 2.125, 'epoch': 11.0}
{'train@zho.rst.sctb_loss': 2.289616107940674, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3712984054669704, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.031338278033148474, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03917724670151855, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04548555265930897, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.2896156311035156, 'train@zho.rst.sctb_runtime': 5.4593, 'train@zho.rst.sctb_samples_per_second': 80.414, 'train@zho.rst.sctb_steps_per_second': 2.564, 'epoch': 12.0}
{'loss': 2.3099, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.399214267730713, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.038810260946483856, 'eval_precision@zho.rst.sctb': 0.040100250626566414, 'eval_recall@zho.rst.sctb': 0.05698529411764706, 'eval_loss@zho.rst.sctb': 2.399214506149292, 'eval_runtime': 1.4182, 'eval_samples_per_second': 66.283, 'eval_steps_per_second': 2.115, 'epoch': 12.0}
{'train_runtime': 213.5204, 'train_samples_per_second': 24.672, 'train_steps_per_second': 0.787, 'train_loss': 2.6331609998430525, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.6332
  train_runtime            = 0:03:33.52
  train_samples_per_second =     24.672
  train_steps_per_second   =      0.787
{'train@rus.rst.rrt_loss': 1.7213950157165527, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.4942058150024287, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.1816557670677138, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.23093112856948902, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.19457385520341008, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.7213951349258423, 'train@rus.rst.rrt_runtime': 344.8582, 'train@rus.rst.rrt_samples_per_second': 83.576, 'train@rus.rst.rrt_steps_per_second': 2.613, 'epoch': 1.0}
{'loss': 2.1463, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.7527562379837036, 'eval_accuracy@rus.rst.rrt': 0.47390542907180383, 'eval_f1@rus.rst.rrt': 0.19982434295378712, 'eval_precision@rus.rst.rrt': 0.2325126809381054, 'eval_recall@rus.rst.rrt': 0.2131659104202517, 'eval_loss@rus.rst.rrt': 1.752756118774414, 'eval_runtime': 34.4669, 'eval_samples_per_second': 82.833, 'eval_steps_per_second': 2.611, 'epoch': 1.0}
{'train@rus.rst.rrt_loss': 1.4979937076568604, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5446533897717022, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.22723760269801083, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.31802159310822975, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.23354696547171044, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.4979937076568604, 'train@rus.rst.rrt_runtime': 344.7474, 'train@rus.rst.rrt_samples_per_second': 83.603, 'train@rus.rst.rrt_steps_per_second': 2.614, 'epoch': 2.0}
{'loss': 1.6398, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.5603095293045044, 'eval_accuracy@rus.rst.rrt': 0.5201401050788091, 'eval_f1@rus.rst.rrt': 0.25663702996802557, 'eval_precision@rus.rst.rrt': 0.3587077725149946, 'eval_recall@rus.rst.rrt': 0.26114033816334437, 'eval_loss@rus.rst.rrt': 1.5603095293045044, 'eval_runtime': 34.4113, 'eval_samples_per_second': 82.967, 'eval_steps_per_second': 2.615, 'epoch': 2.0}
{'train@rus.rst.rrt_loss': 1.4175512790679932, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5667198667684408, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.2834939829099976, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.47246884970330744, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.2760406011041894, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.4175512790679932, 'train@rus.rst.rrt_runtime': 344.7202, 'train@rus.rst.rrt_samples_per_second': 83.61, 'train@rus.rst.rrt_steps_per_second': 2.614, 'epoch': 3.0}
{'loss': 1.5072, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.4866149425506592, 'eval_accuracy@rus.rst.rrt': 0.5432574430823117, 'eval_f1@rus.rst.rrt': 0.31923461086321214, 'eval_precision@rus.rst.rrt': 0.4177644352812554, 'eval_recall@rus.rst.rrt': 0.3121335128525281, 'eval_loss@rus.rst.rrt': 1.4866149425506592, 'eval_runtime': 34.4397, 'eval_samples_per_second': 82.899, 'eval_steps_per_second': 2.613, 'epoch': 3.0}
{'train@rus.rst.rrt_loss': 1.36669921875, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5821941572409964, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.31352024143202306, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4517443932370057, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.2967963207468529, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.36669921875, 'train@rus.rst.rrt_runtime': 344.46, 'train@rus.rst.rrt_samples_per_second': 83.673, 'train@rus.rst.rrt_steps_per_second': 2.616, 'epoch': 4.0}
{'loss': 1.4449, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.4408574104309082, 'eval_accuracy@rus.rst.rrt': 0.559369527145359, 'eval_f1@rus.rst.rrt': 0.35768973099204154, 'eval_precision@rus.rst.rrt': 0.4536769392873982, 'eval_recall@rus.rst.rrt': 0.3436307296166554, 'eval_loss@rus.rst.rrt': 1.4408574104309082, 'eval_runtime': 34.4214, 'eval_samples_per_second': 82.943, 'eval_steps_per_second': 2.615, 'epoch': 4.0}
{'train@rus.rst.rrt_loss': 1.3334907293319702, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.590208868225661, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3286479185705231, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4918793071510658, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.31064034442726235, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3334909677505493, 'train@rus.rst.rrt_runtime': 344.5064, 'train@rus.rst.rrt_samples_per_second': 83.662, 'train@rus.rst.rrt_steps_per_second': 2.615, 'epoch': 5.0}
{'loss': 1.4051, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.4164937734603882, 'eval_accuracy@rus.rst.rrt': 0.5642732049036777, 'eval_f1@rus.rst.rrt': 0.3749909750647788, 'eval_precision@rus.rst.rrt': 0.47081736353802633, 'eval_recall@rus.rst.rrt': 0.3613599897236991, 'eval_loss@rus.rst.rrt': 1.4164937734603882, 'eval_runtime': 34.4489, 'eval_samples_per_second': 82.876, 'eval_steps_per_second': 2.613, 'epoch': 5.0}
{'train@rus.rst.rrt_loss': 1.3102508783340454, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.595760183193394, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.34085909379678536, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.485632718880195, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.32232190393458293, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3102508783340454, 'train@rus.rst.rrt_runtime': 344.5021, 'train@rus.rst.rrt_samples_per_second': 83.663, 'train@rus.rst.rrt_steps_per_second': 2.615, 'epoch': 6.0}
{'loss': 1.3784, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.396334171295166, 'eval_accuracy@rus.rst.rrt': 0.5709281961471103, 'eval_f1@rus.rst.rrt': 0.3876972654635682, 'eval_precision@rus.rst.rrt': 0.4840899020540547, 'eval_recall@rus.rst.rrt': 0.3704091628995621, 'eval_loss@rus.rst.rrt': 1.396334171295166, 'eval_runtime': 34.4238, 'eval_samples_per_second': 82.937, 'eval_steps_per_second': 2.614, 'epoch': 6.0}
{'train@rus.rst.rrt_loss': 1.2946370840072632, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.600582888071612, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3513024977936892, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4589331078347316, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3328862530146715, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2946372032165527, 'train@rus.rst.rrt_runtime': 344.8436, 'train@rus.rst.rrt_samples_per_second': 83.58, 'train@rus.rst.rrt_steps_per_second': 2.613, 'epoch': 7.0}
{'loss': 1.3557, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.3879822492599487, 'eval_accuracy@rus.rst.rrt': 0.5733800350262697, 'eval_f1@rus.rst.rrt': 0.3974710638417176, 'eval_precision@rus.rst.rrt': 0.48500610112184267, 'eval_recall@rus.rst.rrt': 0.3836827681470927, 'eval_loss@rus.rst.rrt': 1.3879822492599487, 'eval_runtime': 34.4894, 'eval_samples_per_second': 82.779, 'eval_steps_per_second': 2.609, 'epoch': 7.0}
{'train@rus.rst.rrt_loss': 1.2822024822235107, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6041218513635417, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3602341265171978, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.45140308280742014, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34362257203895186, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2822023630142212, 'train@rus.rst.rrt_runtime': 344.854, 'train@rus.rst.rrt_samples_per_second': 83.577, 'train@rus.rst.rrt_steps_per_second': 2.613, 'epoch': 8.0}
{'loss': 1.3409, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.3765857219696045, 'eval_accuracy@rus.rst.rrt': 0.5761821366024519, 'eval_f1@rus.rst.rrt': 0.40396647174722405, 'eval_precision@rus.rst.rrt': 0.4735173069992838, 'eval_recall@rus.rst.rrt': 0.3938094866745174, 'eval_loss@rus.rst.rrt': 1.3765857219696045, 'eval_runtime': 34.4742, 'eval_samples_per_second': 82.816, 'eval_steps_per_second': 2.611, 'epoch': 8.0}
{'train@rus.rst.rrt_loss': 1.2702038288116455, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6072097703143432, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36324209040411215, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4643530711986981, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3442867952669358, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2702038288116455, 'train@rus.rst.rrt_runtime': 344.6434, 'train@rus.rst.rrt_samples_per_second': 83.628, 'train@rus.rst.rrt_steps_per_second': 2.614, 'epoch': 9.0}
{'loss': 1.331, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.3707892894744873, 'eval_accuracy@rus.rst.rrt': 0.5747810858143608, 'eval_f1@rus.rst.rrt': 0.4032904646453338, 'eval_precision@rus.rst.rrt': 0.47903664229853765, 'eval_recall@rus.rst.rrt': 0.39049156577944566, 'eval_loss@rus.rst.rrt': 1.3707894086837769, 'eval_runtime': 34.4263, 'eval_samples_per_second': 82.931, 'eval_steps_per_second': 2.614, 'epoch': 9.0}
{'train@rus.rst.rrt_loss': 1.2643969058990479, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6080771632780515, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3640373446667596, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.45951021908000106, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34355597862085724, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2643969058990479, 'train@rus.rst.rrt_runtime': 344.5873, 'train@rus.rst.rrt_samples_per_second': 83.642, 'train@rus.rst.rrt_steps_per_second': 2.615, 'epoch': 10.0}
{'loss': 1.3216, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.3688013553619385, 'eval_accuracy@rus.rst.rrt': 0.5786339754816112, 'eval_f1@rus.rst.rrt': 0.4095395980521714, 'eval_precision@rus.rst.rrt': 0.5037318406612554, 'eval_recall@rus.rst.rrt': 0.3935427463573944, 'eval_loss@rus.rst.rrt': 1.368801474571228, 'eval_runtime': 34.4208, 'eval_samples_per_second': 82.944, 'eval_steps_per_second': 2.615, 'epoch': 10.0}
{'train@rus.rst.rrt_loss': 1.259545922279358, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6093609048643397, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36768515450002687, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4578662972537749, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34883080500674857, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2595460414886475, 'train@rus.rst.rrt_runtime': 344.4512, 'train@rus.rst.rrt_samples_per_second': 83.675, 'train@rus.rst.rrt_steps_per_second': 2.616, 'epoch': 11.0}
{'loss': 1.3158, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.3672943115234375, 'eval_accuracy@rus.rst.rrt': 0.5754816112084064, 'eval_f1@rus.rst.rrt': 0.4098207242542376, 'eval_precision@rus.rst.rrt': 0.49818094730776524, 'eval_recall@rus.rst.rrt': 0.3961678025060943, 'eval_loss@rus.rst.rrt': 1.3672943115234375, 'eval_runtime': 34.4262, 'eval_samples_per_second': 82.931, 'eval_steps_per_second': 2.614, 'epoch': 11.0}
{'train@rus.rst.rrt_loss': 1.2584127187728882, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6104017764207896, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3680271944993478, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.457610848738974, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3493656283731064, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2584127187728882, 'train@rus.rst.rrt_runtime': 344.4742, 'train@rus.rst.rrt_samples_per_second': 83.67, 'train@rus.rst.rrt_steps_per_second': 2.616, 'epoch': 12.0}
{'loss': 1.3098, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.3653733730316162, 'eval_accuracy@rus.rst.rrt': 0.5793345008756567, 'eval_f1@rus.rst.rrt': 0.41293597062302956, 'eval_precision@rus.rst.rrt': 0.506377236065584, 'eval_recall@rus.rst.rrt': 0.39927874846259553, 'eval_loss@rus.rst.rrt': 1.3653733730316162, 'eval_runtime': 34.4104, 'eval_samples_per_second': 82.969, 'eval_steps_per_second': 2.615, 'epoch': 12.0}
{'train_runtime': 13304.8283, 'train_samples_per_second': 25.995, 'train_steps_per_second': 0.813, 'train_loss': 1.4580449574619057, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.6332
  train_runtime            = 0:03:33.52
  train_samples_per_second =     24.672
  train_steps_per_second   =      0.787

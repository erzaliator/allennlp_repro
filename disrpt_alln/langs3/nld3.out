0.3333333333333333
1.0
ASSIGN: 29
CustomBERTModel(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30073, 768, padding_idx=3)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (encoder): CustomPooler2(
    (_dropout): Dropout(p=0.0, inplace=False)
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (relation_decoder): Linear(in_features=768, out_features=29, bias=True)
)
['events.out.tfevents.1668146845.bc01bc43a6e4.23433.0']
Epoch 1: Best val_acc: 0.2927
Epoch 1: Best val_acc: 0.2927
Epoch 1: train_loss: 2.8481 train_acc: 0.2576 | val_loss: 2.5708 val_acc: 0.2927
00:02:25.11
train_size: 397
Epoch 2: Best val_acc: 0.3689
Epoch 2: Best val_acc: 0.3689
Epoch 2: train_loss: 2.5644 train_acc: 0.3149 | val_loss: 2.3886 val_acc: 0.3689
00:02:25.24
train_size: 397
Epoch 3: Best val_acc: 0.3811
Epoch 3: Best val_acc: 0.3811
Epoch 3: train_loss: 2.4265 train_acc: 0.3728 | val_loss: 2.3242 val_acc: 0.3811
00:02:26.54
train_size: 397
Epoch 4: Best val_acc: 0.4146
Epoch 4: Best val_acc: 0.4146
Epoch 4: train_loss: 2.3359 train_acc: 0.3917 | val_loss: 2.2481 val_acc: 0.4146
00:02:25.55
train_size: 397
Epoch 5: Best val_acc: 0.4299
Epoch 5: Best val_acc: 0.4299
Epoch 5: train_loss: 2.2464 train_acc: 0.4137 | val_loss: 2.1825 val_acc: 0.4299
00:02:25.82
train_size: 397
Epoch 6: Best val_acc: 0.4390
Epoch 6: Best val_acc: 0.4390
Epoch 6: train_loss: 2.1747 train_acc: 0.4257 | val_loss: 2.1561 val_acc: 0.4390
00:02:23.21
train_size: 397
Epoch 7: Best val_acc: 0.4665
Epoch 7: Best val_acc: 0.4665
Epoch 7: train_loss: 2.1064 train_acc: 0.4345 | val_loss: 2.0796 val_acc: 0.4665
00:02:25.58
train_size: 397
Epoch 8: Best val_acc: 0.4726
Epoch 8: Best val_acc: 0.4726
Epoch 8: train_loss: 2.0431 train_acc: 0.4553 | val_loss: 2.0485 val_acc: 0.4726
00:02:25.80
train_size: 397
Epoch 9: Best val_acc: 0.4909
Epoch 9: Best val_acc: 0.4909
Epoch 9: train_loss: 1.9818 train_acc: 0.4717 | val_loss: 2.0069 val_acc: 0.4909
00:02:25.91
train_size: 397
Epoch 10: Best val_acc: 0.4909
Epoch 10: train_loss: 1.9236 train_acc: 0.4754 | val_loss: 1.9922 val_acc: 0.4909
00:02:25.03
train_size: 397
Epoch 11: Best val_acc: 0.4939
Epoch 11: Best val_acc: 0.4939
Epoch 11: train_loss: 1.8601 train_acc: 0.5050 | val_loss: 1.9595 val_acc: 0.4939
00:02:25.65
train_size: 397
Epoch 12: Best val_acc: 0.5183
Epoch 12: Best val_acc: 0.5183
Epoch 12: train_loss: 1.8023 train_acc: 0.5088 | val_loss: 1.9264 val_acc: 0.5183
00:02:22.92
train_size: 397
Epoch 13: Best val_acc: 0.5335
Epoch 13: Best val_acc: 0.5335
Epoch 13: train_loss: 1.7370 train_acc: 0.5296 | val_loss: 1.8997 val_acc: 0.5335
00:02:25.20
train_size: 397
Epoch 00014: reducing learning rate of group 0 to 7.0000e-07.
Epoch 14: train_loss: 1.6865 train_acc: 0.5422 | val_loss: 1.8726 val_acc: 0.5274
00:02:23.33
train_size: 397
Epoch 15: Best val_acc: 0.5366
Epoch 15: Best val_acc: 0.5366
Epoch 15: train_loss: 1.6370 train_acc: 0.5529 | val_loss: 1.8519 val_acc: 0.5366
00:02:24.68
train_size: 397
Epoch 16: Best val_acc: 0.5457
Epoch 16: Best val_acc: 0.5457
Epoch 16: train_loss: 1.5879 train_acc: 0.5705 | val_loss: 1.8388 val_acc: 0.5457
00:02:24.84
train_size: 397
Epoch 17: Best val_acc: 0.5610
Epoch 17: Best val_acc: 0.5610
Epoch 17: train_loss: 1.5563 train_acc: 0.5793 | val_loss: 1.8304 val_acc: 0.5610
00:02:22.10
train_size: 397
Epoch 18: train_loss: 1.5128 train_acc: 0.5901 | val_loss: 1.8151 val_acc: 0.5457
00:02:22.99
train_size: 397
Epoch 19: train_loss: 1.4743 train_acc: 0.5982 | val_loss: 1.8333 val_acc: 0.5396
00:02:23.48
train_size: 397
Epoch 20: train_loss: 1.4380 train_acc: 0.6115 | val_loss: 1.8035 val_acc: 0.5518
00:02:23.23
train_size: 397
Epoch 21: train_loss: 1.3959 train_acc: 0.6278 | val_loss: 1.7834 val_acc: 0.5549
00:02:04.40
train_size: 397
Epoch 22: train_loss: 1.3687 train_acc: 0.6304 | val_loss: 1.8122 val_acc: 0.5366
00:02:16.65
train_size: 397
Epoch 23: Best val_acc: 0.5671
Epoch 23: Best val_acc: 0.5671
Epoch 23: train_loss: 1.3191 train_acc: 0.6656 | val_loss: 1.7704 val_acc: 0.5671
00:02:25.03
train_size: 397
Epoch 24: train_loss: 1.2885 train_acc: 0.6650 | val_loss: 1.7987 val_acc: 0.5488
00:02:20.90
train_size: 397
Epoch 25: train_loss: 1.2579 train_acc: 0.6763 | val_loss: 1.7988 val_acc: 0.5427
00:02:24.27
train_size: 397
Epoch 26: train_loss: 1.2208 train_acc: 0.6914 | val_loss: 1.7817 val_acc: 0.5640
00:02:23.55
train_size: 397
Epoch 00027: reducing learning rate of group 0 to 5.0000e-07.
Epoch 27: train_loss: 1.1836 train_acc: 0.7047 | val_loss: 1.7474 val_acc: 0.5640
00:02:23.53
train_size: 397
Epoch 28: Best val_acc: 0.5671
Epoch 28: train_loss: 1.1561 train_acc: 0.7160 | val_loss: 1.7465 val_acc: 0.5671
00:02:24.47
train_size: 397
Epoch 29: Best val_acc: 0.5701
Epoch 29: Best val_acc: 0.5701
Epoch 29: train_loss: 1.1341 train_acc: 0.7292 | val_loss: 1.7474 val_acc: 0.5701
00:02:22.50
train_size: 397
Epoch 30: Best val_acc: 0.5762
Epoch 30: Best val_acc: 0.5762
Epoch 30: train_loss: 1.1074 train_acc: 0.7280 | val_loss: 1.7443 val_acc: 0.5762
00:02:25.25
train_size: 397
Epoch 31: Best val_acc: 0.5823
Epoch 31: Best val_acc: 0.5823
Epoch 31: train_loss: 1.0823 train_acc: 0.7500 | val_loss: 1.7363 val_acc: 0.5823
00:02:25.35
train_size: 397
Epoch 32: train_loss: 1.0634 train_acc: 0.7494 | val_loss: 1.7505 val_acc: 0.5732
00:02:23.31
train_size: 397
Epoch 33: train_loss: 1.0442 train_acc: 0.7670 | val_loss: 1.7601 val_acc: 0.5701
00:02:23.42
train_size: 397
Epoch 34: train_loss: 1.0150 train_acc: 0.7689 | val_loss: 1.7369 val_acc: 0.5793
00:02:23.16
train_size: 397
Epoch 35: train_loss: 1.0037 train_acc: 0.7859 | val_loss: 1.7387 val_acc: 0.5793
00:02:20.62
train_size: 397
Epoch 36: train_loss: 0.9716 train_acc: 0.7890 | val_loss: 1.7438 val_acc: 0.5793
00:02:23.43
train_size: 397
Epoch 37: Best val_acc: 0.5854
Epoch 37: Best val_acc: 0.5854
Epoch 37: train_loss: 0.9556 train_acc: 0.7834 | val_loss: 1.7479 val_acc: 0.5854
00:02:24.59
train_size: 397
Epoch 38: train_loss: 0.9437 train_acc: 0.7935 | val_loss: 1.7519 val_acc: 0.5732
00:02:23.03
train_size: 397
Epoch 39: train_loss: 0.9156 train_acc: 0.8042 | val_loss: 1.7627 val_acc: 0.5701
00:02:22.94
train_size: 397
Epoch 40: train_loss: 0.9037 train_acc: 0.8073 | val_loss: 1.7396 val_acc: 0.5823
00:02:20.16
train_size: 397
Epoch 41: train_loss: 0.8748 train_acc: 0.8186 | val_loss: 1.7421 val_acc: 0.5823
00:02:23.02
train_size: 397
Epoch 42: Best val_acc: 0.5854
Epoch 42: train_loss: 0.8539 train_acc: 0.8237 | val_loss: 1.7525 val_acc: 0.5854
00:02:23.61
train_size: 397
Epoch 43: train_loss: 0.8447 train_acc: 0.8237 | val_loss: 1.7607 val_acc: 0.5823
00:02:22.80
train_size: 397
Epoch 44: Best val_acc: 0.5915
Epoch 44: Best val_acc: 0.5915
Epoch 44: train_loss: 0.8206 train_acc: 0.8356 | val_loss: 1.7515 val_acc: 0.5915
00:02:24.81
train_size: 397
Epoch 45: train_loss: 0.8041 train_acc: 0.8363 | val_loss: 1.7414 val_acc: 0.5823
00:02:23.06
train_size: 397
Epoch 46: train_loss: 0.7872 train_acc: 0.8432 | val_loss: 1.7868 val_acc: 0.5732
00:02:20.29
train_size: 397
Epoch 47: train_loss: 0.7736 train_acc: 0.8394 | val_loss: 1.7491 val_acc: 0.5854
00:02:23.17
train_size: 397
Epoch 48: train_loss: 0.7534 train_acc: 0.8457 | val_loss: 1.7483 val_acc: 0.5793
00:02:23.09
train_size: 397
Epoch 49: train_loss: 0.7402 train_acc: 0.8508 | val_loss: 1.7582 val_acc: 0.5701
00:02:22.88
train_size: 397
Epoch 50: train_loss: 0.7171 train_acc: 0.8615 | val_loss: 1.7758 val_acc: 0.5823
00:02:23.17
train_size: 397
Epoch 51: train_loss: 0.7048 train_acc: 0.8577 | val_loss: 1.7647 val_acc: 0.5732
00:02:21.93
train_size: 397
Epoch 52: train_loss: 0.6825 train_acc: 0.8671 | val_loss: 1.7688 val_acc: 0.5732
00:02:21.80
train_size: 397
Epoch 53: train_loss: 0.6762 train_acc: 0.8678 | val_loss: 1.7890 val_acc: 0.5701
00:02:23.29
train_size: 397
Epoch 54: train_loss: 0.6616 train_acc: 0.8741 | val_loss: 1.7758 val_acc: 0.5732
00:02:23.19
train_size: 397
Epoch 55: train_loss: 0.6386 train_acc: 0.8816 | val_loss: 1.8323 val_acc: 0.5732
00:02:22.99
train_size: 397
Test_loss: 2.0245 test_acc: 0.5152
00:00:09.40
                      precision    recall  f1-score   support

          antithesis       0.00      0.00      0.00         2
          background       0.00      0.00      0.00         3
        circumstance       0.38      0.50      0.43        16
          concession       0.83      0.42      0.56        12
           condition       0.56      0.62      0.59         8
         conjunction       0.41      0.47      0.44        19
            contrast       0.00      0.00      0.00         7
         disjunction       1.00      0.50      0.67         4
         elaboration       0.58      0.85      0.69        95
          enablement       1.00      0.50      0.67         4
          evaluation       0.33      0.50      0.40         2
            evidence       0.00      0.00      0.00         6
      interpretation       0.17      0.20      0.18        10
               joint       0.00      0.00      0.00         3
             justify       0.42      0.50      0.45        10
                list       0.50      0.25      0.33        12
               means       0.00      0.00      0.00         4
          motivation       0.57      0.69      0.62        29
 nonvolitional-cause       0.46      0.46      0.46        13
nonvolitional-result       0.50      0.21      0.30        14
           otherwise       0.00      0.00      0.00         1
         preparation       0.44      0.63      0.52        19
             purpose       1.00      0.67      0.80         6
         restatement       0.00      0.00      0.00         2
            sequence       0.00      0.00      0.00        10
        solutionhood       0.00      0.00      0.00         8
                span       0.00      0.00      0.00         1
             summary       0.00      0.00      0.00         4
    volitional-cause       0.00      0.00      0.00         2

            accuracy                           0.52       326
           macro avg       0.32      0.28      0.28       326
        weighted avg       0.45      0.52      0.47       326

Test Loss: 2.024 |  Test Acc: 51.52%
Test_loss: 1.9610 test_acc: 0.5152
00:00:09.39
                      precision    recall  f1-score   support

          antithesis       0.00      0.00      0.00         2
          background       0.00      0.00      0.00         3
        circumstance       0.33      0.44      0.38        16
          concession       0.83      0.42      0.56        12
           condition       0.56      0.62      0.59         8
         conjunction       0.36      0.47      0.41        19
            contrast       0.00      0.00      0.00         7
         disjunction       1.00      0.50      0.67         4
         elaboration       0.61      0.85      0.71        95
          enablement       1.00      0.25      0.40         4
          evaluation       0.33      0.50      0.40         2
            evidence       0.00      0.00      0.00         6
      interpretation       0.15      0.20      0.17        10
               joint       0.00      0.00      0.00         3
             justify       0.50      0.50      0.50        10
                list       0.50      0.17      0.25        12
               means       0.00      0.00      0.00         4
          motivation       0.54      0.69      0.61        29
 nonvolitional-cause       0.53      0.62      0.57        13
nonvolitional-result       0.44      0.29      0.35        14
           otherwise       0.00      0.00      0.00         1
         preparation       0.41      0.63      0.50        19
             purpose       1.00      0.67      0.80         6
         restatement       0.00      0.00      0.00         2
            sequence       0.00      0.00      0.00        10
        solutionhood       0.00      0.00      0.00         8
                span       0.00      0.00      0.00         1
             summary       0.00      0.00      0.00         4
    volitional-cause       0.00      0.00      0.00         2

            accuracy                           0.52       326
           macro avg       0.31      0.27      0.27       326
        weighted avg       0.46      0.52      0.47       326

Latest Test Loss: 1.961 |  Latest Test Acc: 51.52%
Test_loss: 1.9610 test_acc: 0.5152
00:00:09.29
                      precision    recall  f1-score   support

          antithesis       0.00      0.00      0.00         2
          background       0.00      0.00      0.00         3
        circumstance       0.33      0.44      0.38        16
          concession       0.83      0.42      0.56        12
           condition       0.56      0.62      0.59         8
         conjunction       0.36      0.47      0.41        19
            contrast       0.00      0.00      0.00         7
         disjunction       1.00      0.50      0.67         4
         elaboration       0.61      0.85      0.71        95
          enablement       1.00      0.25      0.40         4
          evaluation       0.33      0.50      0.40         2
            evidence       0.00      0.00      0.00         6
      interpretation       0.15      0.20      0.17        10
               joint       0.00      0.00      0.00         3
             justify       0.50      0.50      0.50        10
                list       0.50      0.17      0.25        12
               means       0.00      0.00      0.00         4
          motivation       0.54      0.69      0.61        29
 nonvolitional-cause       0.53      0.62      0.57        13
nonvolitional-result       0.44      0.29      0.35        14
           otherwise       0.00      0.00      0.00         1
         preparation       0.41      0.63      0.50        19
             purpose       1.00      0.67      0.80         6
         restatement       0.00      0.00      0.00         2
            sequence       0.00      0.00      0.00        10
        solutionhood       0.00      0.00      0.00         8
                span       0.00      0.00      0.00         1
             summary       0.00      0.00      0.00         4
    volitional-cause       0.00      0.00      0.00         2

            accuracy                           0.52       326
           macro avg       0.31      0.27      0.27       326
        weighted avg       0.46      0.52      0.47       326

Best Test Loss: 1.961 |  Best Test Acc: 51.52%
Test_loss: 1.7494 test_acc: 0.5915
00:00:09.38
                      precision    recall  f1-score   support

          antithesis       0.00      0.00      0.00         5
          background       0.00      0.00      0.00         5
        circumstance       0.60      0.54      0.57        28
          concession       0.57      0.44      0.50         9
           condition       0.70      1.00      0.82         7
         conjunction       0.70      0.66      0.68        35
            contrast       0.00      0.00      0.00         4
         disjunction       1.00      0.10      0.18        10
         elaboration       0.63      0.87      0.73        92
          enablement       0.00      0.00      0.00         4
          evaluation       0.00      0.00      0.00         2
            evidence       0.00      0.00      0.00         6
      interpretation       0.36      0.40      0.38        10
               joint       0.00      0.00      0.00         9
             justify       0.54      0.47      0.50        15
                list       0.00      0.00      0.00         2
               means       1.00      0.50      0.67         2
          motivation       0.65      0.61      0.63        18
 nonvolitional-cause       0.50      0.50      0.50        16
nonvolitional-result       0.64      0.58      0.61        12
         preparation       0.63      0.90      0.75        21
         restatement       0.00      0.00      0.00         2
            sequence       0.67      1.00      0.80         2
        solutionhood       0.17      0.40      0.24         5
             summary       0.00      0.00      0.00         2
    volitional-cause       0.00      0.00      0.00         2

            accuracy                           0.59       325
           macro avg       0.36      0.35      0.33       325
        weighted avg       0.54      0.59      0.54       325

Val Loss: 1.749 |  Val Acc: 59.15%

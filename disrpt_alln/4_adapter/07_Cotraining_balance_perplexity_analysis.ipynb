{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seeding for comparing experiment in part 2\n",
    "import torch\n",
    "import json\n",
    "SEED = 2025\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('cuda:2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNLI Bert\n",
    "## Second Tutorial\n",
    "https://towardsdatascience.com/fine-tuning-pre-trained-transformer-models-for-sentence-entailment-d87caf9ec9db\n",
    "Check his Github code for complete notebook. I never referred to it. Medium was enough.\n",
    "BERT in keras-tf: https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define macros\n",
    "BERT_MODEL = 'bert-base-german-cased'\n",
    "batch_size = 4\n",
    "batches_per_epoch = None\n",
    "\n",
    "save_path_suffix = 'cotraining_baseline_de_en_allshuffle_de_labelset_balanceen2000_perplexity_'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# custom reader needed to handle quotechars\n",
    "def read_df_custom(file, perplexity=False):\n",
    "    if perplexity:\n",
    "        header = 'doc\tunit1_toks\tunit2_toks\tunit1_txt\tunit2_txt\tperplexity_score\toriginal_unit1_txt\toriginal_unit2_txt\tsentiment_score_1\tsentiment_score_2\tsentiment_graded_agreement\tsentiment_agreement\ts1_toks\ts2_toks\tunit1_sent\tunit2_sent\tdir\tnuc_children\tsat_children\tgenre\tu1_discontinuous\tu2_discontinuous\tu1_issent\tu2_issent\tu1_length\tu2_length\tlength_ratio\tu1_speaker\tu2_speaker\tsame_speaker\tu1_func\tu1_pos\tu1_depdir\tu2_func\tu2_pos\tu2_depdir\tdoclen\tu1_position\tu2_position\tpercent_distance\tdistance\tlex_overlap_words\tlex_overlap_length\tunit1_case\tunit2_case\tlabel'\n",
    "        extracted_columns = ['unit1_txt', 'unit1_sent', 'unit2_txt', 'unit2_sent', 'dir', 'label', 'distance', 'u1_depdir', 'u2_depdir', 'u2_func', 'u1_position', 'u2_position', 'sat_children', 'nuc_children', 'genre', 'unit1_case', 'unit2_case',\n",
    "                                'u1_discontinuous', 'u2_discontinuous', 'same_speaker', 'lex_overlap_length', 'u1_func', 'perplexity_score']\n",
    "    else:\n",
    "        header = 'doc\tunit1_toks\tunit2_toks\tunit1_txt\tunit2_txt\ts1_toks\ts2_toks\tunit1_sent\tunit2_sent\tdir\tnuc_children\tsat_children\tgenre\tu1_discontinuous\tu2_discontinuous\tu1_issent\tu2_issent\tu1_length\tu2_length\tlength_ratio\tu1_speaker\tu2_speaker\tsame_speaker\tu1_func\tu1_pos\tu1_depdir\tu2_func\tu2_pos\tu2_depdir\tdoclen\tu1_position\tu2_position\tpercent_distance\tdistance\tlex_overlap_words\tlex_overlap_length\tunit1_case\tunit2_case\tlabel'\n",
    "        extracted_columns = ['unit1_txt', 'unit1_sent', 'unit2_txt', 'unit2_sent', 'dir', 'label', 'distance', 'u1_depdir', 'u2_depdir', 'u2_func', 'u1_position', 'u2_position', 'sat_children', 'nuc_children', 'genre', 'unit1_case', 'unit2_case',\n",
    "                                'u1_discontinuous', 'u2_discontinuous', 'same_speaker', 'lex_overlap_length', 'u1_func']\n",
    "    header = header.split()\n",
    "    df = pd.DataFrame(columns=extracted_columns)\n",
    "    file = open(file, 'r')\n",
    "\n",
    "    rows = []\n",
    "    count = 0 \n",
    "    for line in file:\n",
    "        line = line[:-1].split('\\t')\n",
    "        count+=1\n",
    "        if count ==1: continue\n",
    "        row = {}\n",
    "        for column in extracted_columns:\n",
    "            index = header.index(column)\n",
    "            try:\n",
    "                row[column] = line[index]\n",
    "            except:\n",
    "                print(count, line)\n",
    "            row[column] = line[index]\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame.from_records(rows)])\n",
    "    if perplexity:\n",
    "        df['perplexity_score'] = df['perplexity_score'].astype(float)\n",
    "    return df\n",
    "\n",
    "train_df_de = read_df_custom('../../processed/translated/perplexity_top/deu.rst.pcc_train_enriched_translated.rels', perplexity=True)\n",
    "test_df_de = read_df_custom('../../processed/deu.rst.pcc_test_enriched.rels')\n",
    "val_df_de = read_df_custom('../../processed/deu.rst.pcc_dev_enriched.rels')\n",
    "train_df_en = read_df_custom('../../processed/translated/perplexity_top/eng.rst.rstdt_train_enriched_translated.rels', perplexity=True)\n",
    "test_df_en = read_df_custom('../../processed/translated/perplexity_top/eng.rst.rstdt_test_enriched_translated.rels', perplexity=True)\n",
    "val_df_en = read_df_custom('../../processed/translated/perplexity_top/eng.rst.rstdt_dev_enriched_translated.rels', perplexity=True)\n",
    "\n",
    "lang='deu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity average per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "attribution             10954.269459\n",
       "background                645.946950\n",
       "cause                     969.636241\n",
       "comparison               1386.145714\n",
       "condition                 760.808535\n",
       "contrast                  578.961405\n",
       "elaboration              2058.360552\n",
       "enablement                917.842415\n",
       "evaluation               1204.624760\n",
       "explanation               665.707008\n",
       "joint                    2025.612224\n",
       "manner-means              932.480977\n",
       "summary                  3501.154059\n",
       "temporal                 5331.382970\n",
       "textual-organization     3955.837917\n",
       "topic-change             1170.457495\n",
       "topic-comment            1002.787940\n",
       "Name: perplexity_score, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_en.groupby('label')['perplexity_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "antithesis         848.745525\n",
       "background         494.589449\n",
       "cause              784.900340\n",
       "circumstance       504.304861\n",
       "concession         634.819208\n",
       "condition          958.825681\n",
       "conjunction       1541.822773\n",
       "contrast           695.012255\n",
       "disjunction        514.471251\n",
       "e-elaboration      821.025898\n",
       "elaboration        913.735893\n",
       "evaluation-n       753.698329\n",
       "evaluation-s       703.826031\n",
       "evidence           415.832474\n",
       "interpretation    1103.010157\n",
       "joint              838.943208\n",
       "list              1375.893955\n",
       "means              629.372303\n",
       "preparation       1952.404203\n",
       "purpose            690.524908\n",
       "reason             622.669783\n",
       "restatement        395.210284\n",
       "result             628.991563\n",
       "sequence          2979.259654\n",
       "solutionhood       637.784074\n",
       "summary            282.086824\n",
       "Name: perplexity_score, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_de.groupby('label')['perplexity_score'].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3409ea685db85227fbd9509d1b1ace14d085473eb2d57f3ba9dd0302d25f838"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

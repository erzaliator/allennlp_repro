-------------------------------------------------------------------
Lang1:  deu.rst.pcc    Lang2:  por.rst.cstn
Saving run to:  runs/SingleEpochPass=v4_deu.rst.pcc_por.rst.cstn_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2164 examples
read 241 examples
read 260 examples
read 4148 examples
read 573 examples
read 272 examples
Total prediction labels:  42
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (pt): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (pt): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=42, bias=True)
    )
  )
)
{'train@deu.rst.pcc_loss': 3.493467330932617, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.09242144177449169, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.013183172136537101, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.009683152811268149, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.03746378000109343, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.4934675693511963, 'train@deu.rst.pcc_runtime': 26.8095, 'train@deu.rst.pcc_samples_per_second': 80.718, 'train@deu.rst.pcc_steps_per_second': 2.536, 'epoch': 1.0}
{'loss': 3.5906, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 3.5138208866119385, 'eval_accuracy@deu.rst.pcc': 0.058091286307053944, 'eval_f1@deu.rst.pcc': 0.011388204707170226, 'eval_precision@deu.rst.pcc': 0.009071695212999562, 'eval_recall@deu.rst.pcc': 0.03781288156288156, 'eval_loss@deu.rst.pcc': 3.5138206481933594, 'eval_runtime': 3.4141, 'eval_samples_per_second': 70.589, 'eval_steps_per_second': 2.343, 'epoch': 1.0}
{'train_runtime': 103.0782, 'train_samples_per_second': 20.994, 'train_steps_per_second': 0.66, 'train_loss': 3.59057123520795, 'epoch': 1.0}
***** train_log metrics *****
  epoch                    =        1.0
  train_loss               =     3.5906
  train_runtime            = 0:01:43.07
  train_samples_per_second =     20.994
  train_steps_per_second   =       0.66
{'train@por.rst.cstn_loss': 2.8551716804504395, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.2774831243972999, 'train@por.rst.cstn_f1@por.rst.cstn': 0.013575674655595393, 'train@por.rst.cstn_precision@por.rst.cstn': 0.008671347637415621, 'train@por.rst.cstn_recall@por.rst.cstn': 0.03125, 'train@por.rst.cstn_loss@por.rst.cstn': 2.8551714420318604, 'train@por.rst.cstn_runtime': 50.5289, 'train@por.rst.cstn_samples_per_second': 82.092, 'train@por.rst.cstn_steps_per_second': 2.573, 'epoch': 1.0}
{'loss': 3.1462, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 2.9155588150024414, 'eval_accuracy@por.rst.cstn': 0.28097731239092494, 'eval_f1@por.rst.cstn': 0.019940549913301957, 'eval_precision@por.rst.cstn': 0.012771696017769315, 'eval_recall@por.rst.cstn': 0.045454545454545456, 'eval_loss@por.rst.cstn': 2.9155590534210205, 'eval_runtime': 7.388, 'eval_samples_per_second': 77.558, 'eval_steps_per_second': 2.436, 'epoch': 1.0}
{'train_runtime': 164.5363, 'train_samples_per_second': 25.21, 'train_steps_per_second': 0.79, 'train_loss': 3.146167226938101, 'epoch': 1.0}
***** train_logger metrics *****
  epoch                    =        1.0
  train_loss               =     3.5906
  train_runtime            = 0:01:43.07
  train_samples_per_second =     20.994
  train_steps_per_second   =       0.66
-------------------------------------------------------------------
Lang1:  eng.pdtb.pdtb    Lang2:  por.rst.cstn
Saving run to:  runs/SingleEpochPass=v4_eng.pdtb.pdtb_por.rst.cstn_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 43920 examples
read 1674 examples
read 2257 examples
read 4148 examples
read 573 examples
read 272 examples
Total prediction labels:  55
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (pt): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (pt): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=55, bias=True)
    )
  )
)
{'train@eng.pdtb.pdtb_loss': 1.5973142385482788, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.5224726775956284, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.18482240649695694, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.26300568020035575, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.1822717015166946, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.5973142385482788, 'train@eng.pdtb.pdtb_runtime': 521.8145, 'train@eng.pdtb.pdtb_samples_per_second': 84.168, 'train@eng.pdtb.pdtb_steps_per_second': 2.631, 'epoch': 1.0}
{'loss': 2.0167, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 1.5484287738800049, 'eval_accuracy@eng.pdtb.pdtb': 0.5376344086021505, 'eval_f1@eng.pdtb.pdtb': 0.21339379825821747, 'eval_precision@eng.pdtb.pdtb': 0.3036815566027863, 'eval_recall@eng.pdtb.pdtb': 0.2136052684304352, 'eval_loss@eng.pdtb.pdtb': 1.5484291315078735, 'eval_runtime': 20.4554, 'eval_samples_per_second': 81.837, 'eval_steps_per_second': 2.591, 'epoch': 1.0}
{'train_runtime': 1657.0238, 'train_samples_per_second': 26.505, 'train_steps_per_second': 0.829, 'train_loss': 2.016675889361799, 'epoch': 1.0}
***** train_log metrics *****
  epoch                    =        1.0
  train_loss               =     2.0167
  train_runtime            = 0:27:37.02
  train_samples_per_second =     26.505
  train_steps_per_second   =      0.829
{'train@por.rst.cstn_loss': 3.02131724357605, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.21239151398264225, 'train@por.rst.cstn_f1@por.rst.cstn': 0.023913687690556705, 'train@por.rst.cstn_precision@por.rst.cstn': 0.030371799151004762, 'train@por.rst.cstn_recall@por.rst.cstn': 0.0417004506391597, 'train@por.rst.cstn_loss@por.rst.cstn': 3.021317481994629, 'train@por.rst.cstn_runtime': 50.8966, 'train@por.rst.cstn_samples_per_second': 81.498, 'train@por.rst.cstn_steps_per_second': 2.554, 'epoch': 1.0}
{'loss': 3.4353, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 3.093125820159912, 'eval_accuracy@por.rst.cstn': 0.17452006980802792, 'eval_f1@por.rst.cstn': 0.023693526908960993, 'eval_precision@por.rst.cstn': 0.017112299465240642, 'eval_recall@por.rst.cstn': 0.05586124401913875, 'eval_loss@por.rst.cstn': 3.093125581741333, 'eval_runtime': 7.581, 'eval_samples_per_second': 75.584, 'eval_steps_per_second': 2.374, 'epoch': 1.0}
{'train_runtime': 166.1771, 'train_samples_per_second': 24.961, 'train_steps_per_second': 0.782, 'train_loss': 3.4352900578425483, 'epoch': 1.0}
***** train_logger metrics *****
  epoch                    =        1.0
  train_loss               =     2.0167
  train_runtime            = 0:27:37.02
  train_samples_per_second =     26.505
  train_steps_per_second   =      0.829
-------------------------------------------------------------------
Lang1:  eng.rst.gum    Lang2:  por.rst.cstn
Saving run to:  runs/SingleEpochPass=v4_eng.rst.gum_por.rst.cstn_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 13897 examples
read 2149 examples
read 2091 examples
read 4148 examples
read 573 examples
read 272 examples
Total prediction labels:  37
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (pt): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (pt): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=37, bias=True)
    )
  )
)
{'train@eng.rst.gum_loss': 2.6731042861938477, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.22299776930272722, 'train@eng.rst.gum_f1@eng.rst.gum': 0.019324386220063307, 'train@eng.rst.gum_precision@eng.rst.gum': 0.017874424703073998, 'train@eng.rst.gum_recall@eng.rst.gum': 0.04496143083735278, 'train@eng.rst.gum_loss@eng.rst.gum': 2.6731042861938477, 'train@eng.rst.gum_runtime': 166.2848, 'train@eng.rst.gum_samples_per_second': 83.573, 'train@eng.rst.gum_steps_per_second': 2.616, 'epoch': 1.0}
{'loss': 2.8827, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 2.7442069053649902, 'eval_accuracy@eng.rst.gum': 0.21963704048394603, 'eval_f1@eng.rst.gum': 0.021019027670846683, 'eval_precision@eng.rst.gum': 0.019021522691461915, 'eval_recall@eng.rst.gum': 0.04622851451126762, 'eval_loss@eng.rst.gum': 2.7442071437835693, 'eval_runtime': 26.017, 'eval_samples_per_second': 82.6, 'eval_steps_per_second': 2.614, 'epoch': 1.0}
{'train_runtime': 542.0464, 'train_samples_per_second': 25.638, 'train_steps_per_second': 0.803, 'train_loss': 2.8826937971443964, 'epoch': 1.0}
***** train_log metrics *****
  epoch                    =        1.0
  train_loss               =     2.8827
  train_runtime            = 0:09:02.04
  train_samples_per_second =     25.638
  train_steps_per_second   =      0.803
{'train@por.rst.cstn_loss': 2.747143268585205, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.2774831243972999, 'train@por.rst.cstn_f1@por.rst.cstn': 0.013575674655595393, 'train@por.rst.cstn_precision@por.rst.cstn': 0.008671347637415621, 'train@por.rst.cstn_recall@por.rst.cstn': 0.03125, 'train@por.rst.cstn_loss@por.rst.cstn': 2.747143030166626, 'train@por.rst.cstn_runtime': 51.2759, 'train@por.rst.cstn_samples_per_second': 80.896, 'train@por.rst.cstn_steps_per_second': 2.535, 'epoch': 1.0}
{'loss': 3.0712, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 2.7930479049682617, 'eval_accuracy@por.rst.cstn': 0.28097731239092494, 'eval_f1@por.rst.cstn': 0.019940549913301957, 'eval_precision@por.rst.cstn': 0.012771696017769315, 'eval_recall@por.rst.cstn': 0.045454545454545456, 'eval_loss@por.rst.cstn': 2.7930479049682617, 'eval_runtime': 7.3802, 'eval_samples_per_second': 77.64, 'eval_steps_per_second': 2.439, 'epoch': 1.0}
{'train_runtime': 167.1893, 'train_samples_per_second': 24.81, 'train_steps_per_second': 0.778, 'train_loss': 3.0712226280799277, 'epoch': 1.0}
***** train_logger metrics *****
  epoch                    =        1.0
  train_loss               =     2.8827
  train_runtime            = 0:09:02.04
  train_samples_per_second =     25.638
  train_steps_per_second   =      0.803
-------------------------------------------------------------------
Lang1:  eng.rst.rstdt    Lang2:  por.rst.cstn
Saving run to:  runs/SingleEpochPass=v4_eng.rst.rstdt_por.rst.cstn_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 16002 examples
read 1621 examples
read 2155 examples
read 4148 examples
read 573 examples
read 272 examples
Total prediction labels:  39
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (pt): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (pt): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=39, bias=True)
    )
  )
)
{'train@eng.rst.rstdt_loss': 2.034090995788574, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.4237595300587427, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.04961137699508236, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.07405916824805488, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.06774462815418698, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 2.034090995788574, 'train@eng.rst.rstdt_runtime': 191.3914, 'train@eng.rst.rstdt_samples_per_second': 83.609, 'train@eng.rst.rstdt_steps_per_second': 2.618, 'epoch': 1.0}
{'loss': 2.306, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 1.9983338117599487, 'eval_accuracy@eng.rst.rstdt': 0.4244293645897594, 'eval_f1@eng.rst.rstdt': 0.049174978746874314, 'eval_precision@eng.rst.rstdt': 0.07504772820524994, 'eval_recall@eng.rst.rstdt': 0.06743081316874684, 'eval_loss@eng.rst.rstdt': 1.9983338117599487, 'eval_runtime': 19.7883, 'eval_samples_per_second': 81.917, 'eval_steps_per_second': 2.577, 'epoch': 1.0}
{'train_runtime': 617.2837, 'train_samples_per_second': 25.923, 'train_steps_per_second': 0.812, 'train_loss': 2.3060139486651696, 'epoch': 1.0}
***** train_log metrics *****
  epoch                    =        1.0
  train_loss               =      2.306
  train_runtime            = 0:10:17.28
  train_samples_per_second =     25.923
  train_steps_per_second   =      0.812
{'train@por.rst.cstn_loss': 2.7155492305755615, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.3324493731918997, 'train@por.rst.cstn_f1@por.rst.cstn': 0.030748235782482355, 'train@por.rst.cstn_precision@por.rst.cstn': 0.02200806241352141, 'train@por.rst.cstn_recall@por.rst.cstn': 0.05202407295345987, 'train@por.rst.cstn_loss@por.rst.cstn': 2.7155494689941406, 'train@por.rst.cstn_runtime': 50.6727, 'train@por.rst.cstn_samples_per_second': 81.859, 'train@por.rst.cstn_steps_per_second': 2.565, 'epoch': 1.0}
{'loss': 3.0551, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 2.795231580734253, 'eval_accuracy@por.rst.cstn': 0.2862129144851658, 'eval_f1@por.rst.cstn': 0.03688164665523156, 'eval_precision@por.rst.cstn': 0.024270704878409707, 'eval_recall@por.rst.cstn': 0.07714568040654997, 'eval_loss@por.rst.cstn': 2.795231580734253, 'eval_runtime': 7.4487, 'eval_samples_per_second': 76.926, 'eval_steps_per_second': 2.417, 'epoch': 1.0}
{'train_runtime': 164.7146, 'train_samples_per_second': 25.183, 'train_steps_per_second': 0.789, 'train_loss': 3.055133291391226, 'epoch': 1.0}
***** train_logger metrics *****
  epoch                    =        1.0
  train_loss               =      2.306
  train_runtime            = 0:10:17.28
  train_samples_per_second =     25.923
  train_steps_per_second   =      0.812
-------------------------------------------------------------------
Lang1:  eng.sdrt.stac    Lang2:  por.rst.cstn
Saving run to:  runs/SingleEpochPass=v4_eng.sdrt.stac_por.rst.cstn_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 9580 examples
read 1145 examples
read 1510 examples
read 4148 examples
read 573 examples
read 272 examples
Total prediction labels:  48
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (pt): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (pt): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=48, bias=True)
    )
  )
)
{'train@eng.sdrt.stac_loss': 2.3505804538726807, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.28538622129436325, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.047521554925560455, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.03960325674399083, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.08087158614710274, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 2.3505804538726807, 'train@eng.sdrt.stac_runtime': 114.7319, 'train@eng.sdrt.stac_samples_per_second': 83.499, 'train@eng.sdrt.stac_steps_per_second': 2.615, 'epoch': 1.0}
{'loss': 2.7153, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 2.324702739715576, 'eval_accuracy@eng.sdrt.stac': 0.2873362445414847, 'eval_f1@eng.sdrt.stac': 0.04379269263319094, 'eval_precision@eng.sdrt.stac': 0.036481623680684166, 'eval_recall@eng.sdrt.stac': 0.07616151318394172, 'eval_loss@eng.sdrt.stac': 2.324702739715576, 'eval_runtime': 14.2356, 'eval_samples_per_second': 80.432, 'eval_steps_per_second': 2.529, 'epoch': 1.0}
{'train_runtime': 369.6727, 'train_samples_per_second': 25.915, 'train_steps_per_second': 0.812, 'train_loss': 2.7152614339192707, 'epoch': 1.0}
***** train_log metrics *****
  epoch                    =        1.0
  train_loss               =     2.7153
  train_runtime            = 0:06:09.67
  train_samples_per_second =     25.915
  train_steps_per_second   =      0.812
{'train@por.rst.cstn_loss': 2.8184101581573486, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.29532304725168756, 'train@por.rst.cstn_f1@por.rst.cstn': 0.022773890829413078, 'train@por.rst.cstn_precision@por.rst.cstn': 0.03318138603759584, 'train@por.rst.cstn_recall@por.rst.cstn': 0.036586694596306986, 'train@por.rst.cstn_loss@por.rst.cstn': 2.8184094429016113, 'train@por.rst.cstn_runtime': 50.8281, 'train@por.rst.cstn_samples_per_second': 81.608, 'train@por.rst.cstn_steps_per_second': 2.558, 'epoch': 1.0}
{'loss': 3.2046, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 2.922409772872925, 'eval_accuracy@por.rst.cstn': 0.2844677137870855, 'eval_f1@por.rst.cstn': 0.028448007471980074, 'eval_precision@por.rst.cstn': 0.04687250359482345, 'eval_recall@por.rst.cstn': 0.05004234895539243, 'eval_loss@por.rst.cstn': 2.922409772872925, 'eval_runtime': 7.4545, 'eval_samples_per_second': 76.867, 'eval_steps_per_second': 2.415, 'epoch': 1.0}
{'train_runtime': 164.8114, 'train_samples_per_second': 25.168, 'train_steps_per_second': 0.789, 'train_loss': 3.204603224534255, 'epoch': 1.0}
***** train_logger metrics *****
  epoch                    =        1.0
  train_loss               =     2.7153
  train_runtime            = 0:06:09.67
  train_samples_per_second =     25.915
  train_steps_per_second   =      0.812
-------------------------------------------------------------------
Lang1:  fas.rst.prstc    Lang2:  por.rst.cstn
Saving run to:  runs/SingleEpochPass=v4_fas.rst.prstc_por.rst.cstn_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 4100 examples
read 499 examples
read 592 examples
read 4148 examples
read 573 examples
read 272 examples
Total prediction labels:  39
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (ar): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (pt): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (pt): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=39, bias=True)
    )
  )
)
{'train@fas.rst.prstc_loss': 2.6118388175964355, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.23780487804878048, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.02260214430599826, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.013988522238163558, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.058823529411764705, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.6118390560150146, 'train@fas.rst.prstc_runtime': 49.235, 'train@fas.rst.prstc_samples_per_second': 83.274, 'train@fas.rst.prstc_steps_per_second': 2.62, 'epoch': 1.0}
{'loss': 2.9454, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 2.5401394367218018, 'eval_accuracy@fas.rst.prstc': 0.24448897795591182, 'eval_f1@fas.rst.prstc': 0.026194310252281264, 'eval_precision@fas.rst.prstc': 0.016299265197060788, 'eval_recall@fas.rst.prstc': 0.06666666666666667, 'eval_loss@fas.rst.prstc': 2.5401391983032227, 'eval_runtime': 6.377, 'eval_samples_per_second': 78.25, 'eval_steps_per_second': 2.509, 'epoch': 1.0}
{'train_runtime': 158.7297, 'train_samples_per_second': 25.83, 'train_steps_per_second': 0.813, 'train_loss': 2.9453837076822915, 'epoch': 1.0}
***** train_log metrics *****
  epoch                    =        1.0
  train_loss               =     2.9454
  train_runtime            = 0:02:38.72
  train_samples_per_second =      25.83
  train_steps_per_second   =      0.813
{'train@por.rst.cstn_loss': 2.8321752548217773, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.2774831243972999, 'train@por.rst.cstn_f1@por.rst.cstn': 0.013575674655595393, 'train@por.rst.cstn_precision@por.rst.cstn': 0.008671347637415621, 'train@por.rst.cstn_recall@por.rst.cstn': 0.03125, 'train@por.rst.cstn_loss@por.rst.cstn': 2.8321757316589355, 'train@por.rst.cstn_runtime': 50.6928, 'train@por.rst.cstn_samples_per_second': 81.826, 'train@por.rst.cstn_steps_per_second': 2.564, 'epoch': 1.0}
{'loss': 3.1155, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 2.8728227615356445, 'eval_accuracy@por.rst.cstn': 0.28097731239092494, 'eval_f1@por.rst.cstn': 0.019940549913301957, 'eval_precision@por.rst.cstn': 0.012771696017769315, 'eval_recall@por.rst.cstn': 0.045454545454545456, 'eval_loss@por.rst.cstn': 2.8728227615356445, 'eval_runtime': 7.3522, 'eval_samples_per_second': 77.936, 'eval_steps_per_second': 2.448, 'epoch': 1.0}
{'train_runtime': 164.5298, 'train_samples_per_second': 25.211, 'train_steps_per_second': 0.79, 'train_loss': 3.1154879056490383, 'epoch': 1.0}
***** train_logger metrics *****
  epoch                    =        1.0
  train_loss               =     2.9454
  train_runtime            = 0:02:38.72
  train_samples_per_second =      25.83
  train_steps_per_second   =      0.813
-------------------------------------------------------------------
Lang1:  fra.sdrt.annodis    Lang2:  por.rst.cstn
Saving run to:  runs/SingleEpochPass=v4_fra.sdrt.annodis_por.rst.cstn_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2185 examples
read 528 examples
read 625 examples
read 4148 examples
read 573 examples
read 272 examples
Total prediction labels:  45
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (fr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (pt): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (pt): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=45, bias=True)
    )
  )
)
{'train@fra.sdrt.annodis_loss': 3.290771961212158, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.18947368421052632, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.02372612762575083, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.031136637734911085, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.05583805517064022, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 3.2907721996307373, 'train@fra.sdrt.annodis_runtime': 26.944, 'train@fra.sdrt.annodis_samples_per_second': 81.094, 'train@fra.sdrt.annodis_steps_per_second': 2.561, 'epoch': 1.0}
{'loss': 3.4923, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 3.2973756790161133, 'eval_accuracy@fra.sdrt.annodis': 0.2215909090909091, 'eval_f1@fra.sdrt.annodis': 0.026849217638691324, 'eval_precision@fra.sdrt.annodis': 0.023585123585123588, 'eval_recall@fra.sdrt.annodis': 0.05872695125031573, 'eval_loss@fra.sdrt.annodis': 3.2973761558532715, 'eval_runtime': 6.8869, 'eval_samples_per_second': 76.667, 'eval_steps_per_second': 2.468, 'epoch': 1.0}
{'train_runtime': 90.1162, 'train_samples_per_second': 24.246, 'train_steps_per_second': 0.766, 'train_loss': 3.492278168166893, 'epoch': 1.0}
***** train_log metrics *****
  epoch                    =        1.0
  train_loss               =     3.4923
  train_runtime            = 0:01:30.11
  train_samples_per_second =     24.246
  train_steps_per_second   =      0.766
{'train@por.rst.cstn_loss': 2.9737699031829834, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.2774831243972999, 'train@por.rst.cstn_f1@por.rst.cstn': 0.013575674655595393, 'train@por.rst.cstn_precision@por.rst.cstn': 0.008671347637415621, 'train@por.rst.cstn_recall@por.rst.cstn': 0.03125, 'train@por.rst.cstn_loss@por.rst.cstn': 2.9737699031829834, 'train@por.rst.cstn_runtime': 52.3478, 'train@por.rst.cstn_samples_per_second': 79.239, 'train@por.rst.cstn_steps_per_second': 2.483, 'epoch': 1.0}
{'loss': 3.298, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 3.0157217979431152, 'eval_accuracy@por.rst.cstn': 0.28097731239092494, 'eval_f1@por.rst.cstn': 0.019940549913301957, 'eval_precision@por.rst.cstn': 0.012771696017769315, 'eval_recall@por.rst.cstn': 0.045454545454545456, 'eval_loss@por.rst.cstn': 3.0157217979431152, 'eval_runtime': 7.4187, 'eval_samples_per_second': 77.238, 'eval_steps_per_second': 2.426, 'epoch': 1.0}
{'train_runtime': 166.3548, 'train_samples_per_second': 24.935, 'train_steps_per_second': 0.781, 'train_loss': 3.2980187049278844, 'epoch': 1.0}
***** train_logger metrics *****
  epoch                    =        1.0
  train_loss               =     3.4923
  train_runtime            = 0:01:30.11
  train_samples_per_second =     24.246
  train_steps_per_second   =      0.766
-------------------------------------------------------------------
Lang1:  nld.rst.nldt    Lang2:  por.rst.cstn
Saving run to:  runs/SingleEpochPass=v4_nld.rst.nldt_por.rst.cstn_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 1608 examples
read 331 examples
read 326 examples
read 4148 examples
read 573 examples
read 272 examples
Total prediction labels:  42
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (pt): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (pt): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=42, bias=True)
    )
  )
)
{'train@nld.rst.nldt_loss': 3.5199086666107178, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26430348258706465, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01306566650270536, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.00825948383084577, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03125, 'train@nld.rst.nldt_loss@nld.rst.nldt': 3.5199086666107178, 'train@nld.rst.nldt_runtime': 19.8176, 'train@nld.rst.nldt_samples_per_second': 81.14, 'train@nld.rst.nldt_steps_per_second': 2.573, 'epoch': 1.0}
{'loss': 3.6256, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 3.494168996810913, 'eval_accuracy@nld.rst.nldt': 0.27794561933534745, 'eval_f1@nld.rst.nldt': 0.016110673321075214, 'eval_precision@nld.rst.nldt': 0.010294282197605462, 'eval_recall@nld.rst.nldt': 0.037037037037037035, 'eval_loss@nld.rst.nldt': 3.4941697120666504, 'eval_runtime': 4.4621, 'eval_samples_per_second': 74.181, 'eval_steps_per_second': 2.465, 'epoch': 1.0}
{'train_runtime': 65.463, 'train_samples_per_second': 24.564, 'train_steps_per_second': 0.779, 'train_loss': 3.6255654727711395, 'epoch': 1.0}
***** train_log metrics *****
  epoch                    =        1.0
  train_loss               =     3.6256
  train_runtime            = 0:01:05.46
  train_samples_per_second =     24.564
  train_steps_per_second   =      0.779
{'train@por.rst.cstn_loss': 2.9237194061279297, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.2774831243972999, 'train@por.rst.cstn_f1@por.rst.cstn': 0.013575674655595393, 'train@por.rst.cstn_precision@por.rst.cstn': 0.008671347637415621, 'train@por.rst.cstn_recall@por.rst.cstn': 0.03125, 'train@por.rst.cstn_loss@por.rst.cstn': 2.923719644546509, 'train@por.rst.cstn_runtime': 50.7599, 'train@por.rst.cstn_samples_per_second': 81.718, 'train@por.rst.cstn_steps_per_second': 2.561, 'epoch': 1.0}
{'loss': 3.2089, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 2.9302196502685547, 'eval_accuracy@por.rst.cstn': 0.28097731239092494, 'eval_f1@por.rst.cstn': 0.019940549913301957, 'eval_precision@por.rst.cstn': 0.012771696017769315, 'eval_recall@por.rst.cstn': 0.045454545454545456, 'eval_loss@por.rst.cstn': 2.9302194118499756, 'eval_runtime': 7.403, 'eval_samples_per_second': 77.401, 'eval_steps_per_second': 2.431, 'epoch': 1.0}
{'train_runtime': 164.7613, 'train_samples_per_second': 25.176, 'train_steps_per_second': 0.789, 'train_loss': 3.208881084735577, 'epoch': 1.0}
***** train_logger metrics *****
  epoch                    =        1.0
  train_loss               =     3.6256
  train_runtime            = 0:01:05.46
  train_samples_per_second =     24.564
  train_steps_per_second   =      0.779
-------------------------------------------------------------------
Lang1:  rus.rst.rrt    Lang2:  por.rst.cstn
Saving run to:  runs/SingleEpochPass=v4_rus.rst.rrt_por.rst.cstn_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 28822 examples
read 2855 examples
read 2843 examples
read 4148 examples
read 573 examples
read 272 examples
Total prediction labels:  37
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (ru): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (pt): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (pt): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=37, bias=True)
    )
  )
)
{'train@rus.rst.rrt_loss': 2.052159547805786, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.3941433627090417, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.09848208921990743, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.1603366373648264, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.11029014032977048, 'train@rus.rst.rrt_loss@rus.rst.rrt': 2.052159547805786, 'train@rus.rst.rrt_runtime': 350.4327, 'train@rus.rst.rrt_samples_per_second': 82.247, 'train@rus.rst.rrt_steps_per_second': 2.571, 'epoch': 1.0}
{'loss': 2.2975, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 2.077794313430786, 'eval_accuracy@rus.rst.rrt': 0.3607705779334501, 'eval_f1@rus.rst.rrt': 0.09871174022522426, 'eval_precision@rus.rst.rrt': 0.14510976027539763, 'eval_recall@rus.rst.rrt': 0.1131794432098478, 'eval_loss@rus.rst.rrt': 2.077794313430786, 'eval_runtime': 35.1781, 'eval_samples_per_second': 81.158, 'eval_steps_per_second': 2.558, 'epoch': 1.0}
{'train_runtime': 1127.7144, 'train_samples_per_second': 25.558, 'train_steps_per_second': 0.799, 'train_loss': 2.297488467674806, 'epoch': 1.0}
***** train_log metrics *****
  epoch                    =        1.0
  train_loss               =     2.2975
  train_runtime            = 0:18:47.71
  train_samples_per_second =     25.558
  train_steps_per_second   =      0.799
{'train@por.rst.cstn_loss': 2.9310598373413086, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.27917068466730954, 'train@por.rst.cstn_f1@por.rst.cstn': 0.014266582029973848, 'train@por.rst.cstn_precision@por.rst.cstn': 0.027921821225932474, 'train@por.rst.cstn_recall@por.rst.cstn': 0.03156344915095747, 'train@por.rst.cstn_loss@por.rst.cstn': 2.9310595989227295, 'train@por.rst.cstn_runtime': 50.6775, 'train@por.rst.cstn_samples_per_second': 81.851, 'train@por.rst.cstn_steps_per_second': 2.565, 'epoch': 1.0}
{'loss': 3.2583, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 2.945209264755249, 'eval_accuracy@por.rst.cstn': 0.28097731239092494, 'eval_f1@por.rst.cstn': 0.019940549913301957, 'eval_precision@por.rst.cstn': 0.012771696017769315, 'eval_recall@por.rst.cstn': 0.045454545454545456, 'eval_loss@por.rst.cstn': 2.945209264755249, 'eval_runtime': 7.3489, 'eval_samples_per_second': 77.971, 'eval_steps_per_second': 2.449, 'epoch': 1.0}
{'train_runtime': 165.3597, 'train_samples_per_second': 25.085, 'train_steps_per_second': 0.786, 'train_loss': 3.2582789494441107, 'epoch': 1.0}
***** train_logger metrics *****
  epoch                    =        1.0
  train_loss               =     2.2975
  train_runtime            = 0:18:47.71
  train_samples_per_second =     25.558
  train_steps_per_second   =      0.799
-------------------------------------------------------------------
Lang1:  spa.rst.rststb    Lang2:  por.rst.cstn
Saving run to:  runs/SingleEpochPass=v4_spa.rst.rststb_por.rst.cstn_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2240 examples
read 383 examples
read 426 examples
read 4148 examples
read 573 examples
read 272 examples
Total prediction labels:  41
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (pt): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (pt): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=41, bias=True)
    )
  )
)
{'train@spa.rst.rststb_loss': 3.283273696899414, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.2513392857142857, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.037486930994765334, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.03193681488948509, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.05301346142157308, 'train@spa.rst.rststb_loss@spa.rst.rststb': 3.283273935317993, 'train@spa.rst.rststb_runtime': 27.6026, 'train@spa.rst.rststb_samples_per_second': 81.152, 'train@spa.rst.rststb_steps_per_second': 2.536, 'epoch': 1.0}
{'loss': 3.4645, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 3.301534652709961, 'eval_accuracy@spa.rst.rststb': 0.21409921671018275, 'eval_f1@spa.rst.rststb': 0.03493504474274744, 'eval_precision@spa.rst.rststb': 0.03279147145713348, 'eval_recall@spa.rst.rststb': 0.050843682722127595, 'eval_loss@spa.rst.rststb': 3.301534414291382, 'eval_runtime': 5.0241, 'eval_samples_per_second': 76.233, 'eval_steps_per_second': 2.389, 'epoch': 1.0}
{'train_runtime': 89.88, 'train_samples_per_second': 24.922, 'train_steps_per_second': 0.779, 'train_loss': 3.464481898716518, 'epoch': 1.0}
***** train_log metrics *****
  epoch                    =        1.0
  train_loss               =     3.4645
  train_runtime            = 0:01:29.87
  train_samples_per_second =     24.922
  train_steps_per_second   =      0.779
{'train@por.rst.cstn_loss': 2.8033487796783447, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.2774831243972999, 'train@por.rst.cstn_f1@por.rst.cstn': 0.013575674655595393, 'train@por.rst.cstn_precision@por.rst.cstn': 0.008671347637415621, 'train@por.rst.cstn_recall@por.rst.cstn': 0.03125, 'train@por.rst.cstn_loss@por.rst.cstn': 2.8033485412597656, 'train@por.rst.cstn_runtime': 50.7223, 'train@por.rst.cstn_samples_per_second': 81.779, 'train@por.rst.cstn_steps_per_second': 2.563, 'epoch': 1.0}
{'loss': 3.0987, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 2.8503060340881348, 'eval_accuracy@por.rst.cstn': 0.28097731239092494, 'eval_f1@por.rst.cstn': 0.019940549913301957, 'eval_precision@por.rst.cstn': 0.012771696017769315, 'eval_recall@por.rst.cstn': 0.045454545454545456, 'eval_loss@por.rst.cstn': 2.8503060340881348, 'eval_runtime': 7.3873, 'eval_samples_per_second': 77.565, 'eval_steps_per_second': 2.437, 'epoch': 1.0}
{'train_runtime': 164.7165, 'train_samples_per_second': 25.183, 'train_steps_per_second': 0.789, 'train_loss': 3.098699716421274, 'epoch': 1.0}
***** train_logger metrics *****
  epoch                    =        1.0
  train_loss               =     3.4645
  train_runtime            = 0:01:29.87
  train_samples_per_second =     24.922
  train_steps_per_second   =      0.779
-------------------------------------------------------------------
Lang1:  spa.rst.sctb    Lang2:  por.rst.cstn
Saving run to:  runs/SingleEpochPass=v4_spa.rst.sctb_por.rst.cstn_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 439 examples
read 94 examples
read 159 examples
read 4148 examples
read 573 examples
read 272 examples
Total prediction labels:  38
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (pt): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (pt): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=38, bias=True)
    )
  )
)
{'train@spa.rst.sctb_loss': 3.4749133586883545, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.2072892938496583, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02224536387310466, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.028677919480785852, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.02747814013943046, 'train@spa.rst.sctb_loss@spa.rst.sctb': 3.4749133586883545, 'train@spa.rst.sctb_runtime': 5.6515, 'train@spa.rst.sctb_samples_per_second': 77.679, 'train@spa.rst.sctb_steps_per_second': 2.477, 'epoch': 1.0}
{'loss': 3.549, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 3.4677507877349854, 'eval_accuracy@spa.rst.sctb': 0.2872340425531915, 'eval_f1@spa.rst.sctb': 0.04640385565991102, 'eval_precision@spa.rst.sctb': 0.06018349368733, 'eval_recall@spa.rst.sctb': 0.06370203583825874, 'eval_loss@spa.rst.sctb': 3.467750072479248, 'eval_runtime': 1.5579, 'eval_samples_per_second': 60.339, 'eval_steps_per_second': 1.926, 'epoch': 1.0}
{'train_runtime': 18.5257, 'train_samples_per_second': 23.697, 'train_steps_per_second': 0.756, 'train_loss': 3.548992701939174, 'epoch': 1.0}
***** train_log metrics *****
  epoch                    =        1.0
  train_loss               =      3.549
  train_runtime            = 0:00:18.52
  train_samples_per_second =     23.697
  train_steps_per_second   =      0.756
{'train@por.rst.cstn_loss': 2.8516769409179688, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.2774831243972999, 'train@por.rst.cstn_f1@por.rst.cstn': 0.013575674655595393, 'train@por.rst.cstn_precision@por.rst.cstn': 0.008671347637415621, 'train@por.rst.cstn_recall@por.rst.cstn': 0.03125, 'train@por.rst.cstn_loss@por.rst.cstn': 2.8516769409179688, 'train@por.rst.cstn_runtime': 50.7072, 'train@por.rst.cstn_samples_per_second': 81.803, 'train@por.rst.cstn_steps_per_second': 2.564, 'epoch': 1.0}
{'loss': 3.1196, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 2.892380952835083, 'eval_accuracy@por.rst.cstn': 0.28097731239092494, 'eval_f1@por.rst.cstn': 0.019940549913301957, 'eval_precision@por.rst.cstn': 0.012771696017769315, 'eval_recall@por.rst.cstn': 0.045454545454545456, 'eval_loss@por.rst.cstn': 2.892380952835083, 'eval_runtime': 7.4054, 'eval_samples_per_second': 77.376, 'eval_steps_per_second': 2.431, 'epoch': 1.0}
{'train_runtime': 164.5523, 'train_samples_per_second': 25.208, 'train_steps_per_second': 0.79, 'train_loss': 3.119572566105769, 'epoch': 1.0}
***** train_logger metrics *****
  epoch                    =        1.0
  train_loss               =      3.549
  train_runtime            = 0:00:18.52
  train_samples_per_second =     23.697
  train_steps_per_second   =      0.756
-------------------------------------------------------------------
Lang1:  tur.pdtb.tdb    Lang2:  por.rst.cstn
Saving run to:  runs/SingleEpochPass=v4_tur.pdtb.tdb_por.rst.cstn_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2451 examples
read 312 examples
read 422 examples
read 4148 examples
read 573 examples
read 272 examples
Total prediction labels:  55
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (tr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (pt): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (tr): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (pt): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=55, bias=True)
    )
  )
)
{'train@tur.pdtb.tdb_loss': 3.483093738555908, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01741967515426626, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010891738953044898, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 3.483093738555908, 'train@tur.pdtb.tdb_runtime': 30.2598, 'train@tur.pdtb.tdb_samples_per_second': 80.998, 'train@tur.pdtb.tdb_steps_per_second': 2.545, 'epoch': 1.0}
{'loss': 3.6804, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 3.449303150177002, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 3.449303150177002, 'eval_runtime': 4.3592, 'eval_samples_per_second': 71.572, 'eval_steps_per_second': 2.294, 'epoch': 1.0}
{'train_runtime': 97.3741, 'train_samples_per_second': 25.171, 'train_steps_per_second': 0.791, 'train_loss': 3.680373947341721, 'epoch': 1.0}
***** train_log metrics *****
  epoch                    =        1.0
  train_loss               =     3.6804
  train_runtime            = 0:01:37.37
  train_samples_per_second =     25.171
  train_steps_per_second   =      0.791
{'train@por.rst.cstn_loss': 3.0221049785614014, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.2774831243972999, 'train@por.rst.cstn_f1@por.rst.cstn': 0.013575674655595393, 'train@por.rst.cstn_precision@por.rst.cstn': 0.008671347637415621, 'train@por.rst.cstn_recall@por.rst.cstn': 0.03125, 'train@por.rst.cstn_loss@por.rst.cstn': 3.0221049785614014, 'train@por.rst.cstn_runtime': 50.923, 'train@por.rst.cstn_samples_per_second': 81.456, 'train@por.rst.cstn_steps_per_second': 2.553, 'epoch': 1.0}
{'loss': 3.3454, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 3.091031074523926, 'eval_accuracy@por.rst.cstn': 0.28097731239092494, 'eval_f1@por.rst.cstn': 0.019940549913301957, 'eval_precision@por.rst.cstn': 0.012771696017769315, 'eval_recall@por.rst.cstn': 0.045454545454545456, 'eval_loss@por.rst.cstn': 3.0910308361053467, 'eval_runtime': 7.5792, 'eval_samples_per_second': 75.601, 'eval_steps_per_second': 2.375, 'epoch': 1.0}
{'train_runtime': 165.0746, 'train_samples_per_second': 25.128, 'train_steps_per_second': 0.788, 'train_loss': 3.3453693096454327, 'epoch': 1.0}
***** train_logger metrics *****
  epoch                    =        1.0
  train_loss               =     3.6804
  train_runtime            = 0:01:37.37
  train_samples_per_second =     25.171
  train_steps_per_second   =      0.791
-------------------------------------------------------------------
Lang1:  zho.rst.sctb    Lang2:  por.rst.cstn
Saving run to:  runs/SingleEpochPass=v4_zho.rst.sctb_por.rst.cstn_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 439 examples
read 94 examples
read 159 examples
read 4148 examples
read 573 examples
read 272 examples
Total prediction labels:  38
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (zh): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (pt): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (pt): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=38, bias=True)
    )
  )
)
{'train@zho.rst.sctb_loss': 3.5081939697265625, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.21184510250569477, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.018997776244153053, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.02383190118152524, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.033159177343721154, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.5081944465637207, 'train@zho.rst.sctb_runtime': 5.5808, 'train@zho.rst.sctb_samples_per_second': 78.662, 'train@zho.rst.sctb_steps_per_second': 2.509, 'epoch': 1.0}
{'loss': 3.565, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 3.5141584873199463, 'eval_accuracy@zho.rst.sctb': 0.19148936170212766, 'eval_f1@zho.rst.sctb': 0.03298845404108562, 'eval_precision@zho.rst.sctb': 0.034146469004104896, 'eval_recall@zho.rst.sctb': 0.04685887512899897, 'eval_loss@zho.rst.sctb': 3.514157772064209, 'eval_runtime': 2.8483, 'eval_samples_per_second': 33.002, 'eval_steps_per_second': 1.053, 'epoch': 1.0}
{'train_runtime': 19.4755, 'train_samples_per_second': 22.541, 'train_steps_per_second': 0.719, 'train_loss': 3.5650245121547153, 'epoch': 1.0}
***** train_log metrics *****
  epoch                    =        1.0
  train_loss               =      3.565
  train_runtime            = 0:00:19.47
  train_samples_per_second =     22.541
  train_steps_per_second   =      0.719
{'train@por.rst.cstn_loss': 2.849114418029785, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.2774831243972999, 'train@por.rst.cstn_f1@por.rst.cstn': 0.013575674655595393, 'train@por.rst.cstn_precision@por.rst.cstn': 0.008671347637415621, 'train@por.rst.cstn_recall@por.rst.cstn': 0.03125, 'train@por.rst.cstn_loss@por.rst.cstn': 2.8491146564483643, 'train@por.rst.cstn_runtime': 50.7554, 'train@por.rst.cstn_samples_per_second': 81.725, 'train@por.rst.cstn_steps_per_second': 2.561, 'epoch': 1.0}
{'loss': 3.1156, 'learning_rate': 0.0, 'epoch': 1.0}
{'eval_loss': 2.8896796703338623, 'eval_accuracy@por.rst.cstn': 0.28097731239092494, 'eval_f1@por.rst.cstn': 0.019940549913301957, 'eval_precision@por.rst.cstn': 0.012771696017769315, 'eval_recall@por.rst.cstn': 0.045454545454545456, 'eval_loss@por.rst.cstn': 2.889679431915283, 'eval_runtime': 7.4326, 'eval_samples_per_second': 77.093, 'eval_steps_per_second': 2.422, 'epoch': 1.0}
{'train_runtime': 164.5814, 'train_samples_per_second': 25.203, 'train_steps_per_second': 0.79, 'train_loss': 3.1155970646784854, 'epoch': 1.0}
***** train_logger metrics *****
  epoch                    =        1.0
  train_loss               =      3.565
  train_runtime            = 0:00:19.47
  train_samples_per_second =     22.541
  train_steps_per_second   =      0.719

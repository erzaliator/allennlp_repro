-------------------------------------------------------------------
Lang1:  deu.rst.pcc    Lang2:  zho.rst.sctb
Saving run to:  runs/full_shot/FullShot=v4_deu.rst.pcc_zho.rst.sctb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2164 examples
read 241 examples
read 260 examples
read 439 examples
read 94 examples
read 159 examples
Total prediction labels:  31
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (zh): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=31, bias=True)
    )
  )
)
{'train@deu.rst.pcc_loss': 3.124781370162964, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.1044362292051756, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.013070149689965702, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.010934243288989608, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.04150798008682499, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.124781370162964, 'train@deu.rst.pcc_runtime': 26.1796, 'train@deu.rst.pcc_samples_per_second': 82.66, 'train@deu.rst.pcc_steps_per_second': 2.597, 'epoch': 1.0}
{'loss': 3.2705, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.138221025466919, 'eval_accuracy@deu.rst.pcc': 0.11203319502074689, 'eval_f1@deu.rst.pcc': 0.011904761904761904, 'eval_precision@deu.rst.pcc': 0.0081143951833607, 'eval_recall@deu.rst.pcc': 0.04100529100529101, 'eval_loss@deu.rst.pcc': 3.1382205486297607, 'eval_runtime': 3.1592, 'eval_samples_per_second': 76.286, 'eval_steps_per_second': 2.532, 'epoch': 1.0}
{'train@deu.rst.pcc_loss': 2.9544801712036133, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.11645101663585952, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.02023082265761251, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.04567663829936149, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.04673254251620999, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.954479932785034, 'train@deu.rst.pcc_runtime': 26.0243, 'train@deu.rst.pcc_samples_per_second': 83.153, 'train@deu.rst.pcc_steps_per_second': 2.613, 'epoch': 2.0}
{'loss': 3.0436, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.9858601093292236, 'eval_accuracy@deu.rst.pcc': 0.12033195020746888, 'eval_f1@deu.rst.pcc': 0.013455083347556462, 'eval_precision@deu.rst.pcc': 0.008267195767195767, 'eval_recall@deu.rst.pcc': 0.044808201058201054, 'eval_loss@deu.rst.pcc': 2.9858603477478027, 'eval_runtime': 3.1728, 'eval_samples_per_second': 75.958, 'eval_steps_per_second': 2.521, 'epoch': 2.0}
{'train@deu.rst.pcc_loss': 2.8754608631134033, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.13863216266173753, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.03279639485657958, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.06885974256816929, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.05864607500260839, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.875460624694824, 'train@deu.rst.pcc_runtime': 26.1189, 'train@deu.rst.pcc_samples_per_second': 82.852, 'train@deu.rst.pcc_steps_per_second': 2.603, 'epoch': 3.0}
{'loss': 2.9364, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.919147491455078, 'eval_accuracy@deu.rst.pcc': 0.13692946058091288, 'eval_f1@deu.rst.pcc': 0.028628067722895306, 'eval_precision@deu.rst.pcc': 0.06022086466165413, 'eval_recall@deu.rst.pcc': 0.05750152625152625, 'eval_loss@deu.rst.pcc': 2.9191477298736572, 'eval_runtime': 3.1762, 'eval_samples_per_second': 75.876, 'eval_steps_per_second': 2.519, 'epoch': 3.0}
{'train@deu.rst.pcc_loss': 2.8198776245117188, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.18853974121996303, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.06696406636953851, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.08209969743071437, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.10191620541068849, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.8198776245117188, 'train@deu.rst.pcc_runtime': 26.1191, 'train@deu.rst.pcc_samples_per_second': 82.851, 'train@deu.rst.pcc_steps_per_second': 2.603, 'epoch': 4.0}
{'loss': 2.8833, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.8738040924072266, 'eval_accuracy@deu.rst.pcc': 0.15767634854771784, 'eval_f1@deu.rst.pcc': 0.05329658279295398, 'eval_precision@deu.rst.pcc': 0.08013001815782271, 'eval_recall@deu.rst.pcc': 0.09015694953194953, 'eval_loss@deu.rst.pcc': 2.8738040924072266, 'eval_runtime': 3.173, 'eval_samples_per_second': 75.953, 'eval_steps_per_second': 2.521, 'epoch': 4.0}
{'train@deu.rst.pcc_loss': 2.776658058166504, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.19824399260628467, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.06665788364116068, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.1061820352236045, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.11203312353383789, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.776658058166504, 'train@deu.rst.pcc_runtime': 26.1119, 'train@deu.rst.pcc_samples_per_second': 82.874, 'train@deu.rst.pcc_steps_per_second': 2.604, 'epoch': 5.0}
{'loss': 2.8315, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.838361978530884, 'eval_accuracy@deu.rst.pcc': 0.16597510373443983, 'eval_f1@deu.rst.pcc': 0.05136167641238635, 'eval_precision@deu.rst.pcc': 0.07566743917846859, 'eval_recall@deu.rst.pcc': 0.10464362026862024, 'eval_loss@deu.rst.pcc': 2.838362216949463, 'eval_runtime': 3.1736, 'eval_samples_per_second': 75.94, 'eval_steps_per_second': 2.521, 'epoch': 5.0}
{'train@deu.rst.pcc_loss': 2.7387843132019043, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.20748613678373382, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.07198897423270946, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.12226747527112901, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.11969766614335078, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7387843132019043, 'train@deu.rst.pcc_runtime': 26.0844, 'train@deu.rst.pcc_samples_per_second': 82.962, 'train@deu.rst.pcc_steps_per_second': 2.607, 'epoch': 6.0}
{'loss': 2.7958, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.8059380054473877, 'eval_accuracy@deu.rst.pcc': 0.17842323651452283, 'eval_f1@deu.rst.pcc': 0.0611251786334922, 'eval_precision@deu.rst.pcc': 0.08994730830073953, 'eval_recall@deu.rst.pcc': 0.12177205283822932, 'eval_loss@deu.rst.pcc': 2.8059377670288086, 'eval_runtime': 3.1665, 'eval_samples_per_second': 76.109, 'eval_steps_per_second': 2.526, 'epoch': 6.0}
{'train@deu.rst.pcc_loss': 2.7104008197784424, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2144177449168207, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.07913860581690939, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.10903258164442745, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.12484215097453875, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7104008197784424, 'train@deu.rst.pcc_runtime': 26.084, 'train@deu.rst.pcc_samples_per_second': 82.963, 'train@deu.rst.pcc_steps_per_second': 2.607, 'epoch': 7.0}
{'loss': 2.7609, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.7830049991607666, 'eval_accuracy@deu.rst.pcc': 0.18672199170124482, 'eval_f1@deu.rst.pcc': 0.06248383869438389, 'eval_precision@deu.rst.pcc': 0.07020367069386678, 'eval_recall@deu.rst.pcc': 0.12385792992410642, 'eval_loss@deu.rst.pcc': 2.7830049991607666, 'eval_runtime': 3.178, 'eval_samples_per_second': 75.835, 'eval_steps_per_second': 2.517, 'epoch': 7.0}
{'train@deu.rst.pcc_loss': 2.6883881092071533, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.21811460258780038, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08147590178695413, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.10828836583326185, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.12713897018693213, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6883881092071533, 'train@deu.rst.pcc_runtime': 26.084, 'train@deu.rst.pcc_samples_per_second': 82.963, 'train@deu.rst.pcc_steps_per_second': 2.607, 'epoch': 8.0}
{'loss': 2.734, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.7662229537963867, 'eval_accuracy@deu.rst.pcc': 0.17842323651452283, 'eval_f1@deu.rst.pcc': 0.060138510852998685, 'eval_precision@deu.rst.pcc': 0.06923125672507201, 'eval_recall@deu.rst.pcc': 0.12005501987119634, 'eval_loss@deu.rst.pcc': 2.7662229537963867, 'eval_runtime': 3.3653, 'eval_samples_per_second': 71.614, 'eval_steps_per_second': 2.377, 'epoch': 8.0}
{'train@deu.rst.pcc_loss': 2.6711151599884033, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2245841035120148, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08683472109082223, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.10987253521944543, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.13132686603915492, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6711151599884033, 'train@deu.rst.pcc_runtime': 26.0756, 'train@deu.rst.pcc_samples_per_second': 82.99, 'train@deu.rst.pcc_steps_per_second': 2.608, 'epoch': 9.0}
{'loss': 2.7203, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.7551827430725098, 'eval_accuracy@deu.rst.pcc': 0.17842323651452283, 'eval_f1@deu.rst.pcc': 0.06839600572216746, 'eval_precision@deu.rst.pcc': 0.12887538292999806, 'eval_recall@deu.rst.pcc': 0.12431580538198188, 'eval_loss@deu.rst.pcc': 2.755182981491089, 'eval_runtime': 3.1899, 'eval_samples_per_second': 75.551, 'eval_steps_per_second': 2.508, 'epoch': 9.0}
{'train@deu.rst.pcc_loss': 2.6593871116638184, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2255083179297597, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08735286652614252, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.10365255692078866, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.13185108261195386, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6593868732452393, 'train@deu.rst.pcc_runtime': 26.0833, 'train@deu.rst.pcc_samples_per_second': 82.965, 'train@deu.rst.pcc_steps_per_second': 2.607, 'epoch': 10.0}
{'loss': 2.7021, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.743497848510742, 'eval_accuracy@deu.rst.pcc': 0.1825726141078838, 'eval_f1@deu.rst.pcc': 0.07359958491021727, 'eval_precision@deu.rst.pcc': 0.11569882198303864, 'eval_recall@deu.rst.pcc': 0.12752093358711006, 'eval_loss@deu.rst.pcc': 2.743497371673584, 'eval_runtime': 3.1834, 'eval_samples_per_second': 75.706, 'eval_steps_per_second': 2.513, 'epoch': 10.0}
{'train@deu.rst.pcc_loss': 2.6525185108184814, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.22966728280961182, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09003478409422735, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.10614270515210697, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.13431249128834963, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6525182723999023, 'train@deu.rst.pcc_runtime': 26.0968, 'train@deu.rst.pcc_samples_per_second': 82.922, 'train@deu.rst.pcc_steps_per_second': 2.606, 'epoch': 11.0}
{'loss': 2.6919, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.7381067276000977, 'eval_accuracy@deu.rst.pcc': 0.1825726141078838, 'eval_f1@deu.rst.pcc': 0.07198417280823402, 'eval_precision@deu.rst.pcc': 0.11374245895769312, 'eval_recall@deu.rst.pcc': 0.1258039006200771, 'eval_loss@deu.rst.pcc': 2.7381069660186768, 'eval_runtime': 3.1823, 'eval_samples_per_second': 75.732, 'eval_steps_per_second': 2.514, 'epoch': 11.0}
{'train@deu.rst.pcc_loss': 2.6501903533935547, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.22828096118299446, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08974152512683341, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.10396270733226784, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.13379790481399942, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6501903533935547, 'train@deu.rst.pcc_runtime': 26.0974, 'train@deu.rst.pcc_samples_per_second': 82.92, 'train@deu.rst.pcc_steps_per_second': 2.606, 'epoch': 12.0}
{'loss': 2.6907, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.7356762886047363, 'eval_accuracy@deu.rst.pcc': 0.1825726141078838, 'eval_f1@deu.rst.pcc': 0.07209239791645913, 'eval_precision@deu.rst.pcc': 0.11399498421021836, 'eval_recall@deu.rst.pcc': 0.1258039006200771, 'eval_loss@deu.rst.pcc': 2.7356765270233154, 'eval_runtime': 3.1973, 'eval_samples_per_second': 75.376, 'eval_steps_per_second': 2.502, 'epoch': 12.0}
{'train_runtime': 1010.3759, 'train_samples_per_second': 25.701, 'train_steps_per_second': 0.808, 'train_loss': 2.83842752494064, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.8384
  train_runtime            = 0:16:50.37
  train_samples_per_second =     25.701
  train_steps_per_second   =      0.808
{'train@zho.rst.sctb_loss': 3.182720899581909, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019263755112811715, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012849850378454496, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.182720899581909, 'train@zho.rst.sctb_runtime': 5.4509, 'train@zho.rst.sctb_samples_per_second': 80.537, 'train@zho.rst.sctb_steps_per_second': 2.568, 'epoch': 1.0}
{'loss': 3.3128, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.1808085441589355, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 3.1808087825775146, 'eval_runtime': 1.3954, 'eval_samples_per_second': 67.363, 'eval_steps_per_second': 2.15, 'epoch': 1.0}
{'train@zho.rst.sctb_loss': 2.989211082458496, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.989210844039917, 'train@zho.rst.sctb_runtime': 5.4384, 'train@zho.rst.sctb_samples_per_second': 80.723, 'train@zho.rst.sctb_steps_per_second': 2.574, 'epoch': 2.0}
{'loss': 3.0885, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.9962587356567383, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.9962592124938965, 'eval_runtime': 1.408, 'eval_samples_per_second': 66.763, 'eval_steps_per_second': 2.131, 'epoch': 2.0}
{'train@zho.rst.sctb_loss': 2.8053293228149414, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.8053295612335205, 'train@zho.rst.sctb_runtime': 5.4519, 'train@zho.rst.sctb_samples_per_second': 80.522, 'train@zho.rst.sctb_steps_per_second': 2.568, 'epoch': 3.0}
{'loss': 2.9224, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.8225698471069336, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.822570562362671, 'eval_runtime': 1.4236, 'eval_samples_per_second': 66.029, 'eval_steps_per_second': 2.107, 'epoch': 3.0}
{'train@zho.rst.sctb_loss': 2.640220880508423, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.640220880508423, 'train@zho.rst.sctb_runtime': 5.4385, 'train@zho.rst.sctb_samples_per_second': 80.72, 'train@zho.rst.sctb_steps_per_second': 2.574, 'epoch': 4.0}
{'loss': 2.7412, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.6715452671051025, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.6715450286865234, 'eval_runtime': 1.4128, 'eval_samples_per_second': 66.536, 'eval_steps_per_second': 2.123, 'epoch': 4.0}
{'train@zho.rst.sctb_loss': 2.5071470737457275, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.5071470737457275, 'train@zho.rst.sctb_runtime': 5.4497, 'train@zho.rst.sctb_samples_per_second': 80.555, 'train@zho.rst.sctb_steps_per_second': 2.569, 'epoch': 5.0}
{'loss': 2.6065, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.5546793937683105, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.5546791553497314, 'eval_runtime': 1.3894, 'eval_samples_per_second': 67.653, 'eval_steps_per_second': 2.159, 'epoch': 5.0}
{'train@zho.rst.sctb_loss': 2.4075820446014404, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.4075815677642822, 'train@zho.rst.sctb_runtime': 5.4386, 'train@zho.rst.sctb_samples_per_second': 80.719, 'train@zho.rst.sctb_steps_per_second': 2.574, 'epoch': 6.0}
{'loss': 2.4931, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.4714107513427734, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.4714102745056152, 'eval_runtime': 1.3938, 'eval_samples_per_second': 67.441, 'eval_steps_per_second': 2.152, 'epoch': 6.0}
{'train@zho.rst.sctb_loss': 2.3408901691436768, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3416856492027335, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.022440318302387265, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.04370790499822758, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04008097165991902, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.3408901691436768, 'train@zho.rst.sctb_runtime': 5.4682, 'train@zho.rst.sctb_samples_per_second': 80.282, 'train@zho.rst.sctb_steps_per_second': 2.56, 'epoch': 7.0}
{'loss': 2.4249, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.4171159267425537, 'eval_accuracy@zho.rst.sctb': 0.35106382978723405, 'eval_f1@zho.rst.sctb': 0.032795321637426905, 'eval_precision@zho.rst.sctb': 0.07074136955291455, 'eval_recall@zho.rst.sctb': 0.05572755417956656, 'eval_loss@zho.rst.sctb': 2.41711688041687, 'eval_runtime': 1.4112, 'eval_samples_per_second': 66.612, 'eval_steps_per_second': 2.126, 'epoch': 7.0}
{'train@zho.rst.sctb_loss': 2.3030636310577393, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.36674259681093396, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.029761932379701848, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.038582132750192955, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04453441295546559, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.3030636310577393, 'train@zho.rst.sctb_runtime': 5.4486, 'train@zho.rst.sctb_samples_per_second': 80.571, 'train@zho.rst.sctb_steps_per_second': 2.569, 'epoch': 8.0}
{'loss': 2.3654, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.3871779441833496, 'eval_accuracy@zho.rst.sctb': 0.3617021276595745, 'eval_f1@zho.rst.sctb': 0.037635071284769304, 'eval_precision@zho.rst.sctb': 0.04502923976608188, 'eval_recall@zho.rst.sctb': 0.058823529411764705, 'eval_loss@zho.rst.sctb': 2.3871777057647705, 'eval_runtime': 1.4228, 'eval_samples_per_second': 66.069, 'eval_steps_per_second': 2.109, 'epoch': 8.0}
{'train@zho.rst.sctb_loss': 2.2791523933410645, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.4009111617312073, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.03794710485035928, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.042435943569444835, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.050748710554045805, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.2791523933410645, 'train@zho.rst.sctb_runtime': 5.4308, 'train@zho.rst.sctb_samples_per_second': 80.836, 'train@zho.rst.sctb_steps_per_second': 2.578, 'epoch': 9.0}
{'loss': 2.3336, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.36763858795166, 'eval_accuracy@zho.rst.sctb': 0.3617021276595745, 'eval_f1@zho.rst.sctb': 0.037635071284769304, 'eval_precision@zho.rst.sctb': 0.04502923976608188, 'eval_recall@zho.rst.sctb': 0.058823529411764705, 'eval_loss@zho.rst.sctb': 2.3676390647888184, 'eval_runtime': 1.4225, 'eval_samples_per_second': 66.082, 'eval_steps_per_second': 2.109, 'epoch': 9.0}
{'train@zho.rst.sctb_loss': 2.2629528045654297, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.4123006833712984, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.04076892964150744, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.04009621222735977, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.053480117575287, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.2629525661468506, 'train@zho.rst.sctb_runtime': 5.4549, 'train@zho.rst.sctb_samples_per_second': 80.478, 'train@zho.rst.sctb_steps_per_second': 2.566, 'epoch': 10.0}
{'loss': 2.3233, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.3548123836517334, 'eval_accuracy@zho.rst.sctb': 0.39361702127659576, 'eval_f1@zho.rst.sctb': 0.05023588382721509, 'eval_precision@zho.rst.sctb': 0.056952726644196705, 'eval_recall@zho.rst.sctb': 0.06811145510835914, 'eval_loss@zho.rst.sctb': 2.3548123836517334, 'eval_runtime': 1.4077, 'eval_samples_per_second': 66.777, 'eval_steps_per_second': 2.131, 'epoch': 10.0}
{'train@zho.rst.sctb_loss': 2.2544662952423096, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.4145785876993166, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.04098961191634932, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03959935897435897, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.05388497587488214, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.2544660568237305, 'train@zho.rst.sctb_runtime': 5.4404, 'train@zho.rst.sctb_samples_per_second': 80.692, 'train@zho.rst.sctb_steps_per_second': 2.573, 'epoch': 11.0}
{'loss': 2.2837, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.3480138778686523, 'eval_accuracy@zho.rst.sctb': 0.3829787234042553, 'eval_f1@zho.rst.sctb': 0.04870651204281891, 'eval_precision@zho.rst.sctb': 0.05186658506731946, 'eval_recall@zho.rst.sctb': 0.06646671826625387, 'eval_loss@zho.rst.sctb': 2.3480138778686523, 'eval_runtime': 1.3909, 'eval_samples_per_second': 67.581, 'eval_steps_per_second': 2.157, 'epoch': 11.0}
{'train@zho.rst.sctb_loss': 2.251755475997925, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.4145785876993166, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.040816852578601764, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.038911490654117996, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.05388497587488214, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.251755714416504, 'train@zho.rst.sctb_runtime': 5.4217, 'train@zho.rst.sctb_samples_per_second': 80.971, 'train@zho.rst.sctb_steps_per_second': 2.582, 'epoch': 12.0}
{'loss': 2.2862, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.3458609580993652, 'eval_accuracy@zho.rst.sctb': 0.3829787234042553, 'eval_f1@zho.rst.sctb': 0.04870651204281891, 'eval_precision@zho.rst.sctb': 0.05186658506731946, 'eval_recall@zho.rst.sctb': 0.06646671826625387, 'eval_loss@zho.rst.sctb': 2.345860481262207, 'eval_runtime': 1.4109, 'eval_samples_per_second': 66.626, 'eval_steps_per_second': 2.126, 'epoch': 12.0}
{'train_runtime': 213.4239, 'train_samples_per_second': 24.683, 'train_steps_per_second': 0.787, 'train_loss': 2.598466532570975, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.8384
  train_runtime            = 0:16:50.37
  train_samples_per_second =     25.701
  train_steps_per_second   =      0.808
-------------------------------------------------------------------
Lang1:  eng.pdtb.pdtb    Lang2:  zho.rst.sctb
Saving run to:  runs/full_shot/FullShot=v4_eng.pdtb.pdtb_zho.rst.sctb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 43920 examples
read 1674 examples
read 2257 examples
read 439 examples
read 94 examples
read 159 examples
Total prediction labels:  49
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (zh): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=49, bias=True)
    )
  )
)
{'train@eng.pdtb.pdtb_loss': 1.270680546760559, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.5956967213114754, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.2768852002077654, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.3561749159052345, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.2670711754074363, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.2706804275512695, 'train@eng.pdtb.pdtb_runtime': 516.3818, 'train@eng.pdtb.pdtb_samples_per_second': 85.053, 'train@eng.pdtb.pdtb_steps_per_second': 2.659, 'epoch': 1.0}
{'loss': 1.8795, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.198737382888794, 'eval_accuracy@eng.pdtb.pdtb': 0.6224611708482676, 'eval_f1@eng.pdtb.pdtb': 0.3232355596475829, 'eval_precision@eng.pdtb.pdtb': 0.4084060298883161, 'eval_recall@eng.pdtb.pdtb': 0.3140640208968136, 'eval_loss@eng.pdtb.pdtb': 1.1987375020980835, 'eval_runtime': 20.0887, 'eval_samples_per_second': 83.33, 'eval_steps_per_second': 2.638, 'epoch': 1.0}
{'train@eng.pdtb.pdtb_loss': 1.0972111225128174, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6390255009107468, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.35030968400302936, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.43386092181744984, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.3409074693129462, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.097211241722107, 'train@eng.pdtb.pdtb_runtime': 515.0764, 'train@eng.pdtb.pdtb_samples_per_second': 85.269, 'train@eng.pdtb.pdtb_steps_per_second': 2.666, 'epoch': 2.0}
{'loss': 1.2184, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.029132604598999, 'eval_accuracy@eng.pdtb.pdtb': 0.6690561529271206, 'eval_f1@eng.pdtb.pdtb': 0.39615199525128114, 'eval_precision@eng.pdtb.pdtb': 0.446057632151964, 'eval_recall@eng.pdtb.pdtb': 0.3881194709403052, 'eval_loss@eng.pdtb.pdtb': 1.0291324853897095, 'eval_runtime': 20.0526, 'eval_samples_per_second': 83.48, 'eval_steps_per_second': 2.643, 'epoch': 2.0}
{'train@eng.pdtb.pdtb_loss': 1.0483630895614624, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6520036429872496, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.42326849216143464, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.47037483581142325, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4056346221082247, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.0483630895614624, 'train@eng.pdtb.pdtb_runtime': 515.8206, 'train@eng.pdtb.pdtb_samples_per_second': 85.146, 'train@eng.pdtb.pdtb_steps_per_second': 2.662, 'epoch': 3.0}
{'loss': 1.1193, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 0.9890080094337463, 'eval_accuracy@eng.pdtb.pdtb': 0.6780167264038232, 'eval_f1@eng.pdtb.pdtb': 0.4677225195762385, 'eval_precision@eng.pdtb.pdtb': 0.5247133719324897, 'eval_recall@eng.pdtb.pdtb': 0.44888985145802457, 'eval_loss@eng.pdtb.pdtb': 0.9890078902244568, 'eval_runtime': 20.106, 'eval_samples_per_second': 83.259, 'eval_steps_per_second': 2.636, 'epoch': 3.0}
{'train@eng.pdtb.pdtb_loss': 0.9999783039093018, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6670537340619308, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4482116621378605, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.4799456582380368, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.43740440997601976, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9999783039093018, 'train@eng.pdtb.pdtb_runtime': 514.9107, 'train@eng.pdtb.pdtb_samples_per_second': 85.296, 'train@eng.pdtb.pdtb_steps_per_second': 2.666, 'epoch': 4.0}
{'loss': 1.0732, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 0.9468640685081482, 'eval_accuracy@eng.pdtb.pdtb': 0.6869772998805257, 'eval_f1@eng.pdtb.pdtb': 0.5094815937365594, 'eval_precision@eng.pdtb.pdtb': 0.5599770624097705, 'eval_recall@eng.pdtb.pdtb': 0.4890593159430182, 'eval_loss@eng.pdtb.pdtb': 0.9468640685081482, 'eval_runtime': 20.0939, 'eval_samples_per_second': 83.309, 'eval_steps_per_second': 2.638, 'epoch': 4.0}
{'train@eng.pdtb.pdtb_loss': 0.9767343401908875, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6737704918032786, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4552402116101256, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.47749320647332744, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4495378098031164, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9767343997955322, 'train@eng.pdtb.pdtb_runtime': 514.6536, 'train@eng.pdtb.pdtb_samples_per_second': 85.339, 'train@eng.pdtb.pdtb_steps_per_second': 2.668, 'epoch': 5.0}
{'loss': 1.0402, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 0.930342435836792, 'eval_accuracy@eng.pdtb.pdtb': 0.6851851851851852, 'eval_f1@eng.pdtb.pdtb': 0.5269783240250907, 'eval_precision@eng.pdtb.pdtb': 0.5588205347187422, 'eval_recall@eng.pdtb.pdtb': 0.5170817766758729, 'eval_loss@eng.pdtb.pdtb': 0.9303424954414368, 'eval_runtime': 21.0837, 'eval_samples_per_second': 79.398, 'eval_steps_per_second': 2.514, 'epoch': 5.0}
{'train@eng.pdtb.pdtb_loss': 0.9574392437934875, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.678415300546448, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.46496690407823016, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.48473856366829393, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4597819905967106, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.957439124584198, 'train@eng.pdtb.pdtb_runtime': 513.4366, 'train@eng.pdtb.pdtb_samples_per_second': 85.541, 'train@eng.pdtb.pdtb_steps_per_second': 2.674, 'epoch': 6.0}
{'loss': 1.0176, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 0.9232269525527954, 'eval_accuracy@eng.pdtb.pdtb': 0.6827956989247311, 'eval_f1@eng.pdtb.pdtb': 0.5279340745860872, 'eval_precision@eng.pdtb.pdtb': 0.5585195465102688, 'eval_recall@eng.pdtb.pdtb': 0.5165372553428663, 'eval_loss@eng.pdtb.pdtb': 0.9232269525527954, 'eval_runtime': 20.0132, 'eval_samples_per_second': 83.645, 'eval_steps_per_second': 2.648, 'epoch': 6.0}
{'train@eng.pdtb.pdtb_loss': 0.9460048079490662, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6796448087431693, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.46680603004222243, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.490948085224483, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.45805175654387165, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9460048079490662, 'train@eng.pdtb.pdtb_runtime': 514.324, 'train@eng.pdtb.pdtb_samples_per_second': 85.394, 'train@eng.pdtb.pdtb_steps_per_second': 2.67, 'epoch': 7.0}
{'loss': 1.0056, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 0.9092551469802856, 'eval_accuracy@eng.pdtb.pdtb': 0.6851851851851852, 'eval_f1@eng.pdtb.pdtb': 0.5271538050477623, 'eval_precision@eng.pdtb.pdtb': 0.5626678427092695, 'eval_recall@eng.pdtb.pdtb': 0.5128102849423918, 'eval_loss@eng.pdtb.pdtb': 0.9092550873756409, 'eval_runtime': 20.0345, 'eval_samples_per_second': 83.556, 'eval_steps_per_second': 2.645, 'epoch': 7.0}
{'train@eng.pdtb.pdtb_loss': 0.9347687363624573, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.685063752276867, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4761544912483628, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5740882612747938, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.47250077248939293, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9347688555717468, 'train@eng.pdtb.pdtb_runtime': 514.3006, 'train@eng.pdtb.pdtb_samples_per_second': 85.398, 'train@eng.pdtb.pdtb_steps_per_second': 2.67, 'epoch': 8.0}
{'loss': 0.9919, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 0.9057166576385498, 'eval_accuracy@eng.pdtb.pdtb': 0.6929510155316607, 'eval_f1@eng.pdtb.pdtb': 0.5571905630311365, 'eval_precision@eng.pdtb.pdtb': 0.6226920918361106, 'eval_recall@eng.pdtb.pdtb': 0.5417457601726132, 'eval_loss@eng.pdtb.pdtb': 0.9057167172431946, 'eval_runtime': 20.0476, 'eval_samples_per_second': 83.501, 'eval_steps_per_second': 2.644, 'epoch': 8.0}
{'train@eng.pdtb.pdtb_loss': 0.9256757497787476, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6870673952641165, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4784833322408001, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5778478334987353, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.47325695555837244, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9256758689880371, 'train@eng.pdtb.pdtb_runtime': 515.1035, 'train@eng.pdtb.pdtb_samples_per_second': 85.264, 'train@eng.pdtb.pdtb_steps_per_second': 2.665, 'epoch': 9.0}
{'loss': 0.9798, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 0.9025207757949829, 'eval_accuracy@eng.pdtb.pdtb': 0.6911589008363201, 'eval_f1@eng.pdtb.pdtb': 0.5414044744536508, 'eval_precision@eng.pdtb.pdtb': 0.5694550042402406, 'eval_recall@eng.pdtb.pdtb': 0.529600504412445, 'eval_loss@eng.pdtb.pdtb': 0.9025208950042725, 'eval_runtime': 20.0869, 'eval_samples_per_second': 83.338, 'eval_steps_per_second': 2.639, 'epoch': 9.0}
{'train@eng.pdtb.pdtb_loss': 0.9223217368125916, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.688615664845173, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4793634402924742, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5827859113002903, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4724620371458773, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9223218560218811, 'train@eng.pdtb.pdtb_runtime': 515.0518, 'train@eng.pdtb.pdtb_samples_per_second': 85.273, 'train@eng.pdtb.pdtb_steps_per_second': 2.666, 'epoch': 10.0}
{'loss': 0.9749, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 0.8980961441993713, 'eval_accuracy@eng.pdtb.pdtb': 0.6887694145758662, 'eval_f1@eng.pdtb.pdtb': 0.5541974760615246, 'eval_precision@eng.pdtb.pdtb': 0.6236488896449691, 'eval_recall@eng.pdtb.pdtb': 0.5357027836687964, 'eval_loss@eng.pdtb.pdtb': 0.8980960845947266, 'eval_runtime': 20.0494, 'eval_samples_per_second': 83.494, 'eval_steps_per_second': 2.643, 'epoch': 10.0}
{'train@eng.pdtb.pdtb_loss': 0.9178001880645752, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6893214936247724, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.48269792411320583, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.552115573057038, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4768876460763205, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9178001880645752, 'train@eng.pdtb.pdtb_runtime': 514.6203, 'train@eng.pdtb.pdtb_samples_per_second': 85.344, 'train@eng.pdtb.pdtb_steps_per_second': 2.668, 'epoch': 11.0}
{'loss': 0.9698, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.8981212973594666, 'eval_accuracy@eng.pdtb.pdtb': 0.6947431302270012, 'eval_f1@eng.pdtb.pdtb': 0.5667473183267557, 'eval_precision@eng.pdtb.pdtb': 0.6241872884054396, 'eval_recall@eng.pdtb.pdtb': 0.5533050207394195, 'eval_loss@eng.pdtb.pdtb': 0.8981212973594666, 'eval_runtime': 20.0415, 'eval_samples_per_second': 83.527, 'eval_steps_per_second': 2.645, 'epoch': 11.0}
{'train@eng.pdtb.pdtb_loss': 0.916585385799408, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6898907103825137, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.48258043794264976, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.555120671855462, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4755709799728541, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.916585385799408, 'train@eng.pdtb.pdtb_runtime': 513.7858, 'train@eng.pdtb.pdtb_samples_per_second': 85.483, 'train@eng.pdtb.pdtb_steps_per_second': 2.672, 'epoch': 12.0}
{'loss': 0.9663, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.8963273167610168, 'eval_accuracy@eng.pdtb.pdtb': 0.6923536439665472, 'eval_f1@eng.pdtb.pdtb': 0.5557657631555682, 'eval_precision@eng.pdtb.pdtb': 0.6225051947678865, 'eval_recall@eng.pdtb.pdtb': 0.5372689725982707, 'eval_loss@eng.pdtb.pdtb': 0.8963273167610168, 'eval_runtime': 20.0398, 'eval_samples_per_second': 83.534, 'eval_steps_per_second': 2.645, 'epoch': 12.0}
{'train_runtime': 19493.3609, 'train_samples_per_second': 27.037, 'train_steps_per_second': 0.845, 'train_loss': 1.1030203570842627, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =      1.103
  train_runtime            = 5:24:53.36
  train_samples_per_second =     27.037
  train_steps_per_second   =      0.845
{'train@zho.rst.sctb_loss': 4.8230061531066895, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.00683371298405467, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.0076502219068686796, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.007973507973507973, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.015064102564102564, 'train@zho.rst.sctb_loss@zho.rst.sctb': 4.823007106781006, 'train@zho.rst.sctb_runtime': 5.575, 'train@zho.rst.sctb_samples_per_second': 78.745, 'train@zho.rst.sctb_steps_per_second': 2.511, 'epoch': 1.0}
{'loss': 5.9209, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 5.048068523406982, 'eval_accuracy@zho.rst.sctb': 0.010638297872340425, 'eval_f1@zho.rst.sctb': 0.0071428571428571435, 'eval_precision@zho.rst.sctb': 0.0038461538461538464, 'eval_recall@zho.rst.sctb': 0.05, 'eval_loss@zho.rst.sctb': 5.048069477081299, 'eval_runtime': 1.5533, 'eval_samples_per_second': 60.518, 'eval_steps_per_second': 1.931, 'epoch': 1.0}
{'train@zho.rst.sctb_loss': 3.803528070449829, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.009111617312072893, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.011217948717948718, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.010170118343195266, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.015064102564102564, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.803528308868408, 'train@zho.rst.sctb_runtime': 5.5802, 'train@zho.rst.sctb_samples_per_second': 78.671, 'train@zho.rst.sctb_steps_per_second': 2.509, 'epoch': 2.0}
{'loss': 4.2314, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.8307809829711914, 'eval_accuracy@zho.rst.sctb': 0.0, 'eval_f1@zho.rst.sctb': 0.0, 'eval_precision@zho.rst.sctb': 0.0, 'eval_recall@zho.rst.sctb': 0.0, 'eval_loss@zho.rst.sctb': 3.8307809829711914, 'eval_runtime': 1.5383, 'eval_samples_per_second': 61.107, 'eval_steps_per_second': 1.95, 'epoch': 2.0}
{'train@zho.rst.sctb_loss': 3.3710265159606934, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.04100227790432802, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.020606508847329764, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03151538004479181, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.02001694583298017, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.3710267543792725, 'train@zho.rst.sctb_runtime': 5.5784, 'train@zho.rst.sctb_samples_per_second': 78.696, 'train@zho.rst.sctb_steps_per_second': 2.51, 'epoch': 3.0}
{'loss': 3.5952, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 3.3589744567871094, 'eval_accuracy@zho.rst.sctb': 0.05319148936170213, 'eval_f1@zho.rst.sctb': 0.04679649637632831, 'eval_precision@zho.rst.sctb': 0.08418367346938775, 'eval_recall@zho.rst.sctb': 0.08120748299319727, 'eval_loss@zho.rst.sctb': 3.3589730262756348, 'eval_runtime': 1.5362, 'eval_samples_per_second': 61.189, 'eval_steps_per_second': 1.953, 'epoch': 3.0}
{'train@zho.rst.sctb_loss': 3.0319724082946777, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.16856492027334852, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.030823288224240367, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.032251499993435476, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.03596606769822343, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.0319724082946777, 'train@zho.rst.sctb_runtime': 5.5467, 'train@zho.rst.sctb_samples_per_second': 79.146, 'train@zho.rst.sctb_steps_per_second': 2.524, 'epoch': 4.0}
{'loss': 3.2205, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 3.037968158721924, 'eval_accuracy@zho.rst.sctb': 0.14893617021276595, 'eval_f1@zho.rst.sctb': 0.02472166213728412, 'eval_precision@zho.rst.sctb': 0.024921630094043887, 'eval_recall@zho.rst.sctb': 0.02495941558441558, 'eval_loss@zho.rst.sctb': 3.037968397140503, 'eval_runtime': 1.5474, 'eval_samples_per_second': 60.749, 'eval_steps_per_second': 1.939, 'epoch': 4.0}
{'train@zho.rst.sctb_loss': 2.8039538860321045, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3097949886104784, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.036156794244683946, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.05667038275733928, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.046019817721331784, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.8039536476135254, 'train@zho.rst.sctb_runtime': 5.6026, 'train@zho.rst.sctb_samples_per_second': 78.356, 'train@zho.rst.sctb_steps_per_second': 2.499, 'epoch': 5.0}
{'loss': 2.9725, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.8185524940490723, 'eval_accuracy@zho.rst.sctb': 0.2978723404255319, 'eval_f1@zho.rst.sctb': 0.029771398192450827, 'eval_precision@zho.rst.sctb': 0.02199528672427337, 'eval_recall@zho.rst.sctb': 0.046052631578947366, 'eval_loss@zho.rst.sctb': 2.8185524940490723, 'eval_runtime': 1.543, 'eval_samples_per_second': 60.919, 'eval_steps_per_second': 1.944, 'epoch': 5.0}
{'train@zho.rst.sctb_loss': 2.646181344985962, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3621867881548975, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.0467489696847495, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.07650857914015809, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.055086610097424805, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.646181583404541, 'train@zho.rst.sctb_runtime': 5.5667, 'train@zho.rst.sctb_samples_per_second': 78.862, 'train@zho.rst.sctb_steps_per_second': 2.515, 'epoch': 6.0}
{'loss': 2.7782, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.671135663986206, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.03447214527546937, 'eval_precision@zho.rst.sctb': 0.07252888318356868, 'eval_recall@zho.rst.sctb': 0.0540828173374613, 'eval_loss@zho.rst.sctb': 2.671135425567627, 'eval_runtime': 1.5459, 'eval_samples_per_second': 60.807, 'eval_steps_per_second': 1.941, 'epoch': 6.0}
{'train@zho.rst.sctb_loss': 2.537229061126709, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.37585421412300685, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.04741633032444808, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.08059869532103713, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.05507366942118204, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.537228584289551, 'train@zho.rst.sctb_runtime': 5.5779, 'train@zho.rst.sctb_samples_per_second': 78.704, 'train@zho.rst.sctb_steps_per_second': 2.51, 'epoch': 7.0}
{'loss': 2.6147, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.5796573162078857, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.03367083771133824, 'eval_precision@zho.rst.sctb': 0.04573934837092732, 'eval_recall@zho.rst.sctb': 0.0540828173374613, 'eval_loss@zho.rst.sctb': 2.579657793045044, 'eval_runtime': 1.5209, 'eval_samples_per_second': 61.806, 'eval_steps_per_second': 1.973, 'epoch': 7.0}
{'train@zho.rst.sctb_loss': 2.4673385620117188, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.37813211845102507, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.04827316224445411, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.08018190872341673, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.05561995082543028, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.4673385620117188, 'train@zho.rst.sctb_runtime': 5.569, 'train@zho.rst.sctb_samples_per_second': 78.829, 'train@zho.rst.sctb_steps_per_second': 2.514, 'epoch': 8.0}
{'loss': 2.5445, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.5217905044555664, 'eval_accuracy@zho.rst.sctb': 0.3617021276595745, 'eval_f1@zho.rst.sctb': 0.03883237505528527, 'eval_precision@zho.rst.sctb': 0.05444646098003629, 'eval_recall@zho.rst.sctb': 0.058823529411764705, 'eval_loss@zho.rst.sctb': 2.5217902660369873, 'eval_runtime': 1.533, 'eval_samples_per_second': 61.318, 'eval_steps_per_second': 1.957, 'epoch': 8.0}
{'train@zho.rst.sctb_loss': 2.420182704925537, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3826879271070615, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.04925805714166091, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.07996064002738085, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.05642966742462056, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.420182943344116, 'train@zho.rst.sctb_runtime': 5.5835, 'train@zho.rst.sctb_samples_per_second': 78.625, 'train@zho.rst.sctb_steps_per_second': 2.507, 'epoch': 9.0}
{'loss': 2.4956, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.4838101863861084, 'eval_accuracy@zho.rst.sctb': 0.3723404255319149, 'eval_f1@zho.rst.sctb': 0.043343653250773995, 'eval_precision@zho.rst.sctb': 0.058832425892317, 'eval_recall@zho.rst.sctb': 0.06191950464396285, 'eval_loss@zho.rst.sctb': 2.48380970954895, 'eval_runtime': 1.5798, 'eval_samples_per_second': 59.501, 'eval_steps_per_second': 1.899, 'epoch': 9.0}
{'train@zho.rst.sctb_loss': 2.3893070220947266, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.38724373576309795, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.047542555845138876, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.07824082824082824, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.05522156286395652, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.3893072605133057, 'train@zho.rst.sctb_runtime': 5.5251, 'train@zho.rst.sctb_samples_per_second': 79.455, 'train@zho.rst.sctb_steps_per_second': 2.534, 'epoch': 10.0}
{'loss': 2.4359, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.4618444442749023, 'eval_accuracy@zho.rst.sctb': 0.3723404255319149, 'eval_f1@zho.rst.sctb': 0.043343653250773995, 'eval_precision@zho.rst.sctb': 0.058832425892317, 'eval_recall@zho.rst.sctb': 0.06191950464396285, 'eval_loss@zho.rst.sctb': 2.4618442058563232, 'eval_runtime': 1.5255, 'eval_samples_per_second': 61.62, 'eval_steps_per_second': 1.967, 'epoch': 10.0}
{'train@zho.rst.sctb_loss': 2.3724048137664795, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.38724373576309795, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.04424064619648, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.07856909172698646, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.0530623185994491, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.3724048137664795, 'train@zho.rst.sctb_runtime': 5.5734, 'train@zho.rst.sctb_samples_per_second': 78.768, 'train@zho.rst.sctb_steps_per_second': 2.512, 'epoch': 11.0}
{'loss': 2.4171, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.4487504959106445, 'eval_accuracy@zho.rst.sctb': 0.3723404255319149, 'eval_f1@zho.rst.sctb': 0.04287578450257876, 'eval_precision@zho.rst.sctb': 0.058397397989355414, 'eval_recall@zho.rst.sctb': 0.06191950464396285, 'eval_loss@zho.rst.sctb': 2.4487507343292236, 'eval_runtime': 2.3408, 'eval_samples_per_second': 40.156, 'eval_steps_per_second': 1.282, 'epoch': 11.0}
{'train@zho.rst.sctb_loss': 2.3665597438812256, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.38724373576309795, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.04424064619648, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.07856909172698646, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.0530623185994491, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.3665599822998047, 'train@zho.rst.sctb_runtime': 5.5509, 'train@zho.rst.sctb_samples_per_second': 79.086, 'train@zho.rst.sctb_steps_per_second': 2.522, 'epoch': 12.0}
{'loss': 2.4131, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.4448635578155518, 'eval_accuracy@zho.rst.sctb': 0.3723404255319149, 'eval_f1@zho.rst.sctb': 0.04266012625145752, 'eval_precision@zho.rst.sctb': 0.05093768905021173, 'eval_recall@zho.rst.sctb': 0.06191950464396285, 'eval_loss@zho.rst.sctb': 2.4448635578155518, 'eval_runtime': 1.514, 'eval_samples_per_second': 62.085, 'eval_steps_per_second': 1.981, 'epoch': 12.0}
{'train_runtime': 217.3472, 'train_samples_per_second': 24.238, 'train_steps_per_second': 0.773, 'train_loss': 3.136634145464216, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =      1.103
  train_runtime            = 5:24:53.36
  train_samples_per_second =     27.037
  train_steps_per_second   =      0.845
-------------------------------------------------------------------
Lang1:  eng.rst.gum    Lang2:  zho.rst.sctb
Saving run to:  runs/full_shot/FullShot=v4_eng.rst.gum_zho.rst.sctb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 13897 examples
read 2149 examples
read 2091 examples
read 439 examples
read 94 examples
read 159 examples
Total prediction labels:  29
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (zh): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=29, bias=True)
    )
  )
)
{'train@eng.rst.gum_loss': 2.5027787685394287, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.2512772540836152, 'train@eng.rst.gum_f1@eng.rst.gum': 0.04414539249881478, 'train@eng.rst.gum_precision@eng.rst.gum': 0.07675891002914823, 'train@eng.rst.gum_recall@eng.rst.gum': 0.06113113566836463, 'train@eng.rst.gum_loss@eng.rst.gum': 2.502779006958008, 'train@eng.rst.gum_runtime': 163.1151, 'train@eng.rst.gum_samples_per_second': 85.198, 'train@eng.rst.gum_steps_per_second': 2.667, 'epoch': 1.0}
{'loss': 2.7413, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.581841468811035, 'eval_accuracy@eng.rst.gum': 0.25732899022801303, 'eval_f1@eng.rst.gum': 0.051452904767106816, 'eval_precision@eng.rst.gum': 0.07518925888688377, 'eval_recall@eng.rst.gum': 0.06898640913165073, 'eval_loss@eng.rst.gum': 2.5818417072296143, 'eval_runtime': 25.5615, 'eval_samples_per_second': 84.072, 'eval_steps_per_second': 2.66, 'epoch': 1.0}
{'train@eng.rst.gum_loss': 2.085021495819092, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.38907677916097, 'train@eng.rst.gum_f1@eng.rst.gum': 0.1528282745363036, 'train@eng.rst.gum_precision@eng.rst.gum': 0.23940821698134904, 'train@eng.rst.gum_recall@eng.rst.gum': 0.16423943590172732, 'train@eng.rst.gum_loss@eng.rst.gum': 2.085021734237671, 'train@eng.rst.gum_runtime': 162.4776, 'train@eng.rst.gum_samples_per_second': 85.532, 'train@eng.rst.gum_steps_per_second': 2.677, 'epoch': 2.0}
{'loss': 2.3608, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.182474136352539, 'eval_accuracy@eng.rst.gum': 0.3652861796184272, 'eval_f1@eng.rst.gum': 0.14836478470890235, 'eval_precision@eng.rst.gum': 0.25693432422224605, 'eval_recall@eng.rst.gum': 0.16469694756216774, 'eval_loss@eng.rst.gum': 2.182474136352539, 'eval_runtime': 25.4067, 'eval_samples_per_second': 84.584, 'eval_steps_per_second': 2.676, 'epoch': 2.0}
{'train@eng.rst.gum_loss': 1.8071038722991943, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.47650572065913505, 'train@eng.rst.gum_f1@eng.rst.gum': 0.2702036455805009, 'train@eng.rst.gum_precision@eng.rst.gum': 0.4221168371085172, 'train@eng.rst.gum_recall@eng.rst.gum': 0.2810267967384599, 'train@eng.rst.gum_loss@eng.rst.gum': 1.8071039915084839, 'train@eng.rst.gum_runtime': 162.8864, 'train@eng.rst.gum_samples_per_second': 85.317, 'train@eng.rst.gum_steps_per_second': 2.671, 'epoch': 3.0}
{'loss': 2.0095, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.9320095777511597, 'eval_accuracy@eng.rst.gum': 0.44625407166123776, 'eval_f1@eng.rst.gum': 0.260397868787971, 'eval_precision@eng.rst.gum': 0.31592267303689897, 'eval_recall@eng.rst.gum': 0.27327889977814523, 'eval_loss@eng.rst.gum': 1.9320100545883179, 'eval_runtime': 25.5404, 'eval_samples_per_second': 84.141, 'eval_steps_per_second': 2.662, 'epoch': 3.0}
{'train@eng.rst.gum_loss': 1.681108832359314, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5028423400733971, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3050049255216412, 'train@eng.rst.gum_precision@eng.rst.gum': 0.428389044378628, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3125709718172591, 'train@eng.rst.gum_loss@eng.rst.gum': 1.681108832359314, 'train@eng.rst.gum_runtime': 163.1343, 'train@eng.rst.gum_samples_per_second': 85.187, 'train@eng.rst.gum_steps_per_second': 2.667, 'epoch': 4.0}
{'loss': 1.8197, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.8348524570465088, 'eval_accuracy@eng.rst.gum': 0.4676593764541647, 'eval_f1@eng.rst.gum': 0.29499093459155323, 'eval_precision@eng.rst.gum': 0.3752410341126492, 'eval_recall@eng.rst.gum': 0.30739272926838745, 'eval_loss@eng.rst.gum': 1.834852695465088, 'eval_runtime': 25.5462, 'eval_samples_per_second': 84.122, 'eval_steps_per_second': 2.662, 'epoch': 4.0}
{'train@eng.rst.gum_loss': 1.6022387742996216, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5218392458804059, 'train@eng.rst.gum_f1@eng.rst.gum': 0.35043716706272965, 'train@eng.rst.gum_precision@eng.rst.gum': 0.4683014175772166, 'train@eng.rst.gum_recall@eng.rst.gum': 0.35547971065157136, 'train@eng.rst.gum_loss@eng.rst.gum': 1.602238655090332, 'train@eng.rst.gum_runtime': 163.4755, 'train@eng.rst.gum_samples_per_second': 85.01, 'train@eng.rst.gum_steps_per_second': 2.661, 'epoch': 5.0}
{'loss': 1.7159, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.7722073793411255, 'eval_accuracy@eng.rst.gum': 0.4820846905537459, 'eval_f1@eng.rst.gum': 0.3258923593964954, 'eval_precision@eng.rst.gum': 0.41864979416850234, 'eval_recall@eng.rst.gum': 0.3383747710862817, 'eval_loss@eng.rst.gum': 1.7722073793411255, 'eval_runtime': 25.5532, 'eval_samples_per_second': 84.099, 'eval_steps_per_second': 2.661, 'epoch': 5.0}
{'train@eng.rst.gum_loss': 1.547022819519043, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5368065050010794, 'train@eng.rst.gum_f1@eng.rst.gum': 0.38581768133127453, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5263088067762086, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3854735190862502, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5470227003097534, 'train@eng.rst.gum_runtime': 163.5774, 'train@eng.rst.gum_samples_per_second': 84.957, 'train@eng.rst.gum_steps_per_second': 2.659, 'epoch': 6.0}
{'loss': 1.6524, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.7336397171020508, 'eval_accuracy@eng.rst.gum': 0.4965100046533271, 'eval_f1@eng.rst.gum': 0.36097847419260115, 'eval_precision@eng.rst.gum': 0.4280240439286498, 'eval_recall@eng.rst.gum': 0.37037208496226315, 'eval_loss@eng.rst.gum': 1.7336399555206299, 'eval_runtime': 25.6399, 'eval_samples_per_second': 83.815, 'eval_steps_per_second': 2.652, 'epoch': 6.0}
{'train@eng.rst.gum_loss': 1.5103516578674316, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5481758652946679, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3981378110689504, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5481452963412626, 'train@eng.rst.gum_recall@eng.rst.gum': 0.40049228388706765, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5103516578674316, 'train@eng.rst.gum_runtime': 163.3778, 'train@eng.rst.gum_samples_per_second': 85.061, 'train@eng.rst.gum_steps_per_second': 2.663, 'epoch': 7.0}
{'loss': 1.6065, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.7109354734420776, 'eval_accuracy@eng.rst.gum': 0.5039553280595626, 'eval_f1@eng.rst.gum': 0.36851416125781533, 'eval_precision@eng.rst.gum': 0.5121526855024404, 'eval_recall@eng.rst.gum': 0.3813499806074382, 'eval_loss@eng.rst.gum': 1.7109354734420776, 'eval_runtime': 25.5903, 'eval_samples_per_second': 83.977, 'eval_steps_per_second': 2.657, 'epoch': 7.0}
{'train@eng.rst.gum_loss': 1.484998345375061, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5540044613945456, 'train@eng.rst.gum_f1@eng.rst.gum': 0.40600753039828025, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5240166347866649, 'train@eng.rst.gum_recall@eng.rst.gum': 0.40703443683907575, 'train@eng.rst.gum_loss@eng.rst.gum': 1.484998345375061, 'train@eng.rst.gum_runtime': 163.5225, 'train@eng.rst.gum_samples_per_second': 84.985, 'train@eng.rst.gum_steps_per_second': 2.66, 'epoch': 8.0}
{'loss': 1.5751, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.6941698789596558, 'eval_accuracy@eng.rst.gum': 0.5062819916240112, 'eval_f1@eng.rst.gum': 0.37754103509447673, 'eval_precision@eng.rst.gum': 0.4736158744856201, 'eval_recall@eng.rst.gum': 0.38720311901402144, 'eval_loss@eng.rst.gum': 1.6941698789596558, 'eval_runtime': 25.5624, 'eval_samples_per_second': 84.069, 'eval_steps_per_second': 2.66, 'epoch': 8.0}
{'train@eng.rst.gum_loss': 1.4630985260009766, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5650140318054256, 'train@eng.rst.gum_f1@eng.rst.gum': 0.42661051884086576, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5184395942711749, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4260021830324744, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4630986452102661, 'train@eng.rst.gum_runtime': 163.23, 'train@eng.rst.gum_samples_per_second': 85.138, 'train@eng.rst.gum_steps_per_second': 2.665, 'epoch': 9.0}
{'loss': 1.5488, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.6761729717254639, 'eval_accuracy@eng.rst.gum': 0.5058166589111215, 'eval_f1@eng.rst.gum': 0.387364829929734, 'eval_precision@eng.rst.gum': 0.46359851382570944, 'eval_recall@eng.rst.gum': 0.39945469171460785, 'eval_loss@eng.rst.gum': 1.6761729717254639, 'eval_runtime': 25.6062, 'eval_samples_per_second': 83.925, 'eval_steps_per_second': 2.656, 'epoch': 9.0}
{'train@eng.rst.gum_loss': 1.4482169151306152, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5675325609843851, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4305550590201887, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5273003809847893, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4255858702090915, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4482169151306152, 'train@eng.rst.gum_runtime': 163.1342, 'train@eng.rst.gum_samples_per_second': 85.188, 'train@eng.rst.gum_steps_per_second': 2.667, 'epoch': 10.0}
{'loss': 1.5296, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.668280005455017, 'eval_accuracy@eng.rst.gum': 0.5155886458818055, 'eval_f1@eng.rst.gum': 0.3973215089104895, 'eval_precision@eng.rst.gum': 0.481312466695955, 'eval_recall@eng.rst.gum': 0.40239259038067327, 'eval_loss@eng.rst.gum': 1.668280005455017, 'eval_runtime': 25.4869, 'eval_samples_per_second': 84.318, 'eval_steps_per_second': 2.668, 'epoch': 10.0}
{'train@eng.rst.gum_loss': 1.4410252571105957, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5686119306325106, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4360477590583416, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5194994120768829, 'train@eng.rst.gum_recall@eng.rst.gum': 0.43366318721243735, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4410252571105957, 'train@eng.rst.gum_runtime': 163.3258, 'train@eng.rst.gum_samples_per_second': 85.088, 'train@eng.rst.gum_steps_per_second': 2.663, 'epoch': 11.0}
{'loss': 1.5258, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.6619752645492554, 'eval_accuracy@eng.rst.gum': 0.5169846440204746, 'eval_f1@eng.rst.gum': 0.4011146997618772, 'eval_precision@eng.rst.gum': 0.47309957033389166, 'eval_recall@eng.rst.gum': 0.4096859204113755, 'eval_loss@eng.rst.gum': 1.6619752645492554, 'eval_runtime': 25.5417, 'eval_samples_per_second': 84.137, 'eval_steps_per_second': 2.662, 'epoch': 11.0}
{'train@eng.rst.gum_loss': 1.4376894235610962, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5695473843275527, 'train@eng.rst.gum_f1@eng.rst.gum': 0.43662564971734613, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5265839629816551, 'train@eng.rst.gum_recall@eng.rst.gum': 0.43231063608358533, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4376894235610962, 'train@eng.rst.gum_runtime': 162.9399, 'train@eng.rst.gum_samples_per_second': 85.289, 'train@eng.rst.gum_steps_per_second': 2.67, 'epoch': 12.0}
{'loss': 1.5136, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.6606972217559814, 'eval_accuracy@eng.rst.gum': 0.5179153094462541, 'eval_f1@eng.rst.gum': 0.4017138267575189, 'eval_precision@eng.rst.gum': 0.47807376997867906, 'eval_recall@eng.rst.gum': 0.4086875397028776, 'eval_loss@eng.rst.gum': 1.6606972217559814, 'eval_runtime': 26.22, 'eval_samples_per_second': 81.96, 'eval_steps_per_second': 2.593, 'epoch': 12.0}
{'train_runtime': 6410.6553, 'train_samples_per_second': 26.014, 'train_steps_per_second': 0.814, 'train_loss': 1.7999181403967612, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.7999
  train_runtime            = 1:46:50.65
  train_samples_per_second =     26.014
  train_steps_per_second   =      0.814
{'train@zho.rst.sctb_loss': 3.2004663944244385, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.09111617312072894, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.04801454081278353, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.09225981498976013, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.06873532619747472, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.2004663944244385, 'train@zho.rst.sctb_runtime': 5.434, 'train@zho.rst.sctb_samples_per_second': 80.787, 'train@zho.rst.sctb_steps_per_second': 2.576, 'epoch': 1.0}
{'loss': 3.7736, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.29659104347229, 'eval_accuracy@zho.rst.sctb': 0.010638297872340425, 'eval_f1@zho.rst.sctb': 0.0024390243902439024, 'eval_precision@zho.rst.sctb': 0.005555555555555555, 'eval_recall@zho.rst.sctb': 0.0015625, 'eval_loss@zho.rst.sctb': 3.2965903282165527, 'eval_runtime': 1.4151, 'eval_samples_per_second': 66.428, 'eval_steps_per_second': 2.12, 'epoch': 1.0}
{'train@zho.rst.sctb_loss': 2.7344822883605957, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.28018223234624146, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.045049586299505236, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.05592240853407198, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.05270090399866895, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.7344822883605957, 'train@zho.rst.sctb_runtime': 5.4638, 'train@zho.rst.sctb_samples_per_second': 80.347, 'train@zho.rst.sctb_steps_per_second': 2.562, 'epoch': 2.0}
{'loss': 2.9612, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.804023027420044, 'eval_accuracy@zho.rst.sctb': 0.24468085106382978, 'eval_f1@zho.rst.sctb': 0.024959305480195336, 'eval_precision@zho.rst.sctb': 0.01862348178137652, 'eval_recall@zho.rst.sctb': 0.03782894736842105, 'eval_loss@zho.rst.sctb': 2.804023265838623, 'eval_runtime': 1.3878, 'eval_samples_per_second': 67.732, 'eval_steps_per_second': 2.162, 'epoch': 2.0}
{'train@zho.rst.sctb_loss': 2.488166093826294, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3439635535307517, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.02808469725762959, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.028492031605731886, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.041900061006045146, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.488166093826294, 'train@zho.rst.sctb_runtime': 5.4496, 'train@zho.rst.sctb_samples_per_second': 80.557, 'train@zho.rst.sctb_steps_per_second': 2.569, 'epoch': 3.0}
{'loss': 2.6397, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.5750319957733154, 'eval_accuracy@zho.rst.sctb': 0.30851063829787234, 'eval_f1@zho.rst.sctb': 0.02586975914362177, 'eval_precision@zho.rst.sctb': 0.017747858017135864, 'eval_recall@zho.rst.sctb': 0.047697368421052634, 'eval_loss@zho.rst.sctb': 2.5750319957733154, 'eval_runtime': 1.3704, 'eval_samples_per_second': 68.591, 'eval_steps_per_second': 2.189, 'epoch': 3.0}
{'train@zho.rst.sctb_loss': 2.3389923572540283, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.35079726651480636, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.042687429910376454, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.052749333999334, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.052061727025678003, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.3389923572540283, 'train@zho.rst.sctb_runtime': 5.4284, 'train@zho.rst.sctb_samples_per_second': 80.87, 'train@zho.rst.sctb_steps_per_second': 2.579, 'epoch': 4.0}
{'loss': 2.4274, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.4645824432373047, 'eval_accuracy@zho.rst.sctb': 0.30851063829787234, 'eval_f1@zho.rst.sctb': 0.026315789473684206, 'eval_precision@zho.rst.sctb': 0.018170426065162906, 'eval_recall@zho.rst.sctb': 0.047697368421052634, 'eval_loss@zho.rst.sctb': 2.464582681655884, 'eval_runtime': 1.3944, 'eval_samples_per_second': 67.414, 'eval_steps_per_second': 2.152, 'epoch': 4.0}
{'train@zho.rst.sctb_loss': 2.2467281818389893, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3553530751708428, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.047170124025486974, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.05139930249098123, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.05727427763296544, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.24672794342041, 'train@zho.rst.sctb_runtime': 5.4437, 'train@zho.rst.sctb_samples_per_second': 80.644, 'train@zho.rst.sctb_steps_per_second': 2.572, 'epoch': 5.0}
{'loss': 2.3179, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.412729501724243, 'eval_accuracy@zho.rst.sctb': 0.3191489361702128, 'eval_f1@zho.rst.sctb': 0.03059320542158071, 'eval_precision@zho.rst.sctb': 0.024237300077502993, 'eval_recall@zho.rst.sctb': 0.05079334365325078, 'eval_loss@zho.rst.sctb': 2.4127299785614014, 'eval_runtime': 1.3881, 'eval_samples_per_second': 67.717, 'eval_steps_per_second': 2.161, 'epoch': 5.0}
{'train@zho.rst.sctb_loss': 2.1849563121795654, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.36674259681093396, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.056995045599906986, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.07324481074481075, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.06874589935781458, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.1849563121795654, 'train@zho.rst.sctb_runtime': 5.4069, 'train@zho.rst.sctb_samples_per_second': 81.193, 'train@zho.rst.sctb_steps_per_second': 2.589, 'epoch': 6.0}
{'loss': 2.2549, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.382666826248169, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.04329568861424817, 'eval_precision@zho.rst.sctb': 0.08440308087291398, 'eval_recall@zho.rst.sctb': 0.058275283797729616, 'eval_loss@zho.rst.sctb': 2.382666826248169, 'eval_runtime': 1.3901, 'eval_samples_per_second': 67.623, 'eval_steps_per_second': 2.158, 'epoch': 6.0}
{'train@zho.rst.sctb_loss': 2.1430206298828125, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.38496583143507973, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.06108870098214739, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.07627053322892634, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.0726264279652548, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.1430206298828125, 'train@zho.rst.sctb_runtime': 5.4572, 'train@zho.rst.sctb_samples_per_second': 80.445, 'train@zho.rst.sctb_steps_per_second': 2.565, 'epoch': 7.0}
{'loss': 2.1847, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.358701229095459, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.04193175245806824, 'eval_precision@zho.rst.sctb': 0.056432748538011696, 'eval_recall@zho.rst.sctb': 0.056630546955624354, 'eval_loss@zho.rst.sctb': 2.358700752258301, 'eval_runtime': 1.3759, 'eval_samples_per_second': 68.316, 'eval_steps_per_second': 2.18, 'epoch': 7.0}
{'train@zho.rst.sctb_loss': 2.1140036582946777, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.39863325740318906, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.07193436408552688, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.12022407022407022, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.08197647203680315, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.1140036582946777, 'train@zho.rst.sctb_runtime': 5.421, 'train@zho.rst.sctb_samples_per_second': 80.982, 'train@zho.rst.sctb_steps_per_second': 2.583, 'epoch': 8.0}
{'loss': 2.1716, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.341945171356201, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.04814814814814814, 'eval_precision@zho.rst.sctb': 0.06403508771929825, 'eval_recall@zho.rst.sctb': 0.06101651186790505, 'eval_loss@zho.rst.sctb': 2.341945171356201, 'eval_runtime': 1.3965, 'eval_samples_per_second': 67.311, 'eval_steps_per_second': 2.148, 'epoch': 8.0}
{'train@zho.rst.sctb_loss': 2.091822624206543, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.4031890660592255, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.07058859756228177, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.1182037308740306, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.08278618863599342, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.091823101043701, 'train@zho.rst.sctb_runtime': 5.426, 'train@zho.rst.sctb_samples_per_second': 80.907, 'train@zho.rst.sctb_steps_per_second': 2.58, 'epoch': 9.0}
{'loss': 2.1299, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.329875946044922, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.04715843663212085, 'eval_precision@zho.rst.sctb': 0.06264510528334645, 'eval_recall@zho.rst.sctb': 0.05937177502579979, 'eval_loss@zho.rst.sctb': 2.3298754692077637, 'eval_runtime': 1.3864, 'eval_samples_per_second': 67.803, 'eval_steps_per_second': 2.164, 'epoch': 9.0}
{'train@zho.rst.sctb_loss': 2.074706792831421, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.4054669703872437, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.07188969046372724, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.11745129467566606, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.0836533011455812, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.074706792831421, 'train@zho.rst.sctb_runtime': 5.4021, 'train@zho.rst.sctb_samples_per_second': 81.265, 'train@zho.rst.sctb_steps_per_second': 2.592, 'epoch': 10.0}
{'loss': 2.1141, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.321291446685791, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.05313866498077024, 'eval_precision@zho.rst.sctb': 0.06798800799467022, 'eval_recall@zho.rst.sctb': 0.0637577399380805, 'eval_loss@zho.rst.sctb': 2.321291446685791, 'eval_runtime': 1.4033, 'eval_samples_per_second': 66.987, 'eval_steps_per_second': 2.138, 'epoch': 10.0}
{'train@zho.rst.sctb_loss': 2.0653882026672363, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.4054669703872437, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.07188969046372724, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.11745129467566606, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.0836533011455812, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.0653882026672363, 'train@zho.rst.sctb_runtime': 5.4119, 'train@zho.rst.sctb_samples_per_second': 81.117, 'train@zho.rst.sctb_steps_per_second': 2.587, 'epoch': 11.0}
{'loss': 2.0978, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.3162569999694824, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.05313866498077024, 'eval_precision@zho.rst.sctb': 0.06798800799467022, 'eval_recall@zho.rst.sctb': 0.0637577399380805, 'eval_loss@zho.rst.sctb': 2.316256523132324, 'eval_runtime': 1.3839, 'eval_samples_per_second': 67.922, 'eval_steps_per_second': 2.168, 'epoch': 11.0}
{'train@zho.rst.sctb_loss': 2.0625364780426025, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.4054669703872437, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.07188969046372724, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.11745129467566606, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.0836533011455812, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.0625364780426025, 'train@zho.rst.sctb_runtime': 5.4325, 'train@zho.rst.sctb_samples_per_second': 80.811, 'train@zho.rst.sctb_steps_per_second': 2.577, 'epoch': 12.0}
{'loss': 2.0972, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.3142921924591064, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.05227756930543308, 'eval_precision@zho.rst.sctb': 0.06126286179583981, 'eval_recall@zho.rst.sctb': 0.0637577399380805, 'eval_loss@zho.rst.sctb': 2.3142921924591064, 'eval_runtime': 1.4024, 'eval_samples_per_second': 67.03, 'eval_steps_per_second': 2.139, 'epoch': 12.0}
{'train_runtime': 213.1555, 'train_samples_per_second': 24.714, 'train_steps_per_second': 0.788, 'train_loss': 2.430820499147688, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.7999
  train_runtime            = 1:46:50.65
  train_samples_per_second =     26.014
  train_steps_per_second   =      0.814
-------------------------------------------------------------------
Lang1:  eng.rst.rstdt    Lang2:  zho.rst.sctb
Saving run to:  runs/full_shot/FullShot=v4_eng.rst.rstdt_zho.rst.sctb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 16002 examples
read 1621 examples
read 2155 examples
read 439 examples
read 94 examples
read 159 examples
Total prediction labels:  34
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (zh): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=34, bias=True)
    )
  )
)
{'train@eng.rst.rstdt_loss': 1.7493164539337158, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.5095613048368954, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.08420985567496077, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.09625963713772234, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.11087198334514736, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.7493164539337158, 'train@eng.rst.rstdt_runtime': 187.8371, 'train@eng.rst.rstdt_samples_per_second': 85.191, 'train@eng.rst.rstdt_steps_per_second': 2.667, 'epoch': 1.0}
{'loss': 2.1644, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.7340984344482422, 'eval_accuracy@eng.rst.rstdt': 0.5212831585441086, 'eval_f1@eng.rst.rstdt': 0.08567308436070874, 'eval_precision@eng.rst.rstdt': 0.09541468042763919, 'eval_recall@eng.rst.rstdt': 0.11040484419644041, 'eval_loss@eng.rst.rstdt': 1.7340984344482422, 'eval_runtime': 19.3281, 'eval_samples_per_second': 83.867, 'eval_steps_per_second': 2.639, 'epoch': 1.0}
{'train@eng.rst.rstdt_loss': 1.4239555597305298, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6029871266091739, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.20701646016523695, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.3173854385625284, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.2143935412390582, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.4239556789398193, 'train@eng.rst.rstdt_runtime': 187.5499, 'train@eng.rst.rstdt_samples_per_second': 85.321, 'train@eng.rst.rstdt_steps_per_second': 2.671, 'epoch': 2.0}
{'loss': 1.6135, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.4301172494888306, 'eval_accuracy@eng.rst.rstdt': 0.608266502159161, 'eval_f1@eng.rst.rstdt': 0.20745242923782062, 'eval_precision@eng.rst.rstdt': 0.27271132014776683, 'eval_recall@eng.rst.rstdt': 0.21241400696566243, 'eval_loss@eng.rst.rstdt': 1.4301172494888306, 'eval_runtime': 19.3327, 'eval_samples_per_second': 83.847, 'eval_steps_per_second': 2.638, 'epoch': 2.0}
{'train@eng.rst.rstdt_loss': 1.311539888381958, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6335458067741533, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.2995382847136352, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.441819992364689, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.28050974838954146, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.311539888381958, 'train@eng.rst.rstdt_runtime': 187.6668, 'train@eng.rst.rstdt_samples_per_second': 85.268, 'train@eng.rst.rstdt_steps_per_second': 2.67, 'epoch': 3.0}
{'loss': 1.4251, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.341876745223999, 'eval_accuracy@eng.rst.rstdt': 0.6329426280074029, 'eval_f1@eng.rst.rstdt': 0.291943218615781, 'eval_precision@eng.rst.rstdt': 0.39232253905181586, 'eval_recall@eng.rst.rstdt': 0.2791097220265227, 'eval_loss@eng.rst.rstdt': 1.341876745223999, 'eval_runtime': 19.3254, 'eval_samples_per_second': 83.879, 'eval_steps_per_second': 2.639, 'epoch': 3.0}
{'train@eng.rst.rstdt_loss': 1.2417409420013428, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6470441194850644, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3426843215770236, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.44039932781121727, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.32322194286079065, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.2417408227920532, 'train@eng.rst.rstdt_runtime': 187.7486, 'train@eng.rst.rstdt_samples_per_second': 85.231, 'train@eng.rst.rstdt_steps_per_second': 2.668, 'epoch': 4.0}
{'loss': 1.3286, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.303671956062317, 'eval_accuracy@eng.rst.rstdt': 0.6317088217149908, 'eval_f1@eng.rst.rstdt': 0.3266741357162642, 'eval_precision@eng.rst.rstdt': 0.41915202684399233, 'eval_recall@eng.rst.rstdt': 0.3199694136035255, 'eval_loss@eng.rst.rstdt': 1.303671956062317, 'eval_runtime': 19.331, 'eval_samples_per_second': 83.855, 'eval_steps_per_second': 2.638, 'epoch': 4.0}
{'train@eng.rst.rstdt_loss': 1.1904269456863403, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6584176977877765, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3614475700948687, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5425621821207088, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3358932761518731, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1904270648956299, 'train@eng.rst.rstdt_runtime': 187.6525, 'train@eng.rst.rstdt_samples_per_second': 85.275, 'train@eng.rst.rstdt_steps_per_second': 2.67, 'epoch': 5.0}
{'loss': 1.2682, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.2603635787963867, 'eval_accuracy@eng.rst.rstdt': 0.6354102405922271, 'eval_f1@eng.rst.rstdt': 0.327163207366723, 'eval_precision@eng.rst.rstdt': 0.3945853819213122, 'eval_recall@eng.rst.rstdt': 0.32066561803667626, 'eval_loss@eng.rst.rstdt': 1.2603635787963867, 'eval_runtime': 19.3284, 'eval_samples_per_second': 83.866, 'eval_steps_per_second': 2.639, 'epoch': 5.0}
{'train@eng.rst.rstdt_loss': 1.1573361158370972, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6644169478815148, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3847595891669248, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5861854593557342, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.35192204481120243, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1573361158370972, 'train@eng.rst.rstdt_runtime': 187.6809, 'train@eng.rst.rstdt_samples_per_second': 85.262, 'train@eng.rst.rstdt_steps_per_second': 2.669, 'epoch': 6.0}
{'loss': 1.2236, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.2348682880401611, 'eval_accuracy@eng.rst.rstdt': 0.6409623689080814, 'eval_f1@eng.rst.rstdt': 0.3310282462423916, 'eval_precision@eng.rst.rstdt': 0.38987858156589605, 'eval_recall@eng.rst.rstdt': 0.32645382598464223, 'eval_loss@eng.rst.rstdt': 1.2348682880401611, 'eval_runtime': 19.3147, 'eval_samples_per_second': 83.926, 'eval_steps_per_second': 2.64, 'epoch': 6.0}
{'train@eng.rst.rstdt_loss': 1.136570692062378, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6667291588551431, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3968207421945177, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5786645818424461, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.35994116717141295, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1365705728530884, 'train@eng.rst.rstdt_runtime': 187.3388, 'train@eng.rst.rstdt_samples_per_second': 85.417, 'train@eng.rst.rstdt_steps_per_second': 2.674, 'epoch': 7.0}
{'loss': 1.2003, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.217610478401184, 'eval_accuracy@eng.rst.rstdt': 0.6391116594694632, 'eval_f1@eng.rst.rstdt': 0.34671946240840734, 'eval_precision@eng.rst.rstdt': 0.44189045480518035, 'eval_recall@eng.rst.rstdt': 0.3395997918347503, 'eval_loss@eng.rst.rstdt': 1.217610478401184, 'eval_runtime': 19.3063, 'eval_samples_per_second': 83.962, 'eval_steps_per_second': 2.642, 'epoch': 7.0}
{'train@eng.rst.rstdt_loss': 1.119982361793518, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.669978752655918, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.40566202197922413, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5672106667027308, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3701389179308985, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.119982361793518, 'train@eng.rst.rstdt_runtime': 187.4196, 'train@eng.rst.rstdt_samples_per_second': 85.381, 'train@eng.rst.rstdt_steps_per_second': 2.673, 'epoch': 8.0}
{'loss': 1.1811, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.2117135524749756, 'eval_accuracy@eng.rst.rstdt': 0.6397285626156693, 'eval_f1@eng.rst.rstdt': 0.35056707946704596, 'eval_precision@eng.rst.rstdt': 0.44788174977811546, 'eval_recall@eng.rst.rstdt': 0.3428297240256186, 'eval_loss@eng.rst.rstdt': 1.2117135524749756, 'eval_runtime': 19.366, 'eval_samples_per_second': 83.703, 'eval_steps_per_second': 2.633, 'epoch': 8.0}
{'train@eng.rst.rstdt_loss': 1.1083632707595825, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6717910261217348, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.41317665682565163, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5649639434839453, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.37542681984152987, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1083632707595825, 'train@eng.rst.rstdt_runtime': 188.5344, 'train@eng.rst.rstdt_samples_per_second': 84.876, 'train@eng.rst.rstdt_steps_per_second': 2.657, 'epoch': 9.0}
{'loss': 1.1637, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.2021821737289429, 'eval_accuracy@eng.rst.rstdt': 0.6434299814929056, 'eval_f1@eng.rst.rstdt': 0.35386613207504974, 'eval_precision@eng.rst.rstdt': 0.4461006535737274, 'eval_recall@eng.rst.rstdt': 0.34615786694319817, 'eval_loss@eng.rst.rstdt': 1.2021821737289429, 'eval_runtime': 19.3045, 'eval_samples_per_second': 83.97, 'eval_steps_per_second': 2.642, 'epoch': 9.0}
{'train@eng.rst.rstdt_loss': 1.1035380363464355, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6714160729908761, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.42282844162843974, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6007535223569542, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.38736203151748516, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1035380363464355, 'train@eng.rst.rstdt_runtime': 187.4746, 'train@eng.rst.rstdt_samples_per_second': 85.356, 'train@eng.rst.rstdt_steps_per_second': 2.672, 'epoch': 10.0}
{'loss': 1.1564, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.2081490755081177, 'eval_accuracy@eng.rst.rstdt': 0.636027143738433, 'eval_f1@eng.rst.rstdt': 0.3619696373263088, 'eval_precision@eng.rst.rstdt': 0.47740096113610625, 'eval_recall@eng.rst.rstdt': 0.353413097872885, 'eval_loss@eng.rst.rstdt': 1.2081491947174072, 'eval_runtime': 19.331, 'eval_samples_per_second': 83.855, 'eval_steps_per_second': 2.638, 'epoch': 10.0}
{'train@eng.rst.rstdt_loss': 1.0963069200515747, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6735408073990751, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4217278663885705, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.617523788025742, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.38382547965590147, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.0963069200515747, 'train@eng.rst.rstdt_runtime': 187.5054, 'train@eng.rst.rstdt_samples_per_second': 85.342, 'train@eng.rst.rstdt_steps_per_second': 2.672, 'epoch': 11.0}
{'loss': 1.1475, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.1983963251113892, 'eval_accuracy@eng.rst.rstdt': 0.6403454657618753, 'eval_f1@eng.rst.rstdt': 0.357301101601138, 'eval_precision@eng.rst.rstdt': 0.5039076834964363, 'eval_recall@eng.rst.rstdt': 0.3477425399818589, 'eval_loss@eng.rst.rstdt': 1.1983962059020996, 'eval_runtime': 19.3395, 'eval_samples_per_second': 83.818, 'eval_steps_per_second': 2.637, 'epoch': 11.0}
{'train@eng.rst.rstdt_loss': 1.0949137210845947, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6739157605299337, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4228202646418449, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6191013012719324, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3841910807966185, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.0949137210845947, 'train@eng.rst.rstdt_runtime': 187.5744, 'train@eng.rst.rstdt_samples_per_second': 85.31, 'train@eng.rst.rstdt_steps_per_second': 2.671, 'epoch': 12.0}
{'loss': 1.1446, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.1967554092407227, 'eval_accuracy@eng.rst.rstdt': 0.6409623689080814, 'eval_f1@eng.rst.rstdt': 0.35744230454512427, 'eval_precision@eng.rst.rstdt': 0.5036640166443723, 'eval_recall@eng.rst.rstdt': 0.34783289870599365, 'eval_loss@eng.rst.rstdt': 1.1967554092407227, 'eval_runtime': 19.3289, 'eval_samples_per_second': 83.864, 'eval_steps_per_second': 2.639, 'epoch': 12.0}
{'train_runtime': 7256.3858, 'train_samples_per_second': 26.463, 'train_steps_per_second': 0.829, 'train_loss': 1.3347474464320057, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.3347
  train_runtime            = 2:00:56.38
  train_samples_per_second =     26.463
  train_steps_per_second   =      0.829
{'train@zho.rst.sctb_loss': 4.641364097595215, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.004555808656036446, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.007928994082840236, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.019349477682811016, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04326923076923077, 'train@zho.rst.sctb_loss@zho.rst.sctb': 4.641364097595215, 'train@zho.rst.sctb_runtime': 5.4878, 'train@zho.rst.sctb_samples_per_second': 79.996, 'train@zho.rst.sctb_steps_per_second': 2.551, 'epoch': 1.0}
{'loss': 5.6245, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 4.686020374298096, 'eval_accuracy@zho.rst.sctb': 0.0, 'eval_f1@zho.rst.sctb': 0.0, 'eval_precision@zho.rst.sctb': 0.0, 'eval_recall@zho.rst.sctb': 0.0, 'eval_loss@zho.rst.sctb': 4.686020374298096, 'eval_runtime': 1.4321, 'eval_samples_per_second': 65.637, 'eval_steps_per_second': 2.095, 'epoch': 1.0}
{'train@zho.rst.sctb_loss': 3.6401891708374023, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.004555808656036446, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.007921245421245423, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.01934557979334099, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04326923076923077, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.6401896476745605, 'train@zho.rst.sctb_runtime': 5.4599, 'train@zho.rst.sctb_samples_per_second': 80.404, 'train@zho.rst.sctb_steps_per_second': 2.564, 'epoch': 2.0}
{'loss': 4.0624, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.6909706592559814, 'eval_accuracy@zho.rst.sctb': 0.0, 'eval_f1@zho.rst.sctb': 0.0, 'eval_precision@zho.rst.sctb': 0.0, 'eval_recall@zho.rst.sctb': 0.0, 'eval_loss@zho.rst.sctb': 3.690969705581665, 'eval_runtime': 1.4299, 'eval_samples_per_second': 65.74, 'eval_steps_per_second': 2.098, 'epoch': 2.0}
{'train@zho.rst.sctb_loss': 3.1810622215270996, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.00683371298405467, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.007931093296946955, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.017094017094017092, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.005334562697576396, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.181062698364258, 'train@zho.rst.sctb_runtime': 5.4885, 'train@zho.rst.sctb_samples_per_second': 79.985, 'train@zho.rst.sctb_steps_per_second': 2.551, 'epoch': 3.0}
{'loss': 3.4302, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 3.2502782344818115, 'eval_accuracy@zho.rst.sctb': 0.0, 'eval_f1@zho.rst.sctb': 0.0, 'eval_precision@zho.rst.sctb': 0.0, 'eval_recall@zho.rst.sctb': 0.0, 'eval_loss@zho.rst.sctb': 3.250277042388916, 'eval_runtime': 1.428, 'eval_samples_per_second': 65.827, 'eval_steps_per_second': 2.101, 'epoch': 3.0}
{'train@zho.rst.sctb_loss': 2.8849568367004395, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.31890660592255127, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.03194180751358954, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.031955789308730484, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.040133658698907435, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.8849565982818604, 'train@zho.rst.sctb_runtime': 5.4664, 'train@zho.rst.sctb_samples_per_second': 80.309, 'train@zho.rst.sctb_steps_per_second': 2.561, 'epoch': 4.0}
{'loss': 3.0721, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.9945905208587646, 'eval_accuracy@zho.rst.sctb': 0.2553191489361702, 'eval_f1@zho.rst.sctb': 0.024742268041237116, 'eval_precision@zho.rst.sctb': 0.018461538461538463, 'eval_recall@zho.rst.sctb': 0.0375, 'eval_loss@zho.rst.sctb': 2.994591236114502, 'eval_runtime': 1.4439, 'eval_samples_per_second': 65.102, 'eval_steps_per_second': 2.078, 'epoch': 4.0}
{'train@zho.rst.sctb_loss': 2.6554484367370605, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.42369020501138954, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.053844948910738384, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.07540910082668671, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.06278261512580187, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.6554486751556396, 'train@zho.rst.sctb_runtime': 5.4528, 'train@zho.rst.sctb_samples_per_second': 80.509, 'train@zho.rst.sctb_steps_per_second': 2.567, 'epoch': 5.0}
{'loss': 2.8307, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.811802864074707, 'eval_accuracy@zho.rst.sctb': 0.35106382978723405, 'eval_f1@zho.rst.sctb': 0.04416163736552085, 'eval_precision@zho.rst.sctb': 0.03580985915492958, 'eval_recall@zho.rst.sctb': 0.06121323529411764, 'eval_loss@zho.rst.sctb': 2.811802625656128, 'eval_runtime': 1.4268, 'eval_samples_per_second': 65.883, 'eval_steps_per_second': 2.103, 'epoch': 5.0}
{'train@zho.rst.sctb_loss': 2.481501579284668, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.4328018223234624, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.04388514711095356, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.036168628972227175, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.05882091952748045, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.481501579284668, 'train@zho.rst.sctb_runtime': 5.4553, 'train@zho.rst.sctb_samples_per_second': 80.472, 'train@zho.rst.sctb_steps_per_second': 2.566, 'epoch': 6.0}
{'loss': 2.6284, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.6824190616607666, 'eval_accuracy@zho.rst.sctb': 0.3617021276595745, 'eval_f1@zho.rst.sctb': 0.04816372522044469, 'eval_precision@zho.rst.sctb': 0.0384122919334187, 'eval_recall@zho.rst.sctb': 0.06753095975232198, 'eval_loss@zho.rst.sctb': 2.6824193000793457, 'eval_runtime': 1.4168, 'eval_samples_per_second': 66.347, 'eval_steps_per_second': 2.117, 'epoch': 6.0}
{'train@zho.rst.sctb_loss': 2.362553358078003, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.4396355353075171, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.044363179530258076, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03557644541251099, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.06045976374022517, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.362553358078003, 'train@zho.rst.sctb_runtime': 5.4688, 'train@zho.rst.sctb_samples_per_second': 80.273, 'train@zho.rst.sctb_steps_per_second': 2.56, 'epoch': 7.0}
{'loss': 2.4869, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.5986807346343994, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.0444921828380475, 'eval_precision@zho.rst.sctb': 0.03417634996582365, 'eval_recall@zho.rst.sctb': 0.06424148606811146, 'eval_loss@zho.rst.sctb': 2.5986807346343994, 'eval_runtime': 1.4293, 'eval_samples_per_second': 65.766, 'eval_steps_per_second': 2.099, 'epoch': 7.0}
{'train@zho.rst.sctb_loss': 2.301328182220459, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.4419134396355353, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.044722737125548, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03664294678867782, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.060298929621207926, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.30132794380188, 'train@zho.rst.sctb_runtime': 5.4587, 'train@zho.rst.sctb_samples_per_second': 80.422, 'train@zho.rst.sctb_steps_per_second': 2.565, 'epoch': 8.0}
{'loss': 2.4108, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.552339553833008, 'eval_accuracy@zho.rst.sctb': 0.3723404255319149, 'eval_f1@zho.rst.sctb': 0.04772001395635954, 'eval_precision@zho.rst.sctb': 0.03833570412517781, 'eval_recall@zho.rst.sctb': 0.06772445820433436, 'eval_loss@zho.rst.sctb': 2.552340030670166, 'eval_runtime': 1.436, 'eval_samples_per_second': 65.458, 'eval_steps_per_second': 2.089, 'epoch': 8.0}
{'train@zho.rst.sctb_loss': 2.2583062648773193, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.4419134396355353, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.04478681448175247, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.036977058029689605, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.06015750651655482, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.2583060264587402, 'train@zho.rst.sctb_runtime': 5.5019, 'train@zho.rst.sctb_samples_per_second': 79.791, 'train@zho.rst.sctb_steps_per_second': 2.545, 'epoch': 9.0}
{'loss': 2.3233, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.5211708545684814, 'eval_accuracy@zho.rst.sctb': 0.3723404255319149, 'eval_f1@zho.rst.sctb': 0.04772001395635954, 'eval_precision@zho.rst.sctb': 0.03833570412517781, 'eval_recall@zho.rst.sctb': 0.06772445820433436, 'eval_loss@zho.rst.sctb': 2.5211708545684814, 'eval_runtime': 1.4491, 'eval_samples_per_second': 64.867, 'eval_steps_per_second': 2.07, 'epoch': 9.0}
{'train@zho.rst.sctb_loss': 2.22725772857666, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.44874715261959, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.045409903639481976, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03684272716530781, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.061513504519993344, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.22725772857666, 'train@zho.rst.sctb_runtime': 5.4482, 'train@zho.rst.sctb_samples_per_second': 80.577, 'train@zho.rst.sctb_steps_per_second': 2.57, 'epoch': 10.0}
{'loss': 2.3036, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.499281167984009, 'eval_accuracy@zho.rst.sctb': 0.3723404255319149, 'eval_f1@zho.rst.sctb': 0.04746075715604801, 'eval_precision@zho.rst.sctb': 0.03773131458783946, 'eval_recall@zho.rst.sctb': 0.06772445820433436, 'eval_loss@zho.rst.sctb': 2.499281644821167, 'eval_runtime': 1.4314, 'eval_samples_per_second': 65.671, 'eval_steps_per_second': 2.096, 'epoch': 10.0}
{'train@zho.rst.sctb_loss': 2.213550090789795, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.44419134396355353, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.04495354607882739, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.0367072582589824, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.06070378792080307, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.213550090789795, 'train@zho.rst.sctb_runtime': 5.4694, 'train@zho.rst.sctb_samples_per_second': 80.264, 'train@zho.rst.sctb_steps_per_second': 2.56, 'epoch': 11.0}
{'loss': 2.2849, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.489514112472534, 'eval_accuracy@zho.rst.sctb': 0.3723404255319149, 'eval_f1@zho.rst.sctb': 0.04746075715604801, 'eval_precision@zho.rst.sctb': 0.03773131458783946, 'eval_recall@zho.rst.sctb': 0.06772445820433436, 'eval_loss@zho.rst.sctb': 2.4895148277282715, 'eval_runtime': 1.4448, 'eval_samples_per_second': 65.061, 'eval_steps_per_second': 2.076, 'epoch': 11.0}
{'train@zho.rst.sctb_loss': 2.2088000774383545, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.44419134396355353, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.04495354607882739, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.0367072582589824, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.06070378792080307, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.2088005542755127, 'train@zho.rst.sctb_runtime': 5.4361, 'train@zho.rst.sctb_samples_per_second': 80.756, 'train@zho.rst.sctb_steps_per_second': 2.575, 'epoch': 12.0}
{'loss': 2.2588, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.486138105392456, 'eval_accuracy@zho.rst.sctb': 0.3723404255319149, 'eval_f1@zho.rst.sctb': 0.04746075715604801, 'eval_precision@zho.rst.sctb': 0.03773131458783946, 'eval_recall@zho.rst.sctb': 0.06772445820433436, 'eval_loss@zho.rst.sctb': 2.4861373901367188, 'eval_runtime': 1.4239, 'eval_samples_per_second': 66.014, 'eval_steps_per_second': 2.107, 'epoch': 12.0}
{'train_runtime': 214.1035, 'train_samples_per_second': 24.605, 'train_steps_per_second': 0.785, 'train_loss': 2.976380121140253, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.3347
  train_runtime            = 2:00:56.38
  train_samples_per_second =     26.463
  train_steps_per_second   =      0.829
-------------------------------------------------------------------
Lang1:  eng.sdrt.stac    Lang2:  zho.rst.sctb
Saving run to:  runs/full_shot/FullShot=v4_eng.sdrt.stac_zho.rst.sctb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 9580 examples
read 1145 examples
read 1510 examples
read 439 examples
read 94 examples
read 159 examples
Total prediction labels:  42
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (zh): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=42, bias=True)
    )
  )
)
{'train@eng.sdrt.stac_loss': 2.105386257171631, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.3497912317327766, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.06473527510536733, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.09608654842967215, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.10922613750081978, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 2.105386257171631, 'train@eng.sdrt.stac_runtime': 112.371, 'train@eng.sdrt.stac_samples_per_second': 85.253, 'train@eng.sdrt.stac_steps_per_second': 2.67, 'epoch': 1.0}
{'loss': 2.5929, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.0581130981445312, 'eval_accuracy@eng.sdrt.stac': 0.359825327510917, 'eval_f1@eng.sdrt.stac': 0.06577713309965044, 'eval_precision@eng.sdrt.stac': 0.11118625065598015, 'eval_recall@eng.sdrt.stac': 0.11218989149622949, 'eval_loss@eng.sdrt.stac': 2.0581133365631104, 'eval_runtime': 13.8101, 'eval_samples_per_second': 82.91, 'eval_steps_per_second': 2.607, 'epoch': 1.0}
{'train@eng.sdrt.stac_loss': 1.8944666385650635, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4264091858037578, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.1317765586103351, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.14477779431713333, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.17305413294304928, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.8944664001464844, 'train@eng.sdrt.stac_runtime': 112.3167, 'train@eng.sdrt.stac_samples_per_second': 85.295, 'train@eng.sdrt.stac_steps_per_second': 2.671, 'epoch': 2.0}
{'loss': 2.0371, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.8476938009262085, 'eval_accuracy@eng.sdrt.stac': 0.4200873362445415, 'eval_f1@eng.sdrt.stac': 0.1242103714562528, 'eval_precision@eng.sdrt.stac': 0.1222511570908472, 'eval_recall@eng.sdrt.stac': 0.16781463126967958, 'eval_loss@eng.sdrt.stac': 1.8476938009262085, 'eval_runtime': 13.8037, 'eval_samples_per_second': 82.949, 'eval_steps_per_second': 2.608, 'epoch': 2.0}
{'train@eng.sdrt.stac_loss': 1.7776715755462646, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.45156576200417536, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.15719949972253916, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.20426122803327545, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.19489074469656084, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.7776718139648438, 'train@eng.sdrt.stac_runtime': 112.5081, 'train@eng.sdrt.stac_samples_per_second': 85.149, 'train@eng.sdrt.stac_steps_per_second': 2.666, 'epoch': 3.0}
{'loss': 1.8745, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.7340867519378662, 'eval_accuracy@eng.sdrt.stac': 0.4524017467248908, 'eval_f1@eng.sdrt.stac': 0.15003780778636916, 'eval_precision@eng.sdrt.stac': 0.19569478384130368, 'eval_recall@eng.sdrt.stac': 0.19043899240950962, 'eval_loss@eng.sdrt.stac': 1.7340866327285767, 'eval_runtime': 13.8188, 'eval_samples_per_second': 82.858, 'eval_steps_per_second': 2.605, 'epoch': 3.0}
{'train@eng.sdrt.stac_loss': 1.7042820453643799, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4663883089770355, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.179727843097807, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.23335465762911017, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2088612320146559, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.7042820453643799, 'train@eng.sdrt.stac_runtime': 112.1529, 'train@eng.sdrt.stac_samples_per_second': 85.419, 'train@eng.sdrt.stac_steps_per_second': 2.675, 'epoch': 4.0}
{'loss': 1.7855, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.6539515256881714, 'eval_accuracy@eng.sdrt.stac': 0.46550218340611355, 'eval_f1@eng.sdrt.stac': 0.16657665542308547, 'eval_precision@eng.sdrt.stac': 0.26056546124205393, 'eval_recall@eng.sdrt.stac': 0.20013133410946127, 'eval_loss@eng.sdrt.stac': 1.6539515256881714, 'eval_runtime': 13.7976, 'eval_samples_per_second': 82.985, 'eval_steps_per_second': 2.609, 'epoch': 4.0}
{'train@eng.sdrt.stac_loss': 1.6568278074264526, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4859081419624217, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.20914936585410263, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.2160236523966858, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.23216331277837743, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.6568280458450317, 'train@eng.sdrt.stac_runtime': 112.1997, 'train@eng.sdrt.stac_samples_per_second': 85.383, 'train@eng.sdrt.stac_steps_per_second': 2.674, 'epoch': 5.0}
{'loss': 1.7249, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.610839605331421, 'eval_accuracy@eng.sdrt.stac': 0.48995633187772925, 'eval_f1@eng.sdrt.stac': 0.19868976620294654, 'eval_precision@eng.sdrt.stac': 0.212430357983661, 'eval_recall@eng.sdrt.stac': 0.22479871116633443, 'eval_loss@eng.sdrt.stac': 1.610839605331421, 'eval_runtime': 13.7902, 'eval_samples_per_second': 83.03, 'eval_steps_per_second': 2.611, 'epoch': 5.0}
{'train@eng.sdrt.stac_loss': 1.6136188507080078, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.48778705636743214, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.20601024157656883, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.2280636323325067, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.22897862596883212, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.6136189699172974, 'train@eng.sdrt.stac_runtime': 112.1371, 'train@eng.sdrt.stac_samples_per_second': 85.431, 'train@eng.sdrt.stac_steps_per_second': 2.675, 'epoch': 6.0}
{'loss': 1.6749, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.5853688716888428, 'eval_accuracy@eng.sdrt.stac': 0.48646288209606986, 'eval_f1@eng.sdrt.stac': 0.19360133304891997, 'eval_precision@eng.sdrt.stac': 0.2336260378109306, 'eval_recall@eng.sdrt.stac': 0.21955375156303542, 'eval_loss@eng.sdrt.stac': 1.5853688716888428, 'eval_runtime': 13.7563, 'eval_samples_per_second': 83.235, 'eval_steps_per_second': 2.617, 'epoch': 6.0}
{'train@eng.sdrt.stac_loss': 1.591729998588562, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5021920668058455, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2294455270706303, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.24552934396273168, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.25168549765765724, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.591729998588562, 'train@eng.sdrt.stac_runtime': 112.154, 'train@eng.sdrt.stac_samples_per_second': 85.418, 'train@eng.sdrt.stac_steps_per_second': 2.675, 'epoch': 7.0}
{'loss': 1.6449, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.564407229423523, 'eval_accuracy@eng.sdrt.stac': 0.5074235807860262, 'eval_f1@eng.sdrt.stac': 0.22195209660143006, 'eval_precision@eng.sdrt.stac': 0.22146787858972972, 'eval_recall@eng.sdrt.stac': 0.24273988763208115, 'eval_loss@eng.sdrt.stac': 1.564407229423523, 'eval_runtime': 13.7305, 'eval_samples_per_second': 83.391, 'eval_steps_per_second': 2.622, 'epoch': 7.0}
{'train@eng.sdrt.stac_loss': 1.5587869882583618, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5100208768267224, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2481472412180051, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.3563748006124121, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2677323129742735, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5587869882583618, 'train@eng.sdrt.stac_runtime': 112.2071, 'train@eng.sdrt.stac_samples_per_second': 85.378, 'train@eng.sdrt.stac_steps_per_second': 2.674, 'epoch': 8.0}
{'loss': 1.6216, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.5316489934921265, 'eval_accuracy@eng.sdrt.stac': 0.5117903930131005, 'eval_f1@eng.sdrt.stac': 0.23094097890638374, 'eval_precision@eng.sdrt.stac': 0.23630010930143389, 'eval_recall@eng.sdrt.stac': 0.25249924033129223, 'eval_loss@eng.sdrt.stac': 1.5316489934921265, 'eval_runtime': 13.8017, 'eval_samples_per_second': 82.961, 'eval_steps_per_second': 2.608, 'epoch': 8.0}
{'train@eng.sdrt.stac_loss': 1.542279601097107, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5175365344467641, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.27159421015620455, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.3877041924382869, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.28835135602815265, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.542279601097107, 'train@eng.sdrt.stac_runtime': 112.1236, 'train@eng.sdrt.stac_samples_per_second': 85.441, 'train@eng.sdrt.stac_steps_per_second': 2.676, 'epoch': 9.0}
{'loss': 1.5971, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.5203616619110107, 'eval_accuracy@eng.sdrt.stac': 0.519650655021834, 'eval_f1@eng.sdrt.stac': 0.2480265213018674, 'eval_precision@eng.sdrt.stac': 0.24541201692880452, 'eval_recall@eng.sdrt.stac': 0.26772566640123524, 'eval_loss@eng.sdrt.stac': 1.5203617811203003, 'eval_runtime': 13.7923, 'eval_samples_per_second': 83.017, 'eval_steps_per_second': 2.61, 'epoch': 9.0}
{'train@eng.sdrt.stac_loss': 1.5267657041549683, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5206680584551148, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.27826833416666097, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.36780371858419936, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.29987663618662325, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5267657041549683, 'train@eng.sdrt.stac_runtime': 112.2885, 'train@eng.sdrt.stac_samples_per_second': 85.316, 'train@eng.sdrt.stac_steps_per_second': 2.672, 'epoch': 10.0}
{'loss': 1.583, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.5067821741104126, 'eval_accuracy@eng.sdrt.stac': 0.5240174672489083, 'eval_f1@eng.sdrt.stac': 0.2505775211880821, 'eval_precision@eng.sdrt.stac': 0.2435839623687171, 'eval_recall@eng.sdrt.stac': 0.2724053162535023, 'eval_loss@eng.sdrt.stac': 1.5067821741104126, 'eval_runtime': 13.7956, 'eval_samples_per_second': 82.997, 'eval_steps_per_second': 2.61, 'epoch': 10.0}
{'train@eng.sdrt.stac_loss': 1.517626404762268, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5224425887265136, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2826141518430827, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.356846896080239, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.30381352546293505, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5176262855529785, 'train@eng.sdrt.stac_runtime': 112.3038, 'train@eng.sdrt.stac_samples_per_second': 85.304, 'train@eng.sdrt.stac_steps_per_second': 2.671, 'epoch': 11.0}
{'loss': 1.5687, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.5029728412628174, 'eval_accuracy@eng.sdrt.stac': 0.5266375545851528, 'eval_f1@eng.sdrt.stac': 0.2566506400672286, 'eval_precision@eng.sdrt.stac': 0.27729904950229123, 'eval_recall@eng.sdrt.stac': 0.2784660841779502, 'eval_loss@eng.sdrt.stac': 1.502972960472107, 'eval_runtime': 13.7817, 'eval_samples_per_second': 83.081, 'eval_steps_per_second': 2.612, 'epoch': 11.0}
{'train@eng.sdrt.stac_loss': 1.5163288116455078, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5235908141962422, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.28626277482006146, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.36066723024595304, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.30594877745088067, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5163286924362183, 'train@eng.sdrt.stac_runtime': 112.1155, 'train@eng.sdrt.stac_samples_per_second': 85.448, 'train@eng.sdrt.stac_steps_per_second': 2.676, 'epoch': 12.0}
{'loss': 1.5656, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.5007083415985107, 'eval_accuracy@eng.sdrt.stac': 0.525764192139738, 'eval_f1@eng.sdrt.stac': 0.25571703164668314, 'eval_precision@eng.sdrt.stac': 0.24666072855616833, 'eval_recall@eng.sdrt.stac': 0.2792862066262922, 'eval_loss@eng.sdrt.stac': 1.5007083415985107, 'eval_runtime': 13.7558, 'eval_samples_per_second': 83.238, 'eval_steps_per_second': 2.617, 'epoch': 12.0}
{'train_runtime': 4367.2275, 'train_samples_per_second': 26.323, 'train_steps_per_second': 0.824, 'train_loss': 1.7725360361735025, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.7725
  train_runtime            = 1:12:47.22
  train_samples_per_second =     26.323
  train_steps_per_second   =      0.824
{'train@zho.rst.sctb_loss': 4.038482666015625, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.004555808656036446, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.00037071362372567186, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.00018625442354255913, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 4.038482666015625, 'train@zho.rst.sctb_runtime': 5.5159, 'train@zho.rst.sctb_samples_per_second': 79.588, 'train@zho.rst.sctb_steps_per_second': 2.538, 'epoch': 1.0}
{'loss': 4.4896, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 4.033438682556152, 'eval_accuracy@zho.rst.sctb': 0.0, 'eval_f1@zho.rst.sctb': 0.0, 'eval_precision@zho.rst.sctb': 0.0, 'eval_recall@zho.rst.sctb': 0.0, 'eval_loss@zho.rst.sctb': 4.033437728881836, 'eval_runtime': 1.5004, 'eval_samples_per_second': 62.651, 'eval_steps_per_second': 2.0, 'epoch': 1.0}
{'train@zho.rst.sctb_loss': 3.570488452911377, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.018223234624145785, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.004730606945796819, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.010891719745222931, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.02206477732793522, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.5704891681671143, 'train@zho.rst.sctb_runtime': 5.5156, 'train@zho.rst.sctb_samples_per_second': 79.592, 'train@zho.rst.sctb_steps_per_second': 2.538, 'epoch': 2.0}
{'loss': 3.8165, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.57423734664917, 'eval_accuracy@zho.rst.sctb': 0.0, 'eval_f1@zho.rst.sctb': 0.0, 'eval_precision@zho.rst.sctb': 0.0, 'eval_recall@zho.rst.sctb': 0.0, 'eval_loss@zho.rst.sctb': 3.5742383003234863, 'eval_runtime': 1.4888, 'eval_samples_per_second': 63.137, 'eval_steps_per_second': 2.015, 'epoch': 2.0}
{'train@zho.rst.sctb_loss': 3.2539212703704834, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.1776765375854214, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.011928429423459242, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.007352941176470588, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.031578947368421054, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.2539215087890625, 'train@zho.rst.sctb_runtime': 5.4923, 'train@zho.rst.sctb_samples_per_second': 79.93, 'train@zho.rst.sctb_steps_per_second': 2.549, 'epoch': 3.0}
{'loss': 3.4604, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 3.2726738452911377, 'eval_accuracy@zho.rst.sctb': 0.14893617021276595, 'eval_f1@zho.rst.sctb': 0.014432989690721649, 'eval_precision@zho.rst.sctb': 0.008749999999999999, 'eval_recall@zho.rst.sctb': 0.041176470588235294, 'eval_loss@zho.rst.sctb': 3.272674322128296, 'eval_runtime': 1.4926, 'eval_samples_per_second': 62.977, 'eval_steps_per_second': 2.01, 'epoch': 3.0}
{'train@zho.rst.sctb_loss': 3.0167977809906006, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.2164009111617312, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.013710492134507144, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.008342114506498067, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.0167977809906006, 'train@zho.rst.sctb_runtime': 5.5131, 'train@zho.rst.sctb_samples_per_second': 79.628, 'train@zho.rst.sctb_steps_per_second': 2.539, 'epoch': 4.0}
{'loss': 3.1678, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 3.0525195598602295, 'eval_accuracy@zho.rst.sctb': 0.18085106382978725, 'eval_f1@zho.rst.sctb': 0.015596330275229359, 'eval_precision@zho.rst.sctb': 0.009239130434782607, 'eval_recall@zho.rst.sctb': 0.05, 'eval_loss@zho.rst.sctb': 3.0525197982788086, 'eval_runtime': 1.4797, 'eval_samples_per_second': 63.526, 'eval_steps_per_second': 2.027, 'epoch': 4.0}
{'train@zho.rst.sctb_loss': 2.8344998359680176, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.31662870159453305, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.03166592643554038, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.0303003756447634, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04906272530641673, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.8345000743865967, 'train@zho.rst.sctb_runtime': 5.5526, 'train@zho.rst.sctb_samples_per_second': 79.063, 'train@zho.rst.sctb_steps_per_second': 2.521, 'epoch': 5.0}
{'loss': 2.9802, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.88614821434021, 'eval_accuracy@zho.rst.sctb': 0.2553191489361702, 'eval_f1@zho.rst.sctb': 0.03529363314389259, 'eval_precision@zho.rst.sctb': 0.028167641325536064, 'eval_recall@zho.rst.sctb': 0.053986068111455114, 'eval_loss@zho.rst.sctb': 2.8861494064331055, 'eval_runtime': 1.4722, 'eval_samples_per_second': 63.85, 'eval_steps_per_second': 2.038, 'epoch': 5.0}
{'train@zho.rst.sctb_loss': 2.6928865909576416, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3690205011389522, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.03408748114630467, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.030146520146520146, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04720204092951029, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.6928865909576416, 'train@zho.rst.sctb_runtime': 5.5114, 'train@zho.rst.sctb_samples_per_second': 79.653, 'train@zho.rst.sctb_steps_per_second': 2.54, 'epoch': 6.0}
{'loss': 2.8174, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.759443998336792, 'eval_accuracy@zho.rst.sctb': 0.3191489361702128, 'eval_f1@zho.rst.sctb': 0.03339875111507582, 'eval_precision@zho.rst.sctb': 0.0302937576499388, 'eval_recall@zho.rst.sctb': 0.05224458204334365, 'eval_loss@zho.rst.sctb': 2.759443998336792, 'eval_runtime': 1.471, 'eval_samples_per_second': 63.902, 'eval_steps_per_second': 2.039, 'epoch': 6.0}
{'train@zho.rst.sctb_loss': 2.588186740875244, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3621867881548975, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.03160890204685825, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03503273652527384, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04485608119350008, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.588186740875244, 'train@zho.rst.sctb_runtime': 5.5395, 'train@zho.rst.sctb_samples_per_second': 79.249, 'train@zho.rst.sctb_steps_per_second': 2.527, 'epoch': 7.0}
{'loss': 2.6845, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.6674587726593018, 'eval_accuracy@zho.rst.sctb': 0.30851063829787234, 'eval_f1@zho.rst.sctb': 0.024818142918271287, 'eval_precision@zho.rst.sctb': 0.016772700983227296, 'eval_recall@zho.rst.sctb': 0.047697368421052634, 'eval_loss@zho.rst.sctb': 2.6674587726593018, 'eval_runtime': 1.4856, 'eval_samples_per_second': 63.272, 'eval_steps_per_second': 2.019, 'epoch': 7.0}
{'train@zho.rst.sctb_loss': 2.5162506103515625, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3553530751708428, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.029880631149800902, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.037192625784858796, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.043500083190061566, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.5162510871887207, 'train@zho.rst.sctb_runtime': 5.5303, 'train@zho.rst.sctb_samples_per_second': 79.381, 'train@zho.rst.sctb_steps_per_second': 2.532, 'epoch': 8.0}
{'loss': 2.5967, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.6072468757629395, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 2.6072468757629395, 'eval_runtime': 1.5009, 'eval_samples_per_second': 62.631, 'eval_steps_per_second': 1.999, 'epoch': 8.0}
{'train@zho.rst.sctb_loss': 2.467191219329834, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3530751708428246, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.029349816849816853, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03752879970271275, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.043095224890466424, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.467191457748413, 'train@zho.rst.sctb_runtime': 6.1869, 'train@zho.rst.sctb_samples_per_second': 70.956, 'train@zho.rst.sctb_steps_per_second': 2.263, 'epoch': 9.0}
{'loss': 2.523, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.5672314167022705, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 2.5672314167022705, 'eval_runtime': 1.4759, 'eval_samples_per_second': 63.692, 'eval_steps_per_second': 2.033, 'epoch': 9.0}
{'train@zho.rst.sctb_loss': 2.4359915256500244, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.357630979498861, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.030397022332506202, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.036912078669858316, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.0439049414896567, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.4359912872314453, 'train@zho.rst.sctb_runtime': 5.5294, 'train@zho.rst.sctb_samples_per_second': 79.394, 'train@zho.rst.sctb_steps_per_second': 2.532, 'epoch': 10.0}
{'loss': 2.48, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.5426669120788574, 'eval_accuracy@zho.rst.sctb': 0.3191489361702128, 'eval_f1@zho.rst.sctb': 0.025466893039049233, 'eval_precision@zho.rst.sctb': 0.017162471395881007, 'eval_recall@zho.rst.sctb': 0.049342105263157895, 'eval_loss@zho.rst.sctb': 2.5426666736602783, 'eval_runtime': 1.4847, 'eval_samples_per_second': 63.313, 'eval_steps_per_second': 2.021, 'epoch': 10.0}
{'train@zho.rst.sctb_loss': 2.419938564300537, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.357630979498861, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.030453279260040215, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.037732948316889926, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.0439049414896567, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.419938802719116, 'train@zho.rst.sctb_runtime': 5.5186, 'train@zho.rst.sctb_samples_per_second': 79.549, 'train@zho.rst.sctb_steps_per_second': 2.537, 'epoch': 11.0}
{'loss': 2.4554, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.5294580459594727, 'eval_accuracy@zho.rst.sctb': 0.3191489361702128, 'eval_f1@zho.rst.sctb': 0.025466893039049233, 'eval_precision@zho.rst.sctb': 0.017162471395881007, 'eval_recall@zho.rst.sctb': 0.049342105263157895, 'eval_loss@zho.rst.sctb': 2.5294580459594727, 'eval_runtime': 1.4896, 'eval_samples_per_second': 63.103, 'eval_steps_per_second': 2.014, 'epoch': 11.0}
{'train@zho.rst.sctb_loss': 2.414473295211792, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.357630979498861, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.030453279260040215, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.037732948316889926, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.0439049414896567, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.414473295211792, 'train@zho.rst.sctb_runtime': 5.5225, 'train@zho.rst.sctb_samples_per_second': 79.493, 'train@zho.rst.sctb_steps_per_second': 2.535, 'epoch': 12.0}
{'loss': 2.4575, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.525406837463379, 'eval_accuracy@zho.rst.sctb': 0.3191489361702128, 'eval_f1@zho.rst.sctb': 0.025466893039049233, 'eval_precision@zho.rst.sctb': 0.017162471395881007, 'eval_recall@zho.rst.sctb': 0.049342105263157895, 'eval_loss@zho.rst.sctb': 2.5254065990448, 'eval_runtime': 1.4849, 'eval_samples_per_second': 63.302, 'eval_steps_per_second': 2.02, 'epoch': 12.0}
{'train_runtime': 215.8789, 'train_samples_per_second': 24.403, 'train_steps_per_second': 0.778, 'train_loss': 2.994091510772705, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.7725
  train_runtime            = 1:12:47.22
  train_samples_per_second =     26.323
  train_steps_per_second   =      0.824
-------------------------------------------------------------------
Lang1:  fas.rst.prstc    Lang2:  zho.rst.sctb
Saving run to:  runs/full_shot/FullShot=v4_fas.rst.prstc_zho.rst.sctb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 4100 examples
read 499 examples
read 592 examples
read 439 examples
read 94 examples
read 159 examples
Total prediction labels:  34
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (ar): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (zh): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=34, bias=True)
    )
  )
)
{'train@fas.rst.prstc_loss': 2.4179821014404297, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.23780487804878048, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.02260214430599826, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.013988522238163558, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.058823529411764705, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.417982339859009, 'train@fas.rst.prstc_runtime': 48.0849, 'train@fas.rst.prstc_samples_per_second': 85.266, 'train@fas.rst.prstc_steps_per_second': 2.683, 'epoch': 1.0}
{'loss': 2.7824, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.3403241634368896, 'eval_accuracy@fas.rst.prstc': 0.24448897795591182, 'eval_f1@fas.rst.prstc': 0.026194310252281264, 'eval_precision@fas.rst.prstc': 0.016299265197060788, 'eval_recall@fas.rst.prstc': 0.06666666666666667, 'eval_loss@fas.rst.prstc': 2.3403239250183105, 'eval_runtime': 6.1544, 'eval_samples_per_second': 81.081, 'eval_steps_per_second': 2.6, 'epoch': 1.0}
{'train@fas.rst.prstc_loss': 2.3596227169036865, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.28536585365853656, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04483107025801522, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.033065284760743406, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07386326925751206, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3596224784851074, 'train@fas.rst.prstc_runtime': 48.1493, 'train@fas.rst.prstc_samples_per_second': 85.152, 'train@fas.rst.prstc_steps_per_second': 2.679, 'epoch': 2.0}
{'loss': 2.4092, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.273366928100586, 'eval_accuracy@fas.rst.prstc': 0.30060120240480964, 'eval_f1@fas.rst.prstc': 0.052227906773361324, 'eval_precision@fas.rst.prstc': 0.04062991490906158, 'eval_recall@fas.rst.prstc': 0.08339748158707531, 'eval_loss@fas.rst.prstc': 2.273366928100586, 'eval_runtime': 10.0791, 'eval_samples_per_second': 49.509, 'eval_steps_per_second': 1.587, 'epoch': 2.0}
{'train@fas.rst.prstc_loss': 2.3400211334228516, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.24634146341463414, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.029753704907663048, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03183568677792041, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.061496314837991936, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3400211334228516, 'train@fas.rst.prstc_runtime': 48.1489, 'train@fas.rst.prstc_samples_per_second': 85.153, 'train@fas.rst.prstc_steps_per_second': 2.679, 'epoch': 3.0}
{'loss': 2.3636, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.2554500102996826, 'eval_accuracy@fas.rst.prstc': 0.2565130260521042, 'eval_f1@fas.rst.prstc': 0.03513513513513513, 'eval_precision@fas.rst.prstc': 0.03756908779652727, 'eval_recall@fas.rst.prstc': 0.07024471370871942, 'eval_loss@fas.rst.prstc': 2.2554502487182617, 'eval_runtime': 6.1643, 'eval_samples_per_second': 80.95, 'eval_steps_per_second': 2.596, 'epoch': 3.0}
{'train@fas.rst.prstc_loss': 2.322172164916992, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.23878048780487804, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.02370466139870286, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03211958851740538, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.05913845298075585, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.322172164916992, 'train@fas.rst.prstc_runtime': 48.0943, 'train@fas.rst.prstc_samples_per_second': 85.249, 'train@fas.rst.prstc_steps_per_second': 2.682, 'epoch': 4.0}
{'loss': 2.3472, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.243756055831909, 'eval_accuracy@fas.rst.prstc': 0.24849699398797595, 'eval_f1@fas.rst.prstc': 0.02862914862914863, 'eval_precision@fas.rst.prstc': 0.04313090418353576, 'eval_recall@fas.rst.prstc': 0.06782608695652173, 'eval_loss@fas.rst.prstc': 2.2437562942504883, 'eval_runtime': 6.1804, 'eval_samples_per_second': 80.739, 'eval_steps_per_second': 2.589, 'epoch': 4.0}
{'train@fas.rst.prstc_loss': 2.3022384643554688, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.28609756097560973, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.0444813008757618, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.033220906319933144, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07376827872447397, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.302238702774048, 'train@fas.rst.prstc_runtime': 48.2234, 'train@fas.rst.prstc_samples_per_second': 85.021, 'train@fas.rst.prstc_steps_per_second': 2.675, 'epoch': 5.0}
{'loss': 2.3307, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.22178053855896, 'eval_accuracy@fas.rst.prstc': 0.30861723446893785, 'eval_f1@fas.rst.prstc': 0.0537675144310774, 'eval_precision@fas.rst.prstc': 0.042851612903225804, 'eval_recall@fas.rst.prstc': 0.0855832739368021, 'eval_loss@fas.rst.prstc': 2.221780776977539, 'eval_runtime': 6.1471, 'eval_samples_per_second': 81.176, 'eval_steps_per_second': 2.603, 'epoch': 5.0}
{'train@fas.rst.prstc_loss': 2.2711081504821777, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2868292682926829, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04455822740980779, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03308748266538849, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07397687280040222, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.2711081504821777, 'train@fas.rst.prstc_runtime': 48.1394, 'train@fas.rst.prstc_samples_per_second': 85.169, 'train@fas.rst.prstc_steps_per_second': 2.68, 'epoch': 6.0}
{'loss': 2.3047, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.201759099960327, 'eval_accuracy@fas.rst.prstc': 0.30060120240480964, 'eval_f1@fas.rst.prstc': 0.05215198274431724, 'eval_precision@fas.rst.prstc': 0.04093926319283107, 'eval_recall@fas.rst.prstc': 0.08336421952957947, 'eval_loss@fas.rst.prstc': 2.201758623123169, 'eval_runtime': 6.1451, 'eval_samples_per_second': 81.203, 'eval_steps_per_second': 2.604, 'epoch': 6.0}
{'train@fas.rst.prstc_loss': 2.234896183013916, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2941463414634146, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04755215563228779, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.034373679598822225, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07716675759228951, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.234895944595337, 'train@fas.rst.prstc_runtime': 48.2015, 'train@fas.rst.prstc_samples_per_second': 85.06, 'train@fas.rst.prstc_steps_per_second': 2.676, 'epoch': 7.0}
{'loss': 2.2752, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.1537418365478516, 'eval_accuracy@fas.rst.prstc': 0.312625250501002, 'eval_f1@fas.rst.prstc': 0.05599547174308099, 'eval_precision@fas.rst.prstc': 0.041320939334637964, 'eval_recall@fas.rst.prstc': 0.0873414112615823, 'eval_loss@fas.rst.prstc': 2.1537418365478516, 'eval_runtime': 6.1606, 'eval_samples_per_second': 80.998, 'eval_steps_per_second': 2.597, 'epoch': 7.0}
{'train@fas.rst.prstc_loss': 2.1998913288116455, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.30634146341463414, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.05032973577456936, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.036873535533775294, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08088251339815795, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1998913288116455, 'train@fas.rst.prstc_runtime': 48.1089, 'train@fas.rst.prstc_samples_per_second': 85.223, 'train@fas.rst.prstc_steps_per_second': 2.681, 'epoch': 8.0}
{'loss': 2.2413, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.113607883453369, 'eval_accuracy@fas.rst.prstc': 0.3226452905811623, 'eval_f1@fas.rst.prstc': 0.058457863958140975, 'eval_precision@fas.rst.prstc': 0.043264671172620964, 'eval_recall@fas.rst.prstc': 0.09040627227369921, 'eval_loss@fas.rst.prstc': 2.1136083602905273, 'eval_runtime': 6.1426, 'eval_samples_per_second': 81.236, 'eval_steps_per_second': 2.605, 'epoch': 8.0}
{'train@fas.rst.prstc_loss': 2.1805901527404785, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.31902439024390244, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.05306198104358638, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03957852237116845, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08460853845084133, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1805903911590576, 'train@fas.rst.prstc_runtime': 48.0998, 'train@fas.rst.prstc_samples_per_second': 85.239, 'train@fas.rst.prstc_steps_per_second': 2.682, 'epoch': 9.0}
{'loss': 2.2109, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.0872223377227783, 'eval_accuracy@fas.rst.prstc': 0.3366733466933868, 'eval_f1@fas.rst.prstc': 0.06175673521441096, 'eval_precision@fas.rst.prstc': 0.04648928062560058, 'eval_recall@fas.rst.prstc': 0.09453076740318365, 'eval_loss@fas.rst.prstc': 2.087222099304199, 'eval_runtime': 6.1363, 'eval_samples_per_second': 81.319, 'eval_steps_per_second': 2.607, 'epoch': 9.0}
{'train@fas.rst.prstc_loss': 2.1711137294769287, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.32658536585365855, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.05452524214914143, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.04099670428443024, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08680080870318667, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1711134910583496, 'train@fas.rst.prstc_runtime': 48.1603, 'train@fas.rst.prstc_samples_per_second': 85.132, 'train@fas.rst.prstc_steps_per_second': 2.679, 'epoch': 10.0}
{'loss': 2.2003, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.0767767429351807, 'eval_accuracy@fas.rst.prstc': 0.3406813627254509, 'eval_f1@fas.rst.prstc': 0.062663906142167, 'eval_precision@fas.rst.prstc': 0.04739576365663322, 'eval_recall@fas.rst.prstc': 0.09569018769303872, 'eval_loss@fas.rst.prstc': 2.076777219772339, 'eval_runtime': 6.1502, 'eval_samples_per_second': 81.136, 'eval_steps_per_second': 2.602, 'epoch': 10.0}
{'train@fas.rst.prstc_loss': 2.1620092391967773, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.33121951219512197, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.05555809879760263, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.04219741744438646, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08819550078623921, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1620092391967773, 'train@fas.rst.prstc_runtime': 48.0761, 'train@fas.rst.prstc_samples_per_second': 85.281, 'train@fas.rst.prstc_steps_per_second': 2.683, 'epoch': 11.0}
{'loss': 2.1866, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.065645456314087, 'eval_accuracy@fas.rst.prstc': 0.3466933867735471, 'eval_f1@fas.rst.prstc': 0.06415598654974748, 'eval_precision@fas.rst.prstc': 0.0491064491064491, 'eval_recall@fas.rst.prstc': 0.09746258018531719, 'eval_loss@fas.rst.prstc': 2.065645456314087, 'eval_runtime': 6.1564, 'eval_samples_per_second': 81.053, 'eval_steps_per_second': 2.599, 'epoch': 11.0}
{'train@fas.rst.prstc_loss': 2.1616997718811035, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.33195121951219514, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.055579753631102076, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.04207862511383104, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08836729672774729, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1617000102996826, 'train@fas.rst.prstc_runtime': 48.0811, 'train@fas.rst.prstc_samples_per_second': 85.273, 'train@fas.rst.prstc_steps_per_second': 2.683, 'epoch': 12.0}
{'loss': 2.1819, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.0657901763916016, 'eval_accuracy@fas.rst.prstc': 0.342685370741483, 'eval_f1@fas.rst.prstc': 0.0632656141515984, 'eval_precision@fas.rst.prstc': 0.04818924176619617, 'eval_recall@fas.rst.prstc': 0.09630315989546209, 'eval_loss@fas.rst.prstc': 2.0657901763916016, 'eval_runtime': 6.1769, 'eval_samples_per_second': 80.785, 'eval_steps_per_second': 2.59, 'epoch': 12.0}
{'train_runtime': 1876.6519, 'train_samples_per_second': 26.217, 'train_steps_per_second': 0.825, 'train_loss': 2.3194924288017806, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.3195
  train_runtime            = 0:31:16.65
  train_samples_per_second =     26.217
  train_steps_per_second   =      0.825
{'train@zho.rst.sctb_loss': 3.238374710083008, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3553530751708428, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.03372696457318842, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.02791024664596637, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.046187122178470416, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.2383744716644287, 'train@zho.rst.sctb_runtime': 5.4844, 'train@zho.rst.sctb_samples_per_second': 80.045, 'train@zho.rst.sctb_steps_per_second': 2.553, 'epoch': 1.0}
{'loss': 3.4211, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.2487378120422363, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.04213298222514812, 'eval_precision@zho.rst.sctb': 0.032172131147540986, 'eval_recall@zho.rst.sctb': 0.06102941176470589, 'eval_loss@zho.rst.sctb': 3.248737335205078, 'eval_runtime': 1.4232, 'eval_samples_per_second': 66.049, 'eval_steps_per_second': 2.108, 'epoch': 1.0}
{'train@zho.rst.sctb_loss': 3.005239248275757, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3826879271070615, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.03556110230502727, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03317948717948718, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04878265209916255, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.005239248275757, 'train@zho.rst.sctb_runtime': 5.4547, 'train@zho.rst.sctb_samples_per_second': 80.48, 'train@zho.rst.sctb_steps_per_second': 2.567, 'epoch': 2.0}
{'loss': 3.1374, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.0325424671173096, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.040117994100294985, 'eval_precision@zho.rst.sctb': 0.034387964212525615, 'eval_recall@zho.rst.sctb': 0.05843653250773994, 'eval_loss@zho.rst.sctb': 3.0325424671173096, 'eval_runtime': 1.4347, 'eval_samples_per_second': 65.521, 'eval_steps_per_second': 2.091, 'epoch': 2.0}
{'train@zho.rst.sctb_loss': 2.799551486968994, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3895216400911162, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.03587032784793979, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03676689610755545, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04914868837002939, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.7995517253875732, 'train@zho.rst.sctb_runtime': 5.4534, 'train@zho.rst.sctb_samples_per_second': 80.501, 'train@zho.rst.sctb_steps_per_second': 2.567, 'epoch': 3.0}
{'loss': 2.9442, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.844148635864258, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.03850133809099019, 'eval_precision@zho.rst.sctb': 0.03748470012239902, 'eval_recall@zho.rst.sctb': 0.05698529411764706, 'eval_loss@zho.rst.sctb': 2.844148635864258, 'eval_runtime': 1.408, 'eval_samples_per_second': 66.76, 'eval_steps_per_second': 2.131, 'epoch': 3.0}
{'train@zho.rst.sctb_loss': 2.6249845027923584, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.38724373576309795, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.035903522904326975, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03610308395639344, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.049026676279740444, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.6249842643737793, 'train@zho.rst.sctb_runtime': 5.4531, 'train@zho.rst.sctb_samples_per_second': 80.504, 'train@zho.rst.sctb_steps_per_second': 2.567, 'epoch': 4.0}
{'loss': 2.7575, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.689310073852539, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.038236617183985605, 'eval_precision@zho.rst.sctb': 0.035500515995872034, 'eval_recall@zho.rst.sctb': 0.05698529411764706, 'eval_loss@zho.rst.sctb': 2.689309597015381, 'eval_runtime': 1.4215, 'eval_samples_per_second': 66.127, 'eval_steps_per_second': 2.11, 'epoch': 4.0}
{'train@zho.rst.sctb_loss': 2.495256185531616, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.39635535307517084, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.03708221711953055, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03843537414965987, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.05022184016416172, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.4952564239501953, 'train@zho.rst.sctb_runtime': 5.4841, 'train@zho.rst.sctb_samples_per_second': 80.049, 'train@zho.rst.sctb_steps_per_second': 2.553, 'epoch': 5.0}
{'loss': 2.6258, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.579943895339966, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.03850133809099019, 'eval_precision@zho.rst.sctb': 0.03748470012239902, 'eval_recall@zho.rst.sctb': 0.05698529411764706, 'eval_loss@zho.rst.sctb': 2.579944372177124, 'eval_runtime': 1.4259, 'eval_samples_per_second': 65.925, 'eval_steps_per_second': 2.104, 'epoch': 5.0}
{'train@zho.rst.sctb_loss': 2.4033925533294678, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.4009111617312073, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.038150593187906624, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.040005232862375714, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.05103155676335201, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.4033925533294678, 'train@zho.rst.sctb_runtime': 5.4476, 'train@zho.rst.sctb_samples_per_second': 80.586, 'train@zho.rst.sctb_steps_per_second': 2.57, 'epoch': 6.0}
{'loss': 2.5038, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.5085105895996094, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.03850133809099019, 'eval_precision@zho.rst.sctb': 0.03748470012239902, 'eval_recall@zho.rst.sctb': 0.05698529411764706, 'eval_loss@zho.rst.sctb': 2.508510112762451, 'eval_runtime': 1.4201, 'eval_samples_per_second': 66.191, 'eval_steps_per_second': 2.112, 'epoch': 6.0}
{'train@zho.rst.sctb_loss': 2.3418073654174805, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.40774487471526194, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.03924782441892708, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03849241748438893, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.05238755476679053, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.3418076038360596, 'train@zho.rst.sctb_runtime': 5.4516, 'train@zho.rst.sctb_samples_per_second': 80.528, 'train@zho.rst.sctb_steps_per_second': 2.568, 'epoch': 7.0}
{'loss': 2.4346, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.4655256271362305, 'eval_accuracy@zho.rst.sctb': 0.35106382978723405, 'eval_f1@zho.rst.sctb': 0.04191033138401559, 'eval_precision@zho.rst.sctb': 0.03922305764411028, 'eval_recall@zho.rst.sctb': 0.0600812693498452, 'eval_loss@zho.rst.sctb': 2.4655256271362305, 'eval_runtime': 1.4173, 'eval_samples_per_second': 66.324, 'eval_steps_per_second': 2.117, 'epoch': 7.0}
{'train@zho.rst.sctb_loss': 2.310271978378296, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.4054669703872437, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.039039714582272685, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.04057043118595575, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.05184127336254229, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.310271739959717, 'train@zho.rst.sctb_runtime': 5.4615, 'train@zho.rst.sctb_samples_per_second': 80.381, 'train@zho.rst.sctb_steps_per_second': 2.563, 'epoch': 8.0}
{'loss': 2.3776, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.444678544998169, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.03850133809099019, 'eval_precision@zho.rst.sctb': 0.03748470012239902, 'eval_recall@zho.rst.sctb': 0.05698529411764706, 'eval_loss@zho.rst.sctb': 2.444679021835327, 'eval_runtime': 1.4396, 'eval_samples_per_second': 65.295, 'eval_steps_per_second': 2.084, 'epoch': 8.0}
{'train@zho.rst.sctb_loss': 2.2871389389038086, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.4123006833712984, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.04006410256410256, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.040038931400187945, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.05305584826132771, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.2871389389038086, 'train@zho.rst.sctb_runtime': 5.4703, 'train@zho.rst.sctb_samples_per_second': 80.251, 'train@zho.rst.sctb_steps_per_second': 2.559, 'epoch': 9.0}
{'loss': 2.343, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.4307076930999756, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.038236617183985605, 'eval_precision@zho.rst.sctb': 0.035500515995872034, 'eval_recall@zho.rst.sctb': 0.05698529411764706, 'eval_loss@zho.rst.sctb': 2.4307069778442383, 'eval_runtime': 1.4114, 'eval_samples_per_second': 66.601, 'eval_steps_per_second': 2.126, 'epoch': 9.0}
{'train@zho.rst.sctb_loss': 2.2708559036254883, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.4191343963553531, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.041508500952630284, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03907921299225647, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.05469469247407243, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.2708561420440674, 'train@zho.rst.sctb_runtime': 5.4365, 'train@zho.rst.sctb_samples_per_second': 80.751, 'train@zho.rst.sctb_steps_per_second': 2.575, 'epoch': 10.0}
{'loss': 2.32, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.4212708473205566, 'eval_accuracy@zho.rst.sctb': 0.35106382978723405, 'eval_f1@zho.rst.sctb': 0.04191033138401559, 'eval_precision@zho.rst.sctb': 0.03922305764411028, 'eval_recall@zho.rst.sctb': 0.0600812693498452, 'eval_loss@zho.rst.sctb': 2.4212706089019775, 'eval_runtime': 1.4251, 'eval_samples_per_second': 65.96, 'eval_steps_per_second': 2.105, 'epoch': 10.0}
{'train@zho.rst.sctb_loss': 2.263850212097168, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.4191343963553531, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.041508500952630284, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03907921299225647, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.05469469247407243, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.263850212097168, 'train@zho.rst.sctb_runtime': 5.4594, 'train@zho.rst.sctb_samples_per_second': 80.411, 'train@zho.rst.sctb_steps_per_second': 2.564, 'epoch': 11.0}
{'loss': 2.31, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.4173901081085205, 'eval_accuracy@zho.rst.sctb': 0.35106382978723405, 'eval_f1@zho.rst.sctb': 0.04191033138401559, 'eval_precision@zho.rst.sctb': 0.03922305764411028, 'eval_recall@zho.rst.sctb': 0.0600812693498452, 'eval_loss@zho.rst.sctb': 2.4173896312713623, 'eval_runtime': 1.4172, 'eval_samples_per_second': 66.328, 'eval_steps_per_second': 2.117, 'epoch': 11.0}
{'train@zho.rst.sctb_loss': 2.261242389678955, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.4191343963553531, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.041508500952630284, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03907921299225647, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.05469469247407243, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.261242389678955, 'train@zho.rst.sctb_runtime': 5.4588, 'train@zho.rst.sctb_samples_per_second': 80.42, 'train@zho.rst.sctb_steps_per_second': 2.565, 'epoch': 12.0}
{'loss': 2.3071, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.4159955978393555, 'eval_accuracy@zho.rst.sctb': 0.35106382978723405, 'eval_f1@zho.rst.sctb': 0.04191033138401559, 'eval_precision@zho.rst.sctb': 0.03922305764411028, 'eval_recall@zho.rst.sctb': 0.0600812693498452, 'eval_loss@zho.rst.sctb': 2.4159955978393555, 'eval_runtime': 1.4226, 'eval_samples_per_second': 66.076, 'eval_steps_per_second': 2.109, 'epoch': 12.0}
{'train_runtime': 213.7324, 'train_samples_per_second': 24.648, 'train_steps_per_second': 0.786, 'train_loss': 2.6235118366423107, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.3195
  train_runtime            = 0:31:16.65
  train_samples_per_second =     26.217
  train_steps_per_second   =      0.825
-------------------------------------------------------------------
Lang1:  fra.sdrt.annodis    Lang2:  zho.rst.sctb
Saving run to:  runs/full_shot/FullShot=v4_fra.sdrt.annodis_zho.rst.sctb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2185 examples
read 528 examples
read 625 examples
read 439 examples
read 94 examples
read 159 examples
Total prediction labels:  39
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (fr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (zh): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=39, bias=True)
    )
  )
)
{'train@fra.sdrt.annodis_loss': 2.6211366653442383, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.21418764302059495, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.02759836030602615, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.04138732290774691, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.05878401375486583, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.6211366653442383, 'train@fra.sdrt.annodis_runtime': 26.2551, 'train@fra.sdrt.annodis_samples_per_second': 83.222, 'train@fra.sdrt.annodis_steps_per_second': 2.628, 'epoch': 1.0}
{'loss': 3.0885, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.6339659690856934, 'eval_accuracy@fra.sdrt.annodis': 0.20643939393939395, 'eval_f1@fra.sdrt.annodis': 0.024279097919890517, 'eval_precision@fra.sdrt.annodis': 0.050923352479772706, 'eval_recall@fra.sdrt.annodis': 0.05705653732652695, 'eval_loss@fra.sdrt.annodis': 2.6339662075042725, 'eval_runtime': 6.6245, 'eval_samples_per_second': 79.704, 'eval_steps_per_second': 2.566, 'epoch': 1.0}
{'train@fra.sdrt.annodis_loss': 2.388216733932495, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.26910755148741416, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.05710237580994663, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.04606590473067457, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.07951202009704329, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.388216733932495, 'train@fra.sdrt.annodis_runtime': 26.2893, 'train@fra.sdrt.annodis_samples_per_second': 83.114, 'train@fra.sdrt.annodis_steps_per_second': 2.625, 'epoch': 2.0}
{'loss': 2.507, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.3985707759857178, 'eval_accuracy@fra.sdrt.annodis': 0.2518939393939394, 'eval_f1@fra.sdrt.annodis': 0.05322380130081191, 'eval_precision@fra.sdrt.annodis': 0.043785444470999905, 'eval_recall@fra.sdrt.annodis': 0.07319590272169019, 'eval_loss@fra.sdrt.annodis': 2.3985702991485596, 'eval_runtime': 6.6372, 'eval_samples_per_second': 79.552, 'eval_steps_per_second': 2.561, 'epoch': 2.0}
{'train@fra.sdrt.annodis_loss': 2.3100409507751465, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.27276887871853545, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.05602681875571522, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.05246569679383035, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.07878426687853746, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.3100411891937256, 'train@fra.sdrt.annodis_runtime': 26.3077, 'train@fra.sdrt.annodis_samples_per_second': 83.056, 'train@fra.sdrt.annodis_steps_per_second': 2.623, 'epoch': 3.0}
{'loss': 2.3756, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.3266971111297607, 'eval_accuracy@fra.sdrt.annodis': 0.2537878787878788, 'eval_f1@fra.sdrt.annodis': 0.05157649368175684, 'eval_precision@fra.sdrt.annodis': 0.04818067362522381, 'eval_recall@fra.sdrt.annodis': 0.07292096527125949, 'eval_loss@fra.sdrt.annodis': 2.326697587966919, 'eval_runtime': 6.6465, 'eval_samples_per_second': 79.441, 'eval_steps_per_second': 2.558, 'epoch': 3.0}
{'train@fra.sdrt.annodis_loss': 2.2548611164093018, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.28924485125858124, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.061682216810168514, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.08038855617561211, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.08406427949385442, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.2548611164093018, 'train@fra.sdrt.annodis_runtime': 26.3206, 'train@fra.sdrt.annodis_samples_per_second': 83.015, 'train@fra.sdrt.annodis_steps_per_second': 2.622, 'epoch': 4.0}
{'loss': 2.3207, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.277388095855713, 'eval_accuracy@fra.sdrt.annodis': 0.2537878787878788, 'eval_f1@fra.sdrt.annodis': 0.051425380788942335, 'eval_precision@fra.sdrt.annodis': 0.046862958429665896, 'eval_recall@fra.sdrt.annodis': 0.0722900124077002, 'eval_loss@fra.sdrt.annodis': 2.277388572692871, 'eval_runtime': 6.6414, 'eval_samples_per_second': 79.502, 'eval_steps_per_second': 2.56, 'epoch': 4.0}
{'train@fra.sdrt.annodis_loss': 2.205101251602173, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.32402745995423343, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.07619554153435291, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.10749223206177104, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.10046476666909732, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.205101251602173, 'train@fra.sdrt.annodis_runtime': 26.3524, 'train@fra.sdrt.annodis_samples_per_second': 82.915, 'train@fra.sdrt.annodis_steps_per_second': 2.618, 'epoch': 5.0}
{'loss': 2.2681, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.2373156547546387, 'eval_accuracy@fra.sdrt.annodis': 0.2878787878787879, 'eval_f1@fra.sdrt.annodis': 0.06304243093722023, 'eval_precision@fra.sdrt.annodis': 0.1042148650080955, 'eval_recall@fra.sdrt.annodis': 0.08778781122661562, 'eval_loss@fra.sdrt.annodis': 2.2373156547546387, 'eval_runtime': 6.6443, 'eval_samples_per_second': 79.467, 'eval_steps_per_second': 2.559, 'epoch': 5.0}
{'train@fra.sdrt.annodis_loss': 2.1589839458465576, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.3565217391304348, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.1066118760141812, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.1213332102836439, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.12760166295598838, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.1589841842651367, 'train@fra.sdrt.annodis_runtime': 26.3543, 'train@fra.sdrt.annodis_samples_per_second': 82.909, 'train@fra.sdrt.annodis_steps_per_second': 2.618, 'epoch': 6.0}
{'loss': 2.2183, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.1997365951538086, 'eval_accuracy@fra.sdrt.annodis': 0.3087121212121212, 'eval_f1@fra.sdrt.annodis': 0.07853238746969504, 'eval_precision@fra.sdrt.annodis': 0.09300067544748397, 'eval_recall@fra.sdrt.annodis': 0.10040629271790064, 'eval_loss@fra.sdrt.annodis': 2.1997368335723877, 'eval_runtime': 6.6653, 'eval_samples_per_second': 79.216, 'eval_steps_per_second': 2.551, 'epoch': 6.0}
{'train@fra.sdrt.annodis_loss': 2.1155924797058105, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.37940503432494277, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.1177291369743848, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.1225806183563411, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.14297904677201792, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.1155924797058105, 'train@fra.sdrt.annodis_runtime': 26.3241, 'train@fra.sdrt.annodis_samples_per_second': 83.004, 'train@fra.sdrt.annodis_steps_per_second': 2.621, 'epoch': 7.0}
{'loss': 2.1763, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.165755033493042, 'eval_accuracy@fra.sdrt.annodis': 0.3181818181818182, 'eval_f1@fra.sdrt.annodis': 0.0840028582961676, 'eval_precision@fra.sdrt.annodis': 0.07919669393758427, 'eval_recall@fra.sdrt.annodis': 0.1064384941617039, 'eval_loss@fra.sdrt.annodis': 2.165755033493042, 'eval_runtime': 6.6527, 'eval_samples_per_second': 79.366, 'eval_steps_per_second': 2.555, 'epoch': 7.0}
{'train@fra.sdrt.annodis_loss': 2.0777838230133057, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.3903890160183066, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.12302888940241287, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12221312735302442, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.15173182672155974, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.0777838230133057, 'train@fra.sdrt.annodis_runtime': 26.3642, 'train@fra.sdrt.annodis_samples_per_second': 82.878, 'train@fra.sdrt.annodis_steps_per_second': 2.617, 'epoch': 8.0}
{'loss': 2.1478, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.1359684467315674, 'eval_accuracy@fra.sdrt.annodis': 0.32007575757575757, 'eval_f1@fra.sdrt.annodis': 0.08733543505731585, 'eval_precision@fra.sdrt.annodis': 0.076850341622844, 'eval_recall@fra.sdrt.annodis': 0.1114168094753683, 'eval_loss@fra.sdrt.annodis': 2.1359682083129883, 'eval_runtime': 6.6499, 'eval_samples_per_second': 79.399, 'eval_steps_per_second': 2.556, 'epoch': 8.0}
{'train@fra.sdrt.annodis_loss': 2.049093723297119, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.39359267734553777, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.12369750711884536, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12207922946239529, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.15501240555468573, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.049093723297119, 'train@fra.sdrt.annodis_runtime': 26.3808, 'train@fra.sdrt.annodis_samples_per_second': 82.825, 'train@fra.sdrt.annodis_steps_per_second': 2.616, 'epoch': 9.0}
{'loss': 2.113, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.113003730773926, 'eval_accuracy@fra.sdrt.annodis': 0.32575757575757575, 'eval_f1@fra.sdrt.annodis': 0.0885700237630833, 'eval_precision@fra.sdrt.annodis': 0.07635850562295389, 'eval_recall@fra.sdrt.annodis': 0.114070507703558, 'eval_loss@fra.sdrt.annodis': 2.1130032539367676, 'eval_runtime': 6.691, 'eval_samples_per_second': 78.911, 'eval_steps_per_second': 2.541, 'epoch': 9.0}
{'train@fra.sdrt.annodis_loss': 2.0281882286071777, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.40274599542334094, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.12844249661387463, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12441566957991526, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1594532166572076, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.0281882286071777, 'train@fra.sdrt.annodis_runtime': 26.4078, 'train@fra.sdrt.annodis_samples_per_second': 82.741, 'train@fra.sdrt.annodis_steps_per_second': 2.613, 'epoch': 10.0}
{'loss': 2.0811, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.0965700149536133, 'eval_accuracy@fra.sdrt.annodis': 0.3371212121212121, 'eval_f1@fra.sdrt.annodis': 0.09369487527201498, 'eval_precision@fra.sdrt.annodis': 0.09226444721897573, 'eval_recall@fra.sdrt.annodis': 0.11898906729860013, 'eval_loss@fra.sdrt.annodis': 2.0965700149536133, 'eval_runtime': 6.6825, 'eval_samples_per_second': 79.013, 'eval_steps_per_second': 2.544, 'epoch': 10.0}
{'train@fra.sdrt.annodis_loss': 2.0158793926239014, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4064073226544622, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.12951796835550772, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12481107603089489, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.16101786252428024, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.0158791542053223, 'train@fra.sdrt.annodis_runtime': 26.3195, 'train@fra.sdrt.annodis_samples_per_second': 83.018, 'train@fra.sdrt.annodis_steps_per_second': 2.622, 'epoch': 11.0}
{'loss': 2.0587, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.0860202312469482, 'eval_accuracy@fra.sdrt.annodis': 0.3352272727272727, 'eval_f1@fra.sdrt.annodis': 0.09380112612640386, 'eval_precision@fra.sdrt.annodis': 0.0860757320431978, 'eval_recall@fra.sdrt.annodis': 0.11958463262753548, 'eval_loss@fra.sdrt.annodis': 2.0860204696655273, 'eval_runtime': 6.6265, 'eval_samples_per_second': 79.68, 'eval_steps_per_second': 2.565, 'epoch': 11.0}
{'train@fra.sdrt.annodis_loss': 2.011526346206665, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4082379862700229, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.13051696176721808, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12472966367750146, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1618503196448471, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.011526346206665, 'train@fra.sdrt.annodis_runtime': 26.3468, 'train@fra.sdrt.annodis_samples_per_second': 82.932, 'train@fra.sdrt.annodis_steps_per_second': 2.619, 'epoch': 12.0}
{'loss': 2.0605, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.0829200744628906, 'eval_accuracy@fra.sdrt.annodis': 0.3390151515151515, 'eval_f1@fra.sdrt.annodis': 0.09478169374992239, 'eval_precision@fra.sdrt.annodis': 0.08522079787690884, 'eval_recall@fra.sdrt.annodis': 0.12058563362853647, 'eval_loss@fra.sdrt.annodis': 2.0829200744628906, 'eval_runtime': 6.6496, 'eval_samples_per_second': 79.403, 'eval_steps_per_second': 2.557, 'epoch': 12.0}
{'train_runtime': 1060.2598, 'train_samples_per_second': 24.73, 'train_steps_per_second': 0.781, 'train_loss': 2.2846377644561917, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.2846
  train_runtime            = 0:17:40.25
  train_samples_per_second =      24.73
  train_steps_per_second   =      0.781
{'train@zho.rst.sctb_loss': 3.5406320095062256, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.018223234624145785, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.0014049877063575696, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.0007155635062611807, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.5406312942504883, 'train@zho.rst.sctb_runtime': 5.5258, 'train@zho.rst.sctb_samples_per_second': 79.445, 'train@zho.rst.sctb_steps_per_second': 2.534, 'epoch': 1.0}
{'loss': 3.7056, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.5525341033935547, 'eval_accuracy@zho.rst.sctb': 0.02127659574468085, 'eval_f1@zho.rst.sctb': 0.00221606648199446, 'eval_precision@zho.rst.sctb': 0.0011318619128466328, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 3.5525341033935547, 'eval_runtime': 1.4567, 'eval_samples_per_second': 64.529, 'eval_steps_per_second': 2.059, 'epoch': 1.0}
{'train@zho.rst.sctb_loss': 3.283083438873291, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.022779043280182234, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.0024603310546722932, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.026368430623749772, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.03898840885142255, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.283083438873291, 'train@zho.rst.sctb_runtime': 5.4914, 'train@zho.rst.sctb_samples_per_second': 79.943, 'train@zho.rst.sctb_steps_per_second': 2.549, 'epoch': 2.0}
{'loss': 3.4248, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.3015549182891846, 'eval_accuracy@zho.rst.sctb': 0.02127659574468085, 'eval_f1@zho.rst.sctb': 0.002150537634408602, 'eval_precision@zho.rst.sctb': 0.001098901098901099, 'eval_recall@zho.rst.sctb': 0.05, 'eval_loss@zho.rst.sctb': 3.3015551567077637, 'eval_runtime': 1.4746, 'eval_samples_per_second': 63.747, 'eval_steps_per_second': 2.034, 'epoch': 2.0}
{'train@zho.rst.sctb_loss': 3.018967866897583, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.31890660592255127, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.025584583002196324, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.02928824049513705, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.03829515833841717, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.018968105316162, 'train@zho.rst.sctb_runtime': 5.4892, 'train@zho.rst.sctb_samples_per_second': 79.975, 'train@zho.rst.sctb_steps_per_second': 2.55, 'epoch': 3.0}
{'loss': 3.1748, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 3.0498838424682617, 'eval_accuracy@zho.rst.sctb': 0.2978723404255319, 'eval_f1@zho.rst.sctb': 0.025854108956602034, 'eval_precision@zho.rst.sctb': 0.01797175866495507, 'eval_recall@zho.rst.sctb': 0.046052631578947366, 'eval_loss@zho.rst.sctb': 3.04988431930542, 'eval_runtime': 1.472, 'eval_samples_per_second': 63.86, 'eval_steps_per_second': 2.038, 'epoch': 3.0}
{'train@zho.rst.sctb_loss': 2.7543113231658936, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.7543113231658936, 'train@zho.rst.sctb_runtime': 5.4864, 'train@zho.rst.sctb_samples_per_second': 80.016, 'train@zho.rst.sctb_steps_per_second': 2.552, 'epoch': 4.0}
{'loss': 2.8993, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.8042099475860596, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 2.8042101860046387, 'eval_runtime': 1.4475, 'eval_samples_per_second': 64.941, 'eval_steps_per_second': 2.073, 'epoch': 4.0}
{'train@zho.rst.sctb_loss': 2.5449583530426025, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.5449585914611816, 'train@zho.rst.sctb_runtime': 5.4682, 'train@zho.rst.sctb_samples_per_second': 80.283, 'train@zho.rst.sctb_steps_per_second': 2.56, 'epoch': 5.0}
{'loss': 2.6955, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.616450309753418, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.6164498329162598, 'eval_runtime': 1.4592, 'eval_samples_per_second': 64.42, 'eval_steps_per_second': 2.056, 'epoch': 5.0}
{'train@zho.rst.sctb_loss': 2.422027349472046, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.422027349472046, 'train@zho.rst.sctb_runtime': 5.5005, 'train@zho.rst.sctb_samples_per_second': 79.811, 'train@zho.rst.sctb_steps_per_second': 2.545, 'epoch': 6.0}
{'loss': 2.5359, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.51169753074646, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.51169753074646, 'eval_runtime': 1.4469, 'eval_samples_per_second': 64.965, 'eval_steps_per_second': 2.073, 'epoch': 6.0}
{'train@zho.rst.sctb_loss': 2.361220121383667, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.361219882965088, 'train@zho.rst.sctb_runtime': 5.4911, 'train@zho.rst.sctb_samples_per_second': 79.947, 'train@zho.rst.sctb_steps_per_second': 2.55, 'epoch': 7.0}
{'loss': 2.4453, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.4607999324798584, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.4607999324798584, 'eval_runtime': 1.4408, 'eval_samples_per_second': 65.242, 'eval_steps_per_second': 2.082, 'epoch': 7.0}
{'train@zho.rst.sctb_loss': 2.330116033554077, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.330115795135498, 'train@zho.rst.sctb_runtime': 5.536, 'train@zho.rst.sctb_samples_per_second': 79.299, 'train@zho.rst.sctb_steps_per_second': 2.529, 'epoch': 8.0}
{'loss': 2.4009, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.4348196983337402, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 2.4348196983337402, 'eval_runtime': 1.4639, 'eval_samples_per_second': 64.211, 'eval_steps_per_second': 2.049, 'epoch': 8.0}
{'train@zho.rst.sctb_loss': 2.311372995376587, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.311372756958008, 'train@zho.rst.sctb_runtime': 5.5156, 'train@zho.rst.sctb_samples_per_second': 79.592, 'train@zho.rst.sctb_steps_per_second': 2.538, 'epoch': 9.0}
{'loss': 2.3605, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.4183456897735596, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 2.4183459281921387, 'eval_runtime': 1.4489, 'eval_samples_per_second': 64.878, 'eval_steps_per_second': 2.071, 'epoch': 9.0}
{'train@zho.rst.sctb_loss': 2.2988030910491943, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3348519362186788, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.020032051282051284, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.05128205128205128, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038866396761133605, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.2988028526306152, 'train@zho.rst.sctb_runtime': 5.4711, 'train@zho.rst.sctb_samples_per_second': 80.24, 'train@zho.rst.sctb_steps_per_second': 2.559, 'epoch': 10.0}
{'loss': 2.3585, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.4075801372528076, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 2.4075798988342285, 'eval_runtime': 1.4527, 'eval_samples_per_second': 64.707, 'eval_steps_per_second': 2.065, 'epoch': 10.0}
{'train@zho.rst.sctb_loss': 2.2924342155456543, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3348519362186788, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.020032051282051284, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.05128205128205128, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038866396761133605, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.2924344539642334, 'train@zho.rst.sctb_runtime': 5.4918, 'train@zho.rst.sctb_samples_per_second': 79.937, 'train@zho.rst.sctb_steps_per_second': 2.549, 'epoch': 11.0}
{'loss': 2.3407, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.402097225189209, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 2.402097702026367, 'eval_runtime': 1.4458, 'eval_samples_per_second': 65.015, 'eval_steps_per_second': 2.075, 'epoch': 11.0}
{'train@zho.rst.sctb_loss': 2.290418863296509, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3348519362186788, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.020032051282051284, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.05128205128205128, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038866396761133605, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.290418863296509, 'train@zho.rst.sctb_runtime': 5.4866, 'train@zho.rst.sctb_samples_per_second': 80.014, 'train@zho.rst.sctb_steps_per_second': 2.552, 'epoch': 12.0}
{'loss': 2.3342, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.400357723236084, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 2.4003584384918213, 'eval_runtime': 1.4488, 'eval_samples_per_second': 64.88, 'eval_steps_per_second': 2.071, 'epoch': 12.0}
{'train_runtime': 214.5733, 'train_samples_per_second': 24.551, 'train_steps_per_second': 0.783, 'train_loss': 2.722988968803769, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.2846
  train_runtime            = 0:17:40.25
  train_samples_per_second =      24.73
  train_steps_per_second   =      0.781
-------------------------------------------------------------------
Lang1:  nld.rst.nldt    Lang2:  zho.rst.sctb
Saving run to:  runs/full_shot/FullShot=v4_nld.rst.nldt_zho.rst.sctb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 1608 examples
read 331 examples
read 326 examples
read 439 examples
read 94 examples
read 159 examples
Total prediction labels:  36
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (zh): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=36, bias=True)
    )
  )
)
{'train@nld.rst.nldt_loss': 3.0558040142059326, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26430348258706465, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01306566650270536, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.00825948383084577, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03125, 'train@nld.rst.nldt_loss@nld.rst.nldt': 3.0558040142059326, 'train@nld.rst.nldt_runtime': 19.3644, 'train@nld.rst.nldt_samples_per_second': 83.039, 'train@nld.rst.nldt_steps_per_second': 2.634, 'epoch': 1.0}
{'loss': 3.3157, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.0157322883605957, 'eval_accuracy@nld.rst.nldt': 0.27794561933534745, 'eval_f1@nld.rst.nldt': 0.016110673321075214, 'eval_precision@nld.rst.nldt': 0.010294282197605462, 'eval_recall@nld.rst.nldt': 0.037037037037037035, 'eval_loss@nld.rst.nldt': 3.0157320499420166, 'eval_runtime': 4.9803, 'eval_samples_per_second': 66.462, 'eval_steps_per_second': 2.209, 'epoch': 1.0}
{'train@nld.rst.nldt_loss': 2.824125289916992, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26430348258706465, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01306566650270536, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.00825948383084577, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03125, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.824125289916992, 'train@nld.rst.nldt_runtime': 19.4131, 'train@nld.rst.nldt_samples_per_second': 82.83, 'train@nld.rst.nldt_steps_per_second': 2.627, 'epoch': 2.0}
{'loss': 2.9426, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.7481534481048584, 'eval_accuracy@nld.rst.nldt': 0.27794561933534745, 'eval_f1@nld.rst.nldt': 0.016110673321075214, 'eval_precision@nld.rst.nldt': 0.010294282197605462, 'eval_recall@nld.rst.nldt': 0.037037037037037035, 'eval_loss@nld.rst.nldt': 2.7481539249420166, 'eval_runtime': 4.2754, 'eval_samples_per_second': 77.42, 'eval_steps_per_second': 2.573, 'epoch': 2.0}
{'train@nld.rst.nldt_loss': 2.7491486072540283, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.28171641791044777, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.024778007705675727, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.03155531556327456, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.04075863678804855, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.7491486072540283, 'train@nld.rst.nldt_runtime': 19.4773, 'train@nld.rst.nldt_samples_per_second': 82.558, 'train@nld.rst.nldt_steps_per_second': 2.618, 'epoch': 3.0}
{'loss': 2.8151, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.6805317401885986, 'eval_accuracy@nld.rst.nldt': 0.2990936555891239, 'eval_f1@nld.rst.nldt': 0.031247272409880414, 'eval_precision@nld.rst.nldt': 0.035682994822779764, 'eval_recall@nld.rst.nldt': 0.05044091710758377, 'eval_loss@nld.rst.nldt': 2.6805317401885986, 'eval_runtime': 4.259, 'eval_samples_per_second': 77.718, 'eval_steps_per_second': 2.583, 'epoch': 3.0}
{'train@nld.rst.nldt_loss': 2.707808017730713, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2922885572139303, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.0303637611827967, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.029607858057560624, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.04753734827264239, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.707808256149292, 'train@nld.rst.nldt_runtime': 19.4605, 'train@nld.rst.nldt_samples_per_second': 82.629, 'train@nld.rst.nldt_steps_per_second': 2.621, 'epoch': 4.0}
{'loss': 2.7361, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.6433043479919434, 'eval_accuracy@nld.rst.nldt': 0.311178247734139, 'eval_f1@nld.rst.nldt': 0.03679252736300267, 'eval_precision@nld.rst.nldt': 0.038934255281637145, 'eval_recall@nld.rst.nldt': 0.05932699435114894, 'eval_loss@nld.rst.nldt': 2.6433043479919434, 'eval_runtime': 4.2847, 'eval_samples_per_second': 77.251, 'eval_steps_per_second': 2.567, 'epoch': 4.0}
{'train@nld.rst.nldt_loss': 2.6735596656799316, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2972636815920398, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03221169002551155, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.02755428080835219, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05045343137254902, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6735596656799316, 'train@nld.rst.nldt_runtime': 19.4552, 'train@nld.rst.nldt_samples_per_second': 82.651, 'train@nld.rst.nldt_steps_per_second': 2.621, 'epoch': 5.0}
{'loss': 2.7108, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.6131832599639893, 'eval_accuracy@nld.rst.nldt': 0.32326283987915405, 'eval_f1@nld.rst.nldt': 0.04181388648746775, 'eval_precision@nld.rst.nldt': 0.04092414661520353, 'eval_recall@nld.rst.nldt': 0.0645592106944764, 'eval_loss@nld.rst.nldt': 2.6131832599639893, 'eval_runtime': 4.2719, 'eval_samples_per_second': 77.483, 'eval_steps_per_second': 2.575, 'epoch': 5.0}
{'train@nld.rst.nldt_loss': 2.642397165298462, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30472636815920395, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03501522077881805, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.028747825961448252, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.054303221288515405, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.642397165298462, 'train@nld.rst.nldt_runtime': 22.6785, 'train@nld.rst.nldt_samples_per_second': 70.904, 'train@nld.rst.nldt_steps_per_second': 2.249, 'epoch': 6.0}
{'loss': 2.6786, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.5893197059631348, 'eval_accuracy@nld.rst.nldt': 0.32326283987915405, 'eval_f1@nld.rst.nldt': 0.041641342102171595, 'eval_precision@nld.rst.nldt': 0.03714428714428714, 'eval_recall@nld.rst.nldt': 0.06621424737366767, 'eval_loss@nld.rst.nldt': 2.5893194675445557, 'eval_runtime': 4.2709, 'eval_samples_per_second': 77.501, 'eval_steps_per_second': 2.576, 'epoch': 6.0}
{'train@nld.rst.nldt_loss': 2.6166446208953857, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3072139303482587, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03637559572531647, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.028152930596344425, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.057448062558356675, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6166439056396484, 'train@nld.rst.nldt_runtime': 19.451, 'train@nld.rst.nldt_samples_per_second': 82.669, 'train@nld.rst.nldt_steps_per_second': 2.622, 'epoch': 7.0}
{'loss': 2.6508, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.5690972805023193, 'eval_accuracy@nld.rst.nldt': 0.3293051359516616, 'eval_f1@nld.rst.nldt': 0.04303358106930554, 'eval_precision@nld.rst.nldt': 0.03678545557385399, 'eval_recall@nld.rst.nldt': 0.06833064949006978, 'eval_loss@nld.rst.nldt': 2.5690975189208984, 'eval_runtime': 4.2653, 'eval_samples_per_second': 77.604, 'eval_steps_per_second': 2.579, 'epoch': 7.0}
{'train@nld.rst.nldt_loss': 2.5953495502471924, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30970149253731344, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03703047449648025, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.03766459258027672, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05969599977672228, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.5953500270843506, 'train@nld.rst.nldt_runtime': 19.4852, 'train@nld.rst.nldt_samples_per_second': 82.524, 'train@nld.rst.nldt_steps_per_second': 2.617, 'epoch': 8.0}
{'loss': 2.6397, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.553257703781128, 'eval_accuracy@nld.rst.nldt': 0.3323262839879154, 'eval_f1@nld.rst.nldt': 0.04553222336446738, 'eval_precision@nld.rst.nldt': 0.07292271330957052, 'eval_recall@nld.rst.nldt': 0.07009431792040488, 'eval_loss@nld.rst.nldt': 2.553257942199707, 'eval_runtime': 4.2754, 'eval_samples_per_second': 77.42, 'eval_steps_per_second': 2.573, 'epoch': 8.0}
{'train@nld.rst.nldt_loss': 2.582366704940796, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31032338308457713, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.037097378484526336, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.02769404282686743, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05991013071895425, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.582366704940796, 'train@nld.rst.nldt_runtime': 19.476, 'train@nld.rst.nldt_samples_per_second': 82.563, 'train@nld.rst.nldt_steps_per_second': 2.619, 'epoch': 9.0}
{'loss': 2.6156, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.542346715927124, 'eval_accuracy@nld.rst.nldt': 0.33534743202416917, 'eval_f1@nld.rst.nldt': 0.04918546975364841, 'eval_precision@nld.rst.nldt': 0.07350257302810491, 'eval_recall@nld.rst.nldt': 0.07185798635073998, 'eval_loss@nld.rst.nldt': 2.542346715927124, 'eval_runtime': 4.2733, 'eval_samples_per_second': 77.457, 'eval_steps_per_second': 2.574, 'epoch': 9.0}
{'train@nld.rst.nldt_loss': 2.573751449584961, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31094527363184077, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03735834377483723, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.027948445796610703, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05998366013071896, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.57375168800354, 'train@nld.rst.nldt_runtime': 19.4871, 'train@nld.rst.nldt_samples_per_second': 82.516, 'train@nld.rst.nldt_steps_per_second': 2.617, 'epoch': 10.0}
{'loss': 2.6006, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.5349719524383545, 'eval_accuracy@nld.rst.nldt': 0.338368580060423, 'eval_f1@nld.rst.nldt': 0.0522463709970158, 'eval_precision@nld.rst.nldt': 0.073748666297454, 'eval_recall@nld.rst.nldt': 0.07362165478107507, 'eval_loss@nld.rst.nldt': 2.5349719524383545, 'eval_runtime': 4.3027, 'eval_samples_per_second': 76.928, 'eval_steps_per_second': 2.557, 'epoch': 10.0}
{'train@nld.rst.nldt_loss': 2.5671331882476807, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31592039800995025, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03899391019688746, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.03628740426836334, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.062117801830877284, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.5671334266662598, 'train@nld.rst.nldt_runtime': 19.4774, 'train@nld.rst.nldt_samples_per_second': 82.557, 'train@nld.rst.nldt_steps_per_second': 2.618, 'epoch': 11.0}
{'loss': 2.591, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.530174493789673, 'eval_accuracy@nld.rst.nldt': 0.338368580060423, 'eval_f1@nld.rst.nldt': 0.0522463709970158, 'eval_precision@nld.rst.nldt': 0.073748666297454, 'eval_recall@nld.rst.nldt': 0.07362165478107507, 'eval_loss@nld.rst.nldt': 2.530174493789673, 'eval_runtime': 4.2842, 'eval_samples_per_second': 77.261, 'eval_steps_per_second': 2.568, 'epoch': 11.0}
{'train@nld.rst.nldt_loss': 2.5644419193267822, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31592039800995025, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.0389532097981371, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.03622088745458959, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.06235064496813218, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.5644421577453613, 'train@nld.rst.nldt_runtime': 19.4861, 'train@nld.rst.nldt_samples_per_second': 82.52, 'train@nld.rst.nldt_steps_per_second': 2.617, 'epoch': 12.0}
{'loss': 2.5949, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.5286831855773926, 'eval_accuracy@nld.rst.nldt': 0.338368580060423, 'eval_f1@nld.rst.nldt': 0.05193295400624233, 'eval_precision@nld.rst.nldt': 0.07291453366486277, 'eval_recall@nld.rst.nldt': 0.07362165478107507, 'eval_loss@nld.rst.nldt': 2.5286829471588135, 'eval_runtime': 4.2617, 'eval_samples_per_second': 77.668, 'eval_steps_per_second': 2.581, 'epoch': 12.0}
{'train_runtime': 777.4399, 'train_samples_per_second': 24.82, 'train_steps_per_second': 0.787, 'train_loss': 2.7409674700568702, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =      2.741
  train_runtime            = 0:12:57.43
  train_samples_per_second =      24.82
  train_steps_per_second   =      0.787
{'train@zho.rst.sctb_loss': 3.317918062210083, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.08428246013667426, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.008812671192092414, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.008305274971941638, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.009386098427194319, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.317918062210083, 'train@zho.rst.sctb_runtime': 5.4609, 'train@zho.rst.sctb_samples_per_second': 80.389, 'train@zho.rst.sctb_steps_per_second': 2.564, 'epoch': 1.0}
{'loss': 3.466, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.337966203689575, 'eval_accuracy@zho.rst.sctb': 0.1276595744680851, 'eval_f1@zho.rst.sctb': 0.015999999999999997, 'eval_precision@zho.rst.sctb': 0.013953488372093023, 'eval_recall@zho.rst.sctb': 0.01875, 'eval_loss@zho.rst.sctb': 3.3379650115966797, 'eval_runtime': 1.4575, 'eval_samples_per_second': 64.493, 'eval_steps_per_second': 2.058, 'epoch': 1.0}
{'train@zho.rst.sctb_loss': 3.0800833702087402, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.0800836086273193, 'train@zho.rst.sctb_runtime': 5.4585, 'train@zho.rst.sctb_samples_per_second': 80.424, 'train@zho.rst.sctb_steps_per_second': 2.565, 'epoch': 2.0}
{'loss': 3.2086, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.1092090606689453, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 3.109208345413208, 'eval_runtime': 1.4284, 'eval_samples_per_second': 65.806, 'eval_steps_per_second': 2.1, 'epoch': 2.0}
{'train@zho.rst.sctb_loss': 2.842156171798706, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.842155933380127, 'train@zho.rst.sctb_runtime': 5.486, 'train@zho.rst.sctb_samples_per_second': 80.023, 'train@zho.rst.sctb_steps_per_second': 2.552, 'epoch': 3.0}
{'loss': 2.9874, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.8853650093078613, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.885364294052124, 'eval_runtime': 1.4329, 'eval_samples_per_second': 65.599, 'eval_steps_per_second': 2.094, 'epoch': 3.0}
{'train@zho.rst.sctb_loss': 2.6404359340667725, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.6404359340667725, 'train@zho.rst.sctb_runtime': 5.4892, 'train@zho.rst.sctb_samples_per_second': 79.975, 'train@zho.rst.sctb_steps_per_second': 2.55, 'epoch': 4.0}
{'loss': 2.7594, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.7034385204315186, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.7034387588500977, 'eval_runtime': 1.4479, 'eval_samples_per_second': 64.923, 'eval_steps_per_second': 2.072, 'epoch': 4.0}
{'train@zho.rst.sctb_loss': 2.5103840827941895, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.5103840827941895, 'train@zho.rst.sctb_runtime': 5.4677, 'train@zho.rst.sctb_samples_per_second': 80.289, 'train@zho.rst.sctb_steps_per_second': 2.56, 'epoch': 5.0}
{'loss': 2.6194, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.592564582824707, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.592564344406128, 'eval_runtime': 1.4389, 'eval_samples_per_second': 65.329, 'eval_steps_per_second': 2.085, 'epoch': 5.0}
{'train@zho.rst.sctb_loss': 2.427375078201294, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.427375078201294, 'train@zho.rst.sctb_runtime': 5.4733, 'train@zho.rst.sctb_samples_per_second': 80.208, 'train@zho.rst.sctb_steps_per_second': 2.558, 'epoch': 6.0}
{'loss': 2.514, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.5251667499542236, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.5251662731170654, 'eval_runtime': 1.4132, 'eval_samples_per_second': 66.517, 'eval_steps_per_second': 2.123, 'epoch': 6.0}
{'train@zho.rst.sctb_loss': 2.373274326324463, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.373274564743042, 'train@zho.rst.sctb_runtime': 5.4909, 'train@zho.rst.sctb_samples_per_second': 79.951, 'train@zho.rst.sctb_steps_per_second': 2.55, 'epoch': 7.0}
{'loss': 2.444, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.478457450866699, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.478456974029541, 'eval_runtime': 1.4421, 'eval_samples_per_second': 65.182, 'eval_steps_per_second': 2.08, 'epoch': 7.0}
{'train@zho.rst.sctb_loss': 2.3422985076904297, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.3422982692718506, 'train@zho.rst.sctb_runtime': 5.497, 'train@zho.rst.sctb_samples_per_second': 79.861, 'train@zho.rst.sctb_steps_per_second': 2.547, 'epoch': 8.0}
{'loss': 2.4042, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.452042818069458, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.452042579650879, 'eval_runtime': 1.4507, 'eval_samples_per_second': 64.795, 'eval_steps_per_second': 2.068, 'epoch': 8.0}
{'train@zho.rst.sctb_loss': 2.322500467300415, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.322500228881836, 'train@zho.rst.sctb_runtime': 5.4481, 'train@zho.rst.sctb_samples_per_second': 80.579, 'train@zho.rst.sctb_steps_per_second': 2.57, 'epoch': 9.0}
{'loss': 2.3656, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.434373140335083, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.434373617172241, 'eval_runtime': 1.4264, 'eval_samples_per_second': 65.9, 'eval_steps_per_second': 2.103, 'epoch': 9.0}
{'train@zho.rst.sctb_loss': 2.309108257293701, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.309108257293701, 'train@zho.rst.sctb_runtime': 5.4869, 'train@zho.rst.sctb_samples_per_second': 80.008, 'train@zho.rst.sctb_steps_per_second': 2.552, 'epoch': 10.0}
{'loss': 2.3643, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.422588348388672, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.422588586807251, 'eval_runtime': 1.4409, 'eval_samples_per_second': 65.237, 'eval_steps_per_second': 2.082, 'epoch': 10.0}
{'train@zho.rst.sctb_loss': 2.301882266998291, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.301882266998291, 'train@zho.rst.sctb_runtime': 5.462, 'train@zho.rst.sctb_samples_per_second': 80.373, 'train@zho.rst.sctb_steps_per_second': 2.563, 'epoch': 11.0}
{'loss': 2.3443, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.416457414627075, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.416457414627075, 'eval_runtime': 1.425, 'eval_samples_per_second': 65.965, 'eval_steps_per_second': 2.105, 'epoch': 11.0}
{'train@zho.rst.sctb_loss': 2.29954195022583, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.29954195022583, 'train@zho.rst.sctb_runtime': 5.4633, 'train@zho.rst.sctb_samples_per_second': 80.355, 'train@zho.rst.sctb_steps_per_second': 2.563, 'epoch': 12.0}
{'loss': 2.3409, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.41440749168396, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.41440749168396, 'eval_runtime': 1.419, 'eval_samples_per_second': 66.242, 'eval_steps_per_second': 2.114, 'epoch': 12.0}
{'train_runtime': 214.0106, 'train_samples_per_second': 24.616, 'train_steps_per_second': 0.785, 'train_loss': 2.6515004294259206, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =      2.741
  train_runtime            = 0:12:57.43
  train_samples_per_second =      24.82
  train_steps_per_second   =      0.787
-------------------------------------------------------------------
Lang1:  por.rst.cstn    Lang2:  zho.rst.sctb
Saving run to:  runs/full_shot/FullShot=v4_por.rst.cstn_zho.rst.sctb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 4148 examples
read 573 examples
read 272 examples
read 439 examples
read 94 examples
read 159 examples
Total prediction labels:  38
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (pt): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (zh): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (pt): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=38, bias=True)
    )
  )
)
{'train@por.rst.cstn_loss': 2.4717371463775635, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.2774831243972999, 'train@por.rst.cstn_f1@por.rst.cstn': 0.013578237070592677, 'train@por.rst.cstn_precision@por.rst.cstn': 0.008673438630335182, 'train@por.rst.cstn_recall@por.rst.cstn': 0.03125, 'train@por.rst.cstn_loss@por.rst.cstn': 2.4717371463775635, 'train@por.rst.cstn_runtime': 49.6209, 'train@por.rst.cstn_samples_per_second': 83.594, 'train@por.rst.cstn_steps_per_second': 2.62, 'epoch': 1.0}
{'loss': 2.9535, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.588700771331787, 'eval_accuracy@por.rst.cstn': 0.28097731239092494, 'eval_f1@por.rst.cstn': 0.019940549913301957, 'eval_precision@por.rst.cstn': 0.012771696017769315, 'eval_recall@por.rst.cstn': 0.045454545454545456, 'eval_loss@por.rst.cstn': 2.588700294494629, 'eval_runtime': 7.1532, 'eval_samples_per_second': 80.104, 'eval_steps_per_second': 2.516, 'epoch': 1.0}
{'train@por.rst.cstn_loss': 2.2408971786499023, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.37150433944069433, 'train@por.rst.cstn_f1@por.rst.cstn': 0.053754003731251956, 'train@por.rst.cstn_precision@por.rst.cstn': 0.0830191653139371, 'train@por.rst.cstn_recall@por.rst.cstn': 0.061886220111160004, 'train@por.rst.cstn_loss@por.rst.cstn': 2.2408969402313232, 'train@por.rst.cstn_runtime': 49.7107, 'train@por.rst.cstn_samples_per_second': 83.443, 'train@por.rst.cstn_steps_per_second': 2.615, 'epoch': 2.0}
{'loss': 2.3865, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.3604564666748047, 'eval_accuracy@por.rst.cstn': 0.33158813263525305, 'eval_f1@por.rst.cstn': 0.07357491722816802, 'eval_precision@por.rst.cstn': 0.10254228981123621, 'eval_recall@por.rst.cstn': 0.08687524596615505, 'eval_loss@por.rst.cstn': 2.3604562282562256, 'eval_runtime': 7.1853, 'eval_samples_per_second': 79.746, 'eval_steps_per_second': 2.505, 'epoch': 2.0}
{'train@por.rst.cstn_loss': 2.0373446941375732, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.4534715525554484, 'train@por.rst.cstn_f1@por.rst.cstn': 0.07071691396458354, 'train@por.rst.cstn_precision@por.rst.cstn': 0.07230733041470487, 'train@por.rst.cstn_recall@por.rst.cstn': 0.08689322997441871, 'train@por.rst.cstn_loss@por.rst.cstn': 2.0373446941375732, 'train@por.rst.cstn_runtime': 49.726, 'train@por.rst.cstn_samples_per_second': 83.417, 'train@por.rst.cstn_steps_per_second': 2.614, 'epoch': 3.0}
{'loss': 2.194, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.168285369873047, 'eval_accuracy@por.rst.cstn': 0.3769633507853403, 'eval_f1@por.rst.cstn': 0.09347027073199822, 'eval_precision@por.rst.cstn': 0.09522307591247268, 'eval_recall@por.rst.cstn': 0.12499218761960597, 'eval_loss@por.rst.cstn': 2.168285608291626, 'eval_runtime': 7.195, 'eval_samples_per_second': 79.638, 'eval_steps_per_second': 2.502, 'epoch': 3.0}
{'train@por.rst.cstn_loss': 1.8638571500778198, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5149469623915139, 'train@por.rst.cstn_f1@por.rst.cstn': 0.08477968232870936, 'train@por.rst.cstn_precision@por.rst.cstn': 0.11351733763268165, 'train@por.rst.cstn_recall@por.rst.cstn': 0.101116870548381, 'train@por.rst.cstn_loss@por.rst.cstn': 1.8638571500778198, 'train@por.rst.cstn_runtime': 49.656, 'train@por.rst.cstn_samples_per_second': 83.535, 'train@por.rst.cstn_steps_per_second': 2.618, 'epoch': 4.0}
{'loss': 2.0151, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.008544921875, 'eval_accuracy@por.rst.cstn': 0.41535776614310643, 'eval_f1@por.rst.cstn': 0.10472688125107837, 'eval_precision@por.rst.cstn': 0.0874138996490056, 'eval_recall@por.rst.cstn': 0.1398342919822013, 'eval_loss@por.rst.cstn': 2.008544921875, 'eval_runtime': 7.1546, 'eval_samples_per_second': 80.088, 'eval_steps_per_second': 2.516, 'epoch': 4.0}
{'train@por.rst.cstn_loss': 1.7486647367477417, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5325458052073289, 'train@por.rst.cstn_f1@por.rst.cstn': 0.11200533724036782, 'train@por.rst.cstn_precision@por.rst.cstn': 0.11785545824965662, 'train@por.rst.cstn_recall@por.rst.cstn': 0.12198597997829129, 'train@por.rst.cstn_loss@por.rst.cstn': 1.7486646175384521, 'train@por.rst.cstn_runtime': 49.6913, 'train@por.rst.cstn_samples_per_second': 83.475, 'train@por.rst.cstn_steps_per_second': 2.616, 'epoch': 5.0}
{'loss': 1.8643, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.8970763683319092, 'eval_accuracy@por.rst.cstn': 0.4467713787085515, 'eval_f1@por.rst.cstn': 0.1421245688705284, 'eval_precision@por.rst.cstn': 0.15129640642837122, 'eval_recall@por.rst.cstn': 0.16753260261685476, 'eval_loss@por.rst.cstn': 1.8970763683319092, 'eval_runtime': 7.1772, 'eval_samples_per_second': 79.836, 'eval_steps_per_second': 2.508, 'epoch': 5.0}
{'train@por.rst.cstn_loss': 1.6708271503448486, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5612343297974928, 'train@por.rst.cstn_f1@por.rst.cstn': 0.12515571140292606, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13884552378210052, 'train@por.rst.cstn_recall@por.rst.cstn': 0.13513126950787313, 'train@por.rst.cstn_loss@por.rst.cstn': 1.6708271503448486, 'train@por.rst.cstn_runtime': 49.7324, 'train@por.rst.cstn_samples_per_second': 83.406, 'train@por.rst.cstn_steps_per_second': 2.614, 'epoch': 6.0}
{'loss': 1.7735, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.8199454545974731, 'eval_accuracy@por.rst.cstn': 0.4607329842931937, 'eval_f1@por.rst.cstn': 0.1624374197140519, 'eval_precision@por.rst.cstn': 0.18887307962698757, 'eval_recall@por.rst.cstn': 0.178786501677598, 'eval_loss@por.rst.cstn': 1.8199454545974731, 'eval_runtime': 7.2046, 'eval_samples_per_second': 79.532, 'eval_steps_per_second': 2.498, 'epoch': 6.0}
{'train@por.rst.cstn_loss': 1.6227948665618896, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5691899710703954, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1292664436937759, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13152292771361204, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1408319347215641, 'train@por.rst.cstn_loss@por.rst.cstn': 1.6227948665618896, 'train@por.rst.cstn_runtime': 49.7486, 'train@por.rst.cstn_samples_per_second': 83.379, 'train@por.rst.cstn_steps_per_second': 2.613, 'epoch': 7.0}
{'loss': 1.7082, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.7772057056427002, 'eval_accuracy@por.rst.cstn': 0.4799301919720768, 'eval_f1@por.rst.cstn': 0.17533202485764743, 'eval_precision@por.rst.cstn': 0.1793411353212068, 'eval_recall@por.rst.cstn': 0.1898055940546059, 'eval_loss@por.rst.cstn': 1.7772055864334106, 'eval_runtime': 7.1713, 'eval_samples_per_second': 79.902, 'eval_steps_per_second': 2.51, 'epoch': 7.0}
{'train@por.rst.cstn_loss': 1.590559720993042, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5769045323047252, 'train@por.rst.cstn_f1@por.rst.cstn': 0.13331435572422473, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13285220909945517, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14507285116806717, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5905598402023315, 'train@por.rst.cstn_runtime': 49.6883, 'train@por.rst.cstn_samples_per_second': 83.48, 'train@por.rst.cstn_steps_per_second': 2.616, 'epoch': 8.0}
{'loss': 1.6751, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.7483885288238525, 'eval_accuracy@por.rst.cstn': 0.49040139616055844, 'eval_f1@por.rst.cstn': 0.18038054994102998, 'eval_precision@por.rst.cstn': 0.1829700331670498, 'eval_recall@por.rst.cstn': 0.19428278220404283, 'eval_loss@por.rst.cstn': 1.748388648033142, 'eval_runtime': 7.1549, 'eval_samples_per_second': 80.085, 'eval_steps_per_second': 2.516, 'epoch': 8.0}
{'train@por.rst.cstn_loss': 1.5691323280334473, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5797974927675988, 'train@por.rst.cstn_f1@por.rst.cstn': 0.13590288303673792, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13471104606274847, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14695970035131317, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5691322088241577, 'train@por.rst.cstn_runtime': 49.6611, 'train@por.rst.cstn_samples_per_second': 83.526, 'train@por.rst.cstn_steps_per_second': 2.618, 'epoch': 9.0}
{'loss': 1.647, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.7283087968826294, 'eval_accuracy@por.rst.cstn': 0.4956369982547993, 'eval_f1@por.rst.cstn': 0.19197154953083803, 'eval_precision@por.rst.cstn': 0.23090559508066724, 'eval_recall@por.rst.cstn': 0.20157621341051754, 'eval_loss@por.rst.cstn': 1.7283085584640503, 'eval_runtime': 7.1874, 'eval_samples_per_second': 79.723, 'eval_steps_per_second': 2.504, 'epoch': 9.0}
{'train@por.rst.cstn_loss': 1.5544856786727905, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5872709739633558, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1393431117229155, 'train@por.rst.cstn_precision@por.rst.cstn': 0.1487542336257525, 'train@por.rst.cstn_recall@por.rst.cstn': 0.15161934872587118, 'train@por.rst.cstn_loss@por.rst.cstn': 1.554485559463501, 'train@por.rst.cstn_runtime': 49.7346, 'train@por.rst.cstn_samples_per_second': 83.403, 'train@por.rst.cstn_steps_per_second': 2.614, 'epoch': 10.0}
{'loss': 1.619, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.7140631675720215, 'eval_accuracy@por.rst.cstn': 0.5078534031413613, 'eval_f1@por.rst.cstn': 0.19827324264517152, 'eval_precision@por.rst.cstn': 0.2312944709264727, 'eval_recall@por.rst.cstn': 0.21218410140814967, 'eval_loss@por.rst.cstn': 1.7140634059906006, 'eval_runtime': 7.165, 'eval_samples_per_second': 79.972, 'eval_steps_per_second': 2.512, 'epoch': 10.0}
{'train@por.rst.cstn_loss': 1.5459694862365723, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5865477338476374, 'train@por.rst.cstn_f1@por.rst.cstn': 0.13961413346780355, 'train@por.rst.cstn_precision@por.rst.cstn': 0.14884408431554697, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1512394914283697, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5459693670272827, 'train@por.rst.cstn_runtime': 49.6654, 'train@por.rst.cstn_samples_per_second': 83.519, 'train@por.rst.cstn_steps_per_second': 2.618, 'epoch': 11.0}
{'loss': 1.6103, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.7083985805511475, 'eval_accuracy@por.rst.cstn': 0.49912739965095987, 'eval_f1@por.rst.cstn': 0.19577951633250945, 'eval_precision@por.rst.cstn': 0.22929669911379752, 'eval_recall@por.rst.cstn': 0.20971563685834543, 'eval_loss@por.rst.cstn': 1.7083985805511475, 'eval_runtime': 7.182, 'eval_samples_per_second': 79.783, 'eval_steps_per_second': 2.506, 'epoch': 11.0}
{'train@por.rst.cstn_loss': 1.5430665016174316, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5879942140790743, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1408081807970176, 'train@por.rst.cstn_precision@por.rst.cstn': 0.14965647274222468, 'train@por.rst.cstn_recall@por.rst.cstn': 0.15230686314340022, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5430665016174316, 'train@por.rst.cstn_runtime': 49.6422, 'train@por.rst.cstn_samples_per_second': 83.558, 'train@por.rst.cstn_steps_per_second': 2.619, 'epoch': 12.0}
{'loss': 1.6037, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.704927682876587, 'eval_accuracy@por.rst.cstn': 0.5026178010471204, 'eval_f1@por.rst.cstn': 0.1973676174677009, 'eval_precision@por.rst.cstn': 0.23041859078801477, 'eval_recall@por.rst.cstn': 0.2114947920465915, 'eval_loss@por.rst.cstn': 1.7049278020858765, 'eval_runtime': 7.1546, 'eval_samples_per_second': 80.088, 'eval_steps_per_second': 2.516, 'epoch': 12.0}
{'train_runtime': 1947.8618, 'train_samples_per_second': 25.554, 'train_steps_per_second': 0.801, 'train_loss': 1.9208588331173628, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.9209
  train_runtime            = 0:32:27.86
  train_samples_per_second =     25.554
  train_steps_per_second   =      0.801
{'train@zho.rst.sctb_loss': 3.950991630554199, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.01366742596810934, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.03041433192109699, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.05611250348092454, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.06913580246913581, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.950991630554199, 'train@zho.rst.sctb_runtime': 5.5071, 'train@zho.rst.sctb_samples_per_second': 79.715, 'train@zho.rst.sctb_steps_per_second': 2.542, 'epoch': 1.0}
{'loss': 4.3508, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.8275434970855713, 'eval_accuracy@zho.rst.sctb': 0.031914893617021274, 'eval_f1@zho.rst.sctb': 0.050793650793650794, 'eval_precision@zho.rst.sctb': 0.04926108374384237, 'eval_recall@zho.rst.sctb': 0.09523809523809523, 'eval_loss@zho.rst.sctb': 3.827543258666992, 'eval_runtime': 1.4316, 'eval_samples_per_second': 65.661, 'eval_steps_per_second': 2.096, 'epoch': 1.0}
{'train@zho.rst.sctb_loss': 3.5059854984283447, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.01366742596810934, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.024184149184149187, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.04990993365845085, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.06913580246913581, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.5059852600097656, 'train@zho.rst.sctb_runtime': 5.4654, 'train@zho.rst.sctb_samples_per_second': 80.323, 'train@zho.rst.sctb_steps_per_second': 2.562, 'epoch': 2.0}
{'loss': 3.7191, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.436713457107544, 'eval_accuracy@zho.rst.sctb': 0.05319148936170213, 'eval_f1@zho.rst.sctb': 0.07224627224627225, 'eval_precision@zho.rst.sctb': 0.09703504043126684, 'eval_recall@zho.rst.sctb': 0.10884353741496598, 'eval_loss@zho.rst.sctb': 3.436713457107544, 'eval_runtime': 1.4626, 'eval_samples_per_second': 64.267, 'eval_steps_per_second': 2.051, 'epoch': 2.0}
{'train@zho.rst.sctb_loss': 3.196193218231201, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.2255125284738041, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.05328913279074951, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.06438568693470653, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.09222053103331643, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.196193218231201, 'train@zho.rst.sctb_runtime': 5.4574, 'train@zho.rst.sctb_samples_per_second': 80.441, 'train@zho.rst.sctb_steps_per_second': 2.565, 'epoch': 3.0}
{'loss': 3.3758, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 3.1530232429504395, 'eval_accuracy@zho.rst.sctb': 0.22340425531914893, 'eval_f1@zho.rst.sctb': 0.08617181030974135, 'eval_precision@zho.rst.sctb': 0.08118686868686868, 'eval_recall@zho.rst.sctb': 0.11033163265306122, 'eval_loss@zho.rst.sctb': 3.1530230045318604, 'eval_runtime': 1.458, 'eval_samples_per_second': 64.471, 'eval_steps_per_second': 2.058, 'epoch': 3.0}
{'train@zho.rst.sctb_loss': 2.9176294803619385, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.32574031890660593, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.05523516596180956, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.04921124828532236, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.0730593607305936, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.9176294803619385, 'train@zho.rst.sctb_runtime': 5.4788, 'train@zho.rst.sctb_samples_per_second': 80.128, 'train@zho.rst.sctb_steps_per_second': 2.555, 'epoch': 4.0}
{'loss': 3.0723, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.89489483833313, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.07459016393442623, 'eval_precision@zho.rst.sctb': 0.06666666666666667, 'eval_recall@zho.rst.sctb': 0.096875, 'eval_loss@zho.rst.sctb': 2.894894599914551, 'eval_runtime': 1.4417, 'eval_samples_per_second': 65.201, 'eval_steps_per_second': 2.081, 'epoch': 4.0}
{'train@zho.rst.sctb_loss': 2.679165840148926, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.6791651248931885, 'train@zho.rst.sctb_runtime': 5.492, 'train@zho.rst.sctb_samples_per_second': 79.935, 'train@zho.rst.sctb_steps_per_second': 2.549, 'epoch': 5.0}
{'loss': 2.8375, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.6792380809783936, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.0248, 'eval_precision@zho.rst.sctb': 0.016666666666666666, 'eval_recall@zho.rst.sctb': 0.0484375, 'eval_loss@zho.rst.sctb': 2.6792383193969727, 'eval_runtime': 1.4308, 'eval_samples_per_second': 65.695, 'eval_steps_per_second': 2.097, 'epoch': 5.0}
{'train@zho.rst.sctb_loss': 2.5075478553771973, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.507547616958618, 'train@zho.rst.sctb_runtime': 5.4657, 'train@zho.rst.sctb_samples_per_second': 80.32, 'train@zho.rst.sctb_steps_per_second': 2.561, 'epoch': 6.0}
{'loss': 2.6388, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.5338761806488037, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.0248, 'eval_precision@zho.rst.sctb': 0.016666666666666666, 'eval_recall@zho.rst.sctb': 0.0484375, 'eval_loss@zho.rst.sctb': 2.533876657485962, 'eval_runtime': 1.4307, 'eval_samples_per_second': 65.704, 'eval_steps_per_second': 2.097, 'epoch': 6.0}
{'train@zho.rst.sctb_loss': 2.40171480178833, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.401715040206909, 'train@zho.rst.sctb_runtime': 5.4852, 'train@zho.rst.sctb_samples_per_second': 80.034, 'train@zho.rst.sctb_steps_per_second': 2.552, 'epoch': 7.0}
{'loss': 2.5183, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.4477930068969727, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.4477932453155518, 'eval_runtime': 1.4423, 'eval_samples_per_second': 65.175, 'eval_steps_per_second': 2.08, 'epoch': 7.0}
{'train@zho.rst.sctb_loss': 2.3428821563720703, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.3428826332092285, 'train@zho.rst.sctb_runtime': 5.478, 'train@zho.rst.sctb_samples_per_second': 80.139, 'train@zho.rst.sctb_steps_per_second': 2.556, 'epoch': 8.0}
{'loss': 2.4311, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.4004414081573486, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.4004411697387695, 'eval_runtime': 1.4533, 'eval_samples_per_second': 64.681, 'eval_steps_per_second': 2.064, 'epoch': 8.0}
{'train@zho.rst.sctb_loss': 2.3079683780670166, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.3079679012298584, 'train@zho.rst.sctb_runtime': 5.4868, 'train@zho.rst.sctb_samples_per_second': 80.01, 'train@zho.rst.sctb_steps_per_second': 2.552, 'epoch': 9.0}
{'loss': 2.3798, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.3710169792175293, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.3710169792175293, 'eval_runtime': 1.4599, 'eval_samples_per_second': 64.389, 'eval_steps_per_second': 2.055, 'epoch': 9.0}
{'train@zho.rst.sctb_loss': 2.2861952781677246, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3348519362186788, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.020032051282051284, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.05128205128205128, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038866396761133605, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.2861952781677246, 'train@zho.rst.sctb_runtime': 5.479, 'train@zho.rst.sctb_samples_per_second': 80.124, 'train@zho.rst.sctb_steps_per_second': 2.555, 'epoch': 10.0}
{'loss': 2.3519, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.3526618480682373, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.352661609649658, 'eval_runtime': 1.4507, 'eval_samples_per_second': 64.797, 'eval_steps_per_second': 2.068, 'epoch': 10.0}
{'train@zho.rst.sctb_loss': 2.275390625, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3348519362186788, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.020032051282051284, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.05128205128205128, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038866396761133605, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.275390625, 'train@zho.rst.sctb_runtime': 5.474, 'train@zho.rst.sctb_samples_per_second': 80.197, 'train@zho.rst.sctb_steps_per_second': 2.558, 'epoch': 11.0}
{'loss': 2.3353, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.34389591217041, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.343895673751831, 'eval_runtime': 1.4459, 'eval_samples_per_second': 65.011, 'eval_steps_per_second': 2.075, 'epoch': 11.0}
{'train@zho.rst.sctb_loss': 2.271906852722168, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3348519362186788, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.020032051282051284, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.05128205128205128, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038866396761133605, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.271906614303589, 'train@zho.rst.sctb_runtime': 5.5022, 'train@zho.rst.sctb_samples_per_second': 79.786, 'train@zho.rst.sctb_steps_per_second': 2.544, 'epoch': 12.0}
{'loss': 2.3189, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.3408308029174805, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.3408308029174805, 'eval_runtime': 1.4464, 'eval_samples_per_second': 64.989, 'eval_steps_per_second': 2.074, 'epoch': 12.0}
{'train_runtime': 214.1654, 'train_samples_per_second': 24.598, 'train_steps_per_second': 0.784, 'train_loss': 2.8607941355024065, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.9209
  train_runtime            = 0:32:27.86
  train_samples_per_second =     25.554
  train_steps_per_second   =      0.801
-------------------------------------------------------------------
Lang1:  rus.rst.rrt    Lang2:  zho.rst.sctb
Saving run to:  runs/full_shot/FullShot=v4_rus.rst.rrt_zho.rst.sctb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 28822 examples
read 2855 examples
read 2843 examples
read 439 examples
read 94 examples
read 159 examples
Total prediction labels:  32
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (ru): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (zh): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=32, bias=True)
    )
  )
)
{'train@rus.rst.rrt_loss': 1.7441068887710571, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.49049337311775726, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.17554739851500256, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.22322321261672948, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.1933392825631847, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.7441070079803467, 'train@rus.rst.rrt_runtime': 344.2557, 'train@rus.rst.rrt_samples_per_second': 83.723, 'train@rus.rst.rrt_steps_per_second': 2.617, 'epoch': 1.0}
{'loss': 2.1559, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.780383586883545, 'eval_accuracy@rus.rst.rrt': 0.47075306479859896, 'eval_f1@rus.rst.rrt': 0.19098487824201005, 'eval_precision@rus.rst.rrt': 0.1936540188699224, 'eval_recall@rus.rst.rrt': 0.21213850318540076, 'eval_loss@rus.rst.rrt': 1.780383586883545, 'eval_runtime': 34.3804, 'eval_samples_per_second': 83.041, 'eval_steps_per_second': 2.618, 'epoch': 1.0}
{'train@rus.rst.rrt_loss': 1.5192533731460571, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5394490319894525, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.21905013394023412, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.2781955069920961, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.22748485029731255, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.5192533731460571, 'train@rus.rst.rrt_runtime': 344.3934, 'train@rus.rst.rrt_samples_per_second': 83.689, 'train@rus.rst.rrt_steps_per_second': 2.616, 'epoch': 2.0}
{'loss': 1.6605, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.5826516151428223, 'eval_accuracy@rus.rst.rrt': 0.5124343257443082, 'eval_f1@rus.rst.rrt': 0.23967983500291867, 'eval_precision@rus.rst.rrt': 0.2955162060341796, 'eval_recall@rus.rst.rrt': 0.2496244571613526, 'eval_loss@rus.rst.rrt': 1.5826516151428223, 'eval_runtime': 34.4562, 'eval_samples_per_second': 82.859, 'eval_steps_per_second': 2.612, 'epoch': 2.0}
{'train@rus.rst.rrt_loss': 1.4311846494674683, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5637360349732843, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.2692621405387318, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.45041006840935044, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.26591392155468324, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.4311848878860474, 'train@rus.rst.rrt_runtime': 344.4672, 'train@rus.rst.rrt_samples_per_second': 83.671, 'train@rus.rst.rrt_steps_per_second': 2.616, 'epoch': 3.0}
{'loss': 1.5234, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.5011157989501953, 'eval_accuracy@rus.rst.rrt': 0.5341506129597198, 'eval_f1@rus.rst.rrt': 0.30188849397990986, 'eval_precision@rus.rst.rrt': 0.44551104026012717, 'eval_recall@rus.rst.rrt': 0.2975493741712172, 'eval_loss@rus.rst.rrt': 1.5011159181594849, 'eval_runtime': 34.401, 'eval_samples_per_second': 82.992, 'eval_steps_per_second': 2.616, 'epoch': 3.0}
{'train@rus.rst.rrt_loss': 1.3762307167053223, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5787939768232601, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3074559175930555, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.44911768889027637, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.2904484029030788, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3762307167053223, 'train@rus.rst.rrt_runtime': 344.0405, 'train@rus.rst.rrt_samples_per_second': 83.775, 'train@rus.rst.rrt_steps_per_second': 2.619, 'epoch': 4.0}
{'loss': 1.4551, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.4506728649139404, 'eval_accuracy@rus.rst.rrt': 0.5583187390542907, 'eval_f1@rus.rst.rrt': 0.3517533654922727, 'eval_precision@rus.rst.rrt': 0.5156682539304652, 'eval_recall@rus.rst.rrt': 0.33521849716738444, 'eval_loss@rus.rst.rrt': 1.4506726264953613, 'eval_runtime': 34.4378, 'eval_samples_per_second': 82.903, 'eval_steps_per_second': 2.613, 'epoch': 4.0}
{'train@rus.rst.rrt_loss': 1.340330958366394, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.587918950801471, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3263225982633616, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4365172030867159, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.30847930170902144, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3403308391571045, 'train@rus.rst.rrt_runtime': 344.0916, 'train@rus.rst.rrt_samples_per_second': 83.763, 'train@rus.rst.rrt_steps_per_second': 2.618, 'epoch': 5.0}
{'loss': 1.4121, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.4219863414764404, 'eval_accuracy@rus.rst.rrt': 0.5667250437828372, 'eval_f1@rus.rst.rrt': 0.38081694705167607, 'eval_precision@rus.rst.rrt': 0.5090015230453591, 'eval_recall@rus.rst.rrt': 0.36069464983723143, 'eval_loss@rus.rst.rrt': 1.4219863414764404, 'eval_runtime': 34.3794, 'eval_samples_per_second': 83.044, 'eval_steps_per_second': 2.618, 'epoch': 5.0}
{'train@rus.rst.rrt_loss': 1.3170000314712524, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5954132260079106, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3386265400431219, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4411127326422816, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.31974551515612876, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3170000314712524, 'train@rus.rst.rrt_runtime': 344.4668, 'train@rus.rst.rrt_samples_per_second': 83.671, 'train@rus.rst.rrt_steps_per_second': 2.616, 'epoch': 6.0}
{'loss': 1.3834, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.4025202989578247, 'eval_accuracy@rus.rst.rrt': 0.5705779334500876, 'eval_f1@rus.rst.rrt': 0.3859893398237189, 'eval_precision@rus.rst.rrt': 0.49462183661454434, 'eval_recall@rus.rst.rrt': 0.36872026618663883, 'eval_loss@rus.rst.rrt': 1.4025202989578247, 'eval_runtime': 34.4779, 'eval_samples_per_second': 82.807, 'eval_steps_per_second': 2.61, 'epoch': 6.0}
{'train@rus.rst.rrt_loss': 1.301243543624878, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5995073207966137, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.350116948807448, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4329592850558059, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.331682681868282, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.301243543624878, 'train@rus.rst.rrt_runtime': 344.3248, 'train@rus.rst.rrt_samples_per_second': 83.706, 'train@rus.rst.rrt_steps_per_second': 2.617, 'epoch': 7.0}
{'loss': 1.3614, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.3948917388916016, 'eval_accuracy@rus.rst.rrt': 0.568476357267951, 'eval_f1@rus.rst.rrt': 0.38607552331525014, 'eval_precision@rus.rst.rrt': 0.46572919844530203, 'eval_recall@rus.rst.rrt': 0.3716565388490861, 'eval_loss@rus.rst.rrt': 1.3948917388916016, 'eval_runtime': 34.4425, 'eval_samples_per_second': 82.892, 'eval_steps_per_second': 2.613, 'epoch': 7.0}
{'train@rus.rst.rrt_loss': 1.2879173755645752, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6021441954062868, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.35832957673140514, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4361129412316979, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3421747003769903, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2879174947738647, 'train@rus.rst.rrt_runtime': 344.1959, 'train@rus.rst.rrt_samples_per_second': 83.737, 'train@rus.rst.rrt_steps_per_second': 2.618, 'epoch': 8.0}
{'loss': 1.3446, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.3822596073150635, 'eval_accuracy@rus.rst.rrt': 0.5775831873905429, 'eval_f1@rus.rst.rrt': 0.40533141962091895, 'eval_precision@rus.rst.rrt': 0.46866400993521834, 'eval_recall@rus.rst.rrt': 0.3925093776069187, 'eval_loss@rus.rst.rrt': 1.3822596073150635, 'eval_runtime': 34.4279, 'eval_samples_per_second': 82.927, 'eval_steps_per_second': 2.614, 'epoch': 8.0}
{'train@rus.rst.rrt_loss': 1.2751648426055908, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6063076816320866, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3616503037805235, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4512587361161398, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3429728250968434, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2751647233963013, 'train@rus.rst.rrt_runtime': 344.0708, 'train@rus.rst.rrt_samples_per_second': 83.768, 'train@rus.rst.rrt_steps_per_second': 2.619, 'epoch': 9.0}
{'loss': 1.3356, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.3750056028366089, 'eval_accuracy@rus.rst.rrt': 0.5758318739054291, 'eval_f1@rus.rst.rrt': 0.40237422823532537, 'eval_precision@rus.rst.rrt': 0.4772641312138908, 'eval_recall@rus.rst.rrt': 0.3875337274879012, 'eval_loss@rus.rst.rrt': 1.375005841255188, 'eval_runtime': 34.8281, 'eval_samples_per_second': 81.974, 'eval_steps_per_second': 2.584, 'epoch': 9.0}
{'train@rus.rst.rrt_loss': 1.2684262990951538, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6072791617514399, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3638747173681029, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.45248957550236013, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3431702751663944, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2684264183044434, 'train@rus.rst.rrt_runtime': 343.406, 'train@rus.rst.rrt_samples_per_second': 83.93, 'train@rus.rst.rrt_steps_per_second': 2.624, 'epoch': 10.0}
{'loss': 1.3245, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.3720194101333618, 'eval_accuracy@rus.rst.rrt': 0.5810858143607706, 'eval_f1@rus.rst.rrt': 0.4108168142355925, 'eval_precision@rus.rst.rrt': 0.49280034236167936, 'eval_recall@rus.rst.rrt': 0.3925515689167604, 'eval_loss@rus.rst.rrt': 1.3720195293426514, 'eval_runtime': 34.4155, 'eval_samples_per_second': 82.957, 'eval_steps_per_second': 2.615, 'epoch': 10.0}
{'train@rus.rst.rrt_loss': 1.2646352052688599, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6090139476788564, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3659446116190346, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.44717292135652387, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34784031809233595, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2646352052688599, 'train@rus.rst.rrt_runtime': 343.5642, 'train@rus.rst.rrt_samples_per_second': 83.891, 'train@rus.rst.rrt_steps_per_second': 2.623, 'epoch': 11.0}
{'loss': 1.3185, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.3716366291046143, 'eval_accuracy@rus.rst.rrt': 0.578984238178634, 'eval_f1@rus.rst.rrt': 0.4141683873883095, 'eval_precision@rus.rst.rrt': 0.4938050160487218, 'eval_recall@rus.rst.rrt': 0.39831343793457663, 'eval_loss@rus.rst.rrt': 1.3716367483139038, 'eval_runtime': 34.3763, 'eval_samples_per_second': 83.051, 'eval_steps_per_second': 2.618, 'epoch': 11.0}
{'train@rus.rst.rrt_loss': 1.2634375095367432, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6084241204635348, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3657982963805526, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.44775384526899775, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3475880943503326, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2634375095367432, 'train@rus.rst.rrt_runtime': 343.6271, 'train@rus.rst.rrt_samples_per_second': 83.876, 'train@rus.rst.rrt_steps_per_second': 2.622, 'epoch': 12.0}
{'loss': 1.3134, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.3699156045913696, 'eval_accuracy@rus.rst.rrt': 0.5796847635726795, 'eval_f1@rus.rst.rrt': 0.4132160085179289, 'eval_precision@rus.rst.rrt': 0.49216101921705235, 'eval_recall@rus.rst.rrt': 0.3975230295880163, 'eval_loss@rus.rst.rrt': 1.3699156045913696, 'eval_runtime': 34.3484, 'eval_samples_per_second': 83.119, 'eval_steps_per_second': 2.62, 'epoch': 12.0}
{'train_runtime': 13297.1337, 'train_samples_per_second': 26.01, 'train_steps_per_second': 0.813, 'train_loss': 1.4656913438374317, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.4657
  train_runtime            = 3:41:37.13
  train_samples_per_second =      26.01
  train_steps_per_second   =      0.813
{'train@zho.rst.sctb_loss': 3.4699602127075195, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.03416856492027335, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.01817060336940824, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.04510867531627312, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.0560897435897436, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.4699594974517822, 'train@zho.rst.sctb_runtime': 5.4457, 'train@zho.rst.sctb_samples_per_second': 80.614, 'train@zho.rst.sctb_steps_per_second': 2.571, 'epoch': 1.0}
{'loss': 3.7804, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.4369614124298096, 'eval_accuracy@zho.rst.sctb': 0.02127659574468085, 'eval_f1@zho.rst.sctb': 0.003995157384987894, 'eval_precision@zho.rst.sctb': 0.0022640195053988156, 'eval_recall@zho.rst.sctb': 0.05442176870748299, 'eval_loss@zho.rst.sctb': 3.436962366104126, 'eval_runtime': 1.4, 'eval_samples_per_second': 67.145, 'eval_steps_per_second': 2.143, 'epoch': 1.0}
{'train@zho.rst.sctb_loss': 3.1100168228149414, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.09111617312072894, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.038797990454368506, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.06546440721342416, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.05903582718651212, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.1100168228149414, 'train@zho.rst.sctb_runtime': 5.4473, 'train@zho.rst.sctb_samples_per_second': 80.59, 'train@zho.rst.sctb_steps_per_second': 2.57, 'epoch': 2.0}
{'loss': 3.2823, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.125067949295044, 'eval_accuracy@zho.rst.sctb': 0.05319148936170213, 'eval_f1@zho.rst.sctb': 0.012110182979688286, 'eval_precision@zho.rst.sctb': 0.019074986316365627, 'eval_recall@zho.rst.sctb': 0.058886054421768703, 'eval_loss@zho.rst.sctb': 3.1250674724578857, 'eval_runtime': 1.4174, 'eval_samples_per_second': 66.32, 'eval_steps_per_second': 2.117, 'epoch': 2.0}
{'train@zho.rst.sctb_loss': 2.8423430919647217, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.028323331514820877, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.051803459698196545, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04300579557428873, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.842343330383301, 'train@zho.rst.sctb_runtime': 5.4211, 'train@zho.rst.sctb_samples_per_second': 80.98, 'train@zho.rst.sctb_steps_per_second': 2.583, 'epoch': 3.0}
{'loss': 2.988, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.892383098602295, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.024399842581660766, 'eval_precision@zho.rst.sctb': 0.01658640984483681, 'eval_recall@zho.rst.sctb': 0.046130952380952384, 'eval_loss@zho.rst.sctb': 2.8923821449279785, 'eval_runtime': 1.4014, 'eval_samples_per_second': 67.075, 'eval_steps_per_second': 2.141, 'epoch': 3.0}
{'train@zho.rst.sctb_loss': 2.6273140907287598, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.6273138523101807, 'train@zho.rst.sctb_runtime': 5.4524, 'train@zho.rst.sctb_samples_per_second': 80.515, 'train@zho.rst.sctb_steps_per_second': 2.568, 'epoch': 4.0}
{'loss': 2.7458, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.7088871002197266, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.0256, 'eval_precision@zho.rst.sctb': 0.017204301075268817, 'eval_recall@zho.rst.sctb': 0.05, 'eval_loss@zho.rst.sctb': 2.7088875770568848, 'eval_runtime': 1.4158, 'eval_samples_per_second': 66.396, 'eval_steps_per_second': 2.119, 'epoch': 4.0}
{'train@zho.rst.sctb_loss': 2.4713237285614014, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.4713234901428223, 'train@zho.rst.sctb_runtime': 5.405, 'train@zho.rst.sctb_samples_per_second': 81.221, 'train@zho.rst.sctb_steps_per_second': 2.59, 'epoch': 5.0}
{'loss': 2.58, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.584617853164673, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.584618330001831, 'eval_runtime': 1.4221, 'eval_samples_per_second': 66.102, 'eval_steps_per_second': 2.11, 'epoch': 5.0}
{'train@zho.rst.sctb_loss': 2.355283498764038, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019263755112811715, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012849850378454496, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.355283498764038, 'train@zho.rst.sctb_runtime': 5.4444, 'train@zho.rst.sctb_samples_per_second': 80.634, 'train@zho.rst.sctb_steps_per_second': 2.571, 'epoch': 6.0}
{'loss': 2.4495, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.5025880336761475, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.5025882720947266, 'eval_runtime': 1.4046, 'eval_samples_per_second': 66.925, 'eval_steps_per_second': 2.136, 'epoch': 6.0}
{'train@zho.rst.sctb_loss': 2.263213872909546, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3348519362186788, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.02082440787830231, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.022554881313582006, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.039007819865786705, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.263213872909546, 'train@zho.rst.sctb_runtime': 5.4397, 'train@zho.rst.sctb_samples_per_second': 80.703, 'train@zho.rst.sctb_steps_per_second': 2.574, 'epoch': 7.0}
{'loss': 2.3592, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.4354615211486816, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.4354612827301025, 'eval_runtime': 1.4111, 'eval_samples_per_second': 66.617, 'eval_steps_per_second': 2.126, 'epoch': 7.0}
{'train@zho.rst.sctb_loss': 2.1993110179901123, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.35079726651480636, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.026294136798176812, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03244905452542684, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04198325106760578, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.1993110179901123, 'train@zho.rst.sctb_runtime': 5.4463, 'train@zho.rst.sctb_samples_per_second': 80.606, 'train@zho.rst.sctb_steps_per_second': 2.571, 'epoch': 8.0}
{'loss': 2.2734, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.3887939453125, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026947368421052633, 'eval_precision@zho.rst.sctb': 0.018109790605546124, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.3887932300567627, 'eval_runtime': 1.4072, 'eval_samples_per_second': 66.8, 'eval_steps_per_second': 2.132, 'epoch': 8.0}
{'train@zho.rst.sctb_loss': 2.1530065536499023, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.4009111617312073, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.0380952380952381, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.04140743633129419, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.050890133658698905, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.1530067920684814, 'train@zho.rst.sctb_runtime': 5.429, 'train@zho.rst.sctb_samples_per_second': 80.862, 'train@zho.rst.sctb_steps_per_second': 2.579, 'epoch': 9.0}
{'loss': 2.2261, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.3543407917022705, 'eval_accuracy@zho.rst.sctb': 0.35106382978723405, 'eval_f1@zho.rst.sctb': 0.032648694908001714, 'eval_precision@zho.rst.sctb': 0.036051667630615, 'eval_recall@zho.rst.sctb': 0.05572755417956656, 'eval_loss@zho.rst.sctb': 2.3543407917022705, 'eval_runtime': 1.4294, 'eval_samples_per_second': 65.763, 'eval_steps_per_second': 2.099, 'epoch': 9.0}
{'train@zho.rst.sctb_loss': 2.121220588684082, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.4328018223234624, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.04385151178328305, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.04291185897435897, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.056699572957683984, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.121220588684082, 'train@zho.rst.sctb_runtime': 5.4261, 'train@zho.rst.sctb_samples_per_second': 80.905, 'train@zho.rst.sctb_steps_per_second': 2.58, 'epoch': 10.0}
{'loss': 2.1935, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.330963373184204, 'eval_accuracy@zho.rst.sctb': 0.35106382978723405, 'eval_f1@zho.rst.sctb': 0.032648694908001714, 'eval_precision@zho.rst.sctb': 0.036051667630615, 'eval_recall@zho.rst.sctb': 0.05572755417956656, 'eval_loss@zho.rst.sctb': 2.3309624195098877, 'eval_runtime': 1.4085, 'eval_samples_per_second': 66.74, 'eval_steps_per_second': 2.13, 'epoch': 10.0}
{'train@zho.rst.sctb_loss': 2.1041340827941895, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.43507972665148065, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.044251923510505935, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.04243606005537193, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.05724585436193223, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.1041340827941895, 'train@zho.rst.sctb_runtime': 5.4389, 'train@zho.rst.sctb_samples_per_second': 80.715, 'train@zho.rst.sctb_steps_per_second': 2.574, 'epoch': 11.0}
{'loss': 2.1823, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.319641351699829, 'eval_accuracy@zho.rst.sctb': 0.3617021276595745, 'eval_f1@zho.rst.sctb': 0.037635071284769304, 'eval_precision@zho.rst.sctb': 0.04502923976608188, 'eval_recall@zho.rst.sctb': 0.058823529411764705, 'eval_loss@zho.rst.sctb': 2.319641351699829, 'eval_runtime': 1.3998, 'eval_samples_per_second': 67.152, 'eval_steps_per_second': 2.143, 'epoch': 11.0}
{'train@zho.rst.sctb_loss': 2.0985026359558105, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.4328018223234624, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.04390431574897595, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.041619762351469664, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.05698241916699018, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.0985031127929688, 'train@zho.rst.sctb_runtime': 5.4406, 'train@zho.rst.sctb_samples_per_second': 80.69, 'train@zho.rst.sctb_steps_per_second': 2.573, 'epoch': 12.0}
{'loss': 2.1524, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.315600872039795, 'eval_accuracy@zho.rst.sctb': 0.3723404255319149, 'eval_f1@zho.rst.sctb': 0.04219225750326229, 'eval_precision@zho.rst.sctb': 0.05050266114725015, 'eval_recall@zho.rst.sctb': 0.06191950464396285, 'eval_loss@zho.rst.sctb': 2.315601110458374, 'eval_runtime': 1.3937, 'eval_samples_per_second': 67.447, 'eval_steps_per_second': 2.153, 'epoch': 12.0}
{'train_runtime': 213.1219, 'train_samples_per_second': 24.718, 'train_steps_per_second': 0.788, 'train_loss': 2.6010669753665017, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.4657
  train_runtime            = 3:41:37.13
  train_samples_per_second =      26.01
  train_steps_per_second   =      0.813
-------------------------------------------------------------------
Lang1:  spa.rst.rststb    Lang2:  zho.rst.sctb
Saving run to:  runs/full_shot/FullShot=v4_spa.rst.rststb_zho.rst.sctb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2240 examples
read 383 examples
read 426 examples
read 439 examples
read 94 examples
read 159 examples
Total prediction labels:  30
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (zh): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=30, bias=True)
    )
  )
)
{'train@spa.rst.rststb_loss': 2.7112064361572266, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.23482142857142857, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.030140443103006485, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.04336152622111526, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.04551243247462491, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.7112066745758057, 'train@spa.rst.rststb_runtime': 26.7715, 'train@spa.rst.rststb_samples_per_second': 83.671, 'train@spa.rst.rststb_steps_per_second': 2.615, 'epoch': 1.0}
{'loss': 3.0769, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.774235486984253, 'eval_accuracy@spa.rst.rststb': 0.20365535248041775, 'eval_f1@spa.rst.rststb': 0.022463768115942032, 'eval_precision@spa.rst.rststb': 0.03651261265493199, 'eval_recall@spa.rst.rststb': 0.044296329927885965, 'eval_loss@spa.rst.rststb': 2.774235486984253, 'eval_runtime': 4.8491, 'eval_samples_per_second': 78.983, 'eval_steps_per_second': 2.475, 'epoch': 1.0}
{'train@spa.rst.rststb_loss': 2.525395631790161, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.265625, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.03937936522244536, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.04974193603602708, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.05321570820030127, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.525395393371582, 'train@spa.rst.rststb_runtime': 26.8831, 'train@spa.rst.rststb_samples_per_second': 83.324, 'train@spa.rst.rststb_steps_per_second': 2.604, 'epoch': 2.0}
{'loss': 2.644, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.6421501636505127, 'eval_accuracy@spa.rst.rststb': 0.21409921671018275, 'eval_f1@spa.rst.rststb': 0.02854462370153089, 'eval_precision@spa.rst.rststb': 0.05166380499122291, 'eval_recall@spa.rst.rststb': 0.04815032756366265, 'eval_loss@spa.rst.rststb': 2.6421501636505127, 'eval_runtime': 4.8439, 'eval_samples_per_second': 79.069, 'eval_steps_per_second': 2.477, 'epoch': 2.0}
{'train@spa.rst.rststb_loss': 2.4156742095947266, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.3138392857142857, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.05011630111457215, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.06728754790148778, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.06833814676393406, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.4156739711761475, 'train@spa.rst.rststb_runtime': 26.8865, 'train@spa.rst.rststb_samples_per_second': 83.313, 'train@spa.rst.rststb_steps_per_second': 2.604, 'epoch': 3.0}
{'loss': 2.5049, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.5649359226226807, 'eval_accuracy@spa.rst.rststb': 0.2741514360313316, 'eval_f1@spa.rst.rststb': 0.052867749865979106, 'eval_precision@spa.rst.rststb': 0.09001650746854899, 'eval_recall@spa.rst.rststb': 0.06989983254573895, 'eval_loss@spa.rst.rststb': 2.5649356842041016, 'eval_runtime': 4.8336, 'eval_samples_per_second': 79.236, 'eval_steps_per_second': 2.483, 'epoch': 3.0}
{'train@spa.rst.rststb_loss': 2.3211779594421387, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.3464285714285714, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.07259022793201272, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.09015967835407433, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.08934905689523365, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.3211779594421387, 'train@spa.rst.rststb_runtime': 26.8858, 'train@spa.rst.rststb_samples_per_second': 83.315, 'train@spa.rst.rststb_steps_per_second': 2.604, 'epoch': 4.0}
{'loss': 2.4142, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.4969594478607178, 'eval_accuracy@spa.rst.rststb': 0.32114882506527415, 'eval_f1@spa.rst.rststb': 0.07499542463033174, 'eval_precision@spa.rst.rststb': 0.06899750375648296, 'eval_recall@spa.rst.rststb': 0.09584737542088925, 'eval_loss@spa.rst.rststb': 2.4969594478607178, 'eval_runtime': 4.8422, 'eval_samples_per_second': 79.096, 'eval_steps_per_second': 2.478, 'epoch': 4.0}
{'train@spa.rst.rststb_loss': 2.2401485443115234, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.37589285714285714, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.08579436814645149, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.0822702048224059, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.10710601139308981, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.2401480674743652, 'train@spa.rst.rststb_runtime': 26.8726, 'train@spa.rst.rststb_samples_per_second': 83.356, 'train@spa.rst.rststb_steps_per_second': 2.605, 'epoch': 5.0}
{'loss': 2.3253, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.4396071434020996, 'eval_accuracy@spa.rst.rststb': 0.3577023498694517, 'eval_f1@spa.rst.rststb': 0.10357084571282037, 'eval_precision@spa.rst.rststb': 0.10151011392699373, 'eval_recall@spa.rst.rststb': 0.12772255857565415, 'eval_loss@spa.rst.rststb': 2.4396069049835205, 'eval_runtime': 4.8556, 'eval_samples_per_second': 78.878, 'eval_steps_per_second': 2.471, 'epoch': 5.0}
{'train@spa.rst.rststb_loss': 2.174534797668457, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.3830357142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.09006618864721587, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.11896095867919954, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.11304525786974004, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.174534797668457, 'train@spa.rst.rststb_runtime': 26.8808, 'train@spa.rst.rststb_samples_per_second': 83.331, 'train@spa.rst.rststb_steps_per_second': 2.604, 'epoch': 6.0}
{'loss': 2.2541, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.389894962310791, 'eval_accuracy@spa.rst.rststb': 0.360313315926893, 'eval_f1@spa.rst.rststb': 0.10567432720641831, 'eval_precision@spa.rst.rststb': 0.09807785090064591, 'eval_recall@spa.rst.rststb': 0.13296019811598223, 'eval_loss@spa.rst.rststb': 2.389894723892212, 'eval_runtime': 4.8404, 'eval_samples_per_second': 79.126, 'eval_steps_per_second': 2.479, 'epoch': 6.0}
{'train@spa.rst.rststb_loss': 2.118340492248535, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.3964285714285714, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.09439110358214868, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.11760821853316554, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.1212910605928575, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.1183407306671143, 'train@spa.rst.rststb_runtime': 26.8699, 'train@spa.rst.rststb_samples_per_second': 83.365, 'train@spa.rst.rststb_steps_per_second': 2.605, 'epoch': 7.0}
{'loss': 2.2029, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.3485381603240967, 'eval_accuracy@spa.rst.rststb': 0.3629242819843342, 'eval_f1@spa.rst.rststb': 0.10388442998710155, 'eval_precision@spa.rst.rststb': 0.09358323457968915, 'eval_recall@spa.rst.rststb': 0.13441442161285433, 'eval_loss@spa.rst.rststb': 2.348538398742676, 'eval_runtime': 4.8544, 'eval_samples_per_second': 78.897, 'eval_steps_per_second': 2.472, 'epoch': 7.0}
{'train@spa.rst.rststb_loss': 2.0759198665618896, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4044642857142857, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.0967176721852165, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.15383372173969176, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.12461670956942658, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0759198665618896, 'train@spa.rst.rststb_runtime': 26.7965, 'train@spa.rst.rststb_samples_per_second': 83.593, 'train@spa.rst.rststb_steps_per_second': 2.612, 'epoch': 8.0}
{'loss': 2.1526, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.316084861755371, 'eval_accuracy@spa.rst.rststb': 0.3629242819843342, 'eval_f1@spa.rst.rststb': 0.1062347257743677, 'eval_precision@spa.rst.rststb': 0.09226688313070232, 'eval_recall@spa.rst.rststb': 0.1397366310489878, 'eval_loss@spa.rst.rststb': 2.31608510017395, 'eval_runtime': 4.8396, 'eval_samples_per_second': 79.139, 'eval_steps_per_second': 2.48, 'epoch': 8.0}
{'train@spa.rst.rststb_loss': 2.0449934005737305, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.41294642857142855, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.09936473844025463, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.17043712151404902, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.12952047618318177, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0449936389923096, 'train@spa.rst.rststb_runtime': 26.7886, 'train@spa.rst.rststb_samples_per_second': 83.618, 'train@spa.rst.rststb_steps_per_second': 2.613, 'epoch': 9.0}
{'loss': 2.1181, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.292670488357544, 'eval_accuracy@spa.rst.rststb': 0.3629242819843342, 'eval_f1@spa.rst.rststb': 0.10172592235554889, 'eval_precision@spa.rst.rststb': 0.08486180076472129, 'eval_recall@spa.rst.rststb': 0.1399337741173554, 'eval_loss@spa.rst.rststb': 2.292670488357544, 'eval_runtime': 4.8746, 'eval_samples_per_second': 78.57, 'eval_steps_per_second': 2.462, 'epoch': 9.0}
{'train@spa.rst.rststb_loss': 2.0225141048431396, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.41696428571428573, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10215282853252862, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.15091802797865625, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13119164277679798, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0225143432617188, 'train@spa.rst.rststb_runtime': 26.8742, 'train@spa.rst.rststb_samples_per_second': 83.351, 'train@spa.rst.rststb_steps_per_second': 2.605, 'epoch': 10.0}
{'loss': 2.0857, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.2766079902648926, 'eval_accuracy@spa.rst.rststb': 0.370757180156658, 'eval_f1@spa.rst.rststb': 0.10526601561947782, 'eval_precision@spa.rst.rststb': 0.08944904496726673, 'eval_recall@spa.rst.rststb': 0.14188056191748516, 'eval_loss@spa.rst.rststb': 2.2766079902648926, 'eval_runtime': 4.849, 'eval_samples_per_second': 78.986, 'eval_steps_per_second': 2.475, 'epoch': 10.0}
{'train@spa.rst.rststb_loss': 2.0096614360809326, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.41875, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10402931493622934, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.15511082674365714, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13256113771864522, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0096614360809326, 'train@spa.rst.rststb_runtime': 26.8713, 'train@spa.rst.rststb_samples_per_second': 83.36, 'train@spa.rst.rststb_steps_per_second': 2.605, 'epoch': 11.0}
{'loss': 2.068, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.265315294265747, 'eval_accuracy@spa.rst.rststb': 0.3733681462140992, 'eval_f1@spa.rst.rststb': 0.10600507329107071, 'eval_precision@spa.rst.rststb': 0.08999147006635959, 'eval_recall@spa.rst.rststb': 0.14252949118419508, 'eval_loss@spa.rst.rststb': 2.265315294265747, 'eval_runtime': 4.857, 'eval_samples_per_second': 78.856, 'eval_steps_per_second': 2.471, 'epoch': 11.0}
{'train@spa.rst.rststb_loss': 2.005498170852661, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.41830357142857144, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10384168775603464, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.15564968354792558, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13234056886319095, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.005498170852661, 'train@spa.rst.rststb_runtime': 26.9223, 'train@spa.rst.rststb_samples_per_second': 83.202, 'train@spa.rst.rststb_steps_per_second': 2.6, 'epoch': 12.0}
{'loss': 2.0605, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.261500358581543, 'eval_accuracy@spa.rst.rststb': 0.381201044386423, 'eval_f1@spa.rst.rststb': 0.10899485076224515, 'eval_precision@spa.rst.rststb': 0.09377307113287704, 'eval_recall@spa.rst.rststb': 0.14437770745014109, 'eval_loss@spa.rst.rststb': 2.261500358581543, 'eval_runtime': 4.8718, 'eval_samples_per_second': 78.615, 'eval_steps_per_second': 2.463, 'epoch': 12.0}
{'train_runtime': 1060.2679, 'train_samples_per_second': 25.352, 'train_steps_per_second': 0.792, 'train_loss': 2.3255985078357515, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.3256
  train_runtime            = 0:17:40.26
  train_samples_per_second =     25.352
  train_steps_per_second   =      0.792
{'train@zho.rst.sctb_loss': 3.2029526233673096, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.04328018223234624, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.006991711591896834, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.02633491937615649, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04162276080084299, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.2029526233673096, 'train@zho.rst.sctb_runtime': 5.4278, 'train@zho.rst.sctb_samples_per_second': 80.88, 'train@zho.rst.sctb_steps_per_second': 2.579, 'epoch': 1.0}
{'loss': 3.3658, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.2186279296875, 'eval_accuracy@zho.rst.sctb': 0.05319148936170213, 'eval_f1@zho.rst.sctb': 0.010657077476299445, 'eval_precision@zho.rst.sctb': 0.015800370491446006, 'eval_recall@zho.rst.sctb': 0.05921052631578947, 'eval_loss@zho.rst.sctb': 3.2186286449432373, 'eval_runtime': 1.3885, 'eval_samples_per_second': 67.7, 'eval_steps_per_second': 2.161, 'epoch': 1.0}
{'train@zho.rst.sctb_loss': 2.9823334217071533, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.33029612756264237, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019811449651591744, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.013373916251614094, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038198103266596416, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.9823334217071533, 'train@zho.rst.sctb_runtime': 5.4121, 'train@zho.rst.sctb_samples_per_second': 81.115, 'train@zho.rst.sctb_steps_per_second': 2.587, 'epoch': 2.0}
{'loss': 3.1126, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.0079338550567627, 'eval_accuracy@zho.rst.sctb': 0.3191489361702128, 'eval_f1@zho.rst.sctb': 0.025884383088869718, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.049342105263157895, 'eval_loss@zho.rst.sctb': 3.0079338550567627, 'eval_runtime': 1.391, 'eval_samples_per_second': 67.579, 'eval_steps_per_second': 2.157, 'epoch': 2.0}
{'train@zho.rst.sctb_loss': 2.7840378284454346, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019230769230769232, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.01282051282051282, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.7840375900268555, 'train@zho.rst.sctb_runtime': 5.4468, 'train@zho.rst.sctb_samples_per_second': 80.598, 'train@zho.rst.sctb_steps_per_second': 2.57, 'epoch': 3.0}
{'loss': 2.9129, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.8208158016204834, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.8208162784576416, 'eval_runtime': 1.3927, 'eval_samples_per_second': 67.497, 'eval_steps_per_second': 2.154, 'epoch': 3.0}
{'train@zho.rst.sctb_loss': 2.614455461502075, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.614455223083496, 'train@zho.rst.sctb_runtime': 5.4231, 'train@zho.rst.sctb_samples_per_second': 80.95, 'train@zho.rst.sctb_steps_per_second': 2.582, 'epoch': 4.0}
{'loss': 2.7289, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.6633501052856445, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.6633501052856445, 'eval_runtime': 1.3906, 'eval_samples_per_second': 67.595, 'eval_steps_per_second': 2.157, 'epoch': 4.0}
{'train@zho.rst.sctb_loss': 2.475550413131714, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.33712984054669703, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.022060957910014514, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.051311388839992964, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.03991291727140784, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.475550651550293, 'train@zho.rst.sctb_runtime': 5.4503, 'train@zho.rst.sctb_samples_per_second': 80.545, 'train@zho.rst.sctb_steps_per_second': 2.569, 'epoch': 5.0}
{'loss': 2.5824, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.53495717048645, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.534956932067871, 'eval_runtime': 1.3873, 'eval_samples_per_second': 67.755, 'eval_steps_per_second': 2.162, 'epoch': 5.0}
{'train@zho.rst.sctb_loss': 2.36226749420166, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.34851936218678814, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.028404755567385327, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.05146011396011396, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04354136429608128, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.362267255783081, 'train@zho.rst.sctb_runtime': 5.4087, 'train@zho.rst.sctb_samples_per_second': 81.166, 'train@zho.rst.sctb_steps_per_second': 2.588, 'epoch': 6.0}
{'loss': 2.4564, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.4338138103485107, 'eval_accuracy@zho.rst.sctb': 0.35106382978723405, 'eval_f1@zho.rst.sctb': 0.04056482670089859, 'eval_precision@zho.rst.sctb': 0.053017158280316175, 'eval_recall@zho.rst.sctb': 0.05975877192982457, 'eval_loss@zho.rst.sctb': 2.4338138103485107, 'eval_runtime': 1.387, 'eval_samples_per_second': 67.774, 'eval_steps_per_second': 2.163, 'epoch': 6.0}
{'train@zho.rst.sctb_loss': 2.2765491008758545, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.37813211845102507, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.03962977615705702, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.08128888061091451, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.052654495454892675, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.2765491008758545, 'train@zho.rst.sctb_runtime': 5.455, 'train@zho.rst.sctb_samples_per_second': 80.477, 'train@zho.rst.sctb_steps_per_second': 2.566, 'epoch': 7.0}
{'loss': 2.3678, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.3579235076904297, 'eval_accuracy@zho.rst.sctb': 0.39361702127659576, 'eval_f1@zho.rst.sctb': 0.05683880547764395, 'eval_precision@zho.rst.sctb': 0.05100250626566416, 'eval_recall@zho.rst.sctb': 0.07730263157894737, 'eval_loss@zho.rst.sctb': 2.357923746109009, 'eval_runtime': 1.3836, 'eval_samples_per_second': 67.937, 'eval_steps_per_second': 2.168, 'epoch': 7.0}
{'train@zho.rst.sctb_loss': 2.224977970123291, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.41002277904328016, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.05017306852169238, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.08326923076923078, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.058643342754564204, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.224977970123291, 'train@zho.rst.sctb_runtime': 5.4283, 'train@zho.rst.sctb_samples_per_second': 80.873, 'train@zho.rst.sctb_steps_per_second': 2.579, 'epoch': 8.0}
{'loss': 2.2934, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.3193438053131104, 'eval_accuracy@zho.rst.sctb': 0.425531914893617, 'eval_f1@zho.rst.sctb': 0.06677224033535163, 'eval_precision@zho.rst.sctb': 0.05658019693107412, 'eval_recall@zho.rst.sctb': 0.09046052631578948, 'eval_loss@zho.rst.sctb': 2.319343328475952, 'eval_runtime': 1.3765, 'eval_samples_per_second': 68.288, 'eval_steps_per_second': 2.179, 'epoch': 8.0}
{'train@zho.rst.sctb_loss': 2.1895852088928223, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.43735763097949887, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.05854276762413246, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.07735527994433007, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.06488840497968384, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.1895854473114014, 'train@zho.rst.sctb_runtime': 5.4165, 'train@zho.rst.sctb_samples_per_second': 81.049, 'train@zho.rst.sctb_steps_per_second': 2.585, 'epoch': 9.0}
{'loss': 2.2511, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.29120135307312, 'eval_accuracy@zho.rst.sctb': 0.425531914893617, 'eval_f1@zho.rst.sctb': 0.07750118539592224, 'eval_precision@zho.rst.sctb': 0.09388185654008438, 'eval_recall@zho.rst.sctb': 0.09336300309597523, 'eval_loss@zho.rst.sctb': 2.291200876235962, 'eval_runtime': 1.3912, 'eval_samples_per_second': 67.569, 'eval_steps_per_second': 2.156, 'epoch': 9.0}
{'train@zho.rst.sctb_loss': 2.166194200515747, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.4533029612756264, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.06259074087733722, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.07262523600216057, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.06960941481285375, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.166194438934326, 'train@zho.rst.sctb_runtime': 5.4204, 'train@zho.rst.sctb_samples_per_second': 80.99, 'train@zho.rst.sctb_steps_per_second': 2.583, 'epoch': 10.0}
{'loss': 2.2285, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.2718818187713623, 'eval_accuracy@zho.rst.sctb': 0.425531914893617, 'eval_f1@zho.rst.sctb': 0.07834904373018706, 'eval_precision@zho.rst.sctb': 0.08042834358623834, 'eval_recall@zho.rst.sctb': 0.09336300309597523, 'eval_loss@zho.rst.sctb': 2.271881580352783, 'eval_runtime': 1.4035, 'eval_samples_per_second': 66.976, 'eval_steps_per_second': 2.138, 'epoch': 10.0}
{'train@zho.rst.sctb_loss': 2.154172420501709, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.4646924829157175, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.06508481262327417, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.07294676521371438, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.07227536852150857, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.154172658920288, 'train@zho.rst.sctb_runtime': 5.4517, 'train@zho.rst.sctb_samples_per_second': 80.525, 'train@zho.rst.sctb_steps_per_second': 2.568, 'epoch': 11.0}
{'loss': 2.1943, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.262529134750366, 'eval_accuracy@zho.rst.sctb': 0.425531914893617, 'eval_f1@zho.rst.sctb': 0.07834904373018706, 'eval_precision@zho.rst.sctb': 0.08042834358623834, 'eval_recall@zho.rst.sctb': 0.09336300309597523, 'eval_loss@zho.rst.sctb': 2.2625293731689453, 'eval_runtime': 1.3762, 'eval_samples_per_second': 68.303, 'eval_steps_per_second': 2.18, 'epoch': 11.0}
{'train@zho.rst.sctb_loss': 2.1507675647735596, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.4624145785876993, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.06522489353587346, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.07199234114213872, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.07247418753655915, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.1507673263549805, 'train@zho.rst.sctb_runtime': 5.4378, 'train@zho.rst.sctb_samples_per_second': 80.731, 'train@zho.rst.sctb_steps_per_second': 2.575, 'epoch': 12.0}
{'loss': 2.2006, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.260349750518799, 'eval_accuracy@zho.rst.sctb': 0.425531914893617, 'eval_f1@zho.rst.sctb': 0.07834904373018706, 'eval_precision@zho.rst.sctb': 0.08042834358623834, 'eval_recall@zho.rst.sctb': 0.09336300309597523, 'eval_loss@zho.rst.sctb': 2.260350227355957, 'eval_runtime': 3.6717, 'eval_samples_per_second': 25.601, 'eval_steps_per_second': 0.817, 'epoch': 12.0}
{'train_runtime': 215.2359, 'train_samples_per_second': 24.475, 'train_steps_per_second': 0.781, 'train_loss': 2.557899588630313, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.3256
  train_runtime            = 0:17:40.26
  train_samples_per_second =     25.352
  train_steps_per_second   =      0.792
-------------------------------------------------------------------
Lang1:  spa.rst.sctb    Lang2:  zho.rst.sctb
Saving run to:  runs/full_shot/FullShot=v4_spa.rst.sctb_zho.rst.sctb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 439 examples
read 94 examples
read 159 examples
read 439 examples
read 94 examples
read 159 examples
Total prediction labels:  26
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (zh): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=26, bias=True)
    )
  )
)
{'train@spa.rst.sctb_loss': 2.9898083209991455, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3439635535307517, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02306314312702964, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.022804972804972807, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.042284946236559144, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.9898080825805664, 'train@spa.rst.sctb_runtime': 5.446, 'train@spa.rst.sctb_samples_per_second': 80.609, 'train@spa.rst.sctb_steps_per_second': 2.571, 'epoch': 1.0}
{'loss': 3.1503, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.9850006103515625, 'eval_accuracy@spa.rst.sctb': 0.3829787234042553, 'eval_f1@spa.rst.sctb': 0.04647058823529411, 'eval_precision@spa.rst.sctb': 0.05172413793103448, 'eval_recall@spa.rst.sctb': 0.06811145510835914, 'eval_loss@spa.rst.sctb': 2.984999656677246, 'eval_runtime': 1.3676, 'eval_samples_per_second': 68.733, 'eval_steps_per_second': 2.194, 'epoch': 1.0}
{'train@spa.rst.sctb_loss': 2.7399938106536865, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3439635535307517, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02300114547537228, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.026275903880070544, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.042284946236559144, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.7399938106536865, 'train@spa.rst.sctb_runtime': 5.4763, 'train@spa.rst.sctb_samples_per_second': 80.163, 'train@spa.rst.sctb_steps_per_second': 2.556, 'epoch': 2.0}
{'loss': 2.8842, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.7369184494018555, 'eval_accuracy@spa.rst.sctb': 0.3829787234042553, 'eval_f1@spa.rst.sctb': 0.04652844744455159, 'eval_precision@spa.rst.sctb': 0.05710508922670192, 'eval_recall@spa.rst.sctb': 0.06811145510835914, 'eval_loss@spa.rst.sctb': 2.7369186878204346, 'eval_runtime': 1.3744, 'eval_samples_per_second': 68.393, 'eval_steps_per_second': 2.183, 'epoch': 2.0}
{'train@spa.rst.sctb_loss': 2.509404182434082, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.34851936218678814, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.023955129997204363, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03940092165898618, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04301075268817204, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.5094046592712402, 'train@spa.rst.sctb_runtime': 5.4754, 'train@spa.rst.sctb_samples_per_second': 80.177, 'train@spa.rst.sctb_steps_per_second': 2.557, 'epoch': 3.0}
{'loss': 2.6676, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.5157151222229004, 'eval_accuracy@spa.rst.sctb': 0.3829787234042553, 'eval_f1@spa.rst.sctb': 0.04690911359241469, 'eval_precision@spa.rst.sctb': 0.06568627450980392, 'eval_recall@spa.rst.sctb': 0.06811145510835914, 'eval_loss@spa.rst.sctb': 2.5157151222229004, 'eval_runtime': 1.3935, 'eval_samples_per_second': 67.456, 'eval_steps_per_second': 2.153, 'epoch': 3.0}
{'train@spa.rst.sctb_loss': 2.342041254043579, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.35079726651480636, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.024807826694619145, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.04221195791634591, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04345878136200717, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.3420422077178955, 'train@spa.rst.sctb_runtime': 5.4695, 'train@spa.rst.sctb_samples_per_second': 80.264, 'train@spa.rst.sctb_steps_per_second': 2.56, 'epoch': 4.0}
{'loss': 2.4566, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.363999843597412, 'eval_accuracy@spa.rst.sctb': 0.3723404255319149, 'eval_f1@spa.rst.sctb': 0.042004485078488876, 'eval_precision@spa.rst.sctb': 0.06054729584141348, 'eval_recall@spa.rst.sctb': 0.06501547987616098, 'eval_loss@spa.rst.sctb': 2.363999605178833, 'eval_runtime': 1.3792, 'eval_samples_per_second': 68.155, 'eval_steps_per_second': 2.175, 'epoch': 4.0}
{'train@spa.rst.sctb_loss': 2.2444570064544678, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3530751708428246, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.025632707834264926, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.033542197677711695, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04390681003584229, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.244457244873047, 'train@spa.rst.sctb_runtime': 5.4771, 'train@spa.rst.sctb_samples_per_second': 80.151, 'train@spa.rst.sctb_steps_per_second': 2.556, 'epoch': 5.0}
{'loss': 2.3245, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.2771990299224854, 'eval_accuracy@spa.rst.sctb': 0.3723404255319149, 'eval_f1@spa.rst.sctb': 0.042004485078488876, 'eval_precision@spa.rst.sctb': 0.06054729584141348, 'eval_recall@spa.rst.sctb': 0.06501547987616098, 'eval_loss@spa.rst.sctb': 2.2771987915039062, 'eval_runtime': 1.3981, 'eval_samples_per_second': 67.232, 'eval_steps_per_second': 2.146, 'epoch': 5.0}
{'train@spa.rst.sctb_loss': 2.190842390060425, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.36446469248291574, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.029869897502747742, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03572142286171063, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04631720430107527, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.1908421516418457, 'train@spa.rst.sctb_runtime': 5.4872, 'train@spa.rst.sctb_samples_per_second': 80.005, 'train@spa.rst.sctb_steps_per_second': 2.551, 'epoch': 6.0}
{'loss': 2.2475, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.233112096786499, 'eval_accuracy@spa.rst.sctb': 0.3829787234042553, 'eval_f1@spa.rst.sctb': 0.04690911359241469, 'eval_precision@spa.rst.sctb': 0.06568627450980392, 'eval_recall@spa.rst.sctb': 0.06811145510835914, 'eval_loss@spa.rst.sctb': 2.233112335205078, 'eval_runtime': 1.4036, 'eval_samples_per_second': 66.969, 'eval_steps_per_second': 2.137, 'epoch': 6.0}
{'train@spa.rst.sctb_loss': 2.156529664993286, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.38496583143507973, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03597592883307169, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.034805055698371896, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.050860215053763445, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.156529664993286, 'train@spa.rst.sctb_runtime': 5.4908, 'train@spa.rst.sctb_samples_per_second': 79.951, 'train@spa.rst.sctb_steps_per_second': 2.55, 'epoch': 7.0}
{'loss': 2.2064, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.207097053527832, 'eval_accuracy@spa.rst.sctb': 0.4148936170212766, 'eval_f1@spa.rst.sctb': 0.05752327456181411, 'eval_precision@spa.rst.sctb': 0.058403361344537816, 'eval_recall@spa.rst.sctb': 0.07739938080495357, 'eval_loss@spa.rst.sctb': 2.207097053527832, 'eval_runtime': 1.3784, 'eval_samples_per_second': 68.197, 'eval_steps_per_second': 2.177, 'epoch': 7.0}
{'train@spa.rst.sctb_loss': 2.1357650756835938, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4031890660592255, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03969037405349193, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.0366817235168299, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05444444444444444, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.1357648372650146, 'train@spa.rst.sctb_runtime': 5.4986, 'train@spa.rst.sctb_samples_per_second': 79.839, 'train@spa.rst.sctb_steps_per_second': 2.546, 'epoch': 8.0}
{'loss': 2.1708, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.192500114440918, 'eval_accuracy@spa.rst.sctb': 0.425531914893617, 'eval_f1@spa.rst.sctb': 0.06129510627780523, 'eval_precision@spa.rst.sctb': 0.05519980143956317, 'eval_recall@spa.rst.sctb': 0.08180880007505395, 'eval_loss@spa.rst.sctb': 2.192500352859497, 'eval_runtime': 1.3908, 'eval_samples_per_second': 67.586, 'eval_steps_per_second': 2.157, 'epoch': 8.0}
{'train@spa.rst.sctb_loss': 2.120478868484497, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4054669703872437, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.04050380037117156, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03649825783972125, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05523297491039427, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.120478868484497, 'train@spa.rst.sctb_runtime': 5.4794, 'train@spa.rst.sctb_samples_per_second': 80.118, 'train@spa.rst.sctb_steps_per_second': 2.555, 'epoch': 9.0}
{'loss': 2.1729, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.181304693222046, 'eval_accuracy@spa.rst.sctb': 0.44680851063829785, 'eval_f1@spa.rst.sctb': 0.06690433749257278, 'eval_precision@spa.rst.sctb': 0.05904821821776839, 'eval_recall@spa.rst.sctb': 0.08800075053945022, 'eval_loss@spa.rst.sctb': 2.181304693222046, 'eval_runtime': 1.3892, 'eval_samples_per_second': 67.665, 'eval_steps_per_second': 2.16, 'epoch': 9.0}
{'train@spa.rst.sctb_loss': 2.1097567081451416, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.42369020501138954, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.043709808189966344, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03771246458923513, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05898745519713262, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.1097567081451416, 'train@spa.rst.sctb_runtime': 5.4945, 'train@spa.rst.sctb_samples_per_second': 79.898, 'train@spa.rst.sctb_steps_per_second': 2.548, 'epoch': 10.0}
{'loss': 2.1531, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.174511194229126, 'eval_accuracy@spa.rst.sctb': 0.43617021276595747, 'eval_f1@spa.rst.sctb': 0.0647288155028093, 'eval_precision@spa.rst.sctb': 0.0552734778121775, 'eval_recall@spa.rst.sctb': 0.08621821934515432, 'eval_loss@spa.rst.sctb': 2.1745104789733887, 'eval_runtime': 1.3947, 'eval_samples_per_second': 67.396, 'eval_steps_per_second': 2.151, 'epoch': 10.0}
{'train@spa.rst.sctb_loss': 2.1038901805877686, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4214123006833713, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.04335531135531135, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.037035848047084007, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05870967741935484, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.1038897037506104, 'train@spa.rst.sctb_runtime': 5.5117, 'train@spa.rst.sctb_samples_per_second': 79.649, 'train@spa.rst.sctb_steps_per_second': 2.54, 'epoch': 11.0}
{'loss': 2.1523, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.17024827003479, 'eval_accuracy@spa.rst.sctb': 0.43617021276595747, 'eval_f1@spa.rst.sctb': 0.06425057442099774, 'eval_precision@spa.rst.sctb': 0.05405405405405405, 'eval_recall@spa.rst.sctb': 0.08621821934515432, 'eval_loss@spa.rst.sctb': 2.17024827003479, 'eval_runtime': 1.3741, 'eval_samples_per_second': 68.407, 'eval_steps_per_second': 2.183, 'epoch': 11.0}
{'train@spa.rst.sctb_loss': 2.101914167404175, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.4214123006833713, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.04335531135531135, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.037035848047084007, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05870967741935484, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.101914405822754, 'train@spa.rst.sctb_runtime': 5.4976, 'train@spa.rst.sctb_samples_per_second': 79.854, 'train@spa.rst.sctb_steps_per_second': 2.547, 'epoch': 12.0}
{'loss': 2.1352, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.169083595275879, 'eval_accuracy@spa.rst.sctb': 0.43617021276595747, 'eval_f1@spa.rst.sctb': 0.06425057442099774, 'eval_precision@spa.rst.sctb': 0.05405405405405405, 'eval_recall@spa.rst.sctb': 0.08621821934515432, 'eval_loss@spa.rst.sctb': 2.169083595275879, 'eval_runtime': 1.3905, 'eval_samples_per_second': 67.6, 'eval_steps_per_second': 2.157, 'epoch': 12.0}
{'train_runtime': 215.8343, 'train_samples_per_second': 24.408, 'train_steps_per_second': 0.778, 'train_loss': 2.393465518951416, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.3935
  train_runtime            = 0:03:35.83
  train_samples_per_second =     24.408
  train_steps_per_second   =      0.778
{'train@zho.rst.sctb_loss': 3.0392866134643555, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.07972665148063782, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.016886255106150393, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.02171481506896315, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.051821862348178135, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.0392863750457764, 'train@zho.rst.sctb_runtime': 5.4198, 'train@zho.rst.sctb_samples_per_second': 80.999, 'train@zho.rst.sctb_steps_per_second': 2.583, 'epoch': 1.0}
{'loss': 3.1644, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.0505998134613037, 'eval_accuracy@zho.rst.sctb': 0.05319148936170213, 'eval_f1@zho.rst.sctb': 0.015772189093423216, 'eval_precision@zho.rst.sctb': 0.018185708172871202, 'eval_recall@zho.rst.sctb': 0.03869969040247678, 'eval_loss@zho.rst.sctb': 3.050600051879883, 'eval_runtime': 1.3627, 'eval_samples_per_second': 68.982, 'eval_steps_per_second': 2.202, 'epoch': 1.0}
{'train@zho.rst.sctb_loss': 2.841886043548584, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.20956719817767655, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.014712937789860865, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.009166998804304504, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.03724696356275304, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.841886043548584, 'train@zho.rst.sctb_runtime': 5.3814, 'train@zho.rst.sctb_samples_per_second': 81.577, 'train@zho.rst.sctb_steps_per_second': 2.602, 'epoch': 2.0}
{'loss': 2.9521, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.8642966747283936, 'eval_accuracy@zho.rst.sctb': 0.18085106382978725, 'eval_f1@zho.rst.sctb': 0.018448182311448725, 'eval_precision@zho.rst.sctb': 0.01118421052631579, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.8642961978912354, 'eval_runtime': 1.3629, 'eval_samples_per_second': 68.97, 'eval_steps_per_second': 2.201, 'epoch': 2.0}
{'train@zho.rst.sctb_loss': 2.6529362201690674, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.2847380410022779, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.027965729414504856, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.0271240511625127, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04480894015861572, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.6529366970062256, 'train@zho.rst.sctb_runtime': 5.4314, 'train@zho.rst.sctb_samples_per_second': 80.826, 'train@zho.rst.sctb_steps_per_second': 2.578, 'epoch': 3.0}
{'loss': 2.7743, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.6887259483337402, 'eval_accuracy@zho.rst.sctb': 0.2127659574468085, 'eval_f1@zho.rst.sctb': 0.028477645727221283, 'eval_precision@zho.rst.sctb': 0.027239150507848565, 'eval_recall@zho.rst.sctb': 0.05321207430340557, 'eval_loss@zho.rst.sctb': 2.6887269020080566, 'eval_runtime': 1.3709, 'eval_samples_per_second': 68.57, 'eval_steps_per_second': 2.188, 'epoch': 3.0}
{'train@zho.rst.sctb_loss': 2.4882028102874756, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3917995444191344, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.03865723317778113, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.030149758916882204, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.05549331706505463, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.4882028102874756, 'train@zho.rst.sctb_runtime': 5.3864, 'train@zho.rst.sctb_samples_per_second': 81.502, 'train@zho.rst.sctb_steps_per_second': 2.599, 'epoch': 4.0}
{'loss': 2.5789, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.53935170173645, 'eval_accuracy@zho.rst.sctb': 0.30851063829787234, 'eval_f1@zho.rst.sctb': 0.04183535762483131, 'eval_precision@zho.rst.sctb': 0.03270404271548436, 'eval_recall@zho.rst.sctb': 0.06220975232198143, 'eval_loss@zho.rst.sctb': 2.539351224899292, 'eval_runtime': 1.3793, 'eval_samples_per_second': 68.151, 'eval_steps_per_second': 2.175, 'epoch': 4.0}
{'train@zho.rst.sctb_loss': 2.3646273612976074, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.4123006833712984, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.040314986450885285, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03188183572410064, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.05588431035438967, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.3646271228790283, 'train@zho.rst.sctb_runtime': 5.397, 'train@zho.rst.sctb_samples_per_second': 81.341, 'train@zho.rst.sctb_steps_per_second': 2.594, 'epoch': 5.0}
{'loss': 2.4644, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.4305388927459717, 'eval_accuracy@zho.rst.sctb': 0.3191489361702128, 'eval_f1@zho.rst.sctb': 0.04079347304431291, 'eval_precision@zho.rst.sctb': 0.031037775891341258, 'eval_recall@zho.rst.sctb': 0.05950077399380804, 'eval_loss@zho.rst.sctb': 2.4305386543273926, 'eval_runtime': 1.3694, 'eval_samples_per_second': 68.644, 'eval_steps_per_second': 2.191, 'epoch': 5.0}
{'train@zho.rst.sctb_loss': 2.283518075942993, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.4168564920273349, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.04051071878940731, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.034214700112692516, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.054996949697742784, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.2835183143615723, 'train@zho.rst.sctb_runtime': 5.3906, 'train@zho.rst.sctb_samples_per_second': 81.438, 'train@zho.rst.sctb_steps_per_second': 2.597, 'epoch': 6.0}
{'loss': 2.3592, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.3648440837860107, 'eval_accuracy@zho.rst.sctb': 0.30851063829787234, 'eval_f1@zho.rst.sctb': 0.036142989051576306, 'eval_precision@zho.rst.sctb': 0.028049575994781476, 'eval_recall@zho.rst.sctb': 0.05350232198142415, 'eval_loss@zho.rst.sctb': 2.3648436069488525, 'eval_runtime': 1.3712, 'eval_samples_per_second': 68.551, 'eval_steps_per_second': 2.188, 'epoch': 6.0}
{'train@zho.rst.sctb_loss': 2.2399954795837402, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.4191343963553531, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.04091135981504834, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03461121081848157, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.055401807997337925, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.2399957180023193, 'train@zho.rst.sctb_runtime': 5.4035, 'train@zho.rst.sctb_samples_per_second': 81.244, 'train@zho.rst.sctb_steps_per_second': 2.591, 'epoch': 7.0}
{'loss': 2.3093, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.3322341442108154, 'eval_accuracy@zho.rst.sctb': 0.30851063829787234, 'eval_f1@zho.rst.sctb': 0.03609986504723347, 'eval_precision@zho.rst.sctb': 0.02784423179160021, 'eval_recall@zho.rst.sctb': 0.05350232198142415, 'eval_loss@zho.rst.sctb': 2.3322339057922363, 'eval_runtime': 1.3715, 'eval_samples_per_second': 68.538, 'eval_steps_per_second': 2.187, 'epoch': 7.0}
{'train@zho.rst.sctb_loss': 2.2186343669891357, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.41002277904328016, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.03983672499414628, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03557151357784269, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.05349952858965116, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.2186338901519775, 'train@zho.rst.sctb_runtime': 5.3977, 'train@zho.rst.sctb_samples_per_second': 81.331, 'train@zho.rst.sctb_steps_per_second': 2.594, 'epoch': 8.0}
{'loss': 2.2708, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.3175437450408936, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.03859649122807018, 'eval_precision@zho.rst.sctb': 0.03137651821862348, 'eval_recall@zho.rst.sctb': 0.05679179566563468, 'eval_loss@zho.rst.sctb': 2.3175437450408936, 'eval_runtime': 1.3638, 'eval_samples_per_second': 68.925, 'eval_steps_per_second': 2.2, 'epoch': 8.0}
{'train@zho.rst.sctb_loss': 2.2048871517181396, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.41002277904328016, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.03983672499414628, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03557151357784269, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.05349952858965116, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.2048871517181396, 'train@zho.rst.sctb_runtime': 5.3775, 'train@zho.rst.sctb_samples_per_second': 81.637, 'train@zho.rst.sctb_steps_per_second': 2.603, 'epoch': 9.0}
{'loss': 2.2432, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.3082590103149414, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.03859649122807018, 'eval_precision@zho.rst.sctb': 0.03137651821862348, 'eval_recall@zho.rst.sctb': 0.05679179566563468, 'eval_loss@zho.rst.sctb': 2.3082592487335205, 'eval_runtime': 1.3725, 'eval_samples_per_second': 68.486, 'eval_steps_per_second': 2.186, 'epoch': 9.0}
{'train@zho.rst.sctb_loss': 2.1953883171081543, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.4168564920273349, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.04054000336614051, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03511694857848704, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.054714103488436576, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.1953883171081543, 'train@zho.rst.sctb_runtime': 5.3837, 'train@zho.rst.sctb_samples_per_second': 81.542, 'train@zho.rst.sctb_steps_per_second': 2.6, 'epoch': 10.0}
{'loss': 2.2334, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.301849603652954, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.03845826114125032, 'eval_precision@zho.rst.sctb': 0.030839129910337344, 'eval_recall@zho.rst.sctb': 0.05679179566563468, 'eval_loss@zho.rst.sctb': 2.301849603652954, 'eval_runtime': 1.3713, 'eval_samples_per_second': 68.55, 'eval_steps_per_second': 2.188, 'epoch': 10.0}
{'train@zho.rst.sctb_loss': 2.1904664039611816, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.4168564920273349, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.04060366571951373, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.035305391447632825, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.054714103488436576, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.1904661655426025, 'train@zho.rst.sctb_runtime': 5.3794, 'train@zho.rst.sctb_samples_per_second': 81.608, 'train@zho.rst.sctb_steps_per_second': 2.603, 'epoch': 11.0}
{'loss': 2.2266, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.298784017562866, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.03845826114125032, 'eval_precision@zho.rst.sctb': 0.030839129910337344, 'eval_recall@zho.rst.sctb': 0.05679179566563468, 'eval_loss@zho.rst.sctb': 2.2987844944000244, 'eval_runtime': 1.359, 'eval_samples_per_second': 69.17, 'eval_steps_per_second': 2.208, 'epoch': 11.0}
{'train@zho.rst.sctb_loss': 2.1888680458068848, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.4168564920273349, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.04060366571951373, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.035305391447632825, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.054714103488436576, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.1888675689697266, 'train@zho.rst.sctb_runtime': 5.3676, 'train@zho.rst.sctb_samples_per_second': 81.787, 'train@zho.rst.sctb_steps_per_second': 2.608, 'epoch': 12.0}
{'loss': 2.2256, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.297910690307617, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.03845826114125032, 'eval_precision@zho.rst.sctb': 0.030839129910337344, 'eval_recall@zho.rst.sctb': 0.05679179566563468, 'eval_loss@zho.rst.sctb': 2.2979109287261963, 'eval_runtime': 1.374, 'eval_samples_per_second': 68.412, 'eval_steps_per_second': 2.183, 'epoch': 12.0}
{'train_runtime': 212.1665, 'train_samples_per_second': 24.83, 'train_steps_per_second': 0.792, 'train_loss': 2.483510437465849, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.3935
  train_runtime            = 0:03:35.83
  train_samples_per_second =     24.408
  train_steps_per_second   =      0.778
-------------------------------------------------------------------
Lang1:  tur.pdtb.tdb    Lang2:  zho.rst.sctb
Saving run to:  runs/full_shot/FullShot=v4_tur.pdtb.tdb_zho.rst.sctb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2451 examples
read 312 examples
read 422 examples
read 439 examples
read 94 examples
read 159 examples
Total prediction labels:  49
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (tr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (zh): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (tr): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=49, bias=True)
    )
  )
)
{'train@tur.pdtb.tdb_loss': 2.933488607406616, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01741967515426626, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010891738953044898, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.933488607406616, 'train@tur.pdtb.tdb_runtime': 29.4148, 'train@tur.pdtb.tdb_samples_per_second': 83.325, 'train@tur.pdtb.tdb_steps_per_second': 2.618, 'epoch': 1.0}
{'loss': 3.4084, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.8614745140075684, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.86147403717041, 'eval_runtime': 4.1439, 'eval_samples_per_second': 75.291, 'eval_steps_per_second': 2.413, 'epoch': 1.0}
{'train@tur.pdtb.tdb_loss': 2.474914789199829, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01741967515426626, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010891738953044898, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.474915027618408, 'train@tur.pdtb.tdb_runtime': 29.5212, 'train@tur.pdtb.tdb_samples_per_second': 83.025, 'train@tur.pdtb.tdb_steps_per_second': 2.608, 'epoch': 2.0}
{'loss': 2.6554, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.35837721824646, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.35837721824646, 'eval_runtime': 4.1662, 'eval_samples_per_second': 74.889, 'eval_steps_per_second': 2.4, 'epoch': 2.0}
{'train@tur.pdtb.tdb_loss': 2.3959157466888428, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01741967515426626, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010891738953044898, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.395915985107422, 'train@tur.pdtb.tdb_runtime': 29.5412, 'train@tur.pdtb.tdb_samples_per_second': 82.969, 'train@tur.pdtb.tdb_steps_per_second': 2.607, 'epoch': 3.0}
{'loss': 2.4521, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.313339948654175, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.3133397102355957, 'eval_runtime': 4.1506, 'eval_samples_per_second': 75.17, 'eval_steps_per_second': 2.409, 'epoch': 3.0}
{'train@tur.pdtb.tdb_loss': 2.3314950466156006, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.2688698490412077, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.030615827160638553, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.05856166009518662, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.051937154071372206, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.3314950466156006, 'train@tur.pdtb.tdb_runtime': 29.5797, 'train@tur.pdtb.tdb_samples_per_second': 82.861, 'train@tur.pdtb.tdb_steps_per_second': 2.603, 'epoch': 4.0}
{'loss': 2.3896, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.275515556335449, 'eval_accuracy@tur.pdtb.tdb': 0.26282051282051283, 'eval_f1@tur.pdtb.tdb': 0.021342650685820606, 'eval_precision@tur.pdtb.tdb': 0.016859227728792947, 'eval_recall@tur.pdtb.tdb': 0.04569615359835062, 'eval_loss@tur.pdtb.tdb': 2.275515556335449, 'eval_runtime': 4.1722, 'eval_samples_per_second': 74.78, 'eval_steps_per_second': 2.397, 'epoch': 4.0}
{'train@tur.pdtb.tdb_loss': 2.278804063796997, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.2937576499388005, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.04636086345749201, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.05085582943412879, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.06568297491686337, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.278804063796997, 'train@tur.pdtb.tdb_runtime': 29.6341, 'train@tur.pdtb.tdb_samples_per_second': 82.709, 'train@tur.pdtb.tdb_steps_per_second': 2.598, 'epoch': 5.0}
{'loss': 2.3377, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.239680767059326, 'eval_accuracy@tur.pdtb.tdb': 0.27564102564102566, 'eval_f1@tur.pdtb.tdb': 0.033407571722570736, 'eval_precision@tur.pdtb.tdb': 0.07489848958827061, 'eval_recall@tur.pdtb.tdb': 0.05269139022181885, 'eval_loss@tur.pdtb.tdb': 2.2396810054779053, 'eval_runtime': 4.1854, 'eval_samples_per_second': 74.545, 'eval_steps_per_second': 2.389, 'epoch': 5.0}
{'train@tur.pdtb.tdb_loss': 2.2319438457489014, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3219094247246022, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.08182099127075645, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.09906865562687765, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.09533263036341764, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.2319438457489014, 'train@tur.pdtb.tdb_runtime': 29.6747, 'train@tur.pdtb.tdb_samples_per_second': 82.596, 'train@tur.pdtb.tdb_steps_per_second': 2.595, 'epoch': 6.0}
{'loss': 2.2947, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.212238311767578, 'eval_accuracy@tur.pdtb.tdb': 0.2948717948717949, 'eval_f1@tur.pdtb.tdb': 0.07677294017457953, 'eval_precision@tur.pdtb.tdb': 0.07929958344978108, 'eval_recall@tur.pdtb.tdb': 0.10043942335255927, 'eval_loss@tur.pdtb.tdb': 2.212238311767578, 'eval_runtime': 4.1786, 'eval_samples_per_second': 74.666, 'eval_steps_per_second': 2.393, 'epoch': 6.0}
{'train@tur.pdtb.tdb_loss': 2.1993963718414307, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.33374133006935947, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09040518028400886, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.09346981572379302, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.10745336679499362, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1993963718414307, 'train@tur.pdtb.tdb_runtime': 29.6225, 'train@tur.pdtb.tdb_samples_per_second': 82.741, 'train@tur.pdtb.tdb_steps_per_second': 2.599, 'epoch': 7.0}
{'loss': 2.2612, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.1898934841156006, 'eval_accuracy@tur.pdtb.tdb': 0.2980769230769231, 'eval_f1@tur.pdtb.tdb': 0.07874446715994571, 'eval_precision@tur.pdtb.tdb': 0.07546110578883779, 'eval_recall@tur.pdtb.tdb': 0.10372105332124436, 'eval_loss@tur.pdtb.tdb': 2.1898937225341797, 'eval_runtime': 4.1657, 'eval_samples_per_second': 74.898, 'eval_steps_per_second': 2.401, 'epoch': 7.0}
{'train@tur.pdtb.tdb_loss': 2.180250406265259, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.34312525499796004, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09484324802125906, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.10055257211915089, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11144690688531991, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.180250644683838, 'train@tur.pdtb.tdb_runtime': 29.658, 'train@tur.pdtb.tdb_samples_per_second': 82.642, 'train@tur.pdtb.tdb_steps_per_second': 2.596, 'epoch': 8.0}
{'loss': 2.2346, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.173673152923584, 'eval_accuracy@tur.pdtb.tdb': 0.30128205128205127, 'eval_f1@tur.pdtb.tdb': 0.08144488361879666, 'eval_precision@tur.pdtb.tdb': 0.07941232407759603, 'eval_recall@tur.pdtb.tdb': 0.1059214771198255, 'eval_loss@tur.pdtb.tdb': 2.173673391342163, 'eval_runtime': 4.1668, 'eval_samples_per_second': 74.877, 'eval_steps_per_second': 2.4, 'epoch': 8.0}
{'train@tur.pdtb.tdb_loss': 2.155745267868042, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3390452876376989, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09181054922903288, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.09667959804188467, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11268920169939071, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.155745506286621, 'train@tur.pdtb.tdb_runtime': 29.672, 'train@tur.pdtb.tdb_samples_per_second': 82.603, 'train@tur.pdtb.tdb_steps_per_second': 2.595, 'epoch': 9.0}
{'loss': 2.2066, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.15665602684021, 'eval_accuracy@tur.pdtb.tdb': 0.30128205128205127, 'eval_f1@tur.pdtb.tdb': 0.07639673617506978, 'eval_precision@tur.pdtb.tdb': 0.1118785493105265, 'eval_recall@tur.pdtb.tdb': 0.10621856565220812, 'eval_loss@tur.pdtb.tdb': 2.15665602684021, 'eval_runtime': 4.1702, 'eval_samples_per_second': 74.817, 'eval_steps_per_second': 2.398, 'epoch': 9.0}
{'train@tur.pdtb.tdb_loss': 2.1413238048553467, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3459812321501428, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09615836969167547, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.099010408667557, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11622392139799753, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.141324043273926, 'train@tur.pdtb.tdb_runtime': 29.5917, 'train@tur.pdtb.tdb_samples_per_second': 82.827, 'train@tur.pdtb.tdb_steps_per_second': 2.602, 'epoch': 10.0}
{'loss': 2.1846, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.1462438106536865, 'eval_accuracy@tur.pdtb.tdb': 0.3173076923076923, 'eval_f1@tur.pdtb.tdb': 0.08707958461628075, 'eval_precision@tur.pdtb.tdb': 0.1239304393454591, 'eval_recall@tur.pdtb.tdb': 0.11317240596891166, 'eval_loss@tur.pdtb.tdb': 2.1462435722351074, 'eval_runtime': 4.1524, 'eval_samples_per_second': 75.137, 'eval_steps_per_second': 2.408, 'epoch': 10.0}
{'train@tur.pdtb.tdb_loss': 2.135411024093628, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3459812321501428, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09581294231922705, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.09962971622512477, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11592786279128864, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.135411262512207, 'train@tur.pdtb.tdb_runtime': 29.5743, 'train@tur.pdtb.tdb_samples_per_second': 82.876, 'train@tur.pdtb.tdb_steps_per_second': 2.604, 'epoch': 11.0}
{'loss': 2.19, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.141895055770874, 'eval_accuracy@tur.pdtb.tdb': 0.3141025641025641, 'eval_f1@tur.pdtb.tdb': 0.08606791066861699, 'eval_precision@tur.pdtb.tdb': 0.1234768740031898, 'eval_recall@tur.pdtb.tdb': 0.11183550757318975, 'eval_loss@tur.pdtb.tdb': 2.141895055770874, 'eval_runtime': 4.1512, 'eval_samples_per_second': 75.159, 'eval_steps_per_second': 2.409, 'epoch': 11.0}
{'train@tur.pdtb.tdb_loss': 2.132222890853882, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.34557323541411666, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09627994948844289, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.09774864657999885, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11633529893963993, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.132222890853882, 'train@tur.pdtb.tdb_runtime': 29.5733, 'train@tur.pdtb.tdb_samples_per_second': 82.879, 'train@tur.pdtb.tdb_steps_per_second': 2.604, 'epoch': 12.0}
{'loss': 2.1754, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.1394543647766113, 'eval_accuracy@tur.pdtb.tdb': 0.3141025641025641, 'eval_f1@tur.pdtb.tdb': 0.08609391917240054, 'eval_precision@tur.pdtb.tdb': 0.09921669921669922, 'eval_recall@tur.pdtb.tdb': 0.11183550757318975, 'eval_loss@tur.pdtb.tdb': 2.1394548416137695, 'eval_runtime': 4.1664, 'eval_samples_per_second': 74.885, 'eval_steps_per_second': 2.4, 'epoch': 12.0}
{'train_runtime': 1150.7527, 'train_samples_per_second': 25.559, 'train_steps_per_second': 0.803, 'train_loss': 2.399198309167639, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.3992
  train_runtime            = 0:19:10.75
  train_samples_per_second =     25.559
  train_steps_per_second   =      0.803
{'train@zho.rst.sctb_loss': 3.625783920288086, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.004555808656036446, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.0015805278963173701, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.0008026611942274592, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.07692307692307693, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.625783920288086, 'train@zho.rst.sctb_runtime': 5.5558, 'train@zho.rst.sctb_samples_per_second': 79.016, 'train@zho.rst.sctb_steps_per_second': 2.52, 'epoch': 1.0}
{'loss': 3.8982, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.5585758686065674, 'eval_accuracy@zho.rst.sctb': 0.010638297872340425, 'eval_f1@zho.rst.sctb': 0.001234567901234568, 'eval_precision@zho.rst.sctb': 0.000625, 'eval_recall@zho.rst.sctb': 0.05, 'eval_loss@zho.rst.sctb': 3.5585761070251465, 'eval_runtime': 1.5304, 'eval_samples_per_second': 61.423, 'eval_steps_per_second': 1.96, 'epoch': 1.0}
{'train@zho.rst.sctb_loss': 3.3183372020721436, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.09339407744874716, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.011160958529379582, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.01038898882709224, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.013101510361784334, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.3183372020721436, 'train@zho.rst.sctb_runtime': 5.5591, 'train@zho.rst.sctb_samples_per_second': 78.97, 'train@zho.rst.sctb_steps_per_second': 2.518, 'epoch': 2.0}
{'loss': 3.474, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.2741494178771973, 'eval_accuracy@zho.rst.sctb': 0.10638297872340426, 'eval_f1@zho.rst.sctb': 0.014705882352941176, 'eval_precision@zho.rst.sctb': 0.01388888888888889, 'eval_recall@zho.rst.sctb': 0.015625, 'eval_loss@zho.rst.sctb': 3.274149179458618, 'eval_runtime': 1.5367, 'eval_samples_per_second': 61.17, 'eval_steps_per_second': 1.952, 'epoch': 2.0}
{'train@zho.rst.sctb_loss': 3.022247314453125, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.022247314453125, 'train@zho.rst.sctb_runtime': 5.5924, 'train@zho.rst.sctb_samples_per_second': 78.499, 'train@zho.rst.sctb_steps_per_second': 2.503, 'epoch': 3.0}
{'loss': 3.2008, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.9992566108703613, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.9992551803588867, 'eval_runtime': 1.5579, 'eval_samples_per_second': 60.337, 'eval_steps_per_second': 1.926, 'epoch': 3.0}
{'train@zho.rst.sctb_loss': 2.7464544773101807, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.7464542388916016, 'train@zho.rst.sctb_runtime': 5.5752, 'train@zho.rst.sctb_samples_per_second': 78.741, 'train@zho.rst.sctb_steps_per_second': 2.511, 'epoch': 4.0}
{'loss': 2.8983, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.7508420944213867, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.750842571258545, 'eval_runtime': 1.5316, 'eval_samples_per_second': 61.373, 'eval_steps_per_second': 1.959, 'epoch': 4.0}
{'train@zho.rst.sctb_loss': 2.549957275390625, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.549956798553467, 'train@zho.rst.sctb_runtime': 5.5599, 'train@zho.rst.sctb_samples_per_second': 78.959, 'train@zho.rst.sctb_steps_per_second': 2.518, 'epoch': 5.0}
{'loss': 2.674, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.5817995071411133, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.581799030303955, 'eval_runtime': 2.2049, 'eval_samples_per_second': 42.632, 'eval_steps_per_second': 1.361, 'epoch': 5.0}
{'train@zho.rst.sctb_loss': 2.432898759841919, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.432898759841919, 'train@zho.rst.sctb_runtime': 5.5398, 'train@zho.rst.sctb_samples_per_second': 79.245, 'train@zho.rst.sctb_steps_per_second': 2.527, 'epoch': 6.0}
{'loss': 2.5434, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.4879138469696045, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.4879133701324463, 'eval_runtime': 1.5459, 'eval_samples_per_second': 60.807, 'eval_steps_per_second': 1.941, 'epoch': 6.0}
{'train@zho.rst.sctb_loss': 2.3681139945983887, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.3681139945983887, 'train@zho.rst.sctb_runtime': 5.5816, 'train@zho.rst.sctb_samples_per_second': 78.651, 'train@zho.rst.sctb_steps_per_second': 2.508, 'epoch': 7.0}
{'loss': 2.4574, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.437838077545166, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.4378387928009033, 'eval_runtime': 1.5376, 'eval_samples_per_second': 61.133, 'eval_steps_per_second': 1.951, 'epoch': 7.0}
{'train@zho.rst.sctb_loss': 2.335348606109619, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.335348606109619, 'train@zho.rst.sctb_runtime': 5.5522, 'train@zho.rst.sctb_samples_per_second': 79.068, 'train@zho.rst.sctb_steps_per_second': 2.522, 'epoch': 8.0}
{'loss': 2.398, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.414445161819458, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.414444923400879, 'eval_runtime': 1.5341, 'eval_samples_per_second': 61.275, 'eval_steps_per_second': 1.956, 'epoch': 8.0}
{'train@zho.rst.sctb_loss': 2.3171699047088623, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.3171699047088623, 'train@zho.rst.sctb_runtime': 5.5663, 'train@zho.rst.sctb_samples_per_second': 78.868, 'train@zho.rst.sctb_steps_per_second': 2.515, 'epoch': 9.0}
{'loss': 2.3624, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.4015281200408936, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.4015283584594727, 'eval_runtime': 1.5359, 'eval_samples_per_second': 61.202, 'eval_steps_per_second': 1.953, 'epoch': 9.0}
{'train@zho.rst.sctb_loss': 2.3047776222229004, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.33712984054669703, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.02084979793596794, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.051311388839992964, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.039271255060728746, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.3047776222229004, 'train@zho.rst.sctb_runtime': 5.5898, 'train@zho.rst.sctb_samples_per_second': 78.536, 'train@zho.rst.sctb_steps_per_second': 2.505, 'epoch': 10.0}
{'loss': 2.3525, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.3929929733276367, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.3929927349090576, 'eval_runtime': 1.5171, 'eval_samples_per_second': 61.959, 'eval_steps_per_second': 1.977, 'epoch': 10.0}
{'train@zho.rst.sctb_loss': 2.2987558841705322, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.33940774487471526, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.021651642417474522, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.0513408609738885, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.03967611336032389, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.298755407333374, 'train@zho.rst.sctb_runtime': 5.586, 'train@zho.rst.sctb_samples_per_second': 78.589, 'train@zho.rst.sctb_steps_per_second': 2.506, 'epoch': 11.0}
{'loss': 2.3495, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.3890464305877686, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.3890464305877686, 'eval_runtime': 1.5545, 'eval_samples_per_second': 60.468, 'eval_steps_per_second': 1.93, 'epoch': 11.0}
{'train@zho.rst.sctb_loss': 2.2968549728393555, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3416856492027335, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.022438070630841713, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.05137046861184792, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04008097165991902, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.2968554496765137, 'train@zho.rst.sctb_runtime': 5.5924, 'train@zho.rst.sctb_samples_per_second': 78.499, 'train@zho.rst.sctb_steps_per_second': 2.503, 'epoch': 12.0}
{'loss': 2.3335, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.3878252506256104, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.387824773788452, 'eval_runtime': 1.534, 'eval_samples_per_second': 61.279, 'eval_steps_per_second': 1.956, 'epoch': 12.0}
{'train_runtime': 217.184, 'train_samples_per_second': 24.256, 'train_steps_per_second': 0.774, 'train_loss': 2.74516300928025, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.3992
  train_runtime            = 0:19:10.75
  train_samples_per_second =     25.559
  train_steps_per_second   =      0.803

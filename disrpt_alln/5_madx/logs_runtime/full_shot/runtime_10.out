-------------------------------------------------------------------
Lang1:  deu.rst.pcc    Lang2:  spa.rst.rststb
Saving run to:  runs/full_shot/FullShot=v4_deu.rst.pcc_spa.rst.rststb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2164 examples
read 241 examples
read 260 examples
read 2240 examples
read 383 examples
read 426 examples
Total prediction labels:  33
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=33, bias=True)
    )
  )
)
{'train@deu.rst.pcc_loss': 3.1229944229125977, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.10351201478743069, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.014156961796851227, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.01395351747225589, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.041649905475514275, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.122994899749756, 'train@deu.rst.pcc_runtime': 26.2588, 'train@deu.rst.pcc_samples_per_second': 82.411, 'train@deu.rst.pcc_steps_per_second': 2.59, 'epoch': 1.0}
{'loss': 3.3165, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.16290283203125, 'eval_accuracy@deu.rst.pcc': 0.12448132780082988, 'eval_f1@deu.rst.pcc': 0.018719940539256293, 'eval_precision@deu.rst.pcc': 0.024067982456140353, 'eval_recall@deu.rst.pcc': 0.04712301587301587, 'eval_loss@deu.rst.pcc': 3.1629035472869873, 'eval_runtime': 3.167, 'eval_samples_per_second': 76.098, 'eval_steps_per_second': 2.526, 'epoch': 1.0}
{'train@deu.rst.pcc_loss': 2.9383654594421387, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.11737523105360444, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.024309575884439508, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.060731442675481064, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.04824411464836343, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.938365936279297, 'train@deu.rst.pcc_runtime': 26.0903, 'train@deu.rst.pcc_samples_per_second': 82.943, 'train@deu.rst.pcc_steps_per_second': 2.606, 'epoch': 2.0}
{'loss': 3.0329, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.9977619647979736, 'eval_accuracy@deu.rst.pcc': 0.11618257261410789, 'eval_f1@deu.rst.pcc': 0.01558201058201058, 'eval_precision@deu.rst.pcc': 0.01383248730964467, 'eval_recall@deu.rst.pcc': 0.04332010582010582, 'eval_loss@deu.rst.pcc': 2.9977622032165527, 'eval_runtime': 3.1974, 'eval_samples_per_second': 75.373, 'eval_steps_per_second': 2.502, 'epoch': 2.0}
{'train@deu.rst.pcc_loss': 2.869251251220703, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.14833641404805914, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.05164371761110051, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.11628854076431608, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.07004229923398558, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.869251251220703, 'train@deu.rst.pcc_runtime': 26.145, 'train@deu.rst.pcc_samples_per_second': 82.769, 'train@deu.rst.pcc_steps_per_second': 2.601, 'epoch': 3.0}
{'loss': 2.9303, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.9394378662109375, 'eval_accuracy@deu.rst.pcc': 0.12448132780082988, 'eval_f1@deu.rst.pcc': 0.03326788326788327, 'eval_precision@deu.rst.pcc': 0.039453901695143734, 'eval_recall@deu.rst.pcc': 0.05710724460724461, 'eval_loss@deu.rst.pcc': 2.9394378662109375, 'eval_runtime': 3.2012, 'eval_samples_per_second': 75.285, 'eval_steps_per_second': 2.499, 'epoch': 3.0}
{'train@deu.rst.pcc_loss': 2.8150243759155273, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.1899260628465804, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.06966323275109264, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.0874002705879663, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.10785137190316055, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.8150241374969482, 'train@deu.rst.pcc_runtime': 26.3124, 'train@deu.rst.pcc_samples_per_second': 82.243, 'train@deu.rst.pcc_steps_per_second': 2.584, 'epoch': 4.0}
{'loss': 2.8745, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.8998093605041504, 'eval_accuracy@deu.rst.pcc': 0.15767634854771784, 'eval_f1@deu.rst.pcc': 0.05568188659117642, 'eval_precision@deu.rst.pcc': 0.07887949769528717, 'eval_recall@deu.rst.pcc': 0.09677070614570615, 'eval_loss@deu.rst.pcc': 2.8998091220855713, 'eval_runtime': 3.1977, 'eval_samples_per_second': 75.367, 'eval_steps_per_second': 2.502, 'epoch': 4.0}
{'train@deu.rst.pcc_loss': 2.7724802494049072, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.1954713493530499, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.06775746342075298, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.07531383130840139, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.11297741095739054, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7724802494049072, 'train@deu.rst.pcc_runtime': 26.0949, 'train@deu.rst.pcc_samples_per_second': 82.928, 'train@deu.rst.pcc_steps_per_second': 2.606, 'epoch': 5.0}
{'loss': 2.8271, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.8682861328125, 'eval_accuracy@deu.rst.pcc': 0.15767634854771784, 'eval_f1@deu.rst.pcc': 0.059342970290956865, 'eval_precision@deu.rst.pcc': 0.07838127168146108, 'eval_recall@deu.rst.pcc': 0.10220797720797721, 'eval_loss@deu.rst.pcc': 2.868285655975342, 'eval_runtime': 3.1994, 'eval_samples_per_second': 75.327, 'eval_steps_per_second': 2.5, 'epoch': 5.0}
{'train@deu.rst.pcc_loss': 2.737178087234497, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.20194085027726433, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.07088040829272402, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.06984344697467702, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.118116111259364, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.737177848815918, 'train@deu.rst.pcc_runtime': 26.0983, 'train@deu.rst.pcc_samples_per_second': 82.917, 'train@deu.rst.pcc_steps_per_second': 2.606, 'epoch': 6.0}
{'loss': 2.7864, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.8380141258239746, 'eval_accuracy@deu.rst.pcc': 0.15352697095435686, 'eval_f1@deu.rst.pcc': 0.057709587666484215, 'eval_precision@deu.rst.pcc': 0.06125246497270306, 'eval_recall@deu.rst.pcc': 0.10609355921855922, 'eval_loss@deu.rst.pcc': 2.8380141258239746, 'eval_runtime': 3.2205, 'eval_samples_per_second': 74.834, 'eval_steps_per_second': 2.484, 'epoch': 6.0}
{'train@deu.rst.pcc_loss': 2.7114081382751465, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.20425138632162662, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.07279298302832532, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.07528839020358215, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.1201650744347431, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7114078998565674, 'train@deu.rst.pcc_runtime': 26.0967, 'train@deu.rst.pcc_samples_per_second': 82.922, 'train@deu.rst.pcc_steps_per_second': 2.606, 'epoch': 7.0}
{'loss': 2.7588, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.8168458938598633, 'eval_accuracy@deu.rst.pcc': 0.16597510373443983, 'eval_f1@deu.rst.pcc': 0.06529601607094414, 'eval_precision@deu.rst.pcc': 0.07894656950858502, 'eval_recall@deu.rst.pcc': 0.11599511599511599, 'eval_loss@deu.rst.pcc': 2.8168461322784424, 'eval_runtime': 3.205, 'eval_samples_per_second': 75.194, 'eval_steps_per_second': 2.496, 'epoch': 7.0}
{'train@deu.rst.pcc_loss': 2.6913111209869385, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2097966728280961, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.07617721812309854, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.07584706936565862, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.12316983137080882, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6913108825683594, 'train@deu.rst.pcc_runtime': 26.0963, 'train@deu.rst.pcc_samples_per_second': 82.923, 'train@deu.rst.pcc_steps_per_second': 2.606, 'epoch': 8.0}
{'loss': 2.7335, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.801720142364502, 'eval_accuracy@deu.rst.pcc': 0.17842323651452283, 'eval_f1@deu.rst.pcc': 0.06817814902396162, 'eval_precision@deu.rst.pcc': 0.08275189643669874, 'eval_recall@deu.rst.pcc': 0.12335292022792023, 'eval_loss@deu.rst.pcc': 2.801720142364502, 'eval_runtime': 3.2334, 'eval_samples_per_second': 74.536, 'eval_steps_per_second': 2.474, 'epoch': 8.0}
{'train@deu.rst.pcc_loss': 2.6748476028442383, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2158040665434381, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08124701749482235, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.07675063953042367, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.12729112827883002, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6748478412628174, 'train@deu.rst.pcc_runtime': 26.1291, 'train@deu.rst.pcc_samples_per_second': 82.819, 'train@deu.rst.pcc_steps_per_second': 2.602, 'epoch': 9.0}
{'loss': 2.722, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.7920329570770264, 'eval_accuracy@deu.rst.pcc': 0.1825726141078838, 'eval_f1@deu.rst.pcc': 0.07100051139416759, 'eval_precision@deu.rst.pcc': 0.08334379068811064, 'eval_recall@deu.rst.pcc': 0.12277421652421654, 'eval_loss@deu.rst.pcc': 2.7920334339141846, 'eval_runtime': 3.2016, 'eval_samples_per_second': 75.274, 'eval_steps_per_second': 2.499, 'epoch': 9.0}
{'train@deu.rst.pcc_loss': 2.6639719009399414, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.22181146025878004, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.0856995917752629, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.07982548388808584, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.13176426499561195, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6639719009399414, 'train@deu.rst.pcc_runtime': 26.1185, 'train@deu.rst.pcc_samples_per_second': 82.853, 'train@deu.rst.pcc_steps_per_second': 2.604, 'epoch': 10.0}
{'loss': 2.7018, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.7807278633117676, 'eval_accuracy@deu.rst.pcc': 0.1825726141078838, 'eval_f1@deu.rst.pcc': 0.07207642082759798, 'eval_precision@deu.rst.pcc': 0.08684766208558459, 'eval_recall@deu.rst.pcc': 0.12566773504273504, 'eval_loss@deu.rst.pcc': 2.7807278633117676, 'eval_runtime': 3.196, 'eval_samples_per_second': 75.406, 'eval_steps_per_second': 2.503, 'epoch': 10.0}
{'train@deu.rst.pcc_loss': 2.6573736667633057, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.22365988909426987, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08724423617450015, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.08317675986717715, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.13311976996411312, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6573736667633057, 'train@deu.rst.pcc_runtime': 26.1215, 'train@deu.rst.pcc_samples_per_second': 82.844, 'train@deu.rst.pcc_steps_per_second': 2.603, 'epoch': 11.0}
{'loss': 2.6912, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.775920867919922, 'eval_accuracy@deu.rst.pcc': 0.1825726141078838, 'eval_f1@deu.rst.pcc': 0.07226008624944795, 'eval_precision@deu.rst.pcc': 0.0872256900844629, 'eval_recall@deu.rst.pcc': 0.12566773504273504, 'eval_loss@deu.rst.pcc': 2.775921106338501, 'eval_runtime': 3.194, 'eval_samples_per_second': 75.453, 'eval_steps_per_second': 2.505, 'epoch': 11.0}
{'train@deu.rst.pcc_loss': 2.655130624771118, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.22365988909426987, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08752129750979128, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.08281394501232418, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.13335386646350983, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.655130386352539, 'train@deu.rst.pcc_runtime': 26.1475, 'train@deu.rst.pcc_samples_per_second': 82.761, 'train@deu.rst.pcc_steps_per_second': 2.601, 'epoch': 12.0}
{'loss': 2.687, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.7736752033233643, 'eval_accuracy@deu.rst.pcc': 0.17842323651452283, 'eval_f1@deu.rst.pcc': 0.07054853331570889, 'eval_precision@deu.rst.pcc': 0.08587617282827163, 'eval_recall@deu.rst.pcc': 0.12335292022792023, 'eval_loss@deu.rst.pcc': 2.773674964904785, 'eval_runtime': 3.1983, 'eval_samples_per_second': 75.353, 'eval_steps_per_second': 2.501, 'epoch': 12.0}
{'train_runtime': 1010.8156, 'train_samples_per_second': 25.69, 'train_steps_per_second': 0.807, 'train_loss': 2.8384824827605604, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.8385
  train_runtime            = 0:16:50.81
  train_samples_per_second =      25.69
  train_steps_per_second   =      0.807
{'train@spa.rst.rststb_loss': 2.733626127243042, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.21026785714285715, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.022426583304649424, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.023294395053822175, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.04280678823201697, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.733625888824463, 'train@spa.rst.rststb_runtime': 27.0367, 'train@spa.rst.rststb_samples_per_second': 82.85, 'train@spa.rst.rststb_steps_per_second': 2.589, 'epoch': 1.0}
{'loss': 3.2605, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.791522741317749, 'eval_accuracy@spa.rst.rststb': 0.2193211488250653, 'eval_f1@spa.rst.rststb': 0.026566974228444163, 'eval_precision@spa.rst.rststb': 0.03865830115830116, 'eval_recall@spa.rst.rststb': 0.049107142857142856, 'eval_loss@spa.rst.rststb': 2.79152250289917, 'eval_runtime': 4.9042, 'eval_samples_per_second': 78.096, 'eval_steps_per_second': 2.447, 'epoch': 1.0}
{'train@spa.rst.rststb_loss': 2.4674289226531982, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.30758928571428573, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.060083039685706295, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.06701774499108584, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.07921433906940219, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.467428684234619, 'train@spa.rst.rststb_runtime': 27.0319, 'train@spa.rst.rststb_samples_per_second': 82.865, 'train@spa.rst.rststb_steps_per_second': 2.59, 'epoch': 2.0}
{'loss': 2.606, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.5789966583251953, 'eval_accuracy@spa.rst.rststb': 0.29765013054830286, 'eval_f1@spa.rst.rststb': 0.06493190988191146, 'eval_precision@spa.rst.rststb': 0.08044230680187062, 'eval_recall@spa.rst.rststb': 0.08955343894640258, 'eval_loss@spa.rst.rststb': 2.5789966583251953, 'eval_runtime': 4.8704, 'eval_samples_per_second': 78.639, 'eval_steps_per_second': 2.464, 'epoch': 2.0}
{'train@spa.rst.rststb_loss': 2.3493435382843018, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.353125, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.06864083818171098, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.08328588651246228, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.09694431203288971, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.3493432998657227, 'train@spa.rst.rststb_runtime': 27.0954, 'train@spa.rst.rststb_samples_per_second': 82.671, 'train@spa.rst.rststb_steps_per_second': 2.583, 'epoch': 3.0}
{'loss': 2.4412, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.506201982498169, 'eval_accuracy@spa.rst.rststb': 0.3681462140992167, 'eval_f1@spa.rst.rststb': 0.09206185256551758, 'eval_precision@spa.rst.rststb': 0.12183713376837745, 'eval_recall@spa.rst.rststb': 0.11964708030368992, 'eval_loss@spa.rst.rststb': 2.50620174407959, 'eval_runtime': 4.9095, 'eval_samples_per_second': 78.011, 'eval_steps_per_second': 2.444, 'epoch': 3.0}
{'train@spa.rst.rststb_loss': 2.257345199584961, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.37276785714285715, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.08602117890698405, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.11219703783065527, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.11001079513041967, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.25734543800354, 'train@spa.rst.rststb_runtime': 27.069, 'train@spa.rst.rststb_samples_per_second': 82.751, 'train@spa.rst.rststb_steps_per_second': 2.586, 'epoch': 4.0}
{'loss': 2.3406, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.443094253540039, 'eval_accuracy@spa.rst.rststb': 0.3681462140992167, 'eval_f1@spa.rst.rststb': 0.09632204708160706, 'eval_precision@spa.rst.rststb': 0.11047055708138531, 'eval_recall@spa.rst.rststb': 0.12409372061989546, 'eval_loss@spa.rst.rststb': 2.44309401512146, 'eval_runtime': 4.8961, 'eval_samples_per_second': 78.226, 'eval_steps_per_second': 2.451, 'epoch': 4.0}
{'train@spa.rst.rststb_loss': 2.174154281616211, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.39598214285714284, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.09944193732931313, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.10835329248894905, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.12229389727176652, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.174154281616211, 'train@spa.rst.rststb_runtime': 27.0339, 'train@spa.rst.rststb_samples_per_second': 82.859, 'train@spa.rst.rststb_steps_per_second': 2.589, 'epoch': 5.0}
{'loss': 2.2501, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.3819916248321533, 'eval_accuracy@spa.rst.rststb': 0.391644908616188, 'eval_f1@spa.rst.rststb': 0.11336307218660159, 'eval_precision@spa.rst.rststb': 0.12077663456561724, 'eval_recall@spa.rst.rststb': 0.14086264280737704, 'eval_loss@spa.rst.rststb': 2.381991386413574, 'eval_runtime': 4.9302, 'eval_samples_per_second': 77.684, 'eval_steps_per_second': 2.434, 'epoch': 5.0}
{'train@spa.rst.rststb_loss': 2.1005501747131348, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4080357142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10542094375878448, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.13992296305364776, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.1298580472137301, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.1005501747131348, 'train@spa.rst.rststb_runtime': 27.0652, 'train@spa.rst.rststb_samples_per_second': 82.763, 'train@spa.rst.rststb_steps_per_second': 2.586, 'epoch': 6.0}
{'loss': 2.1891, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.3231592178344727, 'eval_accuracy@spa.rst.rststb': 0.3968668407310705, 'eval_f1@spa.rst.rststb': 0.11018649183566724, 'eval_precision@spa.rst.rststb': 0.10800565005242686, 'eval_recall@spa.rst.rststb': 0.1424997143665771, 'eval_loss@spa.rst.rststb': 2.3231592178344727, 'eval_runtime': 4.8825, 'eval_samples_per_second': 78.443, 'eval_steps_per_second': 2.458, 'epoch': 6.0}
{'train@spa.rst.rststb_loss': 2.0409059524536133, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4205357142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.11335732294722496, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.14225188333753042, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13802784377616348, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0409059524536133, 'train@spa.rst.rststb_runtime': 27.0458, 'train@spa.rst.rststb_samples_per_second': 82.823, 'train@spa.rst.rststb_steps_per_second': 2.588, 'epoch': 7.0}
{'loss': 2.116, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.278285026550293, 'eval_accuracy@spa.rst.rststb': 0.4046997389033943, 'eval_f1@spa.rst.rststb': 0.12341188071395257, 'eval_precision@spa.rst.rststb': 0.12726114383121953, 'eval_recall@spa.rst.rststb': 0.15351291565056258, 'eval_loss@spa.rst.rststb': 2.278284788131714, 'eval_runtime': 4.8878, 'eval_samples_per_second': 78.359, 'eval_steps_per_second': 2.455, 'epoch': 7.0}
{'train@spa.rst.rststb_loss': 1.9971566200256348, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.42589285714285713, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.11652236890907057, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.134584566445321, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.14160669733156903, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.9971563816070557, 'train@spa.rst.rststb_runtime': 27.0411, 'train@spa.rst.rststb_samples_per_second': 82.837, 'train@spa.rst.rststb_steps_per_second': 2.589, 'epoch': 8.0}
{'loss': 2.062, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.241689682006836, 'eval_accuracy@spa.rst.rststb': 0.40992167101827676, 'eval_f1@spa.rst.rststb': 0.12498800673257746, 'eval_precision@spa.rst.rststb': 0.13188923132617666, 'eval_recall@spa.rst.rststb': 0.15558777176972657, 'eval_loss@spa.rst.rststb': 2.241689682006836, 'eval_runtime': 4.9056, 'eval_samples_per_second': 78.074, 'eval_steps_per_second': 2.446, 'epoch': 8.0}
{'train@spa.rst.rststb_loss': 1.966647744178772, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.43660714285714286, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.12019937362449284, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.1360616933805913, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.1468667676324753, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.9666473865509033, 'train@spa.rst.rststb_runtime': 27.092, 'train@spa.rst.rststb_samples_per_second': 82.681, 'train@spa.rst.rststb_steps_per_second': 2.584, 'epoch': 9.0}
{'loss': 2.0281, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.2144460678100586, 'eval_accuracy@spa.rst.rststb': 0.4073107049608355, 'eval_f1@spa.rst.rststb': 0.12261457544066239, 'eval_precision@spa.rst.rststb': 0.11412973972629911, 'eval_recall@spa.rst.rststb': 0.1554317001739356, 'eval_loss@spa.rst.rststb': 2.2144455909729004, 'eval_runtime': 4.8802, 'eval_samples_per_second': 78.48, 'eval_steps_per_second': 2.459, 'epoch': 9.0}
{'train@spa.rst.rststb_loss': 1.9453569650650024, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.440625, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.1229441634705727, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.17173277097482392, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.1486812022885841, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.9453569650650024, 'train@spa.rst.rststb_runtime': 27.0565, 'train@spa.rst.rststb_samples_per_second': 82.79, 'train@spa.rst.rststb_steps_per_second': 2.587, 'epoch': 10.0}
{'loss': 2.0081, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.1990978717803955, 'eval_accuracy@spa.rst.rststb': 0.40992167101827676, 'eval_f1@spa.rst.rststb': 0.12418313570487483, 'eval_precision@spa.rst.rststb': 0.11829119371495711, 'eval_recall@spa.rst.rststb': 0.15641984246642573, 'eval_loss@spa.rst.rststb': 2.1990978717803955, 'eval_runtime': 4.8787, 'eval_samples_per_second': 78.505, 'eval_steps_per_second': 2.46, 'epoch': 10.0}
{'train@spa.rst.rststb_loss': 1.9335911273956299, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.44285714285714284, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.1242799920835862, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.20801311006670312, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.14968222230842623, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.933591365814209, 'train@spa.rst.rststb_runtime': 27.0255, 'train@spa.rst.rststb_samples_per_second': 82.885, 'train@spa.rst.rststb_steps_per_second': 2.59, 'epoch': 11.0}
{'loss': 1.9906, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.1884384155273438, 'eval_accuracy@spa.rst.rststb': 0.40992167101827676, 'eval_f1@spa.rst.rststb': 0.1299740475527582, 'eval_precision@spa.rst.rststb': 0.12480753547552226, 'eval_recall@spa.rst.rststb': 0.1607004026082957, 'eval_loss@spa.rst.rststb': 2.188438653945923, 'eval_runtime': 4.8587, 'eval_samples_per_second': 78.827, 'eval_steps_per_second': 2.47, 'epoch': 11.0}
{'train@spa.rst.rststb_loss': 1.92938232421875, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.44375, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.12462384472013512, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.20851870442390497, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.14988456953626922, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.929382562637329, 'train@spa.rst.rststb_runtime': 27.03, 'train@spa.rst.rststb_samples_per_second': 82.871, 'train@spa.rst.rststb_steps_per_second': 2.59, 'epoch': 12.0}
{'loss': 1.976, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.1852433681488037, 'eval_accuracy@spa.rst.rststb': 0.40992167101827676, 'eval_f1@spa.rst.rststb': 0.12991957448479188, 'eval_precision@spa.rst.rststb': 0.12453285494309337, 'eval_recall@spa.rst.rststb': 0.1607004026082957, 'eval_loss@spa.rst.rststb': 2.185243606567383, 'eval_runtime': 5.1095, 'eval_samples_per_second': 74.958, 'eval_steps_per_second': 2.349, 'epoch': 12.0}
{'train_runtime': 1065.4901, 'train_samples_per_second': 25.228, 'train_steps_per_second': 0.788, 'train_loss': 2.2723619370233443, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.8385
  train_runtime            = 0:16:50.81
  train_samples_per_second =      25.69
  train_steps_per_second   =      0.807
-------------------------------------------------------------------
Lang1:  eng.pdtb.pdtb    Lang2:  spa.rst.rststb
Saving run to:  runs/full_shot/FullShot=v4_eng.pdtb.pdtb_spa.rst.rststb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 43920 examples
read 1674 examples
read 2257 examples
read 2240 examples
read 383 examples
read 426 examples
Total prediction labels:  52
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=52, bias=True)
    )
  )
)
{'train@eng.pdtb.pdtb_loss': 1.3115774393081665, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.5890938069216758, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.24766832345812806, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.2829588404795593, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.24470195618691087, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.311577320098877, 'train@eng.pdtb.pdtb_runtime': 518.3584, 'train@eng.pdtb.pdtb_samples_per_second': 84.729, 'train@eng.pdtb.pdtb_steps_per_second': 2.649, 'epoch': 1.0}
{'loss': 1.9039, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.2274705171585083, 'eval_accuracy@eng.pdtb.pdtb': 0.6206690561529271, 'eval_f1@eng.pdtb.pdtb': 0.297591044137045, 'eval_precision@eng.pdtb.pdtb': 0.3360641111621393, 'eval_recall@eng.pdtb.pdtb': 0.29607366794683576, 'eval_loss@eng.pdtb.pdtb': 1.2274705171585083, 'eval_runtime': 20.1239, 'eval_samples_per_second': 83.184, 'eval_steps_per_second': 2.634, 'epoch': 1.0}
{'train@eng.pdtb.pdtb_loss': 1.1218929290771484, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6311247723132969, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.33276458694252314, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.4227062523554997, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.32695603122356826, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.1218929290771484, 'train@eng.pdtb.pdtb_runtime': 516.801, 'train@eng.pdtb.pdtb_samples_per_second': 84.984, 'train@eng.pdtb.pdtb_steps_per_second': 2.657, 'epoch': 2.0}
{'loss': 1.2555, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.0502500534057617, 'eval_accuracy@eng.pdtb.pdtb': 0.6630824372759857, 'eval_f1@eng.pdtb.pdtb': 0.39218073991523306, 'eval_precision@eng.pdtb.pdtb': 0.43947964253387883, 'eval_recall@eng.pdtb.pdtb': 0.38493629332963675, 'eval_loss@eng.pdtb.pdtb': 1.0502500534057617, 'eval_runtime': 20.1551, 'eval_samples_per_second': 83.056, 'eval_steps_per_second': 2.63, 'epoch': 2.0}
{'train@eng.pdtb.pdtb_loss': 1.0646617412567139, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6466530054644809, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.41978087984608853, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.4668419358171671, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4010381844364133, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.0646617412567139, 'train@eng.pdtb.pdtb_runtime': 517.029, 'train@eng.pdtb.pdtb_samples_per_second': 84.947, 'train@eng.pdtb.pdtb_steps_per_second': 2.656, 'epoch': 3.0}
{'loss': 1.1421, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.0069266557693481, 'eval_accuracy@eng.pdtb.pdtb': 0.6714456391875747, 'eval_f1@eng.pdtb.pdtb': 0.4690987802190625, 'eval_precision@eng.pdtb.pdtb': 0.5632758125064422, 'eval_recall@eng.pdtb.pdtb': 0.44432204540107073, 'eval_loss@eng.pdtb.pdtb': 1.0069266557693481, 'eval_runtime': 20.1325, 'eval_samples_per_second': 83.149, 'eval_steps_per_second': 2.633, 'epoch': 3.0}
{'train@eng.pdtb.pdtb_loss': 1.0128761529922485, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6623633879781421, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4439755561175477, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5185027375940124, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4347571135810197, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.0128761529922485, 'train@eng.pdtb.pdtb_runtime': 516.554, 'train@eng.pdtb.pdtb_samples_per_second': 85.025, 'train@eng.pdtb.pdtb_steps_per_second': 2.658, 'epoch': 4.0}
{'loss': 1.0877, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 0.9642996191978455, 'eval_accuracy@eng.pdtb.pdtb': 0.6816009557945042, 'eval_f1@eng.pdtb.pdtb': 0.5028316506735147, 'eval_precision@eng.pdtb.pdtb': 0.5585475840625906, 'eval_recall@eng.pdtb.pdtb': 0.48215941862701994, 'eval_loss@eng.pdtb.pdtb': 0.9642995595932007, 'eval_runtime': 20.1353, 'eval_samples_per_second': 83.138, 'eval_steps_per_second': 2.632, 'epoch': 4.0}
{'train@eng.pdtb.pdtb_loss': 0.9889808297157288, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6692622950819672, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4540118208880238, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5206232357299886, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4486240986786575, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.988980770111084, 'train@eng.pdtb.pdtb_runtime': 516.5859, 'train@eng.pdtb.pdtb_samples_per_second': 85.02, 'train@eng.pdtb.pdtb_steps_per_second': 2.658, 'epoch': 5.0}
{'loss': 1.055, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 0.946060061454773, 'eval_accuracy@eng.pdtb.pdtb': 0.6851851851851852, 'eval_f1@eng.pdtb.pdtb': 0.5147139504916914, 'eval_precision@eng.pdtb.pdtb': 0.548695207785063, 'eval_recall@eng.pdtb.pdtb': 0.5047916404826978, 'eval_loss@eng.pdtb.pdtb': 0.946060061454773, 'eval_runtime': 20.1647, 'eval_samples_per_second': 83.016, 'eval_steps_per_second': 2.628, 'epoch': 5.0}
{'train@eng.pdtb.pdtb_loss': 0.9687308073043823, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6757513661202186, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4624454690159693, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5278558222908771, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.45617078637538055, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9687308073043823, 'train@eng.pdtb.pdtb_runtime': 516.5277, 'train@eng.pdtb.pdtb_samples_per_second': 85.029, 'train@eng.pdtb.pdtb_steps_per_second': 2.658, 'epoch': 6.0}
{'loss': 1.0335, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 0.9361051321029663, 'eval_accuracy@eng.pdtb.pdtb': 0.6816009557945042, 'eval_f1@eng.pdtb.pdtb': 0.5250422809315676, 'eval_precision@eng.pdtb.pdtb': 0.564189650293571, 'eval_recall@eng.pdtb.pdtb': 0.5123614559288574, 'eval_loss@eng.pdtb.pdtb': 0.9361050128936768, 'eval_runtime': 20.1518, 'eval_samples_per_second': 83.07, 'eval_steps_per_second': 2.63, 'epoch': 6.0}
{'train@eng.pdtb.pdtb_loss': 0.956514835357666, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6773224043715848, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4667798061536933, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5350749308045828, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.45757610004379046, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.956514835357666, 'train@eng.pdtb.pdtb_runtime': 516.8149, 'train@eng.pdtb.pdtb_samples_per_second': 84.982, 'train@eng.pdtb.pdtb_steps_per_second': 2.657, 'epoch': 7.0}
{'loss': 1.0151, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 0.9221028089523315, 'eval_accuracy@eng.pdtb.pdtb': 0.6875746714456392, 'eval_f1@eng.pdtb.pdtb': 0.5430010650070372, 'eval_precision@eng.pdtb.pdtb': 0.6311533100272653, 'eval_recall@eng.pdtb.pdtb': 0.5219440449382711, 'eval_loss@eng.pdtb.pdtb': 0.9221028685569763, 'eval_runtime': 20.1488, 'eval_samples_per_second': 83.082, 'eval_steps_per_second': 2.63, 'epoch': 7.0}
{'train@eng.pdtb.pdtb_loss': 0.9445656538009644, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6815573770491803, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.47162291886866425, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5290225546076279, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4688296124328459, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9445656538009644, 'train@eng.pdtb.pdtb_runtime': 516.3017, 'train@eng.pdtb.pdtb_samples_per_second': 85.067, 'train@eng.pdtb.pdtb_steps_per_second': 2.659, 'epoch': 8.0}
{'loss': 1.0034, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 0.9199193716049194, 'eval_accuracy@eng.pdtb.pdtb': 0.6899641577060932, 'eval_f1@eng.pdtb.pdtb': 0.5579226124013646, 'eval_precision@eng.pdtb.pdtb': 0.6192991367125401, 'eval_recall@eng.pdtb.pdtb': 0.5476374939184442, 'eval_loss@eng.pdtb.pdtb': 0.919919490814209, 'eval_runtime': 20.123, 'eval_samples_per_second': 83.188, 'eval_steps_per_second': 2.634, 'epoch': 8.0}
{'train@eng.pdtb.pdtb_loss': 0.9358848333358765, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6838570127504554, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4736900551059874, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5238387871488756, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4693334332536608, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.935884952545166, 'train@eng.pdtb.pdtb_runtime': 517.5488, 'train@eng.pdtb.pdtb_samples_per_second': 84.862, 'train@eng.pdtb.pdtb_steps_per_second': 2.653, 'epoch': 9.0}
{'loss': 0.9905, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 0.9161590337753296, 'eval_accuracy@eng.pdtb.pdtb': 0.6905615292712067, 'eval_f1@eng.pdtb.pdtb': 0.5617353483787024, 'eval_precision@eng.pdtb.pdtb': 0.6250100580797482, 'eval_recall@eng.pdtb.pdtb': 0.5489925280579814, 'eval_loss@eng.pdtb.pdtb': 0.9161591529846191, 'eval_runtime': 20.1602, 'eval_samples_per_second': 83.035, 'eval_steps_per_second': 2.629, 'epoch': 9.0}
{'train@eng.pdtb.pdtb_loss': 0.9316995143890381, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6853597449908926, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4741861187083899, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5355003515051286, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.46920643332516104, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9316995143890381, 'train@eng.pdtb.pdtb_runtime': 517.209, 'train@eng.pdtb.pdtb_samples_per_second': 84.917, 'train@eng.pdtb.pdtb_steps_per_second': 2.655, 'epoch': 10.0}
{'loss': 0.9855, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 0.9095579981803894, 'eval_accuracy@eng.pdtb.pdtb': 0.6905615292712067, 'eval_f1@eng.pdtb.pdtb': 0.5582253885457826, 'eval_precision@eng.pdtb.pdtb': 0.6267723940277774, 'eval_recall@eng.pdtb.pdtb': 0.5442751352605858, 'eval_loss@eng.pdtb.pdtb': 0.9095579981803894, 'eval_runtime': 20.1816, 'eval_samples_per_second': 82.947, 'eval_steps_per_second': 2.626, 'epoch': 10.0}
{'train@eng.pdtb.pdtb_loss': 0.9271067976951599, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6863387978142077, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4756274746226295, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5244962251549437, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4724013009406103, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9271067976951599, 'train@eng.pdtb.pdtb_runtime': 516.5158, 'train@eng.pdtb.pdtb_samples_per_second': 85.031, 'train@eng.pdtb.pdtb_steps_per_second': 2.658, 'epoch': 11.0}
{'loss': 0.9784, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.909718930721283, 'eval_accuracy@eng.pdtb.pdtb': 0.6899641577060932, 'eval_f1@eng.pdtb.pdtb': 0.560792523816543, 'eval_precision@eng.pdtb.pdtb': 0.6221878345391831, 'eval_recall@eng.pdtb.pdtb': 0.5480316595550626, 'eval_loss@eng.pdtb.pdtb': 0.9097189903259277, 'eval_runtime': 20.1821, 'eval_samples_per_second': 82.945, 'eval_steps_per_second': 2.626, 'epoch': 11.0}
{'train@eng.pdtb.pdtb_loss': 0.9259198904037476, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6867486338797815, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4779923522263946, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5705352622323051, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4727873897110507, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9259198307991028, 'train@eng.pdtb.pdtb_runtime': 516.6738, 'train@eng.pdtb.pdtb_samples_per_second': 85.005, 'train@eng.pdtb.pdtb_steps_per_second': 2.657, 'epoch': 12.0}
{'loss': 0.9745, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.9077650308609009, 'eval_accuracy@eng.pdtb.pdtb': 0.6899641577060932, 'eval_f1@eng.pdtb.pdtb': 0.5590661703737541, 'eval_precision@eng.pdtb.pdtb': 0.6238678159576848, 'eval_recall@eng.pdtb.pdtb': 0.5459976853784144, 'eval_loss@eng.pdtb.pdtb': 0.9077650308609009, 'eval_runtime': 20.1509, 'eval_samples_per_second': 83.073, 'eval_steps_per_second': 2.63, 'epoch': 12.0}
{'train_runtime': 19559.6513, 'train_samples_per_second': 26.945, 'train_steps_per_second': 0.842, 'train_loss': 1.118754646910658, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.1188
  train_runtime            = 5:25:59.65
  train_samples_per_second =     26.945
  train_steps_per_second   =      0.842
{'train@spa.rst.rststb_loss': 2.953932523727417, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.22767857142857142, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.045918243209154386, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.0720052436273482, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.05191357398354603, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.953932523727417, 'train@spa.rst.rststb_runtime': 27.1769, 'train@spa.rst.rststb_samples_per_second': 82.423, 'train@spa.rst.rststb_steps_per_second': 2.576, 'epoch': 1.0}
{'loss': 3.7931, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.049339771270752, 'eval_accuracy@spa.rst.rststb': 0.23237597911227154, 'eval_f1@spa.rst.rststb': 0.038319428229311246, 'eval_precision@spa.rst.rststb': 0.05656492897346556, 'eval_recall@spa.rst.rststb': 0.05057618807327079, 'eval_loss@spa.rst.rststb': 3.049339771270752, 'eval_runtime': 5.0749, 'eval_samples_per_second': 75.47, 'eval_steps_per_second': 2.365, 'epoch': 1.0}
{'train@spa.rst.rststb_loss': 2.402263879776001, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.32589285714285715, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.07257175141524533, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.13837567276820445, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.08148236407451438, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.40226411819458, 'train@spa.rst.rststb_runtime': 27.2079, 'train@spa.rst.rststb_samples_per_second': 82.329, 'train@spa.rst.rststb_steps_per_second': 2.573, 'epoch': 2.0}
{'loss': 2.7116, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.5766704082489014, 'eval_accuracy@spa.rst.rststb': 0.3368146214099217, 'eval_f1@spa.rst.rststb': 0.06568645639953724, 'eval_precision@spa.rst.rststb': 0.09725365441373154, 'eval_recall@spa.rst.rststb': 0.08115235666190869, 'eval_loss@spa.rst.rststb': 2.5766704082489014, 'eval_runtime': 5.0936, 'eval_samples_per_second': 75.192, 'eval_steps_per_second': 2.356, 'epoch': 2.0}
{'train@spa.rst.rststb_loss': 2.1356401443481445, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.43125, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.15187963815597158, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.17242346865151484, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.15736147159890784, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.1356401443481445, 'train@spa.rst.rststb_runtime': 27.197, 'train@spa.rst.rststb_samples_per_second': 82.362, 'train@spa.rst.rststb_steps_per_second': 2.574, 'epoch': 3.0}
{'loss': 2.3405, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.3441967964172363, 'eval_accuracy@spa.rst.rststb': 0.3968668407310705, 'eval_f1@spa.rst.rststb': 0.14308806215555128, 'eval_precision@spa.rst.rststb': 0.15146159746431348, 'eval_recall@spa.rst.rststb': 0.15293336001179877, 'eval_loss@spa.rst.rststb': 2.3441970348358154, 'eval_runtime': 6.0295, 'eval_samples_per_second': 63.521, 'eval_steps_per_second': 1.99, 'epoch': 3.0}
{'train@spa.rst.rststb_loss': 1.9692875146865845, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4799107142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.19131205084389408, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.20509657667637024, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.20147914673362352, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.9692875146865845, 'train@spa.rst.rststb_runtime': 27.1608, 'train@spa.rst.rststb_samples_per_second': 82.472, 'train@spa.rst.rststb_steps_per_second': 2.577, 'epoch': 4.0}
{'loss': 2.1444, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.220151662826538, 'eval_accuracy@spa.rst.rststb': 0.4516971279373368, 'eval_f1@spa.rst.rststb': 0.17177943525133205, 'eval_precision@spa.rst.rststb': 0.16086591098977637, 'eval_recall@spa.rst.rststb': 0.19191704777689594, 'eval_loss@spa.rst.rststb': 2.220151662826538, 'eval_runtime': 5.0756, 'eval_samples_per_second': 75.46, 'eval_steps_per_second': 2.364, 'epoch': 4.0}
{'train@spa.rst.rststb_loss': 1.8567308187484741, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.5008928571428571, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.21346063566000764, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.22238747406642093, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.22802752628483727, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.8567306995391846, 'train@spa.rst.rststb_runtime': 27.1679, 'train@spa.rst.rststb_samples_per_second': 82.45, 'train@spa.rst.rststb_steps_per_second': 2.577, 'epoch': 5.0}
{'loss': 2.0002, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.136518955230713, 'eval_accuracy@spa.rst.rststb': 0.4751958224543081, 'eval_f1@spa.rst.rststb': 0.2035580317795914, 'eval_precision@spa.rst.rststb': 0.1881452344089132, 'eval_recall@spa.rst.rststb': 0.23167338857769337, 'eval_loss@spa.rst.rststb': 2.1365184783935547, 'eval_runtime': 5.0636, 'eval_samples_per_second': 75.637, 'eval_steps_per_second': 2.37, 'epoch': 5.0}
{'train@spa.rst.rststb_loss': 1.7763200998306274, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.5169642857142858, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.22732726344821721, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.23602539337238318, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.2436593621287, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.7763200998306274, 'train@spa.rst.rststb_runtime': 27.152, 'train@spa.rst.rststb_samples_per_second': 82.498, 'train@spa.rst.rststb_steps_per_second': 2.578, 'epoch': 6.0}
{'loss': 1.9081, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.0787250995635986, 'eval_accuracy@spa.rst.rststb': 0.4751958224543081, 'eval_f1@spa.rst.rststb': 0.205424735982558, 'eval_precision@spa.rst.rststb': 0.18586384578670018, 'eval_recall@spa.rst.rststb': 0.23678083723398846, 'eval_loss@spa.rst.rststb': 2.0787250995635986, 'eval_runtime': 5.0456, 'eval_samples_per_second': 75.907, 'eval_steps_per_second': 2.378, 'epoch': 6.0}
{'train@spa.rst.rststb_loss': 1.72113835811615, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.5321428571428571, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.24880515760389657, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.31167270434409167, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.2597243195144013, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.721138596534729, 'train@spa.rst.rststb_runtime': 27.1778, 'train@spa.rst.rststb_samples_per_second': 82.42, 'train@spa.rst.rststb_steps_per_second': 2.576, 'epoch': 7.0}
{'loss': 1.8247, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.0319764614105225, 'eval_accuracy@spa.rst.rststb': 0.4751958224543081, 'eval_f1@spa.rst.rststb': 0.20326989995650283, 'eval_precision@spa.rst.rststb': 0.22090766890766886, 'eval_recall@spa.rst.rststb': 0.2360763406808514, 'eval_loss@spa.rst.rststb': 2.0319764614105225, 'eval_runtime': 5.0412, 'eval_samples_per_second': 75.975, 'eval_steps_per_second': 2.38, 'epoch': 7.0}
{'train@spa.rst.rststb_loss': 1.6765044927597046, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.5392857142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.2560622213621211, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.3127130611060694, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.2656527542430026, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.6765044927597046, 'train@spa.rst.rststb_runtime': 27.1769, 'train@spa.rst.rststb_samples_per_second': 82.423, 'train@spa.rst.rststb_steps_per_second': 2.576, 'epoch': 8.0}
{'loss': 1.7908, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.0081937313079834, 'eval_accuracy@spa.rst.rststb': 0.48825065274151436, 'eval_f1@spa.rst.rststb': 0.2230796051130751, 'eval_precision@spa.rst.rststb': 0.24320972789642867, 'eval_recall@spa.rst.rststb': 0.2551559327355338, 'eval_loss@spa.rst.rststb': 2.0081937313079834, 'eval_runtime': 5.0565, 'eval_samples_per_second': 75.744, 'eval_steps_per_second': 2.373, 'epoch': 8.0}
{'train@spa.rst.rststb_loss': 1.6481643915176392, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.5455357142857142, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.2623550743025219, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.3099623772900797, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.2722850101564343, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.6481642723083496, 'train@spa.rst.rststb_runtime': 27.1948, 'train@spa.rst.rststb_samples_per_second': 82.369, 'train@spa.rst.rststb_steps_per_second': 2.574, 'epoch': 9.0}
{'loss': 1.7402, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.9818707704544067, 'eval_accuracy@spa.rst.rststb': 0.4830287206266319, 'eval_f1@spa.rst.rststb': 0.21340763976167204, 'eval_precision@spa.rst.rststb': 0.23029302900489337, 'eval_recall@spa.rst.rststb': 0.24516983851912286, 'eval_loss@spa.rst.rststb': 1.9818707704544067, 'eval_runtime': 5.0293, 'eval_samples_per_second': 76.154, 'eval_steps_per_second': 2.386, 'epoch': 9.0}
{'train@spa.rst.rststb_loss': 1.6262913942337036, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.546875, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.2640795973506583, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.3117421054084871, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.27297634498072576, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.6262913942337036, 'train@spa.rst.rststb_runtime': 27.1986, 'train@spa.rst.rststb_samples_per_second': 82.357, 'train@spa.rst.rststb_steps_per_second': 2.574, 'epoch': 10.0}
{'loss': 1.7272, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.9673641920089722, 'eval_accuracy@spa.rst.rststb': 0.4856396866840731, 'eval_f1@spa.rst.rststb': 0.22815594568991984, 'eval_precision@spa.rst.rststb': 0.24258006448469135, 'eval_recall@spa.rst.rststb': 0.2571968426603182, 'eval_loss@spa.rst.rststb': 1.9673643112182617, 'eval_runtime': 5.0423, 'eval_samples_per_second': 75.957, 'eval_steps_per_second': 2.38, 'epoch': 10.0}
{'train@spa.rst.rststb_loss': 1.6153240203857422, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.5482142857142858, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.26612645451546246, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.31102949700222254, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.27519352399935954, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.6153239011764526, 'train@spa.rst.rststb_runtime': 27.2175, 'train@spa.rst.rststb_samples_per_second': 82.3, 'train@spa.rst.rststb_steps_per_second': 2.572, 'epoch': 11.0}
{'loss': 1.7203, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.9570157527923584, 'eval_accuracy@spa.rst.rststb': 0.48825065274151436, 'eval_f1@spa.rst.rststb': 0.23516661668319191, 'eval_precision@spa.rst.rststb': 0.2880604955119974, 'eval_recall@spa.rst.rststb': 0.259974620438096, 'eval_loss@spa.rst.rststb': 1.9570159912109375, 'eval_runtime': 5.0679, 'eval_samples_per_second': 75.574, 'eval_steps_per_second': 2.368, 'epoch': 11.0}
{'train@spa.rst.rststb_loss': 1.6113266944885254, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.55, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.26725468886946835, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.31017721086003497, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.2759449806731606, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.6113266944885254, 'train@spa.rst.rststb_runtime': 27.2199, 'train@spa.rst.rststb_samples_per_second': 82.293, 'train@spa.rst.rststb_steps_per_second': 2.572, 'epoch': 12.0}
{'loss': 1.6841, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.954321026802063, 'eval_accuracy@spa.rst.rststb': 0.48825065274151436, 'eval_f1@spa.rst.rststb': 0.23468241904924106, 'eval_precision@spa.rst.rststb': 0.2886458736755006, 'eval_recall@spa.rst.rststb': 0.259974620438096, 'eval_loss@spa.rst.rststb': 1.954321026802063, 'eval_runtime': 5.0596, 'eval_samples_per_second': 75.698, 'eval_steps_per_second': 2.372, 'epoch': 12.0}
{'train_runtime': 1069.8733, 'train_samples_per_second': 25.124, 'train_steps_per_second': 0.785, 'train_loss': 2.115440695626395, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.1188
  train_runtime            = 5:25:59.65
  train_samples_per_second =     26.945
  train_steps_per_second   =      0.842
-------------------------------------------------------------------
Lang1:  eng.rst.gum    Lang2:  spa.rst.rststb
Saving run to:  runs/full_shot/FullShot=v4_eng.rst.gum_spa.rst.rststb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 13897 examples
read 2149 examples
read 2091 examples
read 2240 examples
read 383 examples
read 426 examples
Total prediction labels:  32
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=32, bias=True)
    )
  )
)
{'train@eng.rst.gum_loss': 2.5322940349578857, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.24314600273440312, 'train@eng.rst.gum_f1@eng.rst.gum': 0.03924066746877547, 'train@eng.rst.gum_precision@eng.rst.gum': 0.07169612600707918, 'train@eng.rst.gum_recall@eng.rst.gum': 0.05637263155621329, 'train@eng.rst.gum_loss@eng.rst.gum': 2.5322935581207275, 'train@eng.rst.gum_runtime': 163.4333, 'train@eng.rst.gum_samples_per_second': 85.032, 'train@eng.rst.gum_steps_per_second': 2.662, 'epoch': 1.0}
{'loss': 2.7761, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.612731456756592, 'eval_accuracy@eng.rst.gum': 0.23964634713820382, 'eval_f1@eng.rst.gum': 0.04144228952011579, 'eval_precision@eng.rst.gum': 0.06580359332874057, 'eval_recall@eng.rst.gum': 0.058971266551808246, 'eval_loss@eng.rst.gum': 2.612731456756592, 'eval_runtime': 25.6118, 'eval_samples_per_second': 83.907, 'eval_steps_per_second': 2.655, 'epoch': 1.0}
{'train@eng.rst.gum_loss': 2.089158296585083, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.3822407713895085, 'train@eng.rst.gum_f1@eng.rst.gum': 0.1333062139461619, 'train@eng.rst.gum_precision@eng.rst.gum': 0.26825850932917805, 'train@eng.rst.gum_recall@eng.rst.gum': 0.1501764220589748, 'train@eng.rst.gum_loss@eng.rst.gum': 2.089158296585083, 'train@eng.rst.gum_runtime': 163.0021, 'train@eng.rst.gum_samples_per_second': 85.257, 'train@eng.rst.gum_steps_per_second': 2.669, 'epoch': 2.0}
{'loss': 2.3799, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.1895062923431396, 'eval_accuracy@eng.rst.gum': 0.354118194509074, 'eval_f1@eng.rst.gum': 0.12392084329956035, 'eval_precision@eng.rst.gum': 0.2269810678819175, 'eval_recall@eng.rst.gum': 0.1451630988859419, 'eval_loss@eng.rst.gum': 2.1895062923431396, 'eval_runtime': 25.5395, 'eval_samples_per_second': 84.144, 'eval_steps_per_second': 2.663, 'epoch': 2.0}
{'train@eng.rst.gum_loss': 1.8048015832901, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.47657767863567674, 'train@eng.rst.gum_f1@eng.rst.gum': 0.2613349261937904, 'train@eng.rst.gum_precision@eng.rst.gum': 0.41177974252239125, 'train@eng.rst.gum_recall@eng.rst.gum': 0.2726485359892625, 'train@eng.rst.gum_loss@eng.rst.gum': 1.8048014640808105, 'train@eng.rst.gum_runtime': 163.5819, 'train@eng.rst.gum_samples_per_second': 84.954, 'train@eng.rst.gum_steps_per_second': 2.659, 'epoch': 3.0}
{'loss': 2.0112, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.9287798404693604, 'eval_accuracy@eng.rst.gum': 0.44439274080967894, 'eval_f1@eng.rst.gum': 0.2506358332441767, 'eval_precision@eng.rst.gum': 0.32910216239262746, 'eval_recall@eng.rst.gum': 0.26633855293799236, 'eval_loss@eng.rst.gum': 1.9287796020507812, 'eval_runtime': 25.5512, 'eval_samples_per_second': 84.106, 'eval_steps_per_second': 2.661, 'epoch': 3.0}
{'train@eng.rst.gum_loss': 1.6758626699447632, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5057206591350651, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3022178136080025, 'train@eng.rst.gum_precision@eng.rst.gum': 0.4060493203309028, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3100918320752576, 'train@eng.rst.gum_loss@eng.rst.gum': 1.675862431526184, 'train@eng.rst.gum_runtime': 163.4705, 'train@eng.rst.gum_samples_per_second': 85.012, 'train@eng.rst.gum_steps_per_second': 2.661, 'epoch': 4.0}
{'loss': 1.8166, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.8318414688110352, 'eval_accuracy@eng.rst.gum': 0.463936714751047, 'eval_f1@eng.rst.gum': 0.2817716275118379, 'eval_precision@eng.rst.gum': 0.32325624742438913, 'eval_recall@eng.rst.gum': 0.29452664284624197, 'eval_loss@eng.rst.gum': 1.8318414688110352, 'eval_runtime': 25.5718, 'eval_samples_per_second': 84.038, 'eval_steps_per_second': 2.659, 'epoch': 4.0}
{'train@eng.rst.gum_loss': 1.5988125801086426, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5198244225372383, 'train@eng.rst.gum_f1@eng.rst.gum': 0.34325932265165554, 'train@eng.rst.gum_precision@eng.rst.gum': 0.4713442693349796, 'train@eng.rst.gum_recall@eng.rst.gum': 0.34989086526821667, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5988125801086426, 'train@eng.rst.gum_runtime': 163.0583, 'train@eng.rst.gum_samples_per_second': 85.227, 'train@eng.rst.gum_steps_per_second': 2.668, 'epoch': 5.0}
{'loss': 1.7147, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.7743293046951294, 'eval_accuracy@eng.rst.gum': 0.47696603071195903, 'eval_f1@eng.rst.gum': 0.3178585762027183, 'eval_precision@eng.rst.gum': 0.40375506027187313, 'eval_recall@eng.rst.gum': 0.3292642108061642, 'eval_loss@eng.rst.gum': 1.774329423904419, 'eval_runtime': 25.5535, 'eval_samples_per_second': 84.098, 'eval_steps_per_second': 2.661, 'epoch': 5.0}
{'train@eng.rst.gum_loss': 1.5455665588378906, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5370943369072462, 'train@eng.rst.gum_f1@eng.rst.gum': 0.37678726199513085, 'train@eng.rst.gum_precision@eng.rst.gum': 0.49495229797345824, 'train@eng.rst.gum_recall@eng.rst.gum': 0.37912056198495037, 'train@eng.rst.gum_loss@eng.rst.gum': 1.545566439628601, 'train@eng.rst.gum_runtime': 163.2965, 'train@eng.rst.gum_samples_per_second': 85.103, 'train@eng.rst.gum_steps_per_second': 2.664, 'epoch': 6.0}
{'loss': 1.6497, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.735966682434082, 'eval_accuracy@eng.rst.gum': 0.4927873429502094, 'eval_f1@eng.rst.gum': 0.3472688832723505, 'eval_precision@eng.rst.gum': 0.41612815737449854, 'eval_recall@eng.rst.gum': 0.3582516166394089, 'eval_loss@eng.rst.gum': 1.735966682434082, 'eval_runtime': 25.5479, 'eval_samples_per_second': 84.117, 'eval_steps_per_second': 2.662, 'epoch': 6.0}
{'train@eng.rst.gum_loss': 1.5085498094558716, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5441462186083328, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3903003534547638, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5391829914778924, 'train@eng.rst.gum_recall@eng.rst.gum': 0.39311594924295606, 'train@eng.rst.gum_loss@eng.rst.gum': 1.508549690246582, 'train@eng.rst.gum_runtime': 163.7071, 'train@eng.rst.gum_samples_per_second': 84.889, 'train@eng.rst.gum_steps_per_second': 2.657, 'epoch': 7.0}
{'loss': 1.6057, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.7124520540237427, 'eval_accuracy@eng.rst.gum': 0.498371335504886, 'eval_f1@eng.rst.gum': 0.36461569239119324, 'eval_precision@eng.rst.gum': 0.4901539787735903, 'eval_recall@eng.rst.gum': 0.3747981614025822, 'eval_loss@eng.rst.gum': 1.7124522924423218, 'eval_runtime': 25.5751, 'eval_samples_per_second': 84.027, 'eval_steps_per_second': 2.659, 'epoch': 7.0}
{'train@eng.rst.gum_loss': 1.483986258506775, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5509822263797942, 'train@eng.rst.gum_f1@eng.rst.gum': 0.40007491698375225, 'train@eng.rst.gum_precision@eng.rst.gum': 0.54285915409651, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4020777436678728, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4839861392974854, 'train@eng.rst.gum_runtime': 163.2181, 'train@eng.rst.gum_samples_per_second': 85.144, 'train@eng.rst.gum_steps_per_second': 2.665, 'epoch': 8.0}
{'loss': 1.5729, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.6966675519943237, 'eval_accuracy@eng.rst.gum': 0.5039553280595626, 'eval_f1@eng.rst.gum': 0.37211731452554064, 'eval_precision@eng.rst.gum': 0.49297401535831475, 'eval_recall@eng.rst.gum': 0.3844658097455971, 'eval_loss@eng.rst.gum': 1.6966674327850342, 'eval_runtime': 25.5423, 'eval_samples_per_second': 84.135, 'eval_steps_per_second': 2.662, 'epoch': 8.0}
{'train@eng.rst.gum_loss': 1.4629093408584595, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5614161329783407, 'train@eng.rst.gum_f1@eng.rst.gum': 0.41974371053999393, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5372493861389366, 'train@eng.rst.gum_recall@eng.rst.gum': 0.42052522403673726, 'train@eng.rst.gum_loss@eng.rst.gum': 1.46290922164917, 'train@eng.rst.gum_runtime': 163.2723, 'train@eng.rst.gum_samples_per_second': 85.115, 'train@eng.rst.gum_steps_per_second': 2.664, 'epoch': 9.0}
{'loss': 1.5463, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.6788498163223267, 'eval_accuracy@eng.rst.gum': 0.5062819916240112, 'eval_f1@eng.rst.gum': 0.381016981334845, 'eval_precision@eng.rst.gum': 0.4405167505623737, 'eval_recall@eng.rst.gum': 0.39384248249115666, 'eval_loss@eng.rst.gum': 1.6788498163223267, 'eval_runtime': 25.5749, 'eval_samples_per_second': 84.028, 'eval_steps_per_second': 2.659, 'epoch': 9.0}
{'train@eng.rst.gum_loss': 1.447670817375183, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5632150823918831, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4243212983606629, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5403835032287043, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4208037639242506, 'train@eng.rst.gum_loss@eng.rst.gum': 1.447670817375183, 'train@eng.rst.gum_runtime': 163.6211, 'train@eng.rst.gum_samples_per_second': 84.934, 'train@eng.rst.gum_steps_per_second': 2.659, 'epoch': 10.0}
{'loss': 1.5276, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.6691105365753174, 'eval_accuracy@eng.rst.gum': 0.5132619823173569, 'eval_f1@eng.rst.gum': 0.387207115083979, 'eval_precision@eng.rst.gum': 0.46440821226891676, 'eval_recall@eng.rst.gum': 0.3964038383671013, 'eval_loss@eng.rst.gum': 1.6691105365753174, 'eval_runtime': 25.6402, 'eval_samples_per_second': 83.814, 'eval_steps_per_second': 2.652, 'epoch': 10.0}
{'train@eng.rst.gum_loss': 1.4404326677322388, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5653738216881341, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4292057735778339, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5346723729617421, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4288615221513385, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4404325485229492, 'train@eng.rst.gum_runtime': 163.193, 'train@eng.rst.gum_samples_per_second': 85.157, 'train@eng.rst.gum_steps_per_second': 2.666, 'epoch': 11.0}
{'loss': 1.5264, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.6634844541549683, 'eval_accuracy@eng.rst.gum': 0.5123313168915775, 'eval_f1@eng.rst.gum': 0.38995512827124307, 'eval_precision@eng.rst.gum': 0.45154925384451144, 'eval_recall@eng.rst.gum': 0.40157273561885665, 'eval_loss@eng.rst.gum': 1.6634843349456787, 'eval_runtime': 25.5659, 'eval_samples_per_second': 84.057, 'eval_steps_per_second': 2.66, 'epoch': 11.0}
{'train@eng.rst.gum_loss': 1.4372642040252686, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5656616535943009, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4293953279714525, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5358325444264366, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4282477880923997, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4372642040252686, 'train@eng.rst.gum_runtime': 163.6414, 'train@eng.rst.gum_samples_per_second': 84.923, 'train@eng.rst.gum_steps_per_second': 2.658, 'epoch': 12.0}
{'loss': 1.5127, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.6619948148727417, 'eval_accuracy@eng.rst.gum': 0.5132619823173569, 'eval_f1@eng.rst.gum': 0.39067097595892003, 'eval_precision@eng.rst.gum': 0.456776200591801, 'eval_recall@eng.rst.gum': 0.4015646901191118, 'eval_loss@eng.rst.gum': 1.6619949340820312, 'eval_runtime': 25.6005, 'eval_samples_per_second': 83.944, 'eval_steps_per_second': 2.656, 'epoch': 12.0}
{'train_runtime': 6413.5827, 'train_samples_per_second': 26.002, 'train_steps_per_second': 0.814, 'train_loss': 1.8033208737428161, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.8033
  train_runtime            = 1:46:53.58
  train_samples_per_second =     26.002
  train_steps_per_second   =      0.814
{'train@spa.rst.rststb_loss': 2.6428322792053223, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.29017857142857145, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.04954904755412859, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.07553136970117721, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.06203235947257178, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.6428322792053223, 'train@spa.rst.rststb_runtime': 27.0732, 'train@spa.rst.rststb_samples_per_second': 82.739, 'train@spa.rst.rststb_steps_per_second': 2.586, 'epoch': 1.0}
{'loss': 3.1973, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.6791253089904785, 'eval_accuracy@spa.rst.rststb': 0.2741514360313316, 'eval_f1@spa.rst.rststb': 0.0492409732044633, 'eval_precision@spa.rst.rststb': 0.07188159879336349, 'eval_recall@spa.rst.rststb': 0.06387617436408292, 'eval_loss@spa.rst.rststb': 2.6791253089904785, 'eval_runtime': 4.9175, 'eval_samples_per_second': 77.885, 'eval_steps_per_second': 2.44, 'epoch': 1.0}
{'train@spa.rst.rststb_loss': 2.299926280975342, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.36428571428571427, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.09305084153365177, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.12446073638567508, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.09972292236856357, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.299926519393921, 'train@spa.rst.rststb_runtime': 27.0719, 'train@spa.rst.rststb_samples_per_second': 82.743, 'train@spa.rst.rststb_steps_per_second': 2.586, 'epoch': 2.0}
{'loss': 2.5168, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.4132187366485596, 'eval_accuracy@spa.rst.rststb': 0.3681462140992167, 'eval_f1@spa.rst.rststb': 0.12486264613315119, 'eval_precision@spa.rst.rststb': 0.1994864341085271, 'eval_recall@spa.rst.rststb': 0.12963865275106723, 'eval_loss@spa.rst.rststb': 2.4132187366485596, 'eval_runtime': 4.8763, 'eval_samples_per_second': 78.543, 'eval_steps_per_second': 2.461, 'epoch': 2.0}
{'train@spa.rst.rststb_loss': 2.0828046798706055, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4290178571428571, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.12795649016017857, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.1286980014093945, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.1415088231476061, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0828046798706055, 'train@spa.rst.rststb_runtime': 27.0535, 'train@spa.rst.rststb_samples_per_second': 82.799, 'train@spa.rst.rststb_steps_per_second': 2.587, 'epoch': 3.0}
{'loss': 2.2557, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.247934341430664, 'eval_accuracy@spa.rst.rststb': 0.4151436031331593, 'eval_f1@spa.rst.rststb': 0.15145585081132862, 'eval_precision@spa.rst.rststb': 0.15813311205920993, 'eval_recall@spa.rst.rststb': 0.16473993597904488, 'eval_loss@spa.rst.rststb': 2.247934579849243, 'eval_runtime': 4.8821, 'eval_samples_per_second': 78.45, 'eval_steps_per_second': 2.458, 'epoch': 3.0}
{'train@spa.rst.rststb_loss': 1.9347782135009766, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4455357142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.1355684927615526, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.16841002757289744, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.15712380373863963, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.934778094291687, 'train@spa.rst.rststb_runtime': 27.0505, 'train@spa.rst.rststb_samples_per_second': 82.808, 'train@spa.rst.rststb_steps_per_second': 2.588, 'epoch': 4.0}
{'loss': 2.0887, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.133840799331665, 'eval_accuracy@spa.rst.rststb': 0.4360313315926893, 'eval_f1@spa.rst.rststb': 0.1611313585739814, 'eval_precision@spa.rst.rststb': 0.17501883968105414, 'eval_recall@spa.rst.rststb': 0.1847943219806323, 'eval_loss@spa.rst.rststb': 2.133840322494507, 'eval_runtime': 4.8997, 'eval_samples_per_second': 78.169, 'eval_steps_per_second': 2.449, 'epoch': 4.0}
{'train@spa.rst.rststb_loss': 1.8340972661972046, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.46339285714285716, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.14331916026362115, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.1795205602865146, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.16728533439793786, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.8340975046157837, 'train@spa.rst.rststb_runtime': 27.071, 'train@spa.rst.rststb_samples_per_second': 82.745, 'train@spa.rst.rststb_steps_per_second': 2.586, 'epoch': 5.0}
{'loss': 1.9624, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.070986032485962, 'eval_accuracy@spa.rst.rststb': 0.4412532637075718, 'eval_f1@spa.rst.rststb': 0.15710589521225346, 'eval_precision@spa.rst.rststb': 0.19146771840555765, 'eval_recall@spa.rst.rststb': 0.1825350075155182, 'eval_loss@spa.rst.rststb': 2.070985794067383, 'eval_runtime': 4.8834, 'eval_samples_per_second': 78.429, 'eval_steps_per_second': 2.457, 'epoch': 5.0}
{'train@spa.rst.rststb_loss': 1.764273762702942, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.47946428571428573, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.15997374321897254, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.19884529138587181, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.17930860036677884, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.764273762702942, 'train@spa.rst.rststb_runtime': 27.0479, 'train@spa.rst.rststb_samples_per_second': 82.816, 'train@spa.rst.rststb_steps_per_second': 2.588, 'epoch': 6.0}
{'loss': 1.8772, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.023648738861084, 'eval_accuracy@spa.rst.rststb': 0.4464751958224543, 'eval_f1@spa.rst.rststb': 0.17377803523450872, 'eval_precision@spa.rst.rststb': 0.2199478920731781, 'eval_recall@spa.rst.rststb': 0.19176188244434778, 'eval_loss@spa.rst.rststb': 2.023648738861084, 'eval_runtime': 4.875, 'eval_samples_per_second': 78.564, 'eval_steps_per_second': 2.462, 'epoch': 6.0}
{'train@spa.rst.rststb_loss': 1.7135752439498901, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4924107142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.1809685470329114, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.22886000437993823, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.1964601803273924, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.7135753631591797, 'train@spa.rst.rststb_runtime': 27.0442, 'train@spa.rst.rststb_samples_per_second': 82.827, 'train@spa.rst.rststb_steps_per_second': 2.588, 'epoch': 7.0}
{'loss': 1.8118, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.9905498027801514, 'eval_accuracy@spa.rst.rststb': 0.4621409921671018, 'eval_f1@spa.rst.rststb': 0.2000761296260661, 'eval_precision@spa.rst.rststb': 0.23139253836891327, 'eval_recall@spa.rst.rststb': 0.21612393979655206, 'eval_loss@spa.rst.rststb': 1.9905495643615723, 'eval_runtime': 4.9127, 'eval_samples_per_second': 77.962, 'eval_steps_per_second': 2.443, 'epoch': 7.0}
{'train@spa.rst.rststb_loss': 1.6781537532806396, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.5040178571428572, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.20121189057120853, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.24240998266123384, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.21391425396653777, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.6781537532806396, 'train@spa.rst.rststb_runtime': 27.0558, 'train@spa.rst.rststb_samples_per_second': 82.792, 'train@spa.rst.rststb_steps_per_second': 2.587, 'epoch': 8.0}
{'loss': 1.7739, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.97200345993042, 'eval_accuracy@spa.rst.rststb': 0.4621409921671018, 'eval_f1@spa.rst.rststb': 0.20633902432866216, 'eval_precision@spa.rst.rststb': 0.2327054681795698, 'eval_recall@spa.rst.rststb': 0.21978211948633844, 'eval_loss@spa.rst.rststb': 1.9720033407211304, 'eval_runtime': 4.8923, 'eval_samples_per_second': 78.287, 'eval_steps_per_second': 2.453, 'epoch': 8.0}
{'train@spa.rst.rststb_loss': 1.653526782989502, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.5125, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.2119191634393369, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.24947691843280703, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.2262604428408866, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.653526782989502, 'train@spa.rst.rststb_runtime': 27.0393, 'train@spa.rst.rststb_samples_per_second': 82.842, 'train@spa.rst.rststb_steps_per_second': 2.589, 'epoch': 9.0}
{'loss': 1.7337, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.9493563175201416, 'eval_accuracy@spa.rst.rststb': 0.46475195822454307, 'eval_f1@spa.rst.rststb': 0.20857369654366398, 'eval_precision@spa.rst.rststb': 0.22810985446824383, 'eval_recall@spa.rst.rststb': 0.22168817807616312, 'eval_loss@spa.rst.rststb': 1.9493564367294312, 'eval_runtime': 4.9015, 'eval_samples_per_second': 78.139, 'eval_steps_per_second': 2.448, 'epoch': 9.0}
{'train@spa.rst.rststb_loss': 1.6348013877868652, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.5183035714285714, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.2161474671051078, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.25577609178610144, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.22973097322490066, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.6348012685775757, 'train@spa.rst.rststb_runtime': 27.0477, 'train@spa.rst.rststb_samples_per_second': 82.817, 'train@spa.rst.rststb_steps_per_second': 2.588, 'epoch': 10.0}
{'loss': 1.7167, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.94026517868042, 'eval_accuracy@spa.rst.rststb': 0.4621409921671018, 'eval_f1@spa.rst.rststb': 0.20922774271134845, 'eval_precision@spa.rst.rststb': 0.23164088153006054, 'eval_recall@spa.rst.rststb': 0.22116075191582554, 'eval_loss@spa.rst.rststb': 1.94026517868042, 'eval_runtime': 4.8805, 'eval_samples_per_second': 78.476, 'eval_steps_per_second': 2.459, 'epoch': 10.0}
{'train@spa.rst.rststb_loss': 1.6251225471496582, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.521875, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.22239644470913286, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.2595661561473218, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.23572222793790484, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.6251225471496582, 'train@spa.rst.rststb_runtime': 27.0151, 'train@spa.rst.rststb_samples_per_second': 82.917, 'train@spa.rst.rststb_steps_per_second': 2.591, 'epoch': 11.0}
{'loss': 1.7051, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.9326777458190918, 'eval_accuracy@spa.rst.rststb': 0.4595300261096606, 'eval_f1@spa.rst.rststb': 0.2085171136142204, 'eval_precision@spa.rst.rststb': 0.22844604352691658, 'eval_recall@spa.rst.rststb': 0.220633325755488, 'eval_loss@spa.rst.rststb': 1.9326777458190918, 'eval_runtime': 4.9091, 'eval_samples_per_second': 78.018, 'eval_steps_per_second': 2.444, 'epoch': 11.0}
{'train@spa.rst.rststb_loss': 1.6220428943634033, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.5227678571428571, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.22353344907680528, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.2600139289060846, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.23708277215559193, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.6220428943634033, 'train@spa.rst.rststb_runtime': 26.9811, 'train@spa.rst.rststb_samples_per_second': 83.021, 'train@spa.rst.rststb_steps_per_second': 2.594, 'epoch': 12.0}
{'loss': 1.6934, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.9298491477966309, 'eval_accuracy@spa.rst.rststb': 0.4595300261096606, 'eval_f1@spa.rst.rststb': 0.2085171136142204, 'eval_precision@spa.rst.rststb': 0.22844604352691658, 'eval_recall@spa.rst.rststb': 0.220633325755488, 'eval_loss@spa.rst.rststb': 1.9298489093780518, 'eval_runtime': 4.8926, 'eval_samples_per_second': 78.282, 'eval_steps_per_second': 2.453, 'epoch': 12.0}
{'train_runtime': 1065.5018, 'train_samples_per_second': 25.228, 'train_steps_per_second': 0.788, 'train_loss': 2.0277371906098867, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.8033
  train_runtime            = 1:46:53.58
  train_samples_per_second =     26.002
  train_steps_per_second   =      0.814
-------------------------------------------------------------------
Lang1:  eng.rst.rstdt    Lang2:  spa.rst.rststb
Saving run to:  runs/full_shot/FullShot=v4_eng.rst.rstdt_spa.rst.rststb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 16002 examples
read 1621 examples
read 2155 examples
read 2240 examples
read 383 examples
read 426 examples
Total prediction labels:  37
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=37, bias=True)
    )
  )
)
{'train@eng.rst.rstdt_loss': 1.763450026512146, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.5116860392450944, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.08334558364253321, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.10062121992334494, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.1107979020525213, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.7634499073028564, 'train@eng.rst.rstdt_runtime': 188.1435, 'train@eng.rst.rstdt_samples_per_second': 85.052, 'train@eng.rst.rstdt_steps_per_second': 2.663, 'epoch': 1.0}
{'loss': 2.2043, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.7431602478027344, 'eval_accuracy@eng.rst.rstdt': 0.5219000616903147, 'eval_f1@eng.rst.rstdt': 0.08307116987665784, 'eval_precision@eng.rst.rstdt': 0.08138542203845411, 'eval_recall@eng.rst.rstdt': 0.1095470353229689, 'eval_loss@eng.rst.rstdt': 1.7431602478027344, 'eval_runtime': 19.3984, 'eval_samples_per_second': 83.563, 'eval_steps_per_second': 2.629, 'epoch': 1.0}
{'train@eng.rst.rstdt_loss': 1.4404388666152954, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.5984876890388702, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.19227754232538646, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.29936703790207037, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.20242995876818015, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.440438985824585, 'train@eng.rst.rstdt_runtime': 188.2886, 'train@eng.rst.rstdt_samples_per_second': 84.987, 'train@eng.rst.rstdt_steps_per_second': 2.661, 'epoch': 2.0}
{'loss': 1.6288, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.4401367902755737, 'eval_accuracy@eng.rst.rstdt': 0.6045650832819247, 'eval_f1@eng.rst.rstdt': 0.19824005387185706, 'eval_precision@eng.rst.rstdt': 0.2953452040283205, 'eval_recall@eng.rst.rstdt': 0.20473611467366387, 'eval_loss@eng.rst.rstdt': 1.4401367902755737, 'eval_runtime': 19.3957, 'eval_samples_per_second': 83.575, 'eval_steps_per_second': 2.629, 'epoch': 2.0}
{'train@eng.rst.rstdt_loss': 1.329525351524353, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6271716035495563, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.26320910657299945, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.4208211332456218, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.2559974893632005, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.3295254707336426, 'train@eng.rst.rstdt_runtime': 187.753, 'train@eng.rst.rstdt_samples_per_second': 85.229, 'train@eng.rst.rstdt_steps_per_second': 2.668, 'epoch': 3.0}
{'loss': 1.4419, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.3561360836029053, 'eval_accuracy@eng.rst.rstdt': 0.6280074028377545, 'eval_f1@eng.rst.rstdt': 0.24661104080488822, 'eval_precision@eng.rst.rstdt': 0.37358379439584627, 'eval_recall@eng.rst.rstdt': 0.24706174765433744, 'eval_loss@eng.rst.rstdt': 1.3561362028121948, 'eval_runtime': 19.3601, 'eval_samples_per_second': 83.729, 'eval_steps_per_second': 2.634, 'epoch': 3.0}
{'train@eng.rst.rstdt_loss': 1.2617729902267456, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6428571428571429, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.331009929066007, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.4425449748688374, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.31275702575577224, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.2617729902267456, 'train@eng.rst.rstdt_runtime': 188.3762, 'train@eng.rst.rstdt_samples_per_second': 84.947, 'train@eng.rst.rstdt_steps_per_second': 2.66, 'epoch': 4.0}
{'loss': 1.348, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.3146061897277832, 'eval_accuracy@eng.rst.rstdt': 0.634793337446021, 'eval_f1@eng.rst.rstdt': 0.316113544738473, 'eval_precision@eng.rst.rstdt': 0.4159980737187527, 'eval_recall@eng.rst.rstdt': 0.30836833968816757, 'eval_loss@eng.rst.rstdt': 1.3146063089370728, 'eval_runtime': 19.4189, 'eval_samples_per_second': 83.476, 'eval_steps_per_second': 2.626, 'epoch': 4.0}
{'train@eng.rst.rstdt_loss': 1.210026741027832, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6542932133483315, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.34917736439105507, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.477776518143522, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3261949247751255, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.2100268602371216, 'train@eng.rst.rstdt_runtime': 188.1047, 'train@eng.rst.rstdt_samples_per_second': 85.07, 'train@eng.rst.rstdt_steps_per_second': 2.663, 'epoch': 5.0}
{'loss': 1.2896, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.2703689336776733, 'eval_accuracy@eng.rst.rstdt': 0.6421961752004935, 'eval_f1@eng.rst.rstdt': 0.3194672233340883, 'eval_precision@eng.rst.rstdt': 0.413320037931931, 'eval_recall@eng.rst.rstdt': 0.31507398444415236, 'eval_loss@eng.rst.rstdt': 1.270369052886963, 'eval_runtime': 19.3829, 'eval_samples_per_second': 83.63, 'eval_steps_per_second': 2.631, 'epoch': 5.0}
{'train@eng.rst.rstdt_loss': 1.173133134841919, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6613548306461692, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.36936206200858596, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.585998907738711, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.34109226288736544, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1731332540512085, 'train@eng.rst.rstdt_runtime': 188.2577, 'train@eng.rst.rstdt_samples_per_second': 85.001, 'train@eng.rst.rstdt_steps_per_second': 2.661, 'epoch': 6.0}
{'loss': 1.2428, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.2429587841033936, 'eval_accuracy@eng.rst.rstdt': 0.6465144972239358, 'eval_f1@eng.rst.rstdt': 0.3317672518512561, 'eval_precision@eng.rst.rstdt': 0.4079658394709598, 'eval_recall@eng.rst.rstdt': 0.32481572824798743, 'eval_loss@eng.rst.rstdt': 1.2429587841033936, 'eval_runtime': 19.4049, 'eval_samples_per_second': 83.536, 'eval_steps_per_second': 2.628, 'epoch': 6.0}
{'train@eng.rst.rstdt_loss': 1.1522769927978516, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6641044869391326, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.37944940365209207, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5846679472573859, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.34642044333149796, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1522769927978516, 'train@eng.rst.rstdt_runtime': 188.2629, 'train@eng.rst.rstdt_samples_per_second': 84.998, 'train@eng.rst.rstdt_steps_per_second': 2.661, 'epoch': 7.0}
{'loss': 1.2185, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.2235984802246094, 'eval_accuracy@eng.rst.rstdt': 0.6471314003701419, 'eval_f1@eng.rst.rstdt': 0.33858964088463783, 'eval_precision@eng.rst.rstdt': 0.4070486722470199, 'eval_recall@eng.rst.rstdt': 0.3303746474518545, 'eval_loss@eng.rst.rstdt': 1.2235984802246094, 'eval_runtime': 19.4027, 'eval_samples_per_second': 83.545, 'eval_steps_per_second': 2.628, 'epoch': 7.0}
{'train@eng.rst.rstdt_loss': 1.1342827081680298, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6677290338707662, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3915265166208395, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5863370851280746, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.35936792932653183, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1342827081680298, 'train@eng.rst.rstdt_runtime': 187.8176, 'train@eng.rst.rstdt_samples_per_second': 85.2, 'train@eng.rst.rstdt_steps_per_second': 2.667, 'epoch': 8.0}
{'loss': 1.1964, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.2166389226913452, 'eval_accuracy@eng.rst.rstdt': 0.6471314003701419, 'eval_f1@eng.rst.rstdt': 0.3517682613053581, 'eval_precision@eng.rst.rstdt': 0.5279993718401336, 'eval_recall@eng.rst.rstdt': 0.34105715853625623, 'eval_loss@eng.rst.rstdt': 1.2166389226913452, 'eval_runtime': 19.3435, 'eval_samples_per_second': 83.801, 'eval_steps_per_second': 2.637, 'epoch': 8.0}
{'train@eng.rst.rstdt_loss': 1.1222950220108032, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6695413073365829, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3972077140050052, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5888537805176973, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.36251386870045404, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1222951412200928, 'train@eng.rst.rstdt_runtime': 188.3502, 'train@eng.rst.rstdt_samples_per_second': 84.959, 'train@eng.rst.rstdt_steps_per_second': 2.66, 'epoch': 9.0}
{'loss': 1.1778, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.2066712379455566, 'eval_accuracy@eng.rst.rstdt': 0.6483652066625539, 'eval_f1@eng.rst.rstdt': 0.35842075996687733, 'eval_precision@eng.rst.rstdt': 0.5292925587561355, 'eval_recall@eng.rst.rstdt': 0.34760865478409264, 'eval_loss@eng.rst.rstdt': 1.2066712379455566, 'eval_runtime': 19.3988, 'eval_samples_per_second': 83.562, 'eval_steps_per_second': 2.629, 'epoch': 9.0}
{'train@eng.rst.rstdt_loss': 1.1169078350067139, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6695413073365829, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4062970891101231, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5710846501086256, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.37280929306436533, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1169078350067139, 'train@eng.rst.rstdt_runtime': 188.4617, 'train@eng.rst.rstdt_samples_per_second': 84.908, 'train@eng.rst.rstdt_steps_per_second': 2.658, 'epoch': 10.0}
{'loss': 1.1689, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.2116490602493286, 'eval_accuracy@eng.rst.rstdt': 0.6458975940777298, 'eval_f1@eng.rst.rstdt': 0.36600572640425166, 'eval_precision@eng.rst.rstdt': 0.5223097752537361, 'eval_recall@eng.rst.rstdt': 0.3576320985853434, 'eval_loss@eng.rst.rstdt': 1.211648941040039, 'eval_runtime': 19.4528, 'eval_samples_per_second': 83.33, 'eval_steps_per_second': 2.622, 'epoch': 10.0}
{'train@eng.rst.rstdt_loss': 1.1096235513687134, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.671541057367829, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.40514474622804575, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5766468288049371, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.37009031819758637, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1096235513687134, 'train@eng.rst.rstdt_runtime': 188.1241, 'train@eng.rst.rstdt_samples_per_second': 85.061, 'train@eng.rst.rstdt_steps_per_second': 2.663, 'epoch': 11.0}
{'loss': 1.1616, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.2024282217025757, 'eval_accuracy@eng.rst.rstdt': 0.6471314003701419, 'eval_f1@eng.rst.rstdt': 0.3678216378454789, 'eval_precision@eng.rst.rstdt': 0.5316063778855815, 'eval_recall@eng.rst.rstdt': 0.35595295160861723, 'eval_loss@eng.rst.rstdt': 1.2024282217025757, 'eval_runtime': 19.3734, 'eval_samples_per_second': 83.671, 'eval_steps_per_second': 2.632, 'epoch': 11.0}
{'train@eng.rst.rstdt_loss': 1.1081119775772095, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6712285964254469, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.40506719181754497, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5750391554388197, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3695747030633325, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1081119775772095, 'train@eng.rst.rstdt_runtime': 188.5036, 'train@eng.rst.rstdt_samples_per_second': 84.89, 'train@eng.rst.rstdt_steps_per_second': 2.658, 'epoch': 12.0}
{'loss': 1.1589, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.2005749940872192, 'eval_accuracy@eng.rst.rstdt': 0.6483652066625539, 'eval_f1@eng.rst.rstdt': 0.3686857326911323, 'eval_precision@eng.rst.rstdt': 0.5332137833357811, 'eval_recall@eng.rst.rstdt': 0.3558609195747762, 'eval_loss@eng.rst.rstdt': 1.2005751132965088, 'eval_runtime': 19.4084, 'eval_samples_per_second': 83.52, 'eval_steps_per_second': 2.628, 'epoch': 12.0}
{'train_runtime': 7267.8563, 'train_samples_per_second': 26.421, 'train_steps_per_second': 0.827, 'train_loss': 1.353114723921298, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.3531
  train_runtime            = 2:01:07.85
  train_samples_per_second =     26.421
  train_steps_per_second   =      0.827
{'train@spa.rst.rststb_loss': 2.834913969039917, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.234375, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.03288200521466278, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.08292503513214773, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.04658119245650696, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.834913969039917, 'train@spa.rst.rststb_runtime': 27.1043, 'train@spa.rst.rststb_samples_per_second': 82.644, 'train@spa.rst.rststb_steps_per_second': 2.583, 'epoch': 1.0}
{'loss': 3.5635, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.876908540725708, 'eval_accuracy@spa.rst.rststb': 0.2349869451697128, 'eval_f1@spa.rst.rststb': 0.040534194529951245, 'eval_precision@spa.rst.rststb': 0.10229020979020977, 'eval_recall@spa.rst.rststb': 0.05519162315579754, 'eval_loss@spa.rst.rststb': 2.876908779144287, 'eval_runtime': 4.9302, 'eval_samples_per_second': 77.684, 'eval_steps_per_second': 2.434, 'epoch': 1.0}
{'train@spa.rst.rststb_loss': 2.4403860569000244, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.3495535714285714, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.0718228542637339, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.09714726769973081, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.08320960576557347, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.4403862953186035, 'train@spa.rst.rststb_runtime': 27.1048, 'train@spa.rst.rststb_samples_per_second': 82.642, 'train@spa.rst.rststb_steps_per_second': 2.583, 'epoch': 2.0}
{'loss': 2.6736, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.562817335128784, 'eval_accuracy@spa.rst.rststb': 0.32114882506527415, 'eval_f1@spa.rst.rststb': 0.07118182396601923, 'eval_precision@spa.rst.rststb': 0.09960832925949205, 'eval_recall@spa.rst.rststb': 0.08519408495497198, 'eval_loss@spa.rst.rststb': 2.5628175735473633, 'eval_runtime': 4.9134, 'eval_samples_per_second': 77.95, 'eval_steps_per_second': 2.442, 'epoch': 2.0}
{'train@spa.rst.rststb_loss': 2.231867790222168, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4017857142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10525826767057357, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.11328017950494922, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.11702146622930677, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.231868028640747, 'train@spa.rst.rststb_runtime': 27.0966, 'train@spa.rst.rststb_samples_per_second': 82.667, 'train@spa.rst.rststb_steps_per_second': 2.583, 'epoch': 3.0}
{'loss': 2.4082, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.3979036808013916, 'eval_accuracy@spa.rst.rststb': 0.3838120104438642, 'eval_f1@spa.rst.rststb': 0.12516757257660108, 'eval_precision@spa.rst.rststb': 0.16470864661654136, 'eval_recall@spa.rst.rststb': 0.13327211473784265, 'eval_loss@spa.rst.rststb': 2.3979036808013916, 'eval_runtime': 4.9192, 'eval_samples_per_second': 77.858, 'eval_steps_per_second': 2.439, 'epoch': 3.0}
{'train@spa.rst.rststb_loss': 2.0879900455474854, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4348214285714286, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.12485168495343431, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.1559965077809653, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.14031186481299088, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0879902839660645, 'train@spa.rst.rststb_runtime': 27.0995, 'train@spa.rst.rststb_samples_per_second': 82.658, 'train@spa.rst.rststb_steps_per_second': 2.583, 'epoch': 4.0}
{'loss': 2.2432, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.290055751800537, 'eval_accuracy@spa.rst.rststb': 0.4151436031331593, 'eval_f1@spa.rst.rststb': 0.1412094002763017, 'eval_precision@spa.rst.rststb': 0.1611128591052047, 'eval_recall@spa.rst.rststb': 0.1534446930997133, 'eval_loss@spa.rst.rststb': 2.290055513381958, 'eval_runtime': 4.9272, 'eval_samples_per_second': 77.732, 'eval_steps_per_second': 2.435, 'epoch': 4.0}
{'train@spa.rst.rststb_loss': 1.9811967611312866, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4455357142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.12909219974291938, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.15380861249841596, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.14542955165649304, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.9811967611312866, 'train@spa.rst.rststb_runtime': 27.1023, 'train@spa.rst.rststb_samples_per_second': 82.65, 'train@spa.rst.rststb_steps_per_second': 2.583, 'epoch': 5.0}
{'loss': 2.1184, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.2200868129730225, 'eval_accuracy@spa.rst.rststb': 0.4177545691906005, 'eval_f1@spa.rst.rststb': 0.14445675740776162, 'eval_precision@spa.rst.rststb': 0.15880832482260065, 'eval_recall@spa.rst.rststb': 0.15686553449487883, 'eval_loss@spa.rst.rststb': 2.2200872898101807, 'eval_runtime': 4.918, 'eval_samples_per_second': 77.877, 'eval_steps_per_second': 2.44, 'epoch': 5.0}
{'train@spa.rst.rststb_loss': 1.8989946842193604, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.465625, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.1476234032390209, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.19437434942205414, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.16260080844695565, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.8989944458007812, 'train@spa.rst.rststb_runtime': 27.0863, 'train@spa.rst.rststb_samples_per_second': 82.698, 'train@spa.rst.rststb_steps_per_second': 2.584, 'epoch': 6.0}
{'loss': 2.0239, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.1613786220550537, 'eval_accuracy@spa.rst.rststb': 0.43342036553524804, 'eval_f1@spa.rst.rststb': 0.17107958571519696, 'eval_precision@spa.rst.rststb': 0.20564893139943483, 'eval_recall@spa.rst.rststb': 0.1793528027186715, 'eval_loss@spa.rst.rststb': 2.161378860473633, 'eval_runtime': 4.9424, 'eval_samples_per_second': 77.493, 'eval_steps_per_second': 2.428, 'epoch': 6.0}
{'train@spa.rst.rststb_loss': 1.8348288536071777, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4772321428571429, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.16396672973138046, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.20360741630745363, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.17544659474450694, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.8348288536071777, 'train@spa.rst.rststb_runtime': 27.0671, 'train@spa.rst.rststb_samples_per_second': 82.757, 'train@spa.rst.rststb_steps_per_second': 2.586, 'epoch': 7.0}
{'loss': 1.9464, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.126683235168457, 'eval_accuracy@spa.rst.rststb': 0.4464751958224543, 'eval_f1@spa.rst.rststb': 0.19082791941566757, 'eval_precision@spa.rst.rststb': 0.19923881327549528, 'eval_recall@spa.rst.rststb': 0.20435867088573326, 'eval_loss@spa.rst.rststb': 2.126683473587036, 'eval_runtime': 4.921, 'eval_samples_per_second': 77.829, 'eval_steps_per_second': 2.439, 'epoch': 7.0}
{'train@spa.rst.rststb_loss': 1.792496681213379, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4830357142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.16808226198977164, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.20175038595609535, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.1787443081048314, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.7924964427947998, 'train@spa.rst.rststb_runtime': 27.0488, 'train@spa.rst.rststb_samples_per_second': 82.813, 'train@spa.rst.rststb_steps_per_second': 2.588, 'epoch': 8.0}
{'loss': 1.905, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.1013200283050537, 'eval_accuracy@spa.rst.rststb': 0.45430809399477806, 'eval_f1@spa.rst.rststb': 0.19114967430062832, 'eval_precision@spa.rst.rststb': 0.1972524957879649, 'eval_recall@spa.rst.rststb': 0.20610831561749543, 'eval_loss@spa.rst.rststb': 2.1013197898864746, 'eval_runtime': 4.9236, 'eval_samples_per_second': 77.788, 'eval_steps_per_second': 2.437, 'epoch': 8.0}
{'train@spa.rst.rststb_loss': 1.7574161291122437, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4973214285714286, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.18416184696775945, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.2052144249330993, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.1959262879660638, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.7574161291122437, 'train@spa.rst.rststb_runtime': 27.8999, 'train@spa.rst.rststb_samples_per_second': 80.287, 'train@spa.rst.rststb_steps_per_second': 2.509, 'epoch': 9.0}
{'loss': 1.8604, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.071056604385376, 'eval_accuracy@spa.rst.rststb': 0.4516971279373368, 'eval_f1@spa.rst.rststb': 0.1867094378996269, 'eval_precision@spa.rst.rststb': 0.19048791622321035, 'eval_recall@spa.rst.rststb': 0.19975030783206982, 'eval_loss@spa.rst.rststb': 2.071056604385376, 'eval_runtime': 4.9285, 'eval_samples_per_second': 77.711, 'eval_steps_per_second': 2.435, 'epoch': 9.0}
{'train@spa.rst.rststb_loss': 1.7350389957427979, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.5071428571428571, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.19268111678811867, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.21088265846776427, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.20404342569147998, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.7350388765335083, 'train@spa.rst.rststb_runtime': 27.0555, 'train@spa.rst.rststb_samples_per_second': 82.793, 'train@spa.rst.rststb_steps_per_second': 2.587, 'epoch': 10.0}
{'loss': 1.8307, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.0567214488983154, 'eval_accuracy@spa.rst.rststb': 0.4464751958224543, 'eval_f1@spa.rst.rststb': 0.18618755315712168, 'eval_precision@spa.rst.rststb': 0.19011436002453963, 'eval_recall@spa.rst.rststb': 0.19869545551139475, 'eval_loss@spa.rst.rststb': 2.0567214488983154, 'eval_runtime': 4.9276, 'eval_samples_per_second': 77.726, 'eval_steps_per_second': 2.435, 'epoch': 10.0}
{'train@spa.rst.rststb_loss': 1.7222976684570312, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.5075892857142857, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.19425105712518856, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.20844060019853958, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.20753739824382073, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.7222976684570312, 'train@spa.rst.rststb_runtime': 27.076, 'train@spa.rst.rststb_samples_per_second': 82.73, 'train@spa.rst.rststb_steps_per_second': 2.585, 'epoch': 11.0}
{'loss': 1.8203, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.048203229904175, 'eval_accuracy@spa.rst.rststb': 0.4516971279373368, 'eval_f1@spa.rst.rststb': 0.1852400119480577, 'eval_precision@spa.rst.rststb': 0.18709943692387512, 'eval_recall@spa.rst.rststb': 0.19975030783206982, 'eval_loss@spa.rst.rststb': 2.048203468322754, 'eval_runtime': 4.9075, 'eval_samples_per_second': 78.044, 'eval_steps_per_second': 2.445, 'epoch': 11.0}
{'train@spa.rst.rststb_loss': 1.7181040048599243, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.509375, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.19481478186723902, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.21036677687153715, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.20810235593594992, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.7181040048599243, 'train@spa.rst.rststb_runtime': 27.081, 'train@spa.rst.rststb_samples_per_second': 82.715, 'train@spa.rst.rststb_steps_per_second': 2.585, 'epoch': 12.0}
{'loss': 1.8039, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.045276403427124, 'eval_accuracy@spa.rst.rststb': 0.4516971279373368, 'eval_f1@spa.rst.rststb': 0.1843669107312864, 'eval_precision@spa.rst.rststb': 0.18512166932110752, 'eval_recall@spa.rst.rststb': 0.19975030783206982, 'eval_loss@spa.rst.rststb': 2.045276403427124, 'eval_runtime': 4.8996, 'eval_samples_per_second': 78.169, 'eval_steps_per_second': 2.449, 'epoch': 12.0}
{'train_runtime': 1067.9517, 'train_samples_per_second': 25.17, 'train_steps_per_second': 0.787, 'train_loss': 2.1831212724958147, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.3531
  train_runtime            = 2:01:07.85
  train_samples_per_second =     26.421
  train_steps_per_second   =      0.827
-------------------------------------------------------------------
Lang1:  eng.sdrt.stac    Lang2:  spa.rst.rststb
Saving run to:  runs/full_shot/FullShot=v4_eng.sdrt.stac_spa.rst.rststb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 9580 examples
read 1145 examples
read 1510 examples
read 2240 examples
read 383 examples
read 426 examples
Total prediction labels:  45
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=45, bias=True)
    )
  )
)
{'train@eng.sdrt.stac_loss': 2.1018764972686768, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.34937369519832984, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.06474267389095444, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.06865774558281705, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.1090190353116216, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 2.101876735687256, 'train@eng.sdrt.stac_runtime': 112.8581, 'train@eng.sdrt.stac_samples_per_second': 84.885, 'train@eng.sdrt.stac_steps_per_second': 2.658, 'epoch': 1.0}
{'loss': 2.5999, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.063666582107544, 'eval_accuracy@eng.sdrt.stac': 0.35807860262008734, 'eval_f1@eng.sdrt.stac': 0.06644813701174962, 'eval_precision@eng.sdrt.stac': 0.053707055355991525, 'eval_recall@eng.sdrt.stac': 0.11176818468697164, 'eval_loss@eng.sdrt.stac': 2.063666582107544, 'eval_runtime': 13.9132, 'eval_samples_per_second': 82.296, 'eval_steps_per_second': 2.587, 'epoch': 1.0}
{'train@eng.sdrt.stac_loss': 1.8843094110488892, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.41878914405010437, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.11890037179360904, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.13820708229129794, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.17014938391792653, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.88430917263031, 'train@eng.sdrt.stac_runtime': 112.8401, 'train@eng.sdrt.stac_samples_per_second': 84.899, 'train@eng.sdrt.stac_steps_per_second': 2.659, 'epoch': 2.0}
{'loss': 2.0206, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.8426417112350464, 'eval_accuracy@eng.sdrt.stac': 0.41397379912663756, 'eval_f1@eng.sdrt.stac': 0.11185369998801145, 'eval_precision@eng.sdrt.stac': 0.12105849452107673, 'eval_recall@eng.sdrt.stac': 0.16560010967466468, 'eval_loss@eng.sdrt.stac': 1.842641830444336, 'eval_runtime': 13.8817, 'eval_samples_per_second': 82.483, 'eval_steps_per_second': 2.593, 'epoch': 2.0}
{'train@eng.sdrt.stac_loss': 1.7831672430038452, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4465553235908142, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.15023746915755376, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.1377490529871228, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.19001748255536413, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.7831672430038452, 'train@eng.sdrt.stac_runtime': 112.7395, 'train@eng.sdrt.stac_samples_per_second': 84.975, 'train@eng.sdrt.stac_steps_per_second': 2.661, 'epoch': 3.0}
{'loss': 1.8692, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.7403919696807861, 'eval_accuracy@eng.sdrt.stac': 0.4480349344978166, 'eval_f1@eng.sdrt.stac': 0.14437350384150494, 'eval_precision@eng.sdrt.stac': 0.13034076500820313, 'eval_recall@eng.sdrt.stac': 0.18726251062958302, 'eval_loss@eng.sdrt.stac': 1.7403920888900757, 'eval_runtime': 13.8626, 'eval_samples_per_second': 82.596, 'eval_steps_per_second': 2.597, 'epoch': 3.0}
{'train@eng.sdrt.stac_loss': 1.7142627239227295, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.46555323590814196, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.18329122365936373, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.21441136157321272, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.21234217923959053, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.7142627239227295, 'train@eng.sdrt.stac_runtime': 112.4967, 'train@eng.sdrt.stac_samples_per_second': 85.158, 'train@eng.sdrt.stac_steps_per_second': 2.667, 'epoch': 4.0}
{'loss': 1.7892, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.6624226570129395, 'eval_accuracy@eng.sdrt.stac': 0.47510917030567684, 'eval_f1@eng.sdrt.stac': 0.18275477133509868, 'eval_precision@eng.sdrt.stac': 0.18916165503621996, 'eval_recall@eng.sdrt.stac': 0.21302910441662803, 'eval_loss@eng.sdrt.stac': 1.6624226570129395, 'eval_runtime': 13.8671, 'eval_samples_per_second': 82.57, 'eval_steps_per_second': 2.596, 'epoch': 4.0}
{'train@eng.sdrt.stac_loss': 1.6614271402359009, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.48465553235908143, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.20918119179057407, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.21627975693270135, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2335885947778957, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.6614272594451904, 'train@eng.sdrt.stac_runtime': 112.9142, 'train@eng.sdrt.stac_samples_per_second': 84.843, 'train@eng.sdrt.stac_steps_per_second': 2.657, 'epoch': 5.0}
{'loss': 1.7327, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.6105625629425049, 'eval_accuracy@eng.sdrt.stac': 0.48995633187772925, 'eval_f1@eng.sdrt.stac': 0.2012847896052769, 'eval_precision@eng.sdrt.stac': 0.21562633243996407, 'eval_recall@eng.sdrt.stac': 0.22706658038196928, 'eval_loss@eng.sdrt.stac': 1.6105625629425049, 'eval_runtime': 13.8888, 'eval_samples_per_second': 82.441, 'eval_steps_per_second': 2.592, 'epoch': 5.0}
{'train@eng.sdrt.stac_loss': 1.6170073747634888, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4890396659707724, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.21245873624056613, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.23297296463544012, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.23478049920993735, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.6170073747634888, 'train@eng.sdrt.stac_runtime': 112.8895, 'train@eng.sdrt.stac_samples_per_second': 84.862, 'train@eng.sdrt.stac_steps_per_second': 2.657, 'epoch': 6.0}
{'loss': 1.6813, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.5818291902542114, 'eval_accuracy@eng.sdrt.stac': 0.4829694323144105, 'eval_f1@eng.sdrt.stac': 0.19816914294868987, 'eval_precision@eng.sdrt.stac': 0.22346563174535403, 'eval_recall@eng.sdrt.stac': 0.22303598472559225, 'eval_loss@eng.sdrt.stac': 1.5818291902542114, 'eval_runtime': 13.8713, 'eval_samples_per_second': 82.545, 'eval_steps_per_second': 2.595, 'epoch': 6.0}
{'train@eng.sdrt.stac_loss': 1.5974243879318237, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.49843423799582465, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.22645949051906847, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.2252445640812627, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2493857233523406, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5974243879318237, 'train@eng.sdrt.stac_runtime': 112.6944, 'train@eng.sdrt.stac_samples_per_second': 85.009, 'train@eng.sdrt.stac_steps_per_second': 2.662, 'epoch': 7.0}
{'loss': 1.6471, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.5629411935806274, 'eval_accuracy@eng.sdrt.stac': 0.5100436681222708, 'eval_f1@eng.sdrt.stac': 0.22624959695804503, 'eval_precision@eng.sdrt.stac': 0.23649676797163277, 'eval_recall@eng.sdrt.stac': 0.24718833972812906, 'eval_loss@eng.sdrt.stac': 1.5629411935806274, 'eval_runtime': 13.84, 'eval_samples_per_second': 82.731, 'eval_steps_per_second': 2.601, 'epoch': 7.0}
{'train@eng.sdrt.stac_loss': 1.560514211654663, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5077244258872652, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2369182906179485, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.32222280425787864, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.26098358030349283, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.560514211654663, 'train@eng.sdrt.stac_runtime': 113.4662, 'train@eng.sdrt.stac_samples_per_second': 84.43, 'train@eng.sdrt.stac_steps_per_second': 2.644, 'epoch': 8.0}
{'loss': 1.624, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.5245026350021362, 'eval_accuracy@eng.sdrt.stac': 0.5170305676855895, 'eval_f1@eng.sdrt.stac': 0.2307599250487049, 'eval_precision@eng.sdrt.stac': 0.2349019748271022, 'eval_recall@eng.sdrt.stac': 0.2518771736105185, 'eval_loss@eng.sdrt.stac': 1.5245026350021362, 'eval_runtime': 13.8832, 'eval_samples_per_second': 82.474, 'eval_steps_per_second': 2.593, 'epoch': 8.0}
{'train@eng.sdrt.stac_loss': 1.5425387620925903, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5103340292275574, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.24488019271286654, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.29120350039455667, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2667626004776529, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5425388813018799, 'train@eng.sdrt.stac_runtime': 112.8747, 'train@eng.sdrt.stac_samples_per_second': 84.873, 'train@eng.sdrt.stac_steps_per_second': 2.658, 'epoch': 9.0}
{'loss': 1.5963, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.5108503103256226, 'eval_accuracy@eng.sdrt.stac': 0.5205240174672489, 'eval_f1@eng.sdrt.stac': 0.2481212754064621, 'eval_precision@eng.sdrt.stac': 0.2967411155157593, 'eval_recall@eng.sdrt.stac': 0.26157676545670233, 'eval_loss@eng.sdrt.stac': 1.510850191116333, 'eval_runtime': 13.8934, 'eval_samples_per_second': 82.413, 'eval_steps_per_second': 2.591, 'epoch': 9.0}
{'train@eng.sdrt.stac_loss': 1.5254732370376587, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5174321503131524, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.264240259095452, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.36268766186822177, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2823151140274718, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5254732370376587, 'train@eng.sdrt.stac_runtime': 112.6806, 'train@eng.sdrt.stac_samples_per_second': 85.019, 'train@eng.sdrt.stac_steps_per_second': 2.662, 'epoch': 10.0}
{'loss': 1.5851, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.4969292879104614, 'eval_accuracy@eng.sdrt.stac': 0.525764192139738, 'eval_f1@eng.sdrt.stac': 0.2553537041971872, 'eval_precision@eng.sdrt.stac': 0.2619891171990842, 'eval_recall@eng.sdrt.stac': 0.2711251608053632, 'eval_loss@eng.sdrt.stac': 1.4969292879104614, 'eval_runtime': 13.8624, 'eval_samples_per_second': 82.598, 'eval_steps_per_second': 2.597, 'epoch': 10.0}
{'train@eng.sdrt.stac_loss': 1.5169349908828735, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5206680584551148, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2765506268706626, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.35151022986980063, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2927328502790134, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5169349908828735, 'train@eng.sdrt.stac_runtime': 112.6982, 'train@eng.sdrt.stac_samples_per_second': 85.006, 'train@eng.sdrt.stac_steps_per_second': 2.662, 'epoch': 11.0}
{'loss': 1.5658, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.4912852048873901, 'eval_accuracy@eng.sdrt.stac': 0.5275109170305677, 'eval_f1@eng.sdrt.stac': 0.2621556047408679, 'eval_precision@eng.sdrt.stac': 0.269469151281112, 'eval_recall@eng.sdrt.stac': 0.27649896898248016, 'eval_loss@eng.sdrt.stac': 1.4912850856781006, 'eval_runtime': 13.9166, 'eval_samples_per_second': 82.276, 'eval_steps_per_second': 2.587, 'epoch': 11.0}
{'train@eng.sdrt.stac_loss': 1.5152184963226318, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5208768267223383, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2813624592002582, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.3566587268082324, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.296402679903021, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5152186155319214, 'train@eng.sdrt.stac_runtime': 112.8879, 'train@eng.sdrt.stac_samples_per_second': 84.863, 'train@eng.sdrt.stac_steps_per_second': 2.658, 'epoch': 12.0}
{'loss': 1.5663, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.489601969718933, 'eval_accuracy@eng.sdrt.stac': 0.5275109170305677, 'eval_f1@eng.sdrt.stac': 0.26092605791568607, 'eval_precision@eng.sdrt.stac': 0.2664084077132636, 'eval_recall@eng.sdrt.stac': 0.27646064435775847, 'eval_loss@eng.sdrt.stac': 1.489601969718933, 'eval_runtime': 13.8863, 'eval_samples_per_second': 82.455, 'eval_steps_per_second': 2.592, 'epoch': 12.0}
{'train_runtime': 4382.0304, 'train_samples_per_second': 26.234, 'train_steps_per_second': 0.822, 'train_loss': 1.7731094021267362, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.7731
  train_runtime            = 1:13:02.03
  train_samples_per_second =     26.234
  train_steps_per_second   =      0.822
{'train@spa.rst.rststb_loss': 3.109748125076294, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.24553571428571427, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.03576569185905817, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.040405210343738784, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.04999018351138949, 'train@spa.rst.rststb_loss@spa.rst.rststb': 3.109748125076294, 'train@spa.rst.rststb_runtime': 27.2601, 'train@spa.rst.rststb_samples_per_second': 82.171, 'train@spa.rst.rststb_steps_per_second': 2.568, 'epoch': 1.0}
{'loss': 3.8358, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.1594595909118652, 'eval_accuracy@spa.rst.rststb': 0.21148825065274152, 'eval_f1@spa.rst.rststb': 0.027935873333174682, 'eval_precision@spa.rst.rststb': 0.03298434394842712, 'eval_recall@spa.rst.rststb': 0.04713026153566983, 'eval_loss@spa.rst.rststb': 3.1594595909118652, 'eval_runtime': 4.9999, 'eval_samples_per_second': 76.601, 'eval_steps_per_second': 2.4, 'epoch': 1.0}
{'train@spa.rst.rststb_loss': 2.6573948860168457, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.25223214285714285, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.03485348252125205, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.05315615927319306, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.049381220639672874, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.6573946475982666, 'train@spa.rst.rststb_runtime': 27.2097, 'train@spa.rst.rststb_samples_per_second': 82.324, 'train@spa.rst.rststb_steps_per_second': 2.573, 'epoch': 2.0}
{'loss': 2.8698, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.762864351272583, 'eval_accuracy@spa.rst.rststb': 0.23237597911227154, 'eval_f1@spa.rst.rststb': 0.03081903465535179, 'eval_precision@spa.rst.rststb': 0.06563740012221252, 'eval_recall@spa.rst.rststb': 0.051521548708152835, 'eval_loss@spa.rst.rststb': 2.762864589691162, 'eval_runtime': 4.9875, 'eval_samples_per_second': 76.791, 'eval_steps_per_second': 2.406, 'epoch': 2.0}
{'train@spa.rst.rststb_loss': 2.5196895599365234, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.3026785714285714, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.04758809000424058, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.04456320173260571, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.06454164644976636, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.5196897983551025, 'train@spa.rst.rststb_runtime': 27.1516, 'train@spa.rst.rststb_samples_per_second': 82.5, 'train@spa.rst.rststb_steps_per_second': 2.578, 'epoch': 3.0}
{'loss': 2.6277, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.6597371101379395, 'eval_accuracy@spa.rst.rststb': 0.2741514360313316, 'eval_f1@spa.rst.rststb': 0.04935255334302386, 'eval_precision@spa.rst.rststb': 0.052219048630175305, 'eval_recall@spa.rst.rststb': 0.06714625700735329, 'eval_loss@spa.rst.rststb': 2.6597368717193604, 'eval_runtime': 4.9668, 'eval_samples_per_second': 77.112, 'eval_steps_per_second': 2.416, 'epoch': 3.0}
{'train@spa.rst.rststb_loss': 2.4271092414855957, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.31473214285714285, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.049791920538937294, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.04666231484219071, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.0671661658932918, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.4271092414855957, 'train@spa.rst.rststb_runtime': 27.2073, 'train@spa.rst.rststb_samples_per_second': 82.331, 'train@spa.rst.rststb_steps_per_second': 2.573, 'epoch': 4.0}
{'loss': 2.5139, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.5830078125, 'eval_accuracy@spa.rst.rststb': 0.2924281984334204, 'eval_f1@spa.rst.rststb': 0.05536666220504228, 'eval_precision@spa.rst.rststb': 0.057732870011687004, 'eval_recall@spa.rst.rststb': 0.07328625546903995, 'eval_loss@spa.rst.rststb': 2.583008050918579, 'eval_runtime': 4.9828, 'eval_samples_per_second': 76.865, 'eval_steps_per_second': 2.408, 'epoch': 4.0}
{'train@spa.rst.rststb_loss': 2.344440221786499, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.33482142857142855, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.054589119036424584, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.050946413804874094, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.07330960274528965, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.344440221786499, 'train@spa.rst.rststb_runtime': 27.1349, 'train@spa.rst.rststb_samples_per_second': 82.551, 'train@spa.rst.rststb_steps_per_second': 2.58, 'epoch': 5.0}
{'loss': 2.4337, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.519627332687378, 'eval_accuracy@spa.rst.rststb': 0.32114882506527415, 'eval_f1@spa.rst.rststb': 0.061919186643105514, 'eval_precision@spa.rst.rststb': 0.05645892858902069, 'eval_recall@spa.rst.rststb': 0.08241625713430148, 'eval_loss@spa.rst.rststb': 2.519627332687378, 'eval_runtime': 4.987, 'eval_samples_per_second': 76.8, 'eval_steps_per_second': 2.406, 'epoch': 5.0}
{'train@spa.rst.rststb_loss': 2.272679090499878, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.35892857142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.07492965916856098, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.11314620460989129, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.08900097804695407, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.272679090499878, 'train@spa.rst.rststb_runtime': 27.17, 'train@spa.rst.rststb_samples_per_second': 82.444, 'train@spa.rst.rststb_steps_per_second': 2.576, 'epoch': 6.0}
{'loss': 2.3675, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.461430549621582, 'eval_accuracy@spa.rst.rststb': 0.34203655352480417, 'eval_f1@spa.rst.rststb': 0.07601997898044976, 'eval_precision@spa.rst.rststb': 0.07783850931677018, 'eval_recall@spa.rst.rststb': 0.09460008280435588, 'eval_loss@spa.rst.rststb': 2.461430788040161, 'eval_runtime': 5.0293, 'eval_samples_per_second': 76.153, 'eval_steps_per_second': 2.386, 'epoch': 6.0}
{'train@spa.rst.rststb_loss': 2.2080016136169434, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.38705357142857144, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.09616226131549142, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.12805066394016057, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.10839213674661398, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.2080016136169434, 'train@spa.rst.rststb_runtime': 27.1808, 'train@spa.rst.rststb_samples_per_second': 82.411, 'train@spa.rst.rststb_steps_per_second': 2.575, 'epoch': 7.0}
{'loss': 2.3002, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.4095518589019775, 'eval_accuracy@spa.rst.rststb': 0.36553524804177545, 'eval_f1@spa.rst.rststb': 0.10434779811292662, 'eval_precision@spa.rst.rststb': 0.12441247954758462, 'eval_recall@spa.rst.rststb': 0.11687556933328624, 'eval_loss@spa.rst.rststb': 2.4095518589019775, 'eval_runtime': 4.9957, 'eval_samples_per_second': 76.665, 'eval_steps_per_second': 2.402, 'epoch': 7.0}
{'train@spa.rst.rststb_loss': 2.1571435928344727, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.403125, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10452576170764126, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.12798748182912895, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.11816490273471512, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.1571435928344727, 'train@spa.rst.rststb_runtime': 27.1934, 'train@spa.rst.rststb_samples_per_second': 82.373, 'train@spa.rst.rststb_steps_per_second': 2.574, 'epoch': 8.0}
{'loss': 2.2542, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.3689424991607666, 'eval_accuracy@spa.rst.rststb': 0.39425587467362927, 'eval_f1@spa.rst.rststb': 0.1197694283225288, 'eval_precision@spa.rst.rststb': 0.11277967622402771, 'eval_recall@spa.rst.rststb': 0.14015319979308513, 'eval_loss@spa.rst.rststb': 2.3689422607421875, 'eval_runtime': 5.0045, 'eval_samples_per_second': 76.532, 'eval_steps_per_second': 2.398, 'epoch': 8.0}
{'train@spa.rst.rststb_loss': 2.11784291267395, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.421875, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.12294763724309044, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.179313561240166, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13536474337270749, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.1178433895111084, 'train@spa.rst.rststb_runtime': 27.2012, 'train@spa.rst.rststb_samples_per_second': 82.349, 'train@spa.rst.rststb_steps_per_second': 2.573, 'epoch': 9.0}
{'loss': 2.204, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.337379217147827, 'eval_accuracy@spa.rst.rststb': 0.4073107049608355, 'eval_f1@spa.rst.rststb': 0.1275948967193951, 'eval_precision@spa.rst.rststb': 0.1572797221774204, 'eval_recall@spa.rst.rststb': 0.1514059937502891, 'eval_loss@spa.rst.rststb': 2.337378978729248, 'eval_runtime': 5.0078, 'eval_samples_per_second': 76.48, 'eval_steps_per_second': 2.396, 'epoch': 9.0}
{'train@spa.rst.rststb_loss': 2.0911433696746826, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4276785714285714, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.1270459993578494, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.17845918702187377, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.14034324469109985, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0911436080932617, 'train@spa.rst.rststb_runtime': 27.1974, 'train@spa.rst.rststb_samples_per_second': 82.361, 'train@spa.rst.rststb_steps_per_second': 2.574, 'epoch': 10.0}
{'loss': 2.1724, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.3170156478881836, 'eval_accuracy@spa.rst.rststb': 0.3968668407310705, 'eval_f1@spa.rst.rststb': 0.11627314194300516, 'eval_precision@spa.rst.rststb': 0.10572254253785669, 'eval_recall@spa.rst.rststb': 0.14160907360652308, 'eval_loss@spa.rst.rststb': 2.3170158863067627, 'eval_runtime': 5.0146, 'eval_samples_per_second': 76.376, 'eval_steps_per_second': 2.393, 'epoch': 10.0}
{'train@spa.rst.rststb_loss': 2.0744972229003906, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.43392857142857144, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.13306658056342216, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.17715272631956305, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.1457657884685242, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0744974613189697, 'train@spa.rst.rststb_runtime': 27.1915, 'train@spa.rst.rststb_samples_per_second': 82.379, 'train@spa.rst.rststb_steps_per_second': 2.574, 'epoch': 11.0}
{'loss': 2.1559, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.3022732734680176, 'eval_accuracy@spa.rst.rststb': 0.402088772845953, 'eval_f1@spa.rst.rststb': 0.11684114800891117, 'eval_precision@spa.rst.rststb': 0.1062646217235537, 'eval_recall@spa.rst.rststb': 0.1427583903141243, 'eval_loss@spa.rst.rststb': 2.3022732734680176, 'eval_runtime': 4.9929, 'eval_samples_per_second': 76.71, 'eval_steps_per_second': 2.403, 'epoch': 11.0}
{'train@spa.rst.rststb_loss': 2.0696516036987305, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.43526785714285715, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.1336206246171292, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.1776271407042199, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.14631511854957174, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0696516036987305, 'train@spa.rst.rststb_runtime': 27.185, 'train@spa.rst.rststb_samples_per_second': 82.398, 'train@spa.rst.rststb_steps_per_second': 2.575, 'epoch': 12.0}
{'loss': 2.144, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.2983150482177734, 'eval_accuracy@spa.rst.rststb': 0.402088772845953, 'eval_f1@spa.rst.rststb': 0.1171054318120252, 'eval_precision@spa.rst.rststb': 0.10628858024691358, 'eval_recall@spa.rst.rststb': 0.14285285470105044, 'eval_loss@spa.rst.rststb': 2.2983150482177734, 'eval_runtime': 5.0019, 'eval_samples_per_second': 76.57, 'eval_steps_per_second': 2.399, 'epoch': 12.0}
{'train_runtime': 1069.4012, 'train_samples_per_second': 25.136, 'train_steps_per_second': 0.785, 'train_loss': 2.4899310339064824, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.7731
  train_runtime            = 1:13:02.03
  train_samples_per_second =     26.234
  train_steps_per_second   =      0.822
-------------------------------------------------------------------
Lang1:  fas.rst.prstc    Lang2:  spa.rst.rststb
Saving run to:  runs/full_shot/FullShot=v4_fas.rst.prstc_spa.rst.rststb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 4100 examples
read 499 examples
read 592 examples
read 2240 examples
read 383 examples
read 426 examples
Total prediction labels:  37
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (ar): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=37, bias=True)
    )
  )
)
{'train@fas.rst.prstc_loss': 2.4287917613983154, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.23780487804878048, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.02260214430599826, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.013988522238163558, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.058823529411764705, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.4287917613983154, 'train@fas.rst.prstc_runtime': 48.4346, 'train@fas.rst.prstc_samples_per_second': 84.65, 'train@fas.rst.prstc_steps_per_second': 2.663, 'epoch': 1.0}
{'loss': 2.8467, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.3550214767456055, 'eval_accuracy@fas.rst.prstc': 0.24448897795591182, 'eval_f1@fas.rst.prstc': 0.026194310252281264, 'eval_precision@fas.rst.prstc': 0.016299265197060788, 'eval_recall@fas.rst.prstc': 0.06666666666666667, 'eval_loss@fas.rst.prstc': 2.3550212383270264, 'eval_runtime': 6.2124, 'eval_samples_per_second': 80.323, 'eval_steps_per_second': 2.575, 'epoch': 1.0}
{'train@fas.rst.prstc_loss': 2.3646864891052246, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2678048780487805, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.0436564333640178, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03163457232422749, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07126728923975482, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3646864891052246, 'train@fas.rst.prstc_runtime': 48.4143, 'train@fas.rst.prstc_samples_per_second': 84.686, 'train@fas.rst.prstc_steps_per_second': 2.665, 'epoch': 2.0}
{'loss': 2.4168, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.2828216552734375, 'eval_accuracy@fas.rst.prstc': 0.27655310621242485, 'eval_f1@fas.rst.prstc': 0.04995592037515836, 'eval_precision@fas.rst.prstc': 0.03696782914301163, 'eval_recall@fas.rst.prstc': 0.07787122832026609, 'eval_loss@fas.rst.prstc': 2.2828218936920166, 'eval_runtime': 6.1887, 'eval_samples_per_second': 80.631, 'eval_steps_per_second': 2.585, 'epoch': 2.0}
{'train@fas.rst.prstc_loss': 2.345921516418457, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.24829268292682927, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.02896864713856191, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03758079788026069, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.0618685750350331, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.345921516418457, 'train@fas.rst.prstc_runtime': 48.3406, 'train@fas.rst.prstc_samples_per_second': 84.815, 'train@fas.rst.prstc_steps_per_second': 2.669, 'epoch': 3.0}
{'loss': 2.3643, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.26396107673645, 'eval_accuracy@fas.rst.prstc': 0.24649298597194388, 'eval_f1@fas.rst.prstc': 0.029386579378068742, 'eval_precision@fas.rst.prstc': 0.03635991820040899, 'eval_recall@fas.rst.prstc': 0.06731290092658589, 'eval_loss@fas.rst.prstc': 2.2639613151550293, 'eval_runtime': 6.2065, 'eval_samples_per_second': 80.4, 'eval_steps_per_second': 2.578, 'epoch': 3.0}
{'train@fas.rst.prstc_loss': 2.331617593765259, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.23780487804878048, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.02260214430599826, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.013988522238163558, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.058823529411764705, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.331617593765259, 'train@fas.rst.prstc_runtime': 48.4098, 'train@fas.rst.prstc_samples_per_second': 84.694, 'train@fas.rst.prstc_steps_per_second': 2.665, 'epoch': 4.0}
{'loss': 2.3526, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.2544913291931152, 'eval_accuracy@fas.rst.prstc': 0.24448897795591182, 'eval_f1@fas.rst.prstc': 0.026194310252281264, 'eval_precision@fas.rst.prstc': 0.016299265197060788, 'eval_recall@fas.rst.prstc': 0.06666666666666667, 'eval_loss@fas.rst.prstc': 2.2544915676116943, 'eval_runtime': 7.591, 'eval_samples_per_second': 65.736, 'eval_steps_per_second': 2.108, 'epoch': 4.0}
{'train@fas.rst.prstc_loss': 2.3220441341400146, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2675609756097561, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.03774916054439542, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03598866730274987, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.067545543039911, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3220443725585938, 'train@fas.rst.prstc_runtime': 48.519, 'train@fas.rst.prstc_samples_per_second': 84.503, 'train@fas.rst.prstc_steps_per_second': 2.659, 'epoch': 5.0}
{'loss': 2.3448, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.2429168224334717, 'eval_accuracy@fas.rst.prstc': 0.28256513026052105, 'eval_f1@fas.rst.prstc': 0.04615974064249926, 'eval_precision@fas.rst.prstc': 0.05309404622430504, 'eval_recall@fas.rst.prstc': 0.07778094559277739, 'eval_loss@fas.rst.prstc': 2.242917060852051, 'eval_runtime': 6.2127, 'eval_samples_per_second': 80.32, 'eval_steps_per_second': 2.575, 'epoch': 5.0}
{'train@fas.rst.prstc_loss': 2.3110110759735107, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.23853658536585365, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.023047127305320967, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03922252995994391, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.05903212348769295, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3110110759735107, 'train@fas.rst.prstc_runtime': 48.5061, 'train@fas.rst.prstc_samples_per_second': 84.525, 'train@fas.rst.prstc_steps_per_second': 2.659, 'epoch': 6.0}
{'loss': 2.3257, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.2386252880096436, 'eval_accuracy@fas.rst.prstc': 0.24248496993987975, 'eval_f1@fas.rst.prstc': 0.026021505376344085, 'eval_precision@fas.rst.prstc': 0.016198125836680052, 'eval_recall@fas.rst.prstc': 0.06612021857923497, 'eval_loss@fas.rst.prstc': 2.2386252880096436, 'eval_runtime': 6.206, 'eval_samples_per_second': 80.406, 'eval_steps_per_second': 2.578, 'epoch': 6.0}
{'train@fas.rst.prstc_loss': 2.2959561347961426, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.27146341463414636, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.03932169325528263, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03534506098951208, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.06875923964785041, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.2959563732147217, 'train@fas.rst.prstc_runtime': 48.4822, 'train@fas.rst.prstc_samples_per_second': 84.567, 'train@fas.rst.prstc_steps_per_second': 2.661, 'epoch': 7.0}
{'loss': 2.3173, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.219616413116455, 'eval_accuracy@fas.rst.prstc': 0.28256513026052105, 'eval_f1@fas.rst.prstc': 0.04613985741831626, 'eval_precision@fas.rst.prstc': 0.04818708240534521, 'eval_recall@fas.rst.prstc': 0.07781420765027322, 'eval_loss@fas.rst.prstc': 2.219616651535034, 'eval_runtime': 6.2293, 'eval_samples_per_second': 80.105, 'eval_steps_per_second': 2.569, 'epoch': 7.0}
{'train@fas.rst.prstc_loss': 2.277283191680908, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3051219512195122, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.048460443576366224, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03575774418590103, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07922852283302846, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.2772834300994873, 'train@fas.rst.prstc_runtime': 48.4644, 'train@fas.rst.prstc_samples_per_second': 84.598, 'train@fas.rst.prstc_steps_per_second': 2.662, 'epoch': 8.0}
{'loss': 2.3055, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.206803321838379, 'eval_accuracy@fas.rst.prstc': 0.3166332665330661, 'eval_f1@fas.rst.prstc': 0.05617059915341372, 'eval_precision@fas.rst.prstc': 0.04335943966889526, 'eval_recall@fas.rst.prstc': 0.08806842480399144, 'eval_loss@fas.rst.prstc': 2.206803321838379, 'eval_runtime': 6.1958, 'eval_samples_per_second': 80.538, 'eval_steps_per_second': 2.582, 'epoch': 8.0}
{'train@fas.rst.prstc_loss': 2.2607319355010986, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2965853658536585, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04650041918090868, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03535973354471924, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.076500540205171, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.2607319355010986, 'train@fas.rst.prstc_runtime': 48.4198, 'train@fas.rst.prstc_samples_per_second': 84.676, 'train@fas.rst.prstc_steps_per_second': 2.664, 'epoch': 9.0}
{'loss': 2.2903, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.191844940185547, 'eval_accuracy@fas.rst.prstc': 0.3046092184368738, 'eval_f1@fas.rst.prstc': 0.05336045197740113, 'eval_precision@fas.rst.prstc': 0.04291544594574897, 'eval_recall@fas.rst.prstc': 0.08449037776193871, 'eval_loss@fas.rst.prstc': 2.1918447017669678, 'eval_runtime': 6.1901, 'eval_samples_per_second': 80.612, 'eval_steps_per_second': 2.585, 'epoch': 9.0}
{'train@fas.rst.prstc_loss': 2.2428042888641357, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3029268292682927, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04836726197265338, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03505782341777439, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.0790259191510756, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.2428042888641357, 'train@fas.rst.prstc_runtime': 48.3644, 'train@fas.rst.prstc_samples_per_second': 84.773, 'train@fas.rst.prstc_steps_per_second': 2.667, 'epoch': 10.0}
{'loss': 2.2798, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.1756272315979004, 'eval_accuracy@fas.rst.prstc': 0.3066132264529058, 'eval_f1@fas.rst.prstc': 0.054338752450132595, 'eval_precision@fas.rst.prstc': 0.04068965517241379, 'eval_recall@fas.rst.prstc': 0.08540270848182467, 'eval_loss@fas.rst.prstc': 2.1756269931793213, 'eval_runtime': 6.2238, 'eval_samples_per_second': 80.176, 'eval_steps_per_second': 2.571, 'epoch': 10.0}
{'train@fas.rst.prstc_loss': 2.231149911880493, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.30048780487804877, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04897202197107367, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.07394753085587624, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07885736786863194, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.231149911880493, 'train@fas.rst.prstc_runtime': 48.371, 'train@fas.rst.prstc_samples_per_second': 84.761, 'train@fas.rst.prstc_steps_per_second': 2.667, 'epoch': 11.0}
{'loss': 2.2638, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.163557529449463, 'eval_accuracy@fas.rst.prstc': 0.3066132264529058, 'eval_f1@fas.rst.prstc': 0.054554946094193306, 'eval_precision@fas.rst.prstc': 0.040509867313261935, 'eval_recall@fas.rst.prstc': 0.08550249465431219, 'eval_loss@fas.rst.prstc': 2.163557291030884, 'eval_runtime': 6.1942, 'eval_samples_per_second': 80.559, 'eval_steps_per_second': 2.583, 'epoch': 11.0}
{'train@fas.rst.prstc_loss': 2.227846384048462, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2997560975609756, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.048870030006876036, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.07386394950257148, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07867637239351882, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.227846622467041, 'train@fas.rst.prstc_runtime': 48.4599, 'train@fas.rst.prstc_samples_per_second': 84.606, 'train@fas.rst.prstc_steps_per_second': 2.662, 'epoch': 12.0}
{'loss': 2.2532, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.1600756645202637, 'eval_accuracy@fas.rst.prstc': 0.3046092184368738, 'eval_f1@fas.rst.prstc': 0.05429837947823559, 'eval_precision@fas.rst.prstc': 0.04019718621912041, 'eval_recall@fas.rst.prstc': 0.08498930862437633, 'eval_loss@fas.rst.prstc': 2.1600756645202637, 'eval_runtime': 6.2274, 'eval_samples_per_second': 80.129, 'eval_steps_per_second': 2.569, 'epoch': 12.0}
{'train_runtime': 1883.8956, 'train_samples_per_second': 26.116, 'train_steps_per_second': 0.822, 'train_loss': 2.363414084264474, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.3634
  train_runtime            = 0:31:23.89
  train_samples_per_second =     26.116
  train_steps_per_second   =      0.822
{'train@spa.rst.rststb_loss': 3.008899688720703, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.2017857142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.011997664171577216, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.007209851336693677, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.03571428571428571, 'train@spa.rst.rststb_loss@spa.rst.rststb': 3.008899688720703, 'train@spa.rst.rststb_runtime': 27.114, 'train@spa.rst.rststb_samples_per_second': 82.614, 'train@spa.rst.rststb_steps_per_second': 2.582, 'epoch': 1.0}
{'loss': 3.327, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.0113954544067383, 'eval_accuracy@spa.rst.rststb': 0.206266318537859, 'eval_f1@spa.rst.rststb': 0.014869188782232262, 'eval_precision@spa.rst.rststb': 0.00896810080599387, 'eval_recall@spa.rst.rststb': 0.043478260869565216, 'eval_loss@spa.rst.rststb': 3.0113959312438965, 'eval_runtime': 4.9228, 'eval_samples_per_second': 77.801, 'eval_steps_per_second': 2.438, 'epoch': 1.0}
{'train@spa.rst.rststb_loss': 2.666675567626953, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.2375, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.028927675909311713, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.0480069093381919, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.04514444702929984, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.666675567626953, 'train@spa.rst.rststb_runtime': 27.0862, 'train@spa.rst.rststb_samples_per_second': 82.699, 'train@spa.rst.rststb_steps_per_second': 2.584, 'epoch': 2.0}
{'loss': 2.8436, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.753927707672119, 'eval_accuracy@spa.rst.rststb': 0.20887728459530025, 'eval_f1@spa.rst.rststb': 0.022288076644508027, 'eval_precision@spa.rst.rststb': 0.044148865738071134, 'eval_recall@spa.rst.rststb': 0.04529847385875446, 'eval_loss@spa.rst.rststb': 2.7539279460906982, 'eval_runtime': 4.9256, 'eval_samples_per_second': 77.757, 'eval_steps_per_second': 2.436, 'epoch': 2.0}
{'train@spa.rst.rststb_loss': 2.5420737266540527, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.30714285714285716, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.04544052488701171, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.03695553208879825, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.06703254563341592, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.5420734882354736, 'train@spa.rst.rststb_runtime': 27.1305, 'train@spa.rst.rststb_samples_per_second': 82.564, 'train@spa.rst.rststb_steps_per_second': 2.58, 'epoch': 3.0}
{'loss': 2.6388, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.6800365447998047, 'eval_accuracy@spa.rst.rststb': 0.2741514360313316, 'eval_f1@spa.rst.rststb': 0.0482523200526833, 'eval_precision@spa.rst.rststb': 0.03886315090365921, 'eval_recall@spa.rst.rststb': 0.06992660901227589, 'eval_loss@spa.rst.rststb': 2.6800365447998047, 'eval_runtime': 4.9382, 'eval_samples_per_second': 77.559, 'eval_steps_per_second': 2.43, 'epoch': 3.0}
{'train@spa.rst.rststb_loss': 2.4498202800750732, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.31741071428571427, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.04824829764895446, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.04155902968731762, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.0684038057641758, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.4498202800750732, 'train@spa.rst.rststb_runtime': 27.0771, 'train@spa.rst.rststb_samples_per_second': 82.727, 'train@spa.rst.rststb_steps_per_second': 2.585, 'epoch': 4.0}
{'loss': 2.5371, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.6116373538970947, 'eval_accuracy@spa.rst.rststb': 0.2793733681462141, 'eval_f1@spa.rst.rststb': 0.05001611498132025, 'eval_precision@spa.rst.rststb': 0.04385841091574297, 'eval_recall@spa.rst.rststb': 0.07053446680640924, 'eval_loss@spa.rst.rststb': 2.611637592315674, 'eval_runtime': 4.9103, 'eval_samples_per_second': 78.0, 'eval_steps_per_second': 2.444, 'epoch': 4.0}
{'train@spa.rst.rststb_loss': 2.3537771701812744, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.3366071428571429, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.05601590299777026, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.07284022893141089, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.074906349699723, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.3537774085998535, 'train@spa.rst.rststb_runtime': 27.1243, 'train@spa.rst.rststb_samples_per_second': 82.583, 'train@spa.rst.rststb_steps_per_second': 2.581, 'epoch': 5.0}
{'loss': 2.4457, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.540644407272339, 'eval_accuracy@spa.rst.rststb': 0.2950391644908616, 'eval_f1@spa.rst.rststb': 0.054132502077270474, 'eval_precision@spa.rst.rststb': 0.046530700420860605, 'eval_recall@spa.rst.rststb': 0.07520503999241299, 'eval_loss@spa.rst.rststb': 2.5406441688537598, 'eval_runtime': 4.9317, 'eval_samples_per_second': 77.66, 'eval_steps_per_second': 2.433, 'epoch': 5.0}
{'train@spa.rst.rststb_loss': 2.2657394409179688, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.37098214285714287, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.08283356716040094, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.12013618639092646, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.09634662836665087, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.265739679336548, 'train@spa.rst.rststb_runtime': 27.1023, 'train@spa.rst.rststb_samples_per_second': 82.65, 'train@spa.rst.rststb_steps_per_second': 2.583, 'epoch': 6.0}
{'loss': 2.3632, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.469254493713379, 'eval_accuracy@spa.rst.rststb': 0.3263707571801567, 'eval_f1@spa.rst.rststb': 0.08121313410993873, 'eval_precision@spa.rst.rststb': 0.11290326345938075, 'eval_recall@spa.rst.rststb': 0.09597001643819016, 'eval_loss@spa.rst.rststb': 2.4692542552948, 'eval_runtime': 4.9276, 'eval_samples_per_second': 77.726, 'eval_steps_per_second': 2.435, 'epoch': 6.0}
{'train@spa.rst.rststb_loss': 2.194685220718384, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.3933035714285714, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.09704976698662274, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.12217926334541188, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.11137490786044957, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.1946849822998047, 'train@spa.rst.rststb_runtime': 27.0654, 'train@spa.rst.rststb_samples_per_second': 82.763, 'train@spa.rst.rststb_steps_per_second': 2.586, 'epoch': 7.0}
{'loss': 2.2875, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.418137311935425, 'eval_accuracy@spa.rst.rststb': 0.3577023498694517, 'eval_f1@spa.rst.rststb': 0.10875233830281729, 'eval_precision@spa.rst.rststb': 0.11521737315050966, 'eval_recall@spa.rst.rststb': 0.12086982338093322, 'eval_loss@spa.rst.rststb': 2.418137311935425, 'eval_runtime': 4.9088, 'eval_samples_per_second': 78.023, 'eval_steps_per_second': 2.445, 'epoch': 7.0}
{'train@spa.rst.rststb_loss': 2.139970064163208, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4107142857142857, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10671416457187856, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.13572748695778242, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.12372914144077109, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.139970302581787, 'train@spa.rst.rststb_runtime': 27.0871, 'train@spa.rst.rststb_samples_per_second': 82.696, 'train@spa.rst.rststb_steps_per_second': 2.584, 'epoch': 8.0}
{'loss': 2.2306, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.375420570373535, 'eval_accuracy@spa.rst.rststb': 0.37597911227154046, 'eval_f1@spa.rst.rststb': 0.11244304363316891, 'eval_precision@spa.rst.rststb': 0.10324713039563538, 'eval_recall@spa.rst.rststb': 0.13305184884033897, 'eval_loss@spa.rst.rststb': 2.375420331954956, 'eval_runtime': 4.9175, 'eval_samples_per_second': 77.884, 'eval_steps_per_second': 2.44, 'epoch': 8.0}
{'train@spa.rst.rststb_loss': 2.099837064743042, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.42723214285714284, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.11372895752254622, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.14773053417484233, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.1345690380184256, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.099837064743042, 'train@spa.rst.rststb_runtime': 27.0931, 'train@spa.rst.rststb_samples_per_second': 82.678, 'train@spa.rst.rststb_steps_per_second': 2.584, 'epoch': 9.0}
{'loss': 2.1832, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.342149496078491, 'eval_accuracy@spa.rst.rststb': 0.38642297650130547, 'eval_f1@spa.rst.rststb': 0.1145704376948685, 'eval_precision@spa.rst.rststb': 0.10218047085054759, 'eval_recall@spa.rst.rststb': 0.13809022506420218, 'eval_loss@spa.rst.rststb': 2.3421497344970703, 'eval_runtime': 4.9077, 'eval_samples_per_second': 78.041, 'eval_steps_per_second': 2.445, 'epoch': 9.0}
{'train@spa.rst.rststb_loss': 2.0732381343841553, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.43125, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.11599778257263961, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.15449534452500568, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13756863099209812, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0732383728027344, 'train@spa.rst.rststb_runtime': 27.0691, 'train@spa.rst.rststb_samples_per_second': 82.751, 'train@spa.rst.rststb_steps_per_second': 2.586, 'epoch': 10.0}
{'loss': 2.1501, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.323162078857422, 'eval_accuracy@spa.rst.rststb': 0.3838120104438642, 'eval_f1@spa.rst.rststb': 0.1113950589373409, 'eval_precision@spa.rst.rststb': 0.09670215912451936, 'eval_recall@spa.rst.rststb': 0.13753986733167603, 'eval_loss@spa.rst.rststb': 2.3231618404388428, 'eval_runtime': 4.922, 'eval_samples_per_second': 77.814, 'eval_steps_per_second': 2.438, 'epoch': 10.0}
{'train@spa.rst.rststb_loss': 2.058229684829712, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.43392857142857144, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.11685289280848062, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.15474873207100245, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13833556507593028, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.058229684829712, 'train@spa.rst.rststb_runtime': 27.1137, 'train@spa.rst.rststb_samples_per_second': 82.615, 'train@spa.rst.rststb_steps_per_second': 2.582, 'epoch': 11.0}
{'loss': 2.1249, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.3109095096588135, 'eval_accuracy@spa.rst.rststb': 0.38903394255874674, 'eval_f1@spa.rst.rststb': 0.11296468427529892, 'eval_precision@spa.rst.rststb': 0.09892203446886094, 'eval_recall@spa.rst.rststb': 0.13873915433091213, 'eval_loss@spa.rst.rststb': 2.3109095096588135, 'eval_runtime': 4.9352, 'eval_samples_per_second': 77.605, 'eval_steps_per_second': 2.431, 'epoch': 11.0}
{'train@spa.rst.rststb_loss': 2.053623676300049, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.43526785714285715, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.11715150089999934, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.14906471798324014, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13836922732167825, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.053624153137207, 'train@spa.rst.rststb_runtime': 27.0629, 'train@spa.rst.rststb_samples_per_second': 82.77, 'train@spa.rst.rststb_steps_per_second': 2.587, 'epoch': 12.0}
{'loss': 2.1213, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.3076274394989014, 'eval_accuracy@spa.rst.rststb': 0.38642297650130547, 'eval_f1@spa.rst.rststb': 0.11349772538575773, 'eval_precision@spa.rst.rststb': 0.10048362773587774, 'eval_recall@spa.rst.rststb': 0.13818879659838598, 'eval_loss@spa.rst.rststb': 2.3076274394989014, 'eval_runtime': 4.9393, 'eval_samples_per_second': 77.542, 'eval_steps_per_second': 2.43, 'epoch': 12.0}
{'train_runtime': 1066.8171, 'train_samples_per_second': 25.196, 'train_steps_per_second': 0.787, 'train_loss': 2.4377411070324126, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.3634
  train_runtime            = 0:31:23.89
  train_samples_per_second =     26.116
  train_steps_per_second   =      0.822
-------------------------------------------------------------------
Lang1:  fra.sdrt.annodis    Lang2:  spa.rst.rststb
Saving run to:  runs/full_shot/FullShot=v4_fra.sdrt.annodis_spa.rst.rststb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2185 examples
read 528 examples
read 625 examples
read 2240 examples
read 383 examples
read 426 examples
Total prediction labels:  43
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (fr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=43, bias=True)
    )
  )
)
{'train@fra.sdrt.annodis_loss': 2.779409646987915, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.24530892448512587, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.05290266630278086, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.04289416030343555, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.07355382183273253, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.779409408569336, 'train@fra.sdrt.annodis_runtime': 26.3948, 'train@fra.sdrt.annodis_samples_per_second': 82.781, 'train@fra.sdrt.annodis_steps_per_second': 2.614, 'epoch': 1.0}
{'loss': 3.3296, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.791001796722412, 'eval_accuracy@fra.sdrt.annodis': 0.21022727272727273, 'eval_f1@fra.sdrt.annodis': 0.04345972957084068, 'eval_precision@fra.sdrt.annodis': 0.03514274775028046, 'eval_recall@fra.sdrt.annodis': 0.059382158205280394, 'eval_loss@fra.sdrt.annodis': 2.791001796722412, 'eval_runtime': 6.6724, 'eval_samples_per_second': 79.132, 'eval_steps_per_second': 2.548, 'epoch': 1.0}
{'train@fra.sdrt.annodis_loss': 2.4199013710021973, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.2919908466819222, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.06297651504477095, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.04948998088528779, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.08874939368861577, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.4199016094207764, 'train@fra.sdrt.annodis_runtime': 26.4632, 'train@fra.sdrt.annodis_samples_per_second': 82.567, 'train@fra.sdrt.annodis_steps_per_second': 2.607, 'epoch': 2.0}
{'loss': 2.5856, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.433894395828247, 'eval_accuracy@fra.sdrt.annodis': 0.25, 'eval_f1@fra.sdrt.annodis': 0.05333711076968897, 'eval_precision@fra.sdrt.annodis': 0.0421257911699029, 'eval_recall@fra.sdrt.annodis': 0.07354394893335703, 'eval_loss@fra.sdrt.annodis': 2.433894395828247, 'eval_runtime': 6.71, 'eval_samples_per_second': 78.689, 'eval_steps_per_second': 2.534, 'epoch': 2.0}
{'train@fra.sdrt.annodis_loss': 2.3264119625091553, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.28237986270022886, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.06121801135047199, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.056671998514336096, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.08253215856057285, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.3264122009277344, 'train@fra.sdrt.annodis_runtime': 26.4844, 'train@fra.sdrt.annodis_samples_per_second': 82.501, 'train@fra.sdrt.annodis_steps_per_second': 2.605, 'epoch': 3.0}
{'loss': 2.3993, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.3461506366729736, 'eval_accuracy@fra.sdrt.annodis': 0.2537878787878788, 'eval_f1@fra.sdrt.annodis': 0.051882230829599245, 'eval_precision@fra.sdrt.annodis': 0.047436829646897154, 'eval_recall@fra.sdrt.annodis': 0.07204851095470687, 'eval_loss@fra.sdrt.annodis': 2.346151113510132, 'eval_runtime': 6.723, 'eval_samples_per_second': 78.537, 'eval_steps_per_second': 2.529, 'epoch': 3.0}
{'train@fra.sdrt.annodis_loss': 2.2696187496185303, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.3025171624713959, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.06845383852263778, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.10766141487646438, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.08939167635759065, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.269618511199951, 'train@fra.sdrt.annodis_runtime': 26.4592, 'train@fra.sdrt.annodis_samples_per_second': 82.58, 'train@fra.sdrt.annodis_steps_per_second': 2.608, 'epoch': 4.0}
{'loss': 2.3363, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.294097900390625, 'eval_accuracy@fra.sdrt.annodis': 0.2556818181818182, 'eval_f1@fra.sdrt.annodis': 0.054504806617482665, 'eval_precision@fra.sdrt.annodis': 0.04899351532446713, 'eval_recall@fra.sdrt.annodis': 0.07310339068732977, 'eval_loss@fra.sdrt.annodis': 2.294097661972046, 'eval_runtime': 6.6872, 'eval_samples_per_second': 78.957, 'eval_steps_per_second': 2.542, 'epoch': 4.0}
{'train@fra.sdrt.annodis_loss': 2.2237048149108887, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.32585812356979404, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.07218683169122646, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.09263670859291653, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.09924499890256024, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.2237050533294678, 'train@fra.sdrt.annodis_runtime': 26.4937, 'train@fra.sdrt.annodis_samples_per_second': 82.473, 'train@fra.sdrt.annodis_steps_per_second': 2.604, 'epoch': 5.0}
{'loss': 2.2791, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.25590181350708, 'eval_accuracy@fra.sdrt.annodis': 0.2784090909090909, 'eval_f1@fra.sdrt.annodis': 0.05924293792486295, 'eval_precision@fra.sdrt.annodis': 0.04727662899220823, 'eval_recall@fra.sdrt.annodis': 0.08340860763844495, 'eval_loss@fra.sdrt.annodis': 2.25590181350708, 'eval_runtime': 7.7895, 'eval_samples_per_second': 67.784, 'eval_steps_per_second': 2.182, 'epoch': 5.0}
{'train@fra.sdrt.annodis_loss': 2.181117296218872, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.3382151029748284, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.08054730909877923, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.10369086999401486, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.10565550700954235, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.181117296218872, 'train@fra.sdrt.annodis_runtime': 26.4668, 'train@fra.sdrt.annodis_samples_per_second': 82.556, 'train@fra.sdrt.annodis_steps_per_second': 2.607, 'epoch': 6.0}
{'loss': 2.2397, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.2180850505828857, 'eval_accuracy@fra.sdrt.annodis': 0.2897727272727273, 'eval_f1@fra.sdrt.annodis': 0.06164033266837005, 'eval_precision@fra.sdrt.annodis': 0.04946023710626514, 'eval_recall@fra.sdrt.annodis': 0.0876904942325503, 'eval_loss@fra.sdrt.annodis': 2.2180848121643066, 'eval_runtime': 6.6918, 'eval_samples_per_second': 78.903, 'eval_steps_per_second': 2.54, 'epoch': 6.0}
{'train@fra.sdrt.annodis_loss': 2.1402580738067627, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.3652173913043478, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.10290291867017916, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.13362316688769538, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.12085071398566496, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.1402580738067627, 'train@fra.sdrt.annodis_runtime': 26.4433, 'train@fra.sdrt.annodis_samples_per_second': 82.63, 'train@fra.sdrt.annodis_steps_per_second': 2.609, 'epoch': 7.0}
{'loss': 2.1991, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.1850368976593018, 'eval_accuracy@fra.sdrt.annodis': 0.3087121212121212, 'eval_f1@fra.sdrt.annodis': 0.07605935769540319, 'eval_precision@fra.sdrt.annodis': 0.12053475854053887, 'eval_recall@fra.sdrt.annodis': 0.09633889488156357, 'eval_loss@fra.sdrt.annodis': 2.1850366592407227, 'eval_runtime': 6.723, 'eval_samples_per_second': 78.537, 'eval_steps_per_second': 2.529, 'epoch': 7.0}
{'train@fra.sdrt.annodis_loss': 2.103353500366211, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.3903890160183066, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.12273434717842652, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12960677213756927, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.13964512748051602, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.103353500366211, 'train@fra.sdrt.annodis_runtime': 26.4676, 'train@fra.sdrt.annodis_samples_per_second': 82.554, 'train@fra.sdrt.annodis_steps_per_second': 2.607, 'epoch': 8.0}
{'loss': 2.1694, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.155654191970825, 'eval_accuracy@fra.sdrt.annodis': 0.3314393939393939, 'eval_f1@fra.sdrt.annodis': 0.08996351243475467, 'eval_precision@fra.sdrt.annodis': 0.11187560637074018, 'eval_recall@fra.sdrt.annodis': 0.10836559293644507, 'eval_loss@fra.sdrt.annodis': 2.155654191970825, 'eval_runtime': 6.6856, 'eval_samples_per_second': 78.975, 'eval_steps_per_second': 2.543, 'epoch': 8.0}
{'train@fra.sdrt.annodis_loss': 2.074805498123169, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4045766590389016, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.13051077194133703, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.13174062771897754, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.15103865529971006, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.074805498123169, 'train@fra.sdrt.annodis_runtime': 26.4681, 'train@fra.sdrt.annodis_samples_per_second': 82.552, 'train@fra.sdrt.annodis_steps_per_second': 2.607, 'epoch': 9.0}
{'loss': 2.133, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.1319730281829834, 'eval_accuracy@fra.sdrt.annodis': 0.3333333333333333, 'eval_f1@fra.sdrt.annodis': 0.09327471120155535, 'eval_precision@fra.sdrt.annodis': 0.1076439412597003, 'eval_recall@fra.sdrt.annodis': 0.11113306629215372, 'eval_loss@fra.sdrt.annodis': 2.1319727897644043, 'eval_runtime': 6.6866, 'eval_samples_per_second': 78.963, 'eval_steps_per_second': 2.542, 'epoch': 9.0}
{'train@fra.sdrt.annodis_loss': 2.053454875946045, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.41418764302059496, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.13548721105443454, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.13312856117155553, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.158316176452541, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.053454875946045, 'train@fra.sdrt.annodis_runtime': 26.4723, 'train@fra.sdrt.annodis_samples_per_second': 82.539, 'train@fra.sdrt.annodis_steps_per_second': 2.606, 'epoch': 10.0}
{'loss': 2.1107, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.115497350692749, 'eval_accuracy@fra.sdrt.annodis': 0.3333333333333333, 'eval_f1@fra.sdrt.annodis': 0.09231697200485267, 'eval_precision@fra.sdrt.annodis': 0.09826165066365193, 'eval_recall@fra.sdrt.annodis': 0.11131843684789465, 'eval_loss@fra.sdrt.annodis': 2.115497350692749, 'eval_runtime': 6.7096, 'eval_samples_per_second': 78.693, 'eval_steps_per_second': 2.534, 'epoch': 10.0}
{'train@fra.sdrt.annodis_loss': 2.0404462814331055, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4196796338672769, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.13675091746454068, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.1318564244819523, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.162333287272199, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.0404462814331055, 'train@fra.sdrt.annodis_runtime': 26.4985, 'train@fra.sdrt.annodis_samples_per_second': 82.458, 'train@fra.sdrt.annodis_steps_per_second': 2.604, 'epoch': 11.0}
{'loss': 2.0863, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.1044156551361084, 'eval_accuracy@fra.sdrt.annodis': 0.3371212121212121, 'eval_f1@fra.sdrt.annodis': 0.09690472798723652, 'eval_precision@fra.sdrt.annodis': 0.1007069615215175, 'eval_recall@fra.sdrt.annodis': 0.1154602999288591, 'eval_loss@fra.sdrt.annodis': 2.1044156551361084, 'eval_runtime': 6.7378, 'eval_samples_per_second': 78.364, 'eval_steps_per_second': 2.523, 'epoch': 11.0}
{'train@fra.sdrt.annodis_loss': 2.035977363586426, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4205949656750572, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.13718743861574995, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.13115198796939295, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.16284876800871634, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.035977363586426, 'train@fra.sdrt.annodis_runtime': 26.5005, 'train@fra.sdrt.annodis_samples_per_second': 82.451, 'train@fra.sdrt.annodis_steps_per_second': 2.604, 'epoch': 12.0}
{'loss': 2.0843, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.1010711193084717, 'eval_accuracy@fra.sdrt.annodis': 0.3390151515151515, 'eval_f1@fra.sdrt.annodis': 0.09733354079276357, 'eval_precision@fra.sdrt.annodis': 0.1009116905032066, 'eval_recall@fra.sdrt.annodis': 0.11596080042935962, 'eval_loss@fra.sdrt.annodis': 2.1010711193084717, 'eval_runtime': 6.6981, 'eval_samples_per_second': 78.829, 'eval_steps_per_second': 2.538, 'epoch': 12.0}
{'train_runtime': 1064.4915, 'train_samples_per_second': 24.631, 'train_steps_per_second': 0.778, 'train_loss': 2.329375059708305, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.3294
  train_runtime            = 0:17:44.49
  train_samples_per_second =     24.631
  train_steps_per_second   =      0.778
{'train@spa.rst.rststb_loss': 2.999476194381714, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.20133928571428572, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.012002341920374707, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.007216461853558628, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.03563527180783818, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.999476432800293, 'train@spa.rst.rststb_runtime': 27.1725, 'train@spa.rst.rststb_samples_per_second': 82.436, 'train@spa.rst.rststb_steps_per_second': 2.576, 'epoch': 1.0}
{'loss': 3.7696, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.0963573455810547, 'eval_accuracy@spa.rst.rststb': 0.206266318537859, 'eval_f1@spa.rst.rststb': 0.014869188782232262, 'eval_precision@spa.rst.rststb': 0.00896810080599387, 'eval_recall@spa.rst.rststb': 0.043478260869565216, 'eval_loss@spa.rst.rststb': 3.096357822418213, 'eval_runtime': 4.9763, 'eval_samples_per_second': 76.965, 'eval_steps_per_second': 2.411, 'epoch': 1.0}
{'train@spa.rst.rststb_loss': 2.5766634941101074, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.22232142857142856, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.022427751478270197, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.0481358332806807, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.04128847601311205, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.5766634941101074, 'train@spa.rst.rststb_runtime': 27.1928, 'train@spa.rst.rststb_samples_per_second': 82.375, 'train@spa.rst.rststb_steps_per_second': 2.574, 'epoch': 2.0}
{'loss': 2.7698, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.710317611694336, 'eval_accuracy@spa.rst.rststb': 0.2193211488250653, 'eval_f1@spa.rst.rststb': 0.022409742750451444, 'eval_precision@spa.rst.rststb': 0.06284535407881343, 'eval_recall@spa.rst.rststb': 0.047401333254675236, 'eval_loss@spa.rst.rststb': 2.710317850112915, 'eval_runtime': 4.9875, 'eval_samples_per_second': 76.792, 'eval_steps_per_second': 2.406, 'epoch': 2.0}
{'train@spa.rst.rststb_loss': 2.425402879714966, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.3299107142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.0558225684540142, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.1026589594420934, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.07301081632449521, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.425403118133545, 'train@spa.rst.rststb_runtime': 27.1505, 'train@spa.rst.rststb_samples_per_second': 82.503, 'train@spa.rst.rststb_steps_per_second': 2.578, 'epoch': 3.0}
{'loss': 2.5329, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.5959224700927734, 'eval_accuracy@spa.rst.rststb': 0.31592689295039167, 'eval_f1@spa.rst.rststb': 0.0639341838585695, 'eval_precision@spa.rst.rststb': 0.07463482593558432, 'eval_recall@spa.rst.rststb': 0.08459541879193758, 'eval_loss@spa.rst.rststb': 2.5959219932556152, 'eval_runtime': 4.9569, 'eval_samples_per_second': 77.267, 'eval_steps_per_second': 2.421, 'epoch': 3.0}
{'train@spa.rst.rststb_loss': 2.3166821002960205, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.37991071428571427, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.09200940174800525, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.11198864681036806, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.10857284727877665, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.3166821002960205, 'train@spa.rst.rststb_runtime': 27.1307, 'train@spa.rst.rststb_samples_per_second': 82.563, 'train@spa.rst.rststb_steps_per_second': 2.58, 'epoch': 4.0}
{'loss': 2.4207, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.5119736194610596, 'eval_accuracy@spa.rst.rststb': 0.37597911227154046, 'eval_f1@spa.rst.rststb': 0.10907050097408942, 'eval_precision@spa.rst.rststb': 0.11372381661606613, 'eval_recall@spa.rst.rststb': 0.13372111381993176, 'eval_loss@spa.rst.rststb': 2.5119738578796387, 'eval_runtime': 4.9957, 'eval_samples_per_second': 76.665, 'eval_steps_per_second': 2.402, 'epoch': 4.0}
{'train@spa.rst.rststb_loss': 2.2220041751861572, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4075892857142857, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10378587874373994, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.10584070971285028, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.1255084529330063, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.2220041751861572, 'train@spa.rst.rststb_runtime': 27.1391, 'train@spa.rst.rststb_samples_per_second': 82.538, 'train@spa.rst.rststb_steps_per_second': 2.579, 'epoch': 5.0}
{'loss': 2.3137, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.4419445991516113, 'eval_accuracy@spa.rst.rststb': 0.391644908616188, 'eval_f1@spa.rst.rststb': 0.1077232618853384, 'eval_precision@spa.rst.rststb': 0.10035970068578765, 'eval_recall@spa.rst.rststb': 0.13939412430446527, 'eval_loss@spa.rst.rststb': 2.4419448375701904, 'eval_runtime': 4.9652, 'eval_samples_per_second': 77.136, 'eval_steps_per_second': 2.417, 'epoch': 5.0}
{'train@spa.rst.rststb_loss': 2.1380248069763184, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4138392857142857, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10578836400459836, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.10234451806302665, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.12999464201918093, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.1380248069763184, 'train@spa.rst.rststb_runtime': 27.2088, 'train@spa.rst.rststb_samples_per_second': 82.326, 'train@spa.rst.rststb_steps_per_second': 2.573, 'epoch': 6.0}
{'loss': 2.2265, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.3771133422851562, 'eval_accuracy@spa.rst.rststb': 0.38903394255874674, 'eval_f1@spa.rst.rststb': 0.10629967274427302, 'eval_precision@spa.rst.rststb': 0.09458349757467117, 'eval_recall@spa.rst.rststb': 0.13913948117449051, 'eval_loss@spa.rst.rststb': 2.3771135807037354, 'eval_runtime': 4.9528, 'eval_samples_per_second': 77.33, 'eval_steps_per_second': 2.423, 'epoch': 6.0}
{'train@spa.rst.rststb_loss': 2.068742036819458, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.42678571428571427, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.11099604630791983, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.15782941155089988, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13537374994610468, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.068742275238037, 'train@spa.rst.rststb_runtime': 27.1103, 'train@spa.rst.rststb_samples_per_second': 82.625, 'train@spa.rst.rststb_steps_per_second': 2.582, 'epoch': 7.0}
{'loss': 2.1536, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.3248543739318848, 'eval_accuracy@spa.rst.rststb': 0.4073107049608355, 'eval_f1@spa.rst.rststb': 0.11062215712326706, 'eval_precision@spa.rst.rststb': 0.1008456082924168, 'eval_recall@spa.rst.rststb': 0.1451940029676006, 'eval_loss@spa.rst.rststb': 2.3248541355133057, 'eval_runtime': 4.9591, 'eval_samples_per_second': 77.232, 'eval_steps_per_second': 2.42, 'epoch': 7.0}
{'train@spa.rst.rststb_loss': 2.0166101455688477, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.43348214285714287, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.11500777621041662, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.15135853424398898, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13872183649611008, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0166101455688477, 'train@spa.rst.rststb_runtime': 27.1381, 'train@spa.rst.rststb_samples_per_second': 82.541, 'train@spa.rst.rststb_steps_per_second': 2.579, 'epoch': 8.0}
{'loss': 2.0981, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.2845711708068848, 'eval_accuracy@spa.rst.rststb': 0.402088772845953, 'eval_f1@spa.rst.rststb': 0.11057850203405784, 'eval_precision@spa.rst.rststb': 0.0999507273231781, 'eval_recall@spa.rst.rststb': 0.14318942173820232, 'eval_loss@spa.rst.rststb': 2.2845711708068848, 'eval_runtime': 4.9547, 'eval_samples_per_second': 77.3, 'eval_steps_per_second': 2.422, 'epoch': 8.0}
{'train@spa.rst.rststb_loss': 1.9803253412246704, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4357142857142857, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.11483871633098028, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.1481689832537504, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.1398406916169392, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.9803253412246704, 'train@spa.rst.rststb_runtime': 27.1274, 'train@spa.rst.rststb_samples_per_second': 82.573, 'train@spa.rst.rststb_steps_per_second': 2.58, 'epoch': 9.0}
{'loss': 2.0492, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.257274627685547, 'eval_accuracy@spa.rst.rststb': 0.3968668407310705, 'eval_f1@spa.rst.rststb': 0.1096614263489724, 'eval_precision@spa.rst.rststb': 0.09557393959150881, 'eval_recall@spa.rst.rststb': 0.14396671269160774, 'eval_loss@spa.rst.rststb': 2.257274627685547, 'eval_runtime': 4.9749, 'eval_samples_per_second': 76.986, 'eval_steps_per_second': 2.412, 'epoch': 9.0}
{'train@spa.rst.rststb_loss': 1.954869031906128, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.44107142857142856, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.11668826268430109, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.14874800325102397, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.14150011565784687, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.9548689126968384, 'train@spa.rst.rststb_runtime': 27.1635, 'train@spa.rst.rststb_samples_per_second': 82.464, 'train@spa.rst.rststb_steps_per_second': 2.577, 'epoch': 10.0}
{'loss': 2.0252, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.239457130432129, 'eval_accuracy@spa.rst.rststb': 0.4046997389033943, 'eval_f1@spa.rst.rststb': 0.1116206272701487, 'eval_precision@spa.rst.rststb': 0.09699897950184573, 'eval_recall@spa.rst.rststb': 0.14591350049173754, 'eval_loss@spa.rst.rststb': 2.239457368850708, 'eval_runtime': 4.9715, 'eval_samples_per_second': 77.038, 'eval_steps_per_second': 2.414, 'epoch': 10.0}
{'train@spa.rst.rststb_loss': 1.9406380653381348, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.44151785714285713, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.11731495817257316, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.15085604523825236, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.14215729529128737, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.940638542175293, 'train@spa.rst.rststb_runtime': 27.1401, 'train@spa.rst.rststb_samples_per_second': 82.535, 'train@spa.rst.rststb_steps_per_second': 2.579, 'epoch': 11.0}
{'loss': 2.0089, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.2285282611846924, 'eval_accuracy@spa.rst.rststb': 0.4073107049608355, 'eval_f1@spa.rst.rststb': 0.11573403361373406, 'eval_precision@spa.rst.rststb': 0.14084612998717366, 'eval_recall@spa.rst.rststb': 0.1478038596599795, 'eval_loss@spa.rst.rststb': 2.228527784347534, 'eval_runtime': 4.9706, 'eval_samples_per_second': 77.054, 'eval_steps_per_second': 2.414, 'epoch': 11.0}
{'train@spa.rst.rststb_loss': 1.9357759952545166, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.44419642857142855, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.11849919516622658, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.1692773974097998, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.14295404736212888, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.9357759952545166, 'train@spa.rst.rststb_runtime': 27.1263, 'train@spa.rst.rststb_samples_per_second': 82.577, 'train@spa.rst.rststb_steps_per_second': 2.581, 'epoch': 12.0}
{'loss': 1.9968, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.224855661392212, 'eval_accuracy@spa.rst.rststb': 0.4073107049608355, 'eval_f1@spa.rst.rststb': 0.11573403361373406, 'eval_precision@spa.rst.rststb': 0.14084612998717366, 'eval_recall@spa.rst.rststb': 0.1478038596599795, 'eval_loss@spa.rst.rststb': 2.224855661392212, 'eval_runtime': 4.9457, 'eval_samples_per_second': 77.441, 'eval_steps_per_second': 2.426, 'epoch': 12.0}
{'train_runtime': 1068.697, 'train_samples_per_second': 25.152, 'train_steps_per_second': 0.786, 'train_loss': 2.3637555076962427, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.3294
  train_runtime            = 0:17:44.49
  train_samples_per_second =     24.631
  train_steps_per_second   =      0.778
-------------------------------------------------------------------
Lang1:  nld.rst.nldt    Lang2:  spa.rst.rststb
Saving run to:  runs/full_shot/FullShot=v4_nld.rst.nldt_spa.rst.rststb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 1608 examples
read 331 examples
read 326 examples
read 2240 examples
read 383 examples
read 426 examples
Total prediction labels:  37
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=37, bias=True)
    )
  )
)
{'train@nld.rst.nldt_loss': 3.1442227363586426, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26430348258706465, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01306566650270536, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.00825948383084577, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03125, 'train@nld.rst.nldt_loss@nld.rst.nldt': 3.1442229747772217, 'train@nld.rst.nldt_runtime': 19.4582, 'train@nld.rst.nldt_samples_per_second': 82.639, 'train@nld.rst.nldt_steps_per_second': 2.621, 'epoch': 1.0}
{'loss': 3.397, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.121786594390869, 'eval_accuracy@nld.rst.nldt': 0.27794561933534745, 'eval_f1@nld.rst.nldt': 0.016110673321075214, 'eval_precision@nld.rst.nldt': 0.010294282197605462, 'eval_recall@nld.rst.nldt': 0.037037037037037035, 'eval_loss@nld.rst.nldt': 3.1217870712280273, 'eval_runtime': 4.309, 'eval_samples_per_second': 76.815, 'eval_steps_per_second': 2.553, 'epoch': 1.0}
{'train@nld.rst.nldt_loss': 2.867921829223633, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26430348258706465, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01306566650270536, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.00825948383084577, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03125, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.867922067642212, 'train@nld.rst.nldt_runtime': 19.5319, 'train@nld.rst.nldt_samples_per_second': 82.327, 'train@nld.rst.nldt_steps_per_second': 2.611, 'epoch': 2.0}
{'loss': 3.0013, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.8125736713409424, 'eval_accuracy@nld.rst.nldt': 0.27794561933534745, 'eval_f1@nld.rst.nldt': 0.016110673321075214, 'eval_precision@nld.rst.nldt': 0.010294282197605462, 'eval_recall@nld.rst.nldt': 0.037037037037037035, 'eval_loss@nld.rst.nldt': 2.8125736713409424, 'eval_runtime': 4.3018, 'eval_samples_per_second': 76.944, 'eval_steps_per_second': 2.557, 'epoch': 2.0}
{'train@nld.rst.nldt_loss': 2.780529022216797, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26430348258706465, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01310434139121855, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.00829041822721598, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03125, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.7805287837982178, 'train@nld.rst.nldt_runtime': 19.5383, 'train@nld.rst.nldt_samples_per_second': 82.3, 'train@nld.rst.nldt_steps_per_second': 2.61, 'epoch': 3.0}
{'loss': 2.8574, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.7303829193115234, 'eval_accuracy@nld.rst.nldt': 0.27794561933534745, 'eval_f1@nld.rst.nldt': 0.016187208586258467, 'eval_precision@nld.rst.nldt': 0.010356861420691208, 'eval_recall@nld.rst.nldt': 0.037037037037037035, 'eval_loss@nld.rst.nldt': 2.7303826808929443, 'eval_runtime': 4.3117, 'eval_samples_per_second': 76.768, 'eval_steps_per_second': 2.551, 'epoch': 3.0}
{'train@nld.rst.nldt_loss': 2.7353389263153076, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26927860696517414, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.017088167066314795, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.030829656221604208, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03340919701213819, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.7353384494781494, 'train@nld.rst.nldt_runtime': 19.5853, 'train@nld.rst.nldt_samples_per_second': 82.103, 'train@nld.rst.nldt_steps_per_second': 2.604, 'epoch': 4.0}
{'loss': 2.7626, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.6907896995544434, 'eval_accuracy@nld.rst.nldt': 0.283987915407855, 'eval_f1@nld.rst.nldt': 0.022101355434688767, 'eval_precision@nld.rst.nldt': 0.05674846625766871, 'eval_recall@nld.rst.nldt': 0.04015285126396237, 'eval_loss@nld.rst.nldt': 2.6907894611358643, 'eval_runtime': 4.2927, 'eval_samples_per_second': 77.108, 'eval_steps_per_second': 2.563, 'epoch': 4.0}
{'train@nld.rst.nldt_loss': 2.7016444206237793, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2798507462686567, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.024232019767807998, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.026330509932285563, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.038673552754435105, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.7016444206237793, 'train@nld.rst.nldt_runtime': 19.5534, 'train@nld.rst.nldt_samples_per_second': 82.236, 'train@nld.rst.nldt_steps_per_second': 2.608, 'epoch': 5.0}
{'loss': 2.7377, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.6593825817108154, 'eval_accuracy@nld.rst.nldt': 0.30513595166163143, 'eval_f1@nld.rst.nldt': 0.03624757181510214, 'eval_precision@nld.rst.nldt': 0.03829407412469302, 'eval_recall@nld.rst.nldt': 0.052557319223985884, 'eval_loss@nld.rst.nldt': 2.6593828201293945, 'eval_runtime': 4.2889, 'eval_samples_per_second': 77.176, 'eval_steps_per_second': 2.565, 'epoch': 5.0}
{'train@nld.rst.nldt_loss': 2.6677231788635254, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.292910447761194, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.031095518897239344, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.02748168358456815, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.046386554621848736, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.667722702026367, 'train@nld.rst.nldt_runtime': 19.5777, 'train@nld.rst.nldt_samples_per_second': 82.134, 'train@nld.rst.nldt_steps_per_second': 2.605, 'epoch': 6.0}
{'loss': 2.7063, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.629985809326172, 'eval_accuracy@nld.rst.nldt': 0.31419939577039274, 'eval_f1@nld.rst.nldt': 0.039284583729028176, 'eval_precision@nld.rst.nldt': 0.038183224238406935, 'eval_recall@nld.rst.nldt': 0.05673133450911228, 'eval_loss@nld.rst.nldt': 2.629986047744751, 'eval_runtime': 4.3001, 'eval_samples_per_second': 76.974, 'eval_steps_per_second': 2.558, 'epoch': 6.0}
{'train@nld.rst.nldt_loss': 2.6413443088531494, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30472636815920395, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03533742875608058, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.028781343856953273, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05302112511671336, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6413450241088867, 'train@nld.rst.nldt_runtime': 19.5857, 'train@nld.rst.nldt_samples_per_second': 82.101, 'train@nld.rst.nldt_steps_per_second': 2.604, 'epoch': 7.0}
{'loss': 2.677, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.6073989868164062, 'eval_accuracy@nld.rst.nldt': 0.311178247734139, 'eval_f1@nld.rst.nldt': 0.03757688817308627, 'eval_precision@nld.rst.nldt': 0.034762550006452446, 'eval_recall@nld.rst.nldt': 0.056984382588247326, 'eval_loss@nld.rst.nldt': 2.6073992252349854, 'eval_runtime': 4.2911, 'eval_samples_per_second': 77.137, 'eval_steps_per_second': 2.563, 'epoch': 7.0}
{'train@nld.rst.nldt_loss': 2.619011163711548, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30970149253731344, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03733309199303236, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.05934966087981433, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.057774898002679334, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.619011163711548, 'train@nld.rst.nldt_runtime': 20.2952, 'train@nld.rst.nldt_samples_per_second': 79.231, 'train@nld.rst.nldt_steps_per_second': 2.513, 'epoch': 8.0}
{'loss': 2.6583, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.5898640155792236, 'eval_accuracy@nld.rst.nldt': 0.3293051359516616, 'eval_f1@nld.rst.nldt': 0.045675626646445436, 'eval_precision@nld.rst.nldt': 0.048718911836006906, 'eval_recall@nld.rst.nldt': 0.06969174143087187, 'eval_loss@nld.rst.nldt': 2.5898642539978027, 'eval_runtime': 4.3291, 'eval_samples_per_second': 76.46, 'eval_steps_per_second': 2.541, 'epoch': 8.0}
{'train@nld.rst.nldt_loss': 2.6062541007995605, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31156716417910446, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03814214692396596, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.05997522825418046, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05869343208297812, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6062541007995605, 'train@nld.rst.nldt_runtime': 19.5463, 'train@nld.rst.nldt_samples_per_second': 82.266, 'train@nld.rst.nldt_steps_per_second': 2.609, 'epoch': 9.0}
{'loss': 2.6398, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.578524112701416, 'eval_accuracy@nld.rst.nldt': 0.3293051359516616, 'eval_f1@nld.rst.nldt': 0.045204366513664156, 'eval_precision@nld.rst.nldt': 0.04856766209893451, 'eval_recall@nld.rst.nldt': 0.0680367047516806, 'eval_loss@nld.rst.nldt': 2.578524589538574, 'eval_runtime': 4.2991, 'eval_samples_per_second': 76.992, 'eval_steps_per_second': 2.559, 'epoch': 9.0}
{'train@nld.rst.nldt_loss': 2.5970823764801025, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31218905472636815, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.038367920543173506, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.03934243966909382, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05894144795599399, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.5970823764801025, 'train@nld.rst.nldt_runtime': 19.5905, 'train@nld.rst.nldt_samples_per_second': 82.08, 'train@nld.rst.nldt_steps_per_second': 2.603, 'epoch': 10.0}
{'loss': 2.6291, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.571101665496826, 'eval_accuracy@nld.rst.nldt': 0.3293051359516616, 'eval_f1@nld.rst.nldt': 0.045204366513664156, 'eval_precision@nld.rst.nldt': 0.04856766209893451, 'eval_recall@nld.rst.nldt': 0.0680367047516806, 'eval_loss@nld.rst.nldt': 2.571101427078247, 'eval_runtime': 4.2981, 'eval_samples_per_second': 77.011, 'eval_steps_per_second': 2.559, 'epoch': 10.0}
{'train@nld.rst.nldt_loss': 2.5902578830718994, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31156716417910446, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03804556670483898, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.03871686514198046, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05921689146673162, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.5902578830718994, 'train@nld.rst.nldt_runtime': 19.5387, 'train@nld.rst.nldt_samples_per_second': 82.298, 'train@nld.rst.nldt_steps_per_second': 2.61, 'epoch': 11.0}
{'loss': 2.6199, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.565967082977295, 'eval_accuracy@nld.rst.nldt': 0.3293051359516616, 'eval_f1@nld.rst.nldt': 0.044832527704209583, 'eval_precision@nld.rst.nldt': 0.04787965016841806, 'eval_recall@nld.rst.nldt': 0.06969174143087187, 'eval_loss@nld.rst.nldt': 2.5659666061401367, 'eval_runtime': 4.3134, 'eval_samples_per_second': 76.737, 'eval_steps_per_second': 2.55, 'epoch': 11.0}
{'train@nld.rst.nldt_loss': 2.5876410007476807, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31032338308457713, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03801259228022694, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.03854471036546375, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05953551891771201, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.5876410007476807, 'train@nld.rst.nldt_runtime': 19.5243, 'train@nld.rst.nldt_samples_per_second': 82.359, 'train@nld.rst.nldt_steps_per_second': 2.612, 'epoch': 12.0}
{'loss': 2.6237, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.564223051071167, 'eval_accuracy@nld.rst.nldt': 0.3293051359516616, 'eval_f1@nld.rst.nldt': 0.044832527704209583, 'eval_precision@nld.rst.nldt': 0.04787965016841806, 'eval_recall@nld.rst.nldt': 0.06969174143087187, 'eval_loss@nld.rst.nldt': 2.564223051071167, 'eval_runtime': 4.3162, 'eval_samples_per_second': 76.687, 'eval_steps_per_second': 2.549, 'epoch': 12.0}
{'train_runtime': 777.019, 'train_samples_per_second': 24.833, 'train_steps_per_second': 0.788, 'train_loss': 2.7758383657418046, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.7758
  train_runtime            = 0:12:57.01
  train_samples_per_second =     24.833
  train_steps_per_second   =      0.788
{'train@spa.rst.rststb_loss': 2.820986032485962, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.2174107142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.022130631298162416, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.015649567511269637, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.04149349265640724, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.820986032485962, 'train@spa.rst.rststb_runtime': 27.0836, 'train@spa.rst.rststb_samples_per_second': 82.707, 'train@spa.rst.rststb_steps_per_second': 2.585, 'epoch': 1.0}
{'loss': 3.3505, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.848942995071411, 'eval_accuracy@spa.rst.rststb': 0.21671018276762402, 'eval_f1@spa.rst.rststb': 0.026970819522973942, 'eval_precision@spa.rst.rststb': 0.01868686124575801, 'eval_recall@spa.rst.rststb': 0.048833980893550955, 'eval_loss@spa.rst.rststb': 2.848942518234253, 'eval_runtime': 4.9306, 'eval_samples_per_second': 77.678, 'eval_steps_per_second': 2.434, 'epoch': 1.0}
{'train@spa.rst.rststb_loss': 2.5420169830322266, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.2683035714285714, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.03905431460324321, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.05188842542186849, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.0536525392542802, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.5420172214508057, 'train@spa.rst.rststb_runtime': 27.1084, 'train@spa.rst.rststb_samples_per_second': 82.631, 'train@spa.rst.rststb_steps_per_second': 2.582, 'epoch': 2.0}
{'loss': 2.6916, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.6444263458251953, 'eval_accuracy@spa.rst.rststb': 0.2506527415143603, 'eval_f1@spa.rst.rststb': 0.03984898307148947, 'eval_precision@spa.rst.rststb': 0.06371726044186833, 'eval_recall@spa.rst.rststb': 0.058067407994300775, 'eval_loss@spa.rst.rststb': 2.6444265842437744, 'eval_runtime': 4.9456, 'eval_samples_per_second': 77.442, 'eval_steps_per_second': 2.426, 'epoch': 2.0}
{'train@spa.rst.rststb_loss': 2.4197299480438232, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.33482142857142855, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.06113934331530903, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.06322626071859526, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.07718051101408219, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.4197299480438232, 'train@spa.rst.rststb_runtime': 27.0968, 'train@spa.rst.rststb_samples_per_second': 82.667, 'train@spa.rst.rststb_steps_per_second': 2.583, 'epoch': 3.0}
{'loss': 2.5148, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.5648183822631836, 'eval_accuracy@spa.rst.rststb': 0.2898172323759791, 'eval_f1@spa.rst.rststb': 0.06164781139037728, 'eval_precision@spa.rst.rststb': 0.06284249215395583, 'eval_recall@spa.rst.rststb': 0.07737905438108332, 'eval_loss@spa.rst.rststb': 2.5648186206817627, 'eval_runtime': 4.8995, 'eval_samples_per_second': 78.172, 'eval_steps_per_second': 2.449, 'epoch': 3.0}
{'train@spa.rst.rststb_loss': 2.3308475017547607, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.3575892857142857, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.07447907579650022, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.09567160837773603, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.09340216821662452, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.3308475017547607, 'train@spa.rst.rststb_runtime': 27.1248, 'train@spa.rst.rststb_samples_per_second': 82.581, 'train@spa.rst.rststb_steps_per_second': 2.581, 'epoch': 4.0}
{'loss': 2.4145, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.4981205463409424, 'eval_accuracy@spa.rst.rststb': 0.3263707571801567, 'eval_f1@spa.rst.rststb': 0.0786293572405282, 'eval_precision@spa.rst.rststb': 0.07284265091448457, 'eval_recall@spa.rst.rststb': 0.09556142196052168, 'eval_loss@spa.rst.rststb': 2.4981207847595215, 'eval_runtime': 4.9396, 'eval_samples_per_second': 77.537, 'eval_steps_per_second': 2.429, 'epoch': 4.0}
{'train@spa.rst.rststb_loss': 2.255251169204712, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.3767857142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.0831060022724184, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.12247391795417169, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.10479352777574844, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.255250930786133, 'train@spa.rst.rststb_runtime': 27.0985, 'train@spa.rst.rststb_samples_per_second': 82.661, 'train@spa.rst.rststb_steps_per_second': 2.583, 'epoch': 5.0}
{'loss': 2.3347, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.446369171142578, 'eval_accuracy@spa.rst.rststb': 0.3577023498694517, 'eval_f1@spa.rst.rststb': 0.09580860369709669, 'eval_precision@spa.rst.rststb': 0.12004899402051711, 'eval_recall@spa.rst.rststb': 0.11460355681086097, 'eval_loss@spa.rst.rststb': 2.446369171142578, 'eval_runtime': 4.9286, 'eval_samples_per_second': 77.709, 'eval_steps_per_second': 2.435, 'epoch': 5.0}
{'train@spa.rst.rststb_loss': 2.1876983642578125, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4004464285714286, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.09458430668089646, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.1273554210817841, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.11711465918983226, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.1876978874206543, 'train@spa.rst.rststb_runtime': 27.1242, 'train@spa.rst.rststb_samples_per_second': 82.583, 'train@spa.rst.rststb_steps_per_second': 2.581, 'epoch': 6.0}
{'loss': 2.2658, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.3965327739715576, 'eval_accuracy@spa.rst.rststb': 0.3733681462140992, 'eval_f1@spa.rst.rststb': 0.10447715005483875, 'eval_precision@spa.rst.rststb': 0.1158932715968118, 'eval_recall@spa.rst.rststb': 0.1278877912434127, 'eval_loss@spa.rst.rststb': 2.3965327739715576, 'eval_runtime': 4.9173, 'eval_samples_per_second': 77.888, 'eval_steps_per_second': 2.44, 'epoch': 6.0}
{'train@spa.rst.rststb_loss': 2.1276628971099854, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4133928571428571, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10161381945091898, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.15930242739314524, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.12387576812199981, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.1276633739471436, 'train@spa.rst.rststb_runtime': 27.0884, 'train@spa.rst.rststb_samples_per_second': 82.692, 'train@spa.rst.rststb_steps_per_second': 2.584, 'epoch': 7.0}
{'loss': 2.206, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.352386951446533, 'eval_accuracy@spa.rst.rststb': 0.3968668407310705, 'eval_f1@spa.rst.rststb': 0.11821232241022347, 'eval_precision@spa.rst.rststb': 0.11092663232972252, 'eval_recall@spa.rst.rststb': 0.14314621668263475, 'eval_loss@spa.rst.rststb': 2.3523871898651123, 'eval_runtime': 4.9223, 'eval_samples_per_second': 77.809, 'eval_steps_per_second': 2.438, 'epoch': 7.0}
{'train@spa.rst.rststb_loss': 2.0794577598571777, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.42142857142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.1058807936598629, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.15884308418466922, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.1278879960957311, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0794575214385986, 'train@spa.rst.rststb_runtime': 27.0931, 'train@spa.rst.rststb_samples_per_second': 82.678, 'train@spa.rst.rststb_steps_per_second': 2.584, 'epoch': 8.0}
{'loss': 2.1553, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.3129780292510986, 'eval_accuracy@spa.rst.rststb': 0.39425587467362927, 'eval_f1@spa.rst.rststb': 0.11802893899999078, 'eval_precision@spa.rst.rststb': 0.11104209799861973, 'eval_recall@spa.rst.rststb': 0.14259585895010862, 'eval_loss@spa.rst.rststb': 2.3129780292510986, 'eval_runtime': 4.9411, 'eval_samples_per_second': 77.513, 'eval_steps_per_second': 2.429, 'epoch': 8.0}
{'train@spa.rst.rststb_loss': 2.043095111846924, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.42276785714285714, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10760647349149481, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.1356996992566943, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13123415975048774, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.043095111846924, 'train@spa.rst.rststb_runtime': 27.1002, 'train@spa.rst.rststb_samples_per_second': 82.656, 'train@spa.rst.rststb_steps_per_second': 2.583, 'epoch': 9.0}
{'loss': 2.1156, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.28574275970459, 'eval_accuracy@spa.rst.rststb': 0.391644908616188, 'eval_f1@spa.rst.rststb': 0.11386426709275922, 'eval_precision@spa.rst.rststb': 0.10063258921002861, 'eval_recall@spa.rst.rststb': 0.14268042884591403, 'eval_loss@spa.rst.rststb': 2.28574275970459, 'eval_runtime': 4.9652, 'eval_samples_per_second': 77.137, 'eval_steps_per_second': 2.417, 'epoch': 9.0}
{'train@spa.rst.rststb_loss': 2.0168724060058594, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.43125, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.112844849269524, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.14499859075989718, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13535542477992293, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0168724060058594, 'train@spa.rst.rststb_runtime': 27.0828, 'train@spa.rst.rststb_samples_per_second': 82.709, 'train@spa.rst.rststb_steps_per_second': 2.585, 'epoch': 10.0}
{'loss': 2.0821, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.2654857635498047, 'eval_accuracy@spa.rst.rststb': 0.39947780678851175, 'eval_f1@spa.rst.rststb': 0.1168756885173646, 'eval_precision@spa.rst.rststb': 0.10467031132228488, 'eval_recall@spa.rst.rststb': 0.14452864511186003, 'eval_loss@spa.rst.rststb': 2.265486240386963, 'eval_runtime': 4.9545, 'eval_samples_per_second': 77.303, 'eval_steps_per_second': 2.422, 'epoch': 10.0}
{'train@spa.rst.rststb_loss': 2.0019450187683105, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.43214285714285716, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.11210613845328489, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.13708051816260666, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13540745223042056, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0019450187683105, 'train@spa.rst.rststb_runtime': 27.0382, 'train@spa.rst.rststb_samples_per_second': 82.846, 'train@spa.rst.rststb_steps_per_second': 2.589, 'epoch': 11.0}
{'loss': 2.0612, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.253314733505249, 'eval_accuracy@spa.rst.rststb': 0.39947780678851175, 'eval_f1@spa.rst.rststb': 0.1155275186416886, 'eval_precision@spa.rst.rststb': 0.10192016108353327, 'eval_recall@spa.rst.rststb': 0.14452864511186003, 'eval_loss@spa.rst.rststb': 2.253314971923828, 'eval_runtime': 4.9255, 'eval_samples_per_second': 77.759, 'eval_steps_per_second': 2.436, 'epoch': 11.0}
{'train@spa.rst.rststb_loss': 1.9968372583389282, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.43348214285714287, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.11300380193759618, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.13790785721748502, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13593636867611522, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.9968370199203491, 'train@spa.rst.rststb_runtime': 27.0759, 'train@spa.rst.rststb_samples_per_second': 82.73, 'train@spa.rst.rststb_steps_per_second': 2.585, 'epoch': 12.0}
{'loss': 2.0529, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.248852252960205, 'eval_accuracy@spa.rst.rststb': 0.39947780678851175, 'eval_f1@spa.rst.rststb': 0.11547305838206472, 'eval_precision@spa.rst.rststb': 0.10174764478895378, 'eval_recall@spa.rst.rststb': 0.14452864511186003, 'eval_loss@spa.rst.rststb': 2.248852491378784, 'eval_runtime': 4.9358, 'eval_samples_per_second': 77.596, 'eval_steps_per_second': 2.431, 'epoch': 12.0}
{'train_runtime': 1066.9652, 'train_samples_per_second': 25.193, 'train_steps_per_second': 0.787, 'train_loss': 2.3537522815522696, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.7758
  train_runtime            = 0:12:57.01
  train_samples_per_second =     24.833
  train_steps_per_second   =      0.788
-------------------------------------------------------------------
Lang1:  por.rst.cstn    Lang2:  spa.rst.rststb
Saving run to:  runs/full_shot/FullShot=v4_por.rst.cstn_spa.rst.rststb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 4148 examples
read 573 examples
read 272 examples
read 2240 examples
read 383 examples
read 426 examples
Total prediction labels:  41
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (pt): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (pt): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=41, bias=True)
    )
  )
)
{'train@por.rst.cstn_loss': 2.467775821685791, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.2777242044358727, 'train@por.rst.cstn_f1@por.rst.cstn': 0.013745797392308495, 'train@por.rst.cstn_precision@por.rst.cstn': 0.03992343863033518, 'train@por.rst.cstn_recall@por.rst.cstn': 0.031334005376344086, 'train@por.rst.cstn_loss@por.rst.cstn': 2.467775583267212, 'train@por.rst.cstn_runtime': 49.765, 'train@por.rst.cstn_samples_per_second': 83.352, 'train@por.rst.cstn_steps_per_second': 2.612, 'epoch': 1.0}
{'loss': 2.9947, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.5683772563934326, 'eval_accuracy@por.rst.cstn': 0.28097731239092494, 'eval_f1@por.rst.cstn': 0.019940549913301957, 'eval_precision@por.rst.cstn': 0.012771696017769315, 'eval_recall@por.rst.cstn': 0.045454545454545456, 'eval_loss@por.rst.cstn': 2.5683774948120117, 'eval_runtime': 7.2108, 'eval_samples_per_second': 79.464, 'eval_steps_per_second': 2.496, 'epoch': 1.0}
{'train@por.rst.cstn_loss': 2.2303617000579834, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.3700578592092575, 'train@por.rst.cstn_f1@por.rst.cstn': 0.052996282358053176, 'train@por.rst.cstn_precision@por.rst.cstn': 0.06841345956500255, 'train@por.rst.cstn_recall@por.rst.cstn': 0.06235101562254274, 'train@por.rst.cstn_loss@por.rst.cstn': 2.2303617000579834, 'train@por.rst.cstn_runtime': 49.894, 'train@por.rst.cstn_samples_per_second': 83.136, 'train@por.rst.cstn_steps_per_second': 2.606, 'epoch': 2.0}
{'loss': 2.3775, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.34985613822937, 'eval_accuracy@por.rst.cstn': 0.3298429319371728, 'eval_f1@por.rst.cstn': 0.07029119438472907, 'eval_precision@por.rst.cstn': 0.09788359788359789, 'eval_recall@por.rst.cstn': 0.0863469534418151, 'eval_loss@por.rst.cstn': 2.3498566150665283, 'eval_runtime': 7.226, 'eval_samples_per_second': 79.297, 'eval_steps_per_second': 2.491, 'epoch': 2.0}
{'train@por.rst.cstn_loss': 2.0447542667388916, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.4339440694310511, 'train@por.rst.cstn_f1@por.rst.cstn': 0.06840211063080456, 'train@por.rst.cstn_precision@por.rst.cstn': 0.08132489338474456, 'train@por.rst.cstn_recall@por.rst.cstn': 0.08180722220588915, 'train@por.rst.cstn_loss@por.rst.cstn': 2.0447545051574707, 'train@por.rst.cstn_runtime': 49.9189, 'train@por.rst.cstn_samples_per_second': 83.095, 'train@por.rst.cstn_steps_per_second': 2.604, 'epoch': 3.0}
{'loss': 2.1834, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.1826226711273193, 'eval_accuracy@por.rst.cstn': 0.3542757417102967, 'eval_f1@por.rst.cstn': 0.09085508858762145, 'eval_precision@por.rst.cstn': 0.08198905167230959, 'eval_recall@por.rst.cstn': 0.11528754512944238, 'eval_loss@por.rst.cstn': 2.1826226711273193, 'eval_runtime': 7.2, 'eval_samples_per_second': 79.584, 'eval_steps_per_second': 2.5, 'epoch': 3.0}
{'train@por.rst.cstn_loss': 1.8793830871582031, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5055448408871746, 'train@por.rst.cstn_f1@por.rst.cstn': 0.08795340270803345, 'train@por.rst.cstn_precision@por.rst.cstn': 0.12840372521895133, 'train@por.rst.cstn_recall@por.rst.cstn': 0.10006425838255648, 'train@por.rst.cstn_loss@por.rst.cstn': 1.8793830871582031, 'train@por.rst.cstn_runtime': 49.8619, 'train@por.rst.cstn_samples_per_second': 83.19, 'train@por.rst.cstn_steps_per_second': 2.607, 'epoch': 4.0}
{'loss': 2.0181, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.031766176223755, 'eval_accuracy@por.rst.cstn': 0.42757417102966844, 'eval_f1@por.rst.cstn': 0.11860140780472431, 'eval_precision@por.rst.cstn': 0.15978458454804387, 'eval_recall@por.rst.cstn': 0.14327439226284658, 'eval_loss@por.rst.cstn': 2.031766176223755, 'eval_runtime': 7.2772, 'eval_samples_per_second': 78.74, 'eval_steps_per_second': 2.473, 'epoch': 4.0}
{'train@por.rst.cstn_loss': 1.7569289207458496, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5294117647058824, 'train@por.rst.cstn_f1@por.rst.cstn': 0.11456754977484755, 'train@por.rst.cstn_precision@por.rst.cstn': 0.1525038566285045, 'train@por.rst.cstn_recall@por.rst.cstn': 0.12241873546549586, 'train@por.rst.cstn_loss@por.rst.cstn': 1.7569289207458496, 'train@por.rst.cstn_runtime': 49.9644, 'train@por.rst.cstn_samples_per_second': 83.019, 'train@por.rst.cstn_steps_per_second': 2.602, 'epoch': 5.0}
{'loss': 1.8786, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.904919981956482, 'eval_accuracy@por.rst.cstn': 0.43804537521815007, 'eval_f1@por.rst.cstn': 0.14217298424529687, 'eval_precision@por.rst.cstn': 0.20036656691888252, 'eval_recall@por.rst.cstn': 0.1624222101546836, 'eval_loss@por.rst.cstn': 1.904919981956482, 'eval_runtime': 7.2248, 'eval_samples_per_second': 79.31, 'eval_steps_per_second': 2.491, 'epoch': 5.0}
{'train@por.rst.cstn_loss': 1.6797809600830078, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5607521697203471, 'train@por.rst.cstn_f1@por.rst.cstn': 0.12465128256032673, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13729235952673535, 'train@por.rst.cstn_recall@por.rst.cstn': 0.13429139558520975, 'train@por.rst.cstn_loss@por.rst.cstn': 1.6797810792922974, 'train@por.rst.cstn_runtime': 49.8512, 'train@por.rst.cstn_samples_per_second': 83.208, 'train@por.rst.cstn_steps_per_second': 2.608, 'epoch': 6.0}
{'loss': 1.7818, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.8260563611984253, 'eval_accuracy@por.rst.cstn': 0.4694589877835951, 'eval_f1@por.rst.cstn': 0.15905483806933673, 'eval_precision@por.rst.cstn': 0.18619454836463634, 'eval_recall@por.rst.cstn': 0.1782282954661777, 'eval_loss@por.rst.cstn': 1.8260560035705566, 'eval_runtime': 7.2268, 'eval_samples_per_second': 79.289, 'eval_steps_per_second': 2.491, 'epoch': 6.0}
{'train@por.rst.cstn_loss': 1.6283061504364014, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5718418514946962, 'train@por.rst.cstn_f1@por.rst.cstn': 0.13002425906626602, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13474320311129734, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14280017690397512, 'train@por.rst.cstn_loss@por.rst.cstn': 1.6283059120178223, 'train@por.rst.cstn_runtime': 49.8437, 'train@por.rst.cstn_samples_per_second': 83.22, 'train@por.rst.cstn_steps_per_second': 2.608, 'epoch': 7.0}
{'loss': 1.7116, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.7744572162628174, 'eval_accuracy@por.rst.cstn': 0.4869109947643979, 'eval_f1@por.rst.cstn': 0.16948910233332784, 'eval_precision@por.rst.cstn': 0.17438682640888523, 'eval_recall@por.rst.cstn': 0.18594785037212747, 'eval_loss@por.rst.cstn': 1.7744572162628174, 'eval_runtime': 7.2173, 'eval_samples_per_second': 79.392, 'eval_steps_per_second': 2.494, 'epoch': 7.0}
{'train@por.rst.cstn_loss': 1.596572756767273, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5769045323047252, 'train@por.rst.cstn_f1@por.rst.cstn': 0.13205960090785412, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13466709125112705, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1442049140464046, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5965726375579834, 'train@por.rst.cstn_runtime': 49.9082, 'train@por.rst.cstn_samples_per_second': 83.113, 'train@por.rst.cstn_steps_per_second': 2.605, 'epoch': 8.0}
{'loss': 1.6776, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.7489266395568848, 'eval_accuracy@por.rst.cstn': 0.4956369982547993, 'eval_f1@por.rst.cstn': 0.1771018116407327, 'eval_precision@por.rst.cstn': 0.18253423774495392, 'eval_recall@por.rst.cstn': 0.19289753308892063, 'eval_loss@por.rst.cstn': 1.7489265203475952, 'eval_runtime': 7.2219, 'eval_samples_per_second': 79.342, 'eval_steps_per_second': 2.492, 'epoch': 8.0}
{'train@por.rst.cstn_loss': 1.5746744871139526, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5805207328833173, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1340103906712023, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13551676405557975, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14586486682764277, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5746747255325317, 'train@por.rst.cstn_runtime': 49.9552, 'train@por.rst.cstn_samples_per_second': 83.034, 'train@por.rst.cstn_steps_per_second': 2.602, 'epoch': 9.0}
{'loss': 1.6471, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.7285690307617188, 'eval_accuracy@por.rst.cstn': 0.5008726003490401, 'eval_f1@por.rst.cstn': 0.1809095896268435, 'eval_precision@por.rst.cstn': 0.1832134795538756, 'eval_recall@por.rst.cstn': 0.1953893597330252, 'eval_loss@por.rst.cstn': 1.7285689115524292, 'eval_runtime': 7.237, 'eval_samples_per_second': 79.176, 'eval_steps_per_second': 2.487, 'epoch': 9.0}
{'train@por.rst.cstn_loss': 1.56002676486969, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5826904532304725, 'train@por.rst.cstn_f1@por.rst.cstn': 0.13596863936422496, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13383830520447176, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14871206046538793, 'train@por.rst.cstn_loss@por.rst.cstn': 1.56002676486969, 'train@por.rst.cstn_runtime': 50.0269, 'train@por.rst.cstn_samples_per_second': 82.915, 'train@por.rst.cstn_steps_per_second': 2.599, 'epoch': 10.0}
{'loss': 1.6203, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.7138972282409668, 'eval_accuracy@por.rst.cstn': 0.5113438045375218, 'eval_f1@por.rst.cstn': 0.1890621772125499, 'eval_precision@por.rst.cstn': 0.18552135554566673, 'eval_recall@por.rst.cstn': 0.20520302658070497, 'eval_loss@por.rst.cstn': 1.7138972282409668, 'eval_runtime': 7.2157, 'eval_samples_per_second': 79.41, 'eval_steps_per_second': 2.495, 'epoch': 10.0}
{'train@por.rst.cstn_loss': 1.5505722761154175, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.585824493731919, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1385991826968628, 'train@por.rst.cstn_precision@por.rst.cstn': 0.16727686945161246, 'train@por.rst.cstn_recall@por.rst.cstn': 0.15041286286445638, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5505722761154175, 'train@por.rst.cstn_runtime': 49.9698, 'train@por.rst.cstn_samples_per_second': 83.01, 'train@por.rst.cstn_steps_per_second': 2.602, 'epoch': 11.0}
{'loss': 1.6106, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.7061406373977661, 'eval_accuracy@por.rst.cstn': 0.5078534031413613, 'eval_f1@por.rst.cstn': 0.18919962763622086, 'eval_precision@por.rst.cstn': 0.1852548946569577, 'eval_recall@por.rst.cstn': 0.20589112768746637, 'eval_loss@por.rst.cstn': 1.706140398979187, 'eval_runtime': 7.2669, 'eval_samples_per_second': 78.851, 'eval_steps_per_second': 2.477, 'epoch': 11.0}
{'train@por.rst.cstn_loss': 1.5476347208023071, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5867888138862102, 'train@por.rst.cstn_f1@por.rst.cstn': 0.13915188004276358, 'train@por.rst.cstn_precision@por.rst.cstn': 0.16716349792555388, 'train@por.rst.cstn_recall@por.rst.cstn': 0.15081485032905795, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5476347208023071, 'train@por.rst.cstn_runtime': 49.9502, 'train@por.rst.cstn_samples_per_second': 83.043, 'train@por.rst.cstn_steps_per_second': 2.603, 'epoch': 12.0}
{'loss': 1.6057, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.702693223953247, 'eval_accuracy@por.rst.cstn': 0.5043630017452007, 'eval_f1@por.rst.cstn': 0.18684436883516445, 'eval_precision@por.rst.cstn': 0.1845571891625677, 'eval_recall@por.rst.cstn': 0.20198846469389425, 'eval_loss@por.rst.cstn': 1.702693223953247, 'eval_runtime': 7.2069, 'eval_samples_per_second': 79.507, 'eval_steps_per_second': 2.498, 'epoch': 12.0}
{'train_runtime': 1954.3403, 'train_samples_per_second': 25.469, 'train_steps_per_second': 0.798, 'train_loss': 1.9255822303967598, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.9256
  train_runtime            = 0:32:34.34
  train_samples_per_second =     25.469
  train_steps_per_second   =      0.798
{'train@spa.rst.rststb_loss': 2.715087890625, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.24107142857142858, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.02498849805351675, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.019146466097535977, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.04442694857540018, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.715088129043579, 'train@spa.rst.rststb_runtime': 27.1599, 'train@spa.rst.rststb_samples_per_second': 82.474, 'train@spa.rst.rststb_steps_per_second': 2.577, 'epoch': 1.0}
{'loss': 3.0941, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.769369602203369, 'eval_accuracy@spa.rst.rststb': 0.2558746736292428, 'eval_f1@spa.rst.rststb': 0.032303656071601565, 'eval_precision@spa.rst.rststb': 0.02366310641205452, 'eval_recall@spa.rst.rststb': 0.05708934688144309, 'eval_loss@spa.rst.rststb': 2.7693700790405273, 'eval_runtime': 4.9616, 'eval_samples_per_second': 77.193, 'eval_steps_per_second': 2.419, 'epoch': 1.0}
{'train@spa.rst.rststb_loss': 2.431352376937866, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.328125, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.06093365785990313, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.09363483389514575, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.07324598458721805, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.431352138519287, 'train@spa.rst.rststb_runtime': 27.1206, 'train@spa.rst.rststb_samples_per_second': 82.594, 'train@spa.rst.rststb_steps_per_second': 2.581, 'epoch': 2.0}
{'loss': 2.6067, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.556060552597046, 'eval_accuracy@spa.rst.rststb': 0.3263707571801567, 'eval_f1@spa.rst.rststb': 0.07017401320926203, 'eval_precision@spa.rst.rststb': 0.08537816828054474, 'eval_recall@spa.rst.rststb': 0.08588483633764932, 'eval_loss@spa.rst.rststb': 2.556060552597046, 'eval_runtime': 4.9662, 'eval_samples_per_second': 77.121, 'eval_steps_per_second': 2.416, 'epoch': 2.0}
{'train@spa.rst.rststb_loss': 2.266486406326294, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.40089285714285716, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.09559479629601729, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.08608916298633416, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.11554463784607705, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.266486406326294, 'train@spa.rst.rststb_runtime': 27.8468, 'train@spa.rst.rststb_samples_per_second': 80.44, 'train@spa.rst.rststb_steps_per_second': 2.514, 'epoch': 3.0}
{'loss': 2.4076, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.4349725246429443, 'eval_accuracy@spa.rst.rststb': 0.39425587467362927, 'eval_f1@spa.rst.rststb': 0.11189834009127081, 'eval_precision@spa.rst.rststb': 0.10414359360304519, 'eval_recall@spa.rst.rststb': 0.13217205255911013, 'eval_loss@spa.rst.rststb': 2.4349727630615234, 'eval_runtime': 4.9585, 'eval_samples_per_second': 77.242, 'eval_steps_per_second': 2.42, 'epoch': 3.0}
{'train@spa.rst.rststb_loss': 2.1465535163879395, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.41830357142857144, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10125521594145337, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.0877360253259158, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.126891471547961, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.1465535163879395, 'train@spa.rst.rststb_runtime': 27.1021, 'train@spa.rst.rststb_samples_per_second': 82.65, 'train@spa.rst.rststb_steps_per_second': 2.583, 'epoch': 4.0}
{'loss': 2.2707, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.342916965484619, 'eval_accuracy@spa.rst.rststb': 0.4046997389033943, 'eval_f1@spa.rst.rststb': 0.11366614500813584, 'eval_precision@spa.rst.rststb': 0.09919199370668276, 'eval_recall@spa.rst.rststb': 0.14079911537382028, 'eval_loss@spa.rst.rststb': 2.34291672706604, 'eval_runtime': 4.958, 'eval_samples_per_second': 77.25, 'eval_steps_per_second': 2.42, 'epoch': 4.0}
{'train@spa.rst.rststb_loss': 2.0560302734375, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.42544642857142856, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10432757235979005, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.13942630709161616, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.1304685966316473, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.056030511856079, 'train@spa.rst.rststb_runtime': 27.1136, 'train@spa.rst.rststb_samples_per_second': 82.615, 'train@spa.rst.rststb_steps_per_second': 2.582, 'epoch': 5.0}
{'loss': 2.1653, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.2805144786834717, 'eval_accuracy@spa.rst.rststb': 0.4073107049608355, 'eval_f1@spa.rst.rststb': 0.10991542880929368, 'eval_precision@spa.rst.rststb': 0.09354772364175284, 'eval_recall@spa.rst.rststb': 0.1416886861321266, 'eval_loss@spa.rst.rststb': 2.2805144786834717, 'eval_runtime': 4.9903, 'eval_samples_per_second': 76.749, 'eval_steps_per_second': 2.405, 'epoch': 5.0}
{'train@spa.rst.rststb_loss': 1.9875717163085938, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.43660714285714286, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10949888776476911, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.18090606921941527, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13590743804161404, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.9875719547271729, 'train@spa.rst.rststb_runtime': 27.1533, 'train@spa.rst.rststb_samples_per_second': 82.495, 'train@spa.rst.rststb_steps_per_second': 2.578, 'epoch': 6.0}
{'loss': 2.0975, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.2330563068389893, 'eval_accuracy@spa.rst.rststb': 0.4151436031331593, 'eval_f1@spa.rst.rststb': 0.11132298008368802, 'eval_precision@spa.rst.rststb': 0.09302240262808822, 'eval_recall@spa.rst.rststb': 0.1462202030466925, 'eval_loss@spa.rst.rststb': 2.2330563068389893, 'eval_runtime': 4.9517, 'eval_samples_per_second': 77.347, 'eval_steps_per_second': 2.423, 'epoch': 6.0}
{'train@spa.rst.rststb_loss': 1.9353079795837402, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4450892857142857, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.11298874774869705, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.17231897272455826, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.1380804248461299, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.9353082180023193, 'train@spa.rst.rststb_runtime': 27.1754, 'train@spa.rst.rststb_samples_per_second': 82.428, 'train@spa.rst.rststb_steps_per_second': 2.576, 'epoch': 7.0}
{'loss': 2.0356, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.2017085552215576, 'eval_accuracy@spa.rst.rststb': 0.412532637075718, 'eval_f1@spa.rst.rststb': 0.1116510715670757, 'eval_precision@spa.rst.rststb': 0.09547542101245954, 'eval_recall@spa.rst.rststb': 0.145374130711615, 'eval_loss@spa.rst.rststb': 2.2017085552215576, 'eval_runtime': 4.9952, 'eval_samples_per_second': 76.673, 'eval_steps_per_second': 2.402, 'epoch': 7.0}
{'train@spa.rst.rststb_loss': 1.896081566810608, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.44821428571428573, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.11857954790444385, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.1651266439690928, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.14104442588083457, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.896081566810608, 'train@spa.rst.rststb_runtime': 27.144, 'train@spa.rst.rststb_samples_per_second': 82.523, 'train@spa.rst.rststb_steps_per_second': 2.579, 'epoch': 8.0}
{'loss': 1.9918, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.175642251968384, 'eval_accuracy@spa.rst.rststb': 0.40992167101827676, 'eval_f1@spa.rst.rststb': 0.11244081666230457, 'eval_precision@spa.rst.rststb': 0.0979305335136228, 'eval_recall@spa.rst.rststb': 0.14472520144490506, 'eval_loss@spa.rst.rststb': 2.1756420135498047, 'eval_runtime': 4.9689, 'eval_samples_per_second': 77.079, 'eval_steps_per_second': 2.415, 'epoch': 8.0}
{'train@spa.rst.rststb_loss': 1.8676636219024658, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4602678571428571, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.1274294812221751, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.18711651791890963, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.14929052253759184, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.8676636219024658, 'train@spa.rst.rststb_runtime': 27.1514, 'train@spa.rst.rststb_samples_per_second': 82.5, 'train@spa.rst.rststb_steps_per_second': 2.578, 'epoch': 9.0}
{'loss': 1.9591, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.1557419300079346, 'eval_accuracy@spa.rst.rststb': 0.412532637075718, 'eval_f1@spa.rst.rststb': 0.10636290874819303, 'eval_precision@spa.rst.rststb': 0.08925935421687176, 'eval_recall@spa.rst.rststb': 0.14102216702939355, 'eval_loss@spa.rst.rststb': 2.1557421684265137, 'eval_runtime': 4.972, 'eval_samples_per_second': 77.031, 'eval_steps_per_second': 2.413, 'epoch': 9.0}
{'train@spa.rst.rststb_loss': 1.8458080291748047, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4660714285714286, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.13251960453868974, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.19197442491872768, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.15210910390831872, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.8458081483840942, 'train@spa.rst.rststb_runtime': 27.1705, 'train@spa.rst.rststb_samples_per_second': 82.442, 'train@spa.rst.rststb_steps_per_second': 2.576, 'epoch': 10.0}
{'loss': 1.9407, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.1372480392456055, 'eval_accuracy@spa.rst.rststb': 0.4255874673629243, 'eval_f1@spa.rst.rststb': 0.11010041084407167, 'eval_precision@spa.rst.rststb': 0.09308749539012696, 'eval_recall@spa.rst.rststb': 0.1430348003121005, 'eval_loss@spa.rst.rststb': 2.1372478008270264, 'eval_runtime': 4.9574, 'eval_samples_per_second': 77.258, 'eval_steps_per_second': 2.421, 'epoch': 10.0}
{'train@spa.rst.rststb_loss': 1.8345507383346558, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4669642857142857, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.13443401714245767, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.20309015630860042, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.15340189579498548, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.8345508575439453, 'train@spa.rst.rststb_runtime': 27.1515, 'train@spa.rst.rststb_samples_per_second': 82.5, 'train@spa.rst.rststb_steps_per_second': 2.578, 'epoch': 11.0}
{'loss': 1.9112, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.130509614944458, 'eval_accuracy@spa.rst.rststb': 0.4255874673629243, 'eval_f1@spa.rst.rststb': 0.11323567412928731, 'eval_precision@spa.rst.rststb': 0.13419086676492226, 'eval_recall@spa.rst.rststb': 0.1445495831174414, 'eval_loss@spa.rst.rststb': 2.130509853363037, 'eval_runtime': 4.9697, 'eval_samples_per_second': 77.067, 'eval_steps_per_second': 2.415, 'epoch': 11.0}
{'train@spa.rst.rststb_loss': 1.8306185007095337, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4674107142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.13557351196207415, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.20152079559070396, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.1540562098603717, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.8306187391281128, 'train@spa.rst.rststb_runtime': 27.1365, 'train@spa.rst.rststb_samples_per_second': 82.546, 'train@spa.rst.rststb_steps_per_second': 2.58, 'epoch': 12.0}
{'loss': 1.908, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.1274802684783936, 'eval_accuracy@spa.rst.rststb': 0.4255874673629243, 'eval_f1@spa.rst.rststb': 0.1135502402056258, 'eval_precision@spa.rst.rststb': 0.13481110809162702, 'eval_recall@spa.rst.rststb': 0.1445495831174414, 'eval_loss@spa.rst.rststb': 2.1274800300598145, 'eval_runtime': 4.9562, 'eval_samples_per_second': 77.276, 'eval_steps_per_second': 2.421, 'epoch': 12.0}
{'train_runtime': 1070.3523, 'train_samples_per_second': 25.113, 'train_steps_per_second': 0.785, 'train_loss': 2.1990274883451915, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.9256
  train_runtime            = 0:32:34.34
  train_samples_per_second =     25.469
  train_steps_per_second   =      0.798
-------------------------------------------------------------------
Lang1:  rus.rst.rrt    Lang2:  spa.rst.rststb
Saving run to:  runs/full_shot/FullShot=v4_rus.rst.rrt_spa.rst.rststb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 28822 examples
read 2855 examples
read 2843 examples
read 2240 examples
read 383 examples
read 426 examples
Total prediction labels:  35
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (ru): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=35, bias=True)
    )
  )
)
{'train@rus.rst.rrt_loss': 1.7303928136825562, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.48889737006453404, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.17602021560294007, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.2160599240618929, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.19282321148301554, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.7303928136825562, 'train@rus.rst.rrt_runtime': 344.7192, 'train@rus.rst.rrt_samples_per_second': 83.61, 'train@rus.rst.rrt_steps_per_second': 2.614, 'epoch': 1.0}
{'loss': 2.1777, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.7744061946868896, 'eval_accuracy@rus.rst.rrt': 0.4714535901926445, 'eval_f1@rus.rst.rrt': 0.193515751572955, 'eval_precision@rus.rst.rrt': 0.20200626836248134, 'eval_recall@rus.rst.rrt': 0.21184298608343904, 'eval_loss@rus.rst.rrt': 1.7744063138961792, 'eval_runtime': 34.5129, 'eval_samples_per_second': 82.723, 'eval_steps_per_second': 2.608, 'epoch': 1.0}
{'train@rus.rst.rrt_loss': 1.5051885843276978, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5403858163902574, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.22314156304212587, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.27159235212088073, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.2294715380365126, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.5051885843276978, 'train@rus.rst.rrt_runtime': 345.26, 'train@rus.rst.rrt_samples_per_second': 83.479, 'train@rus.rst.rrt_steps_per_second': 2.61, 'epoch': 2.0}
{'loss': 1.6447, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.5761759281158447, 'eval_accuracy@rus.rst.rrt': 0.5159369527145359, 'eval_f1@rus.rst.rrt': 0.2411589263716283, 'eval_precision@rus.rst.rrt': 0.29365076268541607, 'eval_recall@rus.rst.rrt': 0.2502280733956885, 'eval_loss@rus.rst.rrt': 1.5761758089065552, 'eval_runtime': 34.535, 'eval_samples_per_second': 82.67, 'eval_steps_per_second': 2.606, 'epoch': 2.0}
{'train@rus.rst.rrt_loss': 1.4235032796859741, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5658524738047325, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.28090219137184363, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4545856433014888, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.2755401360758656, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.4235032796859741, 'train@rus.rst.rrt_runtime': 345.7448, 'train@rus.rst.rrt_samples_per_second': 83.362, 'train@rus.rst.rrt_steps_per_second': 2.606, 'epoch': 3.0}
{'loss': 1.5106, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.4969803094863892, 'eval_accuracy@rus.rst.rrt': 0.5422066549912434, 'eval_f1@rus.rst.rrt': 0.31228311113737933, 'eval_precision@rus.rst.rrt': 0.43620693110045433, 'eval_recall@rus.rst.rrt': 0.30851871501806205, 'eval_loss@rus.rst.rrt': 1.4969801902770996, 'eval_runtime': 34.6696, 'eval_samples_per_second': 82.349, 'eval_steps_per_second': 2.596, 'epoch': 3.0}
{'train@rus.rst.rrt_loss': 1.3694928884506226, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5800777184095482, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.31212307538235273, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4620674293868218, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.2951173303410352, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.369492769241333, 'train@rus.rst.rrt_runtime': 344.7703, 'train@rus.rst.rrt_samples_per_second': 83.598, 'train@rus.rst.rrt_steps_per_second': 2.613, 'epoch': 4.0}
{'loss': 1.4477, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.4454938173294067, 'eval_accuracy@rus.rst.rrt': 0.5597197898423818, 'eval_f1@rus.rst.rrt': 0.3622884330641231, 'eval_precision@rus.rst.rrt': 0.5010889681132333, 'eval_recall@rus.rst.rrt': 0.34368126413674444, 'eval_loss@rus.rst.rrt': 1.4454938173294067, 'eval_runtime': 34.5038, 'eval_samples_per_second': 82.744, 'eval_steps_per_second': 2.608, 'epoch': 4.0}
{'train@rus.rst.rrt_loss': 1.335615873336792, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5887169523280827, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.32852832861679127, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4459281851903324, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3104352172575122, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3356159925460815, 'train@rus.rst.rrt_runtime': 345.5335, 'train@rus.rst.rrt_samples_per_second': 83.413, 'train@rus.rst.rrt_steps_per_second': 2.608, 'epoch': 5.0}
{'loss': 1.4065, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.4198808670043945, 'eval_accuracy@rus.rst.rrt': 0.5646234676007005, 'eval_f1@rus.rst.rrt': 0.37710855868468957, 'eval_precision@rus.rst.rrt': 0.47887723128724985, 'eval_recall@rus.rst.rrt': 0.3605465911621878, 'eval_loss@rus.rst.rrt': 1.4198808670043945, 'eval_runtime': 34.505, 'eval_samples_per_second': 82.742, 'eval_steps_per_second': 2.608, 'epoch': 5.0}
{'train@rus.rst.rrt_loss': 1.3134760856628418, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5949968773853307, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.34127476941937823, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.45334544685619577, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3220588202668706, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3134762048721313, 'train@rus.rst.rrt_runtime': 346.1257, 'train@rus.rst.rrt_samples_per_second': 83.27, 'train@rus.rst.rrt_steps_per_second': 2.603, 'epoch': 6.0}
{'loss': 1.3804, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.4015268087387085, 'eval_accuracy@rus.rst.rrt': 0.5705779334500876, 'eval_f1@rus.rst.rrt': 0.3856091881707455, 'eval_precision@rus.rst.rrt': 0.46944877771418375, 'eval_recall@rus.rst.rrt': 0.3718028242922662, 'eval_loss@rus.rst.rrt': 1.401526927947998, 'eval_runtime': 34.6794, 'eval_samples_per_second': 82.325, 'eval_steps_per_second': 2.595, 'epoch': 6.0}
{'train@rus.rst.rrt_loss': 1.2966166734695435, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5997154951079037, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3522785381041706, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.43778895545225255, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3331448397900814, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2966166734695435, 'train@rus.rst.rrt_runtime': 345.4843, 'train@rus.rst.rrt_samples_per_second': 83.425, 'train@rus.rst.rrt_steps_per_second': 2.608, 'epoch': 7.0}
{'loss': 1.358, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.3913742303848267, 'eval_accuracy@rus.rst.rrt': 0.5691768826619965, 'eval_f1@rus.rst.rrt': 0.38958931193125396, 'eval_precision@rus.rst.rrt': 0.4656393876009602, 'eval_recall@rus.rst.rrt': 0.3759296746685666, 'eval_loss@rus.rst.rrt': 1.3913742303848267, 'eval_runtime': 34.6659, 'eval_samples_per_second': 82.358, 'eval_steps_per_second': 2.596, 'epoch': 7.0}
{'train@rus.rst.rrt_loss': 1.2841178178787231, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6032197626812851, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3620502078961723, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4337424496717555, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3451522953781995, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2841176986694336, 'train@rus.rst.rrt_runtime': 345.6144, 'train@rus.rst.rrt_samples_per_second': 83.394, 'train@rus.rst.rrt_steps_per_second': 2.607, 'epoch': 8.0}
{'loss': 1.3408, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.3795868158340454, 'eval_accuracy@rus.rst.rrt': 0.5726795096322241, 'eval_f1@rus.rst.rrt': 0.4005290523774436, 'eval_precision@rus.rst.rrt': 0.454251510380121, 'eval_recall@rus.rst.rrt': 0.3898117199988093, 'eval_loss@rus.rst.rrt': 1.3795868158340454, 'eval_runtime': 34.578, 'eval_samples_per_second': 82.567, 'eval_steps_per_second': 2.603, 'epoch': 8.0}
{'train@rus.rst.rrt_loss': 1.2715524435043335, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6049545486087017, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36410954107798754, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.44013040752241483, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3452816235548278, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2715520858764648, 'train@rus.rst.rrt_runtime': 345.9957, 'train@rus.rst.rrt_samples_per_second': 83.302, 'train@rus.rst.rrt_steps_per_second': 2.604, 'epoch': 9.0}
{'loss': 1.333, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.373144507408142, 'eval_accuracy@rus.rst.rrt': 0.5747810858143608, 'eval_f1@rus.rst.rrt': 0.4053540323246869, 'eval_precision@rus.rst.rrt': 0.4739332002926596, 'eval_recall@rus.rst.rrt': 0.3908577515449808, 'eval_loss@rus.rst.rrt': 1.3731443881988525, 'eval_runtime': 34.5956, 'eval_samples_per_second': 82.525, 'eval_steps_per_second': 2.601, 'epoch': 9.0}
{'train@rus.rst.rrt_loss': 1.2650855779647827, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6076955103740198, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3653180450129822, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.44356329308078535, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3443220738178049, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2650856971740723, 'train@rus.rst.rrt_runtime': 345.585, 'train@rus.rst.rrt_samples_per_second': 83.401, 'train@rus.rst.rrt_steps_per_second': 2.607, 'epoch': 10.0}
{'loss': 1.3214, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.3706562519073486, 'eval_accuracy@rus.rst.rrt': 0.5768826619964974, 'eval_f1@rus.rst.rrt': 0.4068533763590658, 'eval_precision@rus.rst.rrt': 0.469176150991526, 'eval_recall@rus.rst.rrt': 0.3902045008445338, 'eval_loss@rus.rst.rrt': 1.3706563711166382, 'eval_runtime': 34.5551, 'eval_samples_per_second': 82.622, 'eval_steps_per_second': 2.605, 'epoch': 10.0}
{'train@rus.rst.rrt_loss': 1.2615288496017456, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6087016862119214, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36816412891511907, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4518685447701696, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34916999314122016, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.261528730392456, 'train@rus.rst.rrt_runtime': 344.9028, 'train@rus.rst.rrt_samples_per_second': 83.566, 'train@rus.rst.rrt_steps_per_second': 2.612, 'epoch': 11.0}
{'loss': 1.3147, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.3704005479812622, 'eval_accuracy@rus.rst.rrt': 0.5775831873905429, 'eval_f1@rus.rst.rrt': 0.41136462041549393, 'eval_precision@rus.rst.rrt': 0.47434925510411735, 'eval_recall@rus.rst.rrt': 0.3974527189010754, 'eval_loss@rus.rst.rrt': 1.3704005479812622, 'eval_runtime': 34.519, 'eval_samples_per_second': 82.708, 'eval_steps_per_second': 2.607, 'epoch': 11.0}
{'train@rus.rst.rrt_loss': 1.2601937055587769, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6089792519603081, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36789955702032917, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4545658143958431, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34951937328385535, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.260193943977356, 'train@rus.rst.rrt_runtime': 345.4678, 'train@rus.rst.rrt_samples_per_second': 83.429, 'train@rus.rst.rrt_steps_per_second': 2.608, 'epoch': 12.0}
{'loss': 1.3115, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.3685684204101562, 'eval_accuracy@rus.rst.rrt': 0.5782837127845885, 'eval_f1@rus.rst.rrt': 0.4118231121345148, 'eval_precision@rus.rst.rrt': 0.4770402040703004, 'eval_recall@rus.rst.rrt': 0.39917620168440243, 'eval_loss@rus.rst.rrt': 1.3685684204101562, 'eval_runtime': 34.6024, 'eval_samples_per_second': 82.509, 'eval_steps_per_second': 2.601, 'epoch': 12.0}
{'train_runtime': 13340.6765, 'train_samples_per_second': 25.926, 'train_steps_per_second': 0.81, 'train_loss': 1.4622516187526013, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.4623
  train_runtime            = 3:42:20.67
  train_samples_per_second =     25.926
  train_steps_per_second   =       0.81
{'train@spa.rst.rststb_loss': 2.7637012004852295, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.2544642857142857, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.05074640706421718, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.0772855361175717, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.05817960694797808, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.7637012004852295, 'train@spa.rst.rststb_runtime': 27.1303, 'train@spa.rst.rststb_samples_per_second': 82.564, 'train@spa.rst.rststb_steps_per_second': 2.58, 'epoch': 1.0}
{'loss': 3.7392, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.9375548362731934, 'eval_accuracy@spa.rst.rststb': 0.25326370757180156, 'eval_f1@spa.rst.rststb': 0.05436392816874295, 'eval_precision@spa.rst.rststb': 0.08895577395577396, 'eval_recall@spa.rst.rststb': 0.06479533802131858, 'eval_loss@spa.rst.rststb': 2.9375548362731934, 'eval_runtime': 4.9025, 'eval_samples_per_second': 78.123, 'eval_steps_per_second': 2.448, 'epoch': 1.0}
{'train@spa.rst.rststb_loss': 2.3353099822998047, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.3575892857142857, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.08385330580722339, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.135643458359594, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.0915072480103353, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.335310220718384, 'train@spa.rst.rststb_runtime': 27.0491, 'train@spa.rst.rststb_samples_per_second': 82.812, 'train@spa.rst.rststb_steps_per_second': 2.588, 'epoch': 2.0}
{'loss': 2.5668, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.523904800415039, 'eval_accuracy@spa.rst.rststb': 0.3263707571801567, 'eval_f1@spa.rst.rststb': 0.07643282587004413, 'eval_precision@spa.rst.rststb': 0.08863862886085108, 'eval_recall@spa.rst.rststb': 0.08822087937297766, 'eval_loss@spa.rst.rststb': 2.523905038833618, 'eval_runtime': 4.9209, 'eval_samples_per_second': 77.831, 'eval_steps_per_second': 2.439, 'epoch': 2.0}
{'train@spa.rst.rststb_loss': 2.107332944869995, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.42142857142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.1328061067755517, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.19790495638474428, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.1341794635235214, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.107333183288574, 'train@spa.rst.rststb_runtime': 27.0665, 'train@spa.rst.rststb_samples_per_second': 82.759, 'train@spa.rst.rststb_steps_per_second': 2.586, 'epoch': 3.0}
{'loss': 2.2835, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.323800802230835, 'eval_accuracy@spa.rst.rststb': 0.391644908616188, 'eval_f1@spa.rst.rststb': 0.12158512900942077, 'eval_precision@spa.rst.rststb': 0.1367448883126705, 'eval_recall@spa.rst.rststb': 0.1282105218943479, 'eval_loss@spa.rst.rststb': 2.323801040649414, 'eval_runtime': 4.904, 'eval_samples_per_second': 78.1, 'eval_steps_per_second': 2.447, 'epoch': 3.0}
{'train@spa.rst.rststb_loss': 1.9464579820632935, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4575892857142857, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.16104053745674793, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.20499119889203973, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.16034637256403578, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.9464582204818726, 'train@spa.rst.rststb_runtime': 27.0396, 'train@spa.rst.rststb_samples_per_second': 82.841, 'train@spa.rst.rststb_steps_per_second': 2.589, 'epoch': 4.0}
{'loss': 2.096, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.188551664352417, 'eval_accuracy@spa.rst.rststb': 0.43342036553524804, 'eval_f1@spa.rst.rststb': 0.15883609128739296, 'eval_precision@spa.rst.rststb': 0.1802075087651878, 'eval_recall@spa.rst.rststb': 0.15941971683813075, 'eval_loss@spa.rst.rststb': 2.188551187515259, 'eval_runtime': 4.901, 'eval_samples_per_second': 78.147, 'eval_steps_per_second': 2.448, 'epoch': 4.0}
{'train@spa.rst.rststb_loss': 1.8337944746017456, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4830357142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.17425422030846888, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.2169088680541296, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.1772491710244357, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.8337947130203247, 'train@spa.rst.rststb_runtime': 27.0478, 'train@spa.rst.rststb_samples_per_second': 82.816, 'train@spa.rst.rststb_steps_per_second': 2.588, 'epoch': 5.0}
{'loss': 1.9686, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.1003706455230713, 'eval_accuracy@spa.rst.rststb': 0.4516971279373368, 'eval_f1@spa.rst.rststb': 0.17833777718494015, 'eval_precision@spa.rst.rststb': 0.20570784600389866, 'eval_recall@spa.rst.rststb': 0.17763248041221427, 'eval_loss@spa.rst.rststb': 2.100370407104492, 'eval_runtime': 4.8985, 'eval_samples_per_second': 78.188, 'eval_steps_per_second': 2.45, 'epoch': 5.0}
{'train@spa.rst.rststb_loss': 1.7547038793563843, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.5008928571428571, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.1913940729119093, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.23878671923655706, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.19343872230117648, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.7547041177749634, 'train@spa.rst.rststb_runtime': 27.0535, 'train@spa.rst.rststb_samples_per_second': 82.799, 'train@spa.rst.rststb_steps_per_second': 2.587, 'epoch': 6.0}
{'loss': 1.8691, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.0321662425994873, 'eval_accuracy@spa.rst.rststb': 0.4699738903394256, 'eval_f1@spa.rst.rststb': 0.19789199441277128, 'eval_precision@spa.rst.rststb': 0.2093433777453291, 'eval_recall@spa.rst.rststb': 0.20017889425690583, 'eval_loss@spa.rst.rststb': 2.0321662425994873, 'eval_runtime': 4.8987, 'eval_samples_per_second': 78.184, 'eval_steps_per_second': 2.45, 'epoch': 6.0}
{'train@spa.rst.rststb_loss': 1.6935324668884277, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.5142857142857142, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.20218649173870265, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.2523930760493029, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.2053123398877664, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.6935323476791382, 'train@spa.rst.rststb_runtime': 27.0592, 'train@spa.rst.rststb_samples_per_second': 82.782, 'train@spa.rst.rststb_steps_per_second': 2.587, 'epoch': 7.0}
{'loss': 1.7967, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.9866409301757812, 'eval_accuracy@spa.rst.rststb': 0.4830287206266319, 'eval_f1@spa.rst.rststb': 0.2059512636849532, 'eval_precision@spa.rst.rststb': 0.21497784576672915, 'eval_recall@spa.rst.rststb': 0.21073493249334238, 'eval_loss@spa.rst.rststb': 1.9866408109664917, 'eval_runtime': 4.8964, 'eval_samples_per_second': 78.221, 'eval_steps_per_second': 2.451, 'epoch': 7.0}
{'train@spa.rst.rststb_loss': 1.650491714477539, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.5236607142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.2106397555377008, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.2579509048090299, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.2127401167010013, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.650491714477539, 'train@spa.rst.rststb_runtime': 27.0561, 'train@spa.rst.rststb_samples_per_second': 82.791, 'train@spa.rst.rststb_steps_per_second': 2.587, 'epoch': 8.0}
{'loss': 1.7493, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.9635899066925049, 'eval_accuracy@spa.rst.rststb': 0.48825065274151436, 'eval_f1@spa.rst.rststb': 0.20429197685252476, 'eval_precision@spa.rst.rststb': 0.21897836787975183, 'eval_recall@spa.rst.rststb': 0.20913686841734694, 'eval_loss@spa.rst.rststb': 1.9635899066925049, 'eval_runtime': 4.8968, 'eval_samples_per_second': 78.214, 'eval_steps_per_second': 2.451, 'epoch': 8.0}
{'train@spa.rst.rststb_loss': 1.6209890842437744, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.53125, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.22289959334330695, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.2746028023462315, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.22472581479686823, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.6209890842437744, 'train@spa.rst.rststb_runtime': 27.0774, 'train@spa.rst.rststb_samples_per_second': 82.726, 'train@spa.rst.rststb_steps_per_second': 2.585, 'epoch': 9.0}
{'loss': 1.7176, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.9325202703475952, 'eval_accuracy@spa.rst.rststb': 0.4960835509138381, 'eval_f1@spa.rst.rststb': 0.21505710023978244, 'eval_precision@spa.rst.rststb': 0.2566971132920456, 'eval_recall@spa.rst.rststb': 0.21761512928691215, 'eval_loss@spa.rst.rststb': 1.9325201511383057, 'eval_runtime': 4.9022, 'eval_samples_per_second': 78.128, 'eval_steps_per_second': 2.448, 'epoch': 9.0}
{'train@spa.rst.rststb_loss': 1.6005338430404663, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.5383928571428571, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.2330387512012297, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.2833271003048935, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.23311413212249946, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.6005338430404663, 'train@spa.rst.rststb_runtime': 27.0704, 'train@spa.rst.rststb_samples_per_second': 82.747, 'train@spa.rst.rststb_steps_per_second': 2.586, 'epoch': 10.0}
{'loss': 1.6826, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.9177443981170654, 'eval_accuracy@spa.rst.rststb': 0.49869451697127937, 'eval_f1@spa.rst.rststb': 0.21520485971034034, 'eval_precision@spa.rst.rststb': 0.25002846370679577, 'eval_recall@spa.rst.rststb': 0.2210081440333962, 'eval_loss@spa.rst.rststb': 1.9177443981170654, 'eval_runtime': 4.8977, 'eval_samples_per_second': 78.2, 'eval_steps_per_second': 2.45, 'epoch': 10.0}
{'train@spa.rst.rststb_loss': 1.5893769264221191, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.54375, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.2423102763332136, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.2840748106692875, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.24160096487334032, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.5893768072128296, 'train@spa.rst.rststb_runtime': 27.0381, 'train@spa.rst.rststb_samples_per_second': 82.846, 'train@spa.rst.rststb_steps_per_second': 2.589, 'epoch': 11.0}
{'loss': 1.6763, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.9101591110229492, 'eval_accuracy@spa.rst.rststb': 0.49869451697127937, 'eval_f1@spa.rst.rststb': 0.22195017556024405, 'eval_precision@spa.rst.rststb': 0.24999342349632322, 'eval_recall@spa.rst.rststb': 0.22541112910802305, 'eval_loss@spa.rst.rststb': 1.9101591110229492, 'eval_runtime': 4.9166, 'eval_samples_per_second': 77.899, 'eval_steps_per_second': 2.441, 'epoch': 11.0}
{'train@spa.rst.rststb_loss': 1.5858159065246582, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.5441964285714286, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.2443851264925434, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.2846797231115658, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.24410247858297665, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.5858159065246582, 'train@spa.rst.rststb_runtime': 27.1119, 'train@spa.rst.rststb_samples_per_second': 82.62, 'train@spa.rst.rststb_steps_per_second': 2.582, 'epoch': 12.0}
{'loss': 1.6634, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.9065461158752441, 'eval_accuracy@spa.rst.rststb': 0.49869451697127937, 'eval_f1@spa.rst.rststb': 0.2218562784140244, 'eval_precision@spa.rst.rststb': 0.24818929052237895, 'eval_recall@spa.rst.rststb': 0.22541112910802305, 'eval_loss@spa.rst.rststb': 1.9065463542938232, 'eval_runtime': 4.9033, 'eval_samples_per_second': 78.11, 'eval_steps_per_second': 2.447, 'epoch': 12.0}
{'train_runtime': 1066.5496, 'train_samples_per_second': 25.203, 'train_steps_per_second': 0.788, 'train_loss': 2.0674207778204057, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.4623
  train_runtime            = 3:42:20.67
  train_samples_per_second =     25.926
  train_steps_per_second   =       0.81
-------------------------------------------------------------------
Lang1:  spa.rst.sctb    Lang2:  spa.rst.rststb
Saving run to:  runs/full_shot/FullShot=v4_spa.rst.sctb_spa.rst.rststb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 439 examples
read 94 examples
read 159 examples
read 2240 examples
read 383 examples
read 426 examples
Total prediction labels:  30
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=30, bias=True)
    )
  )
)
{'train@spa.rst.sctb_loss': 3.12501859664917, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.35990888382687924, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.031188674846885797, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.027713644380311046, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04661290322580645, 'train@spa.rst.sctb_loss@spa.rst.sctb': 3.125018358230591, 'train@spa.rst.sctb_runtime': 5.47, 'train@spa.rst.sctb_samples_per_second': 80.256, 'train@spa.rst.sctb_steps_per_second': 2.559, 'epoch': 1.0}
{'loss': 3.2685, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.116117000579834, 'eval_accuracy@spa.rst.sctb': 0.3617021276595745, 'eval_f1@spa.rst.sctb': 0.04066216650594663, 'eval_precision@spa.rst.sctb': 0.04467944481163252, 'eval_recall@spa.rst.sctb': 0.06323294868186509, 'eval_loss@spa.rst.sctb': 3.116117000579834, 'eval_runtime': 1.4091, 'eval_samples_per_second': 66.71, 'eval_steps_per_second': 2.129, 'epoch': 1.0}
{'train@spa.rst.sctb_loss': 2.8941690921783447, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3621867881548975, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.029884336677814937, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.028749831921473717, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04620967741935483, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.894169569015503, 'train@spa.rst.sctb_runtime': 5.4978, 'train@spa.rst.sctb_samples_per_second': 79.851, 'train@spa.rst.sctb_steps_per_second': 2.546, 'epoch': 2.0}
{'loss': 3.0276, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.884138345718384, 'eval_accuracy@spa.rst.sctb': 0.3617021276595745, 'eval_f1@spa.rst.sctb': 0.036656891495601175, 'eval_precision@spa.rst.sctb': 0.04093945270415859, 'eval_recall@spa.rst.sctb': 0.06191950464396284, 'eval_loss@spa.rst.sctb': 2.8841373920440674, 'eval_runtime': 1.4232, 'eval_samples_per_second': 66.048, 'eval_steps_per_second': 2.108, 'epoch': 2.0}
{'train@spa.rst.sctb_loss': 2.6679186820983887, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.357630979498861, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.028659485338120887, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.028069973382473384, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04531362007168458, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.6679189205169678, 'train@spa.rst.sctb_runtime': 5.5188, 'train@spa.rst.sctb_samples_per_second': 79.546, 'train@spa.rst.sctb_steps_per_second': 2.537, 'epoch': 3.0}
{'loss': 2.8191, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.6635689735412598, 'eval_accuracy@spa.rst.sctb': 0.3723404255319149, 'eval_f1@spa.rst.sctb': 0.041794024078348195, 'eval_precision@spa.rst.sctb': 0.050980392156862744, 'eval_recall@spa.rst.sctb': 0.06501547987616098, 'eval_loss@spa.rst.sctb': 2.663569450378418, 'eval_runtime': 1.3955, 'eval_samples_per_second': 67.357, 'eval_steps_per_second': 2.15, 'epoch': 3.0}
{'train@spa.rst.sctb_loss': 2.479905366897583, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.35990888382687924, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.029307506375851704, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.02897509578544061, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04576164874551971, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.479905366897583, 'train@spa.rst.sctb_runtime': 5.5541, 'train@spa.rst.sctb_samples_per_second': 79.041, 'train@spa.rst.sctb_steps_per_second': 2.521, 'epoch': 4.0}
{'loss': 2.5997, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.4877541065216064, 'eval_accuracy@spa.rst.sctb': 0.3723404255319149, 'eval_f1@spa.rst.sctb': 0.041794024078348195, 'eval_precision@spa.rst.sctb': 0.050980392156862744, 'eval_recall@spa.rst.sctb': 0.06501547987616098, 'eval_loss@spa.rst.sctb': 2.4877543449401855, 'eval_runtime': 1.4204, 'eval_samples_per_second': 66.18, 'eval_steps_per_second': 2.112, 'epoch': 4.0}
{'train@spa.rst.sctb_loss': 2.349719524383545, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3621867881548975, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.029946087820103568, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.029829339143064636, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04620967741935483, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.349719285964966, 'train@spa.rst.sctb_runtime': 5.5364, 'train@spa.rst.sctb_samples_per_second': 79.293, 'train@spa.rst.sctb_steps_per_second': 2.529, 'epoch': 5.0}
{'loss': 2.4491, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.367915630340576, 'eval_accuracy@spa.rst.sctb': 0.3617021276595745, 'eval_f1@spa.rst.sctb': 0.04083754392531137, 'eval_precision@spa.rst.sctb': 0.05032679738562092, 'eval_recall@spa.rst.sctb': 0.06323294868186509, 'eval_loss@spa.rst.sctb': 2.3679158687591553, 'eval_runtime': 1.4153, 'eval_samples_per_second': 66.417, 'eval_steps_per_second': 2.12, 'epoch': 5.0}
{'train@spa.rst.sctb_loss': 2.271549940109253, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.36446469248291574, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.030167264038231778, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.031243411343031834, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04648745519713262, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.271550178527832, 'train@spa.rst.sctb_runtime': 5.559, 'train@spa.rst.sctb_samples_per_second': 78.972, 'train@spa.rst.sctb_steps_per_second': 2.518, 'epoch': 6.0}
{'loss': 2.3451, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.298902988433838, 'eval_accuracy@spa.rst.sctb': 0.3617021276595745, 'eval_f1@spa.rst.sctb': 0.04083754392531137, 'eval_precision@spa.rst.sctb': 0.05032679738562092, 'eval_recall@spa.rst.sctb': 0.06323294868186509, 'eval_loss@spa.rst.sctb': 2.298902750015259, 'eval_runtime': 1.4246, 'eval_samples_per_second': 65.982, 'eval_steps_per_second': 2.106, 'epoch': 6.0}
{'train@spa.rst.sctb_loss': 2.2243916988372803, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3735763097949886, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.032817696823589815, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.0316048139624089, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04844982078853047, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.2243916988372803, 'train@spa.rst.sctb_runtime': 5.5446, 'train@spa.rst.sctb_samples_per_second': 79.176, 'train@spa.rst.sctb_steps_per_second': 2.525, 'epoch': 7.0}
{'loss': 2.2902, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.2595436573028564, 'eval_accuracy@spa.rst.sctb': 0.3829787234042553, 'eval_f1@spa.rst.sctb': 0.04993680116674768, 'eval_precision@spa.rst.sctb': 0.06060606060606061, 'eval_recall@spa.rst.sctb': 0.06942489914626138, 'eval_loss@spa.rst.sctb': 2.2595431804656982, 'eval_runtime': 1.4309, 'eval_samples_per_second': 65.695, 'eval_steps_per_second': 2.097, 'epoch': 7.0}
{'train@spa.rst.sctb_loss': 2.1972146034240723, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3826879271070615, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03500912884715701, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03356227106227106, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.050241935483870966, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.1972148418426514, 'train@spa.rst.sctb_runtime': 5.528, 'train@spa.rst.sctb_samples_per_second': 79.414, 'train@spa.rst.sctb_steps_per_second': 2.533, 'epoch': 8.0}
{'loss': 2.2377, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.2377336025238037, 'eval_accuracy@spa.rst.sctb': 0.3829787234042553, 'eval_f1@spa.rst.sctb': 0.04993680116674768, 'eval_precision@spa.rst.sctb': 0.06060606060606061, 'eval_recall@spa.rst.sctb': 0.06942489914626138, 'eval_loss@spa.rst.sctb': 2.2377336025238037, 'eval_runtime': 1.4117, 'eval_samples_per_second': 66.584, 'eval_steps_per_second': 2.125, 'epoch': 8.0}
{'train@spa.rst.sctb_loss': 2.1780126094818115, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3917995444191344, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03716058992385023, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.034106566989152744, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05220430107526882, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.1780123710632324, 'train@spa.rst.sctb_runtime': 5.5453, 'train@spa.rst.sctb_samples_per_second': 79.166, 'train@spa.rst.sctb_steps_per_second': 2.525, 'epoch': 9.0}
{'loss': 2.2227, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.2223284244537354, 'eval_accuracy@spa.rst.sctb': 0.39361702127659576, 'eval_f1@spa.rst.sctb': 0.051711904958336774, 'eval_precision@spa.rst.sctb': 0.04746532759445241, 'eval_recall@spa.rst.sctb': 0.07252087437845951, 'eval_loss@spa.rst.sctb': 2.2223281860351562, 'eval_runtime': 1.4174, 'eval_samples_per_second': 66.317, 'eval_steps_per_second': 2.116, 'epoch': 9.0}
{'train@spa.rst.sctb_loss': 2.165339469909668, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.39863325740318906, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03847755362535413, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.033630573645731965, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.053718637992831546, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.165339708328247, 'train@spa.rst.sctb_runtime': 5.5332, 'train@spa.rst.sctb_samples_per_second': 79.339, 'train@spa.rst.sctb_steps_per_second': 2.53, 'epoch': 10.0}
{'loss': 2.2166, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.213141441345215, 'eval_accuracy@spa.rst.sctb': 0.39361702127659576, 'eval_f1@spa.rst.sctb': 0.05140608875128999, 'eval_precision@spa.rst.sctb': 0.04586335958884978, 'eval_recall@spa.rst.sctb': 0.07252087437845951, 'eval_loss@spa.rst.sctb': 2.213142156600952, 'eval_runtime': 1.4228, 'eval_samples_per_second': 66.069, 'eval_steps_per_second': 2.109, 'epoch': 10.0}
{'train@spa.rst.sctb_loss': 2.1584041118621826, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.39863325740318906, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03847755362535413, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.033630573645731965, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.053718637992831546, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.1584043502807617, 'train@spa.rst.sctb_runtime': 5.5477, 'train@spa.rst.sctb_samples_per_second': 79.131, 'train@spa.rst.sctb_steps_per_second': 2.524, 'epoch': 11.0}
{'loss': 2.2092, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.2076876163482666, 'eval_accuracy@spa.rst.sctb': 0.39361702127659576, 'eval_f1@spa.rst.sctb': 0.05140608875128999, 'eval_precision@spa.rst.sctb': 0.04586335958884978, 'eval_recall@spa.rst.sctb': 0.07252087437845951, 'eval_loss@spa.rst.sctb': 2.2076878547668457, 'eval_runtime': 1.4322, 'eval_samples_per_second': 65.635, 'eval_steps_per_second': 2.095, 'epoch': 11.0}
{'train@spa.rst.sctb_loss': 2.1560850143432617, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.39863325740318906, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.038432843051760555, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.033444403307417006, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.053718637992831546, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.1560850143432617, 'train@spa.rst.sctb_runtime': 5.5267, 'train@spa.rst.sctb_samples_per_second': 79.432, 'train@spa.rst.sctb_steps_per_second': 2.533, 'epoch': 12.0}
{'loss': 2.1979, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.206148624420166, 'eval_accuracy@spa.rst.sctb': 0.39361702127659576, 'eval_f1@spa.rst.sctb': 0.05140608875128999, 'eval_precision@spa.rst.sctb': 0.04586335958884978, 'eval_recall@spa.rst.sctb': 0.07252087437845951, 'eval_loss@spa.rst.sctb': 2.206148862838745, 'eval_runtime': 1.432, 'eval_samples_per_second': 65.642, 'eval_steps_per_second': 2.095, 'epoch': 12.0}
{'train_runtime': 216.8276, 'train_samples_per_second': 24.296, 'train_steps_per_second': 0.775, 'train_loss': 2.4902865205492293, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.4903
  train_runtime            = 0:03:36.82
  train_samples_per_second =     24.296
  train_steps_per_second   =      0.775
{'train@spa.rst.rststb_loss': 2.7316153049468994, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.20625, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.01441415355017842, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.026653005022742458, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.03692751167783578, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.7316150665283203, 'train@spa.rst.rststb_runtime': 27.0063, 'train@spa.rst.rststb_samples_per_second': 82.944, 'train@spa.rst.rststb_steps_per_second': 2.592, 'epoch': 1.0}
{'loss': 3.2344, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.7368762493133545, 'eval_accuracy@spa.rst.rststb': 0.20887728459530025, 'eval_f1@spa.rst.rststb': 0.016272565471650142, 'eval_precision@spa.rst.rststb': 0.017806481374697265, 'eval_recall@spa.rst.rststb': 0.04412719013627515, 'eval_loss@spa.rst.rststb': 2.7368762493133545, 'eval_runtime': 4.8833, 'eval_samples_per_second': 78.431, 'eval_steps_per_second': 2.457, 'epoch': 1.0}
{'train@spa.rst.rststb_loss': 2.509784698486328, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.2598214285714286, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.03634897545903023, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.08528737515072919, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.051373342485282525, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.5097851753234863, 'train@spa.rst.rststb_runtime': 27.0223, 'train@spa.rst.rststb_samples_per_second': 82.894, 'train@spa.rst.rststb_steps_per_second': 2.59, 'epoch': 2.0}
{'loss': 2.6352, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.5996973514556885, 'eval_accuracy@spa.rst.rststb': 0.24804177545691905, 'eval_f1@spa.rst.rststb': 0.039483754626183414, 'eval_precision@spa.rst.rststb': 0.06299049344984872, 'eval_recall@spa.rst.rststb': 0.05736340561663589, 'eval_loss@spa.rst.rststb': 2.5996975898742676, 'eval_runtime': 4.8723, 'eval_samples_per_second': 78.608, 'eval_steps_per_second': 2.463, 'epoch': 2.0}
{'train@spa.rst.rststb_loss': 2.3970730304718018, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.34151785714285715, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.06296559807732009, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.05538386389455317, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.08089100768898447, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.3970730304718018, 'train@spa.rst.rststb_runtime': 27.0393, 'train@spa.rst.rststb_samples_per_second': 82.842, 'train@spa.rst.rststb_steps_per_second': 2.589, 'epoch': 3.0}
{'loss': 2.4873, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.5293071269989014, 'eval_accuracy@spa.rst.rststb': 0.3133159268929504, 'eval_f1@spa.rst.rststb': 0.07113638465300288, 'eval_precision@spa.rst.rststb': 0.0676933268228055, 'eval_recall@spa.rst.rststb': 0.08735966224527818, 'eval_loss@spa.rst.rststb': 2.5293076038360596, 'eval_runtime': 4.8608, 'eval_samples_per_second': 78.794, 'eval_steps_per_second': 2.469, 'epoch': 3.0}
{'train@spa.rst.rststb_loss': 2.3058996200561523, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.35982142857142857, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.06972110234404523, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.09458403298485045, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.0956764112353639, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.3058993816375732, 'train@spa.rst.rststb_runtime': 26.9913, 'train@spa.rst.rststb_samples_per_second': 82.99, 'train@spa.rst.rststb_steps_per_second': 2.593, 'epoch': 4.0}
{'loss': 2.3941, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.467768430709839, 'eval_accuracy@spa.rst.rststb': 0.3368146214099217, 'eval_f1@spa.rst.rststb': 0.07822152962025801, 'eval_precision@spa.rst.rststb': 0.0686992298947949, 'eval_recall@spa.rst.rststb': 0.10352526050248227, 'eval_loss@spa.rst.rststb': 2.4677681922912598, 'eval_runtime': 4.8769, 'eval_samples_per_second': 78.534, 'eval_steps_per_second': 2.461, 'epoch': 4.0}
{'train@spa.rst.rststb_loss': 2.2249257564544678, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.3776785714285714, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.0795861885832891, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.08885452225083237, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.10445732957262209, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.2249257564544678, 'train@spa.rst.rststb_runtime': 26.9964, 'train@spa.rst.rststb_samples_per_second': 82.974, 'train@spa.rst.rststb_steps_per_second': 2.593, 'epoch': 5.0}
{'loss': 2.306, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.41334867477417, 'eval_accuracy@spa.rst.rststb': 0.3681462140992167, 'eval_f1@spa.rst.rststb': 0.08728059326074933, 'eval_precision@spa.rst.rststb': 0.07638705535110768, 'eval_recall@spa.rst.rststb': 0.11562150259075649, 'eval_loss@spa.rst.rststb': 2.413349151611328, 'eval_runtime': 4.8764, 'eval_samples_per_second': 78.542, 'eval_steps_per_second': 2.461, 'epoch': 5.0}
{'train@spa.rst.rststb_loss': 2.151456117630005, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.39598214285714284, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.08884206284719477, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.10594599299209048, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.11407076923829941, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.151455879211426, 'train@spa.rst.rststb_runtime': 27.0268, 'train@spa.rst.rststb_samples_per_second': 82.881, 'train@spa.rst.rststb_steps_per_second': 2.59, 'epoch': 6.0}
{'loss': 2.2352, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.3584611415863037, 'eval_accuracy@spa.rst.rststb': 0.36553524804177545, 'eval_f1@spa.rst.rststb': 0.10709622939216665, 'eval_precision@spa.rst.rststb': 0.11468218008152571, 'eval_recall@spa.rst.rststb': 0.13020570246993168, 'eval_loss@spa.rst.rststb': 2.3584611415863037, 'eval_runtime': 4.8859, 'eval_samples_per_second': 78.389, 'eval_steps_per_second': 2.456, 'epoch': 6.0}
{'train@spa.rst.rststb_loss': 2.0891096591949463, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4089285714285714, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.09694360642392146, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.10586414935743553, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.12193319562776782, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0891098976135254, 'train@spa.rst.rststb_runtime': 26.9999, 'train@spa.rst.rststb_samples_per_second': 82.963, 'train@spa.rst.rststb_steps_per_second': 2.593, 'epoch': 7.0}
{'loss': 2.161, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.310758113861084, 'eval_accuracy@spa.rst.rststb': 0.38642297650130547, 'eval_f1@spa.rst.rststb': 0.11461963313627867, 'eval_precision@spa.rst.rststb': 0.11284234203488862, 'eval_recall@spa.rst.rststb': 0.13969013227467791, 'eval_loss@spa.rst.rststb': 2.310757875442505, 'eval_runtime': 4.8642, 'eval_samples_per_second': 78.738, 'eval_steps_per_second': 2.467, 'epoch': 7.0}
{'train@spa.rst.rststb_loss': 2.0424273014068604, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4174107142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10045736285298688, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.10732251491623858, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.12564589969735038, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0424275398254395, 'train@spa.rst.rststb_runtime': 27.0209, 'train@spa.rst.rststb_samples_per_second': 82.899, 'train@spa.rst.rststb_steps_per_second': 2.591, 'epoch': 8.0}
{'loss': 2.1173, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.2745778560638428, 'eval_accuracy@spa.rst.rststb': 0.39425587467362927, 'eval_f1@spa.rst.rststb': 0.11600284617185243, 'eval_precision@spa.rst.rststb': 0.10976640786294686, 'eval_recall@spa.rst.rststb': 0.1416369200748077, 'eval_loss@spa.rst.rststb': 2.2745778560638428, 'eval_runtime': 4.8872, 'eval_samples_per_second': 78.368, 'eval_steps_per_second': 2.455, 'epoch': 8.0}
{'train@spa.rst.rststb_loss': 2.009190559387207, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.42276785714285714, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10195501838191845, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.10237850094737581, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.12903283463264842, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.009190797805786, 'train@spa.rst.rststb_runtime': 27.0118, 'train@spa.rst.rststb_samples_per_second': 82.927, 'train@spa.rst.rststb_steps_per_second': 2.591, 'epoch': 9.0}
{'loss': 2.0768, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.248333215713501, 'eval_accuracy@spa.rst.rststb': 0.402088772845953, 'eval_f1@spa.rst.rststb': 0.12081385886892912, 'eval_precision@spa.rst.rststb': 0.11076360134922665, 'eval_recall@spa.rst.rststb': 0.14880734577688717, 'eval_loss@spa.rst.rststb': 2.24833345413208, 'eval_runtime': 4.8821, 'eval_samples_per_second': 78.451, 'eval_steps_per_second': 2.458, 'epoch': 9.0}
{'train@spa.rst.rststb_loss': 1.9861478805541992, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.42678571428571427, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.1042167818549173, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.10461181715057698, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.1305970249450723, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.9861476421356201, 'train@spa.rst.rststb_runtime': 27.0422, 'train@spa.rst.rststb_samples_per_second': 82.833, 'train@spa.rst.rststb_steps_per_second': 2.589, 'epoch': 10.0}
{'loss': 2.0498, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.2324745655059814, 'eval_accuracy@spa.rst.rststb': 0.402088772845953, 'eval_f1@spa.rst.rststb': 0.12132071480146403, 'eval_precision@spa.rst.rststb': 0.11130682665806735, 'eval_recall@spa.rst.rststb': 0.14880734577688717, 'eval_loss@spa.rst.rststb': 2.2324748039245605, 'eval_runtime': 4.8755, 'eval_samples_per_second': 78.556, 'eval_steps_per_second': 2.461, 'epoch': 10.0}
{'train@spa.rst.rststb_loss': 1.9729801416397095, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4294642857142857, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10644092095320766, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.10423435342574414, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13283613898962135, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.9729799032211304, 'train@spa.rst.rststb_runtime': 27.0339, 'train@spa.rst.rststb_samples_per_second': 82.859, 'train@spa.rst.rststb_steps_per_second': 2.589, 'epoch': 11.0}
{'loss': 2.0294, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.2219302654266357, 'eval_accuracy@spa.rst.rststb': 0.402088772845953, 'eval_f1@spa.rst.rststb': 0.12137623906552919, 'eval_precision@spa.rst.rststb': 0.11155058544580997, 'eval_recall@spa.rst.rststb': 0.14880734577688717, 'eval_loss@spa.rst.rststb': 2.221930503845215, 'eval_runtime': 4.879, 'eval_samples_per_second': 78.499, 'eval_steps_per_second': 2.46, 'epoch': 11.0}
{'train@spa.rst.rststb_loss': 1.9685544967651367, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4316964285714286, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10754486761192246, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.12129396224827907, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13355880979225512, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.9685544967651367, 'train@spa.rst.rststb_runtime': 27.0109, 'train@spa.rst.rststb_samples_per_second': 82.93, 'train@spa.rst.rststb_steps_per_second': 2.592, 'epoch': 12.0}
{'loss': 2.0201, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.2184436321258545, 'eval_accuracy@spa.rst.rststb': 0.4046997389033943, 'eval_f1@spa.rst.rststb': 0.12241961386911777, 'eval_precision@spa.rst.rststb': 0.11295287936579852, 'eval_recall@spa.rst.rststb': 0.1494562750435971, 'eval_loss@spa.rst.rststb': 2.2184433937072754, 'eval_runtime': 4.8573, 'eval_samples_per_second': 78.85, 'eval_steps_per_second': 2.47, 'epoch': 12.0}
{'train_runtime': 1064.1489, 'train_samples_per_second': 25.26, 'train_steps_per_second': 0.789, 'train_loss': 2.312232989356631, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.4903
  train_runtime            = 0:03:36.82
  train_samples_per_second =     24.296
  train_steps_per_second   =      0.775
-------------------------------------------------------------------
Lang1:  tur.pdtb.tdb    Lang2:  spa.rst.rststb
Saving run to:  runs/full_shot/FullShot=v4_tur.pdtb.tdb_spa.rst.rststb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2451 examples
read 312 examples
read 422 examples
read 2240 examples
read 383 examples
read 426 examples
Total prediction labels:  52
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (tr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (tr): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=52, bias=True)
    )
  )
)
{'train@tur.pdtb.tdb_loss': 3.001589298248291, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01741967515426626, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010891738953044898, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 3.001589298248291, 'train@tur.pdtb.tdb_runtime': 32.3467, 'train@tur.pdtb.tdb_samples_per_second': 75.773, 'train@tur.pdtb.tdb_steps_per_second': 2.38, 'epoch': 1.0}
{'loss': 3.4757, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.943430185317993, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.9434304237365723, 'eval_runtime': 4.2325, 'eval_samples_per_second': 73.715, 'eval_steps_per_second': 2.363, 'epoch': 1.0}
{'train@tur.pdtb.tdb_loss': 2.4909608364105225, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01741967515426626, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010891738953044898, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.4909608364105225, 'train@tur.pdtb.tdb_runtime': 29.6965, 'train@tur.pdtb.tdb_samples_per_second': 82.535, 'train@tur.pdtb.tdb_steps_per_second': 2.593, 'epoch': 2.0}
{'loss': 2.7071, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.380988597869873, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.380988359451294, 'eval_runtime': 4.243, 'eval_samples_per_second': 73.532, 'eval_steps_per_second': 2.357, 'epoch': 2.0}
{'train@tur.pdtb.tdb_loss': 2.3924551010131836, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.251733986128111, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01849474683381471, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.029556474977381543, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.04401064773735581, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.3924551010131836, 'train@tur.pdtb.tdb_runtime': 29.7563, 'train@tur.pdtb.tdb_samples_per_second': 82.369, 'train@tur.pdtb.tdb_steps_per_second': 2.588, 'epoch': 3.0}
{'loss': 2.4595, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.312620162963867, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.3126204013824463, 'eval_runtime': 4.2078, 'eval_samples_per_second': 74.149, 'eval_steps_per_second': 2.377, 'epoch': 3.0}
{'train@tur.pdtb.tdb_loss': 2.3312742710113525, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.26805385556915545, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.032924274664853384, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.04266236986274035, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.052474477512290854, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.3312745094299316, 'train@tur.pdtb.tdb_runtime': 29.7932, 'train@tur.pdtb.tdb_samples_per_second': 82.267, 'train@tur.pdtb.tdb_steps_per_second': 2.584, 'epoch': 4.0}
{'loss': 2.3881, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.2693240642547607, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.02360632024034717, 'eval_precision@tur.pdtb.tdb': 0.025383707201889018, 'eval_recall@tur.pdtb.tdb': 0.04688450772788122, 'eval_loss@tur.pdtb.tdb': 2.2693240642547607, 'eval_runtime': 4.2353, 'eval_samples_per_second': 73.667, 'eval_steps_per_second': 2.361, 'epoch': 4.0}
{'train@tur.pdtb.tdb_loss': 2.2797772884368896, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.28600571195430435, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.04143067533957801, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.0444614868958463, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.06231628173150014, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.2797772884368896, 'train@tur.pdtb.tdb_runtime': 29.7849, 'train@tur.pdtb.tdb_samples_per_second': 82.29, 'train@tur.pdtb.tdb_steps_per_second': 2.585, 'epoch': 5.0}
{'loss': 2.3365, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.231449604034424, 'eval_accuracy@tur.pdtb.tdb': 0.2724358974358974, 'eval_f1@tur.pdtb.tdb': 0.028334222854770805, 'eval_precision@tur.pdtb.tdb': 0.022834730281538793, 'eval_recall@tur.pdtb.tdb': 0.05083972252646951, 'eval_loss@tur.pdtb.tdb': 2.231449604034424, 'eval_runtime': 4.2272, 'eval_samples_per_second': 73.808, 'eval_steps_per_second': 2.366, 'epoch': 5.0}
{'train@tur.pdtb.tdb_loss': 2.233973503112793, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.31334149326805383, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.07508027706648968, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.08060450945168449, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.09010203214553064, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.233973264694214, 'train@tur.pdtb.tdb_runtime': 29.793, 'train@tur.pdtb.tdb_samples_per_second': 82.268, 'train@tur.pdtb.tdb_steps_per_second': 2.585, 'epoch': 6.0}
{'loss': 2.301, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.201237440109253, 'eval_accuracy@tur.pdtb.tdb': 0.2916666666666667, 'eval_f1@tur.pdtb.tdb': 0.06077962771818393, 'eval_precision@tur.pdtb.tdb': 0.08314967860422405, 'eval_recall@tur.pdtb.tdb': 0.07205381239754026, 'eval_loss@tur.pdtb.tdb': 2.201237440109253, 'eval_runtime': 4.2555, 'eval_samples_per_second': 73.317, 'eval_steps_per_second': 2.35, 'epoch': 6.0}
{'train@tur.pdtb.tdb_loss': 2.2029333114624023, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.32843737250102, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.08574060997282863, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.09719126449035731, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.10493958893106817, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.2029330730438232, 'train@tur.pdtb.tdb_runtime': 29.7916, 'train@tur.pdtb.tdb_samples_per_second': 82.272, 'train@tur.pdtb.tdb_steps_per_second': 2.585, 'epoch': 7.0}
{'loss': 2.2645, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.181499481201172, 'eval_accuracy@tur.pdtb.tdb': 0.2948717948717949, 'eval_f1@tur.pdtb.tdb': 0.07395202092612772, 'eval_precision@tur.pdtb.tdb': 0.08429438994643254, 'eval_recall@tur.pdtb.tdb': 0.1014514930216098, 'eval_loss@tur.pdtb.tdb': 2.181499719619751, 'eval_runtime': 4.2793, 'eval_samples_per_second': 72.909, 'eval_steps_per_second': 2.337, 'epoch': 7.0}
{'train@tur.pdtb.tdb_loss': 2.1874024868011475, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3317013463892289, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.08947095213540279, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.10305668936524989, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.10663059370778831, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1874027252197266, 'train@tur.pdtb.tdb_runtime': 29.8049, 'train@tur.pdtb.tdb_samples_per_second': 82.235, 'train@tur.pdtb.tdb_steps_per_second': 2.583, 'epoch': 8.0}
{'loss': 2.2385, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.170753002166748, 'eval_accuracy@tur.pdtb.tdb': 0.2948717948717949, 'eval_f1@tur.pdtb.tdb': 0.07050886290989561, 'eval_precision@tur.pdtb.tdb': 0.07270988716771849, 'eval_recall@tur.pdtb.tdb': 0.10073651188494193, 'eval_loss@tur.pdtb.tdb': 2.170753002166748, 'eval_runtime': 4.2245, 'eval_samples_per_second': 73.854, 'eval_steps_per_second': 2.367, 'epoch': 8.0}
{'train@tur.pdtb.tdb_loss': 2.1619791984558105, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.332109343125255, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.08684550887802973, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.09815901733345944, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.10922669223090578, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1619791984558105, 'train@tur.pdtb.tdb_runtime': 29.7717, 'train@tur.pdtb.tdb_samples_per_second': 82.327, 'train@tur.pdtb.tdb_steps_per_second': 2.586, 'epoch': 9.0}
{'loss': 2.2083, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.151283025741577, 'eval_accuracy@tur.pdtb.tdb': 0.30128205128205127, 'eval_f1@tur.pdtb.tdb': 0.06776988441488477, 'eval_precision@tur.pdtb.tdb': 0.07570179181432378, 'eval_recall@tur.pdtb.tdb': 0.10419956194614932, 'eval_loss@tur.pdtb.tdb': 2.151282787322998, 'eval_runtime': 4.2512, 'eval_samples_per_second': 73.39, 'eval_steps_per_second': 2.352, 'epoch': 9.0}
{'train@tur.pdtb.tdb_loss': 2.1492316722869873, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3398612811097511, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09082913764381369, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.10433831410518661, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11282520011970698, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1492319107055664, 'train@tur.pdtb.tdb_runtime': 29.724, 'train@tur.pdtb.tdb_samples_per_second': 82.459, 'train@tur.pdtb.tdb_steps_per_second': 2.59, 'epoch': 10.0}
{'loss': 2.1986, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.1401636600494385, 'eval_accuracy@tur.pdtb.tdb': 0.30128205128205127, 'eval_f1@tur.pdtb.tdb': 0.06748962787724067, 'eval_precision@tur.pdtb.tdb': 0.07566661126223026, 'eval_recall@tur.pdtb.tdb': 0.10412528981305366, 'eval_loss@tur.pdtb.tdb': 2.1401636600494385, 'eval_runtime': 4.2142, 'eval_samples_per_second': 74.035, 'eval_steps_per_second': 2.373, 'epoch': 10.0}
{'train@tur.pdtb.tdb_loss': 2.143433094024658, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.339453284373725, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09041389440147403, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.10520622965008958, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11264235381964936, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.143433094024658, 'train@tur.pdtb.tdb_runtime': 29.7536, 'train@tur.pdtb.tdb_samples_per_second': 82.377, 'train@tur.pdtb.tdb_steps_per_second': 2.588, 'epoch': 11.0}
{'loss': 2.1951, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.136491537094116, 'eval_accuracy@tur.pdtb.tdb': 0.3076923076923077, 'eval_f1@tur.pdtb.tdb': 0.06999057212114028, 'eval_precision@tur.pdtb.tdb': 0.07932499599166265, 'eval_recall@tur.pdtb.tdb': 0.1059355612016383, 'eval_loss@tur.pdtb.tdb': 2.136491537094116, 'eval_runtime': 4.2436, 'eval_samples_per_second': 73.523, 'eval_steps_per_second': 2.357, 'epoch': 11.0}
{'train@tur.pdtb.tdb_loss': 2.1401898860931396, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.34394124847001223, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09284448910453624, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.11017676012585166, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11465131984440284, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1401898860931396, 'train@tur.pdtb.tdb_runtime': 29.7129, 'train@tur.pdtb.tdb_samples_per_second': 82.489, 'train@tur.pdtb.tdb_steps_per_second': 2.591, 'epoch': 12.0}
{'loss': 2.1831, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.133676290512085, 'eval_accuracy@tur.pdtb.tdb': 0.3108974358974359, 'eval_f1@tur.pdtb.tdb': 0.07131367584197773, 'eval_precision@tur.pdtb.tdb': 0.0694568868980963, 'eval_recall@tur.pdtb.tdb': 0.10791316860093246, 'eval_loss@tur.pdtb.tdb': 2.133676290512085, 'eval_runtime': 4.1872, 'eval_samples_per_second': 74.512, 'eval_steps_per_second': 2.388, 'epoch': 12.0}
{'train_runtime': 1158.7997, 'train_samples_per_second': 25.381, 'train_steps_per_second': 0.797, 'train_loss': 2.412998083866004, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =      2.413
  train_runtime            = 0:19:18.79
  train_samples_per_second =     25.381
  train_steps_per_second   =      0.797
{'train@spa.rst.rststb_loss': 2.889484167098999, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.2017857142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.011993207386966675, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.007206632653061225, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.03571428571428571, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.889484405517578, 'train@spa.rst.rststb_runtime': 27.1811, 'train@spa.rst.rststb_samples_per_second': 82.41, 'train@spa.rst.rststb_steps_per_second': 2.575, 'epoch': 1.0}
{'loss': 3.4359, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.9532642364501953, 'eval_accuracy@spa.rst.rststb': 0.206266318537859, 'eval_f1@spa.rst.rststb': 0.014869188782232262, 'eval_precision@spa.rst.rststb': 0.00896810080599387, 'eval_recall@spa.rst.rststb': 0.043478260869565216, 'eval_loss@spa.rst.rststb': 2.9532642364501953, 'eval_runtime': 5.0595, 'eval_samples_per_second': 75.699, 'eval_steps_per_second': 2.372, 'epoch': 1.0}
{'train@spa.rst.rststb_loss': 2.5916950702667236, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.2558035714285714, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.03506915468981047, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.05845810439560439, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.05028077805486196, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.5916953086853027, 'train@spa.rst.rststb_runtime': 27.223, 'train@spa.rst.rststb_samples_per_second': 82.283, 'train@spa.rst.rststb_steps_per_second': 2.571, 'epoch': 2.0}
{'loss': 2.7563, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.7193832397460938, 'eval_accuracy@spa.rst.rststb': 0.22193211488250653, 'eval_f1@spa.rst.rststb': 0.02559938573759478, 'eval_precision@spa.rst.rststb': 0.052662171588002786, 'eval_recall@spa.rst.rststb': 0.04940711462450592, 'eval_loss@spa.rst.rststb': 2.719383955001831, 'eval_runtime': 5.0668, 'eval_samples_per_second': 75.59, 'eval_steps_per_second': 2.368, 'epoch': 2.0}
{'train@spa.rst.rststb_loss': 2.4500441551208496, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.3308035714285714, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.05331858914118281, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.061148112506785535, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.07284271452378564, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.4500441551208496, 'train@spa.rst.rststb_runtime': 27.2713, 'train@spa.rst.rststb_samples_per_second': 82.138, 'train@spa.rst.rststb_steps_per_second': 2.567, 'epoch': 3.0}
{'loss': 2.5565, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.614992618560791, 'eval_accuracy@spa.rst.rststb': 0.3185378590078329, 'eval_f1@spa.rst.rststb': 0.06427237271070367, 'eval_precision@spa.rst.rststb': 0.10034317479225162, 'eval_recall@spa.rst.rststb': 0.0851326816588565, 'eval_loss@spa.rst.rststb': 2.614992380142212, 'eval_runtime': 5.05, 'eval_samples_per_second': 75.842, 'eval_steps_per_second': 2.376, 'epoch': 3.0}
{'train@spa.rst.rststb_loss': 2.3376476764678955, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.36294642857142856, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.07051816164291602, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.0610102982053672, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.09476533959201514, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.3376474380493164, 'train@spa.rst.rststb_runtime': 27.3047, 'train@spa.rst.rststb_samples_per_second': 82.037, 'train@spa.rst.rststb_steps_per_second': 2.564, 'epoch': 4.0}
{'loss': 2.4434, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.5302305221557617, 'eval_accuracy@spa.rst.rststb': 0.3629242819843342, 'eval_f1@spa.rst.rststb': 0.08549593311220337, 'eval_precision@spa.rst.rststb': 0.07956584767321251, 'eval_recall@spa.rst.rststb': 0.11277327597693299, 'eval_loss@spa.rst.rststb': 2.5302305221557617, 'eval_runtime': 5.0274, 'eval_samples_per_second': 76.182, 'eval_steps_per_second': 2.387, 'epoch': 4.0}
{'train@spa.rst.rststb_loss': 2.2419471740722656, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.38035714285714284, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.08375801547457708, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.09236046288595488, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.10723793938732777, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.2419471740722656, 'train@spa.rst.rststb_runtime': 27.2789, 'train@spa.rst.rststb_samples_per_second': 82.115, 'train@spa.rst.rststb_steps_per_second': 2.566, 'epoch': 5.0}
{'loss': 2.3437, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.4587881565093994, 'eval_accuracy@spa.rst.rststb': 0.3838120104438642, 'eval_f1@spa.rst.rststb': 0.09878951171632203, 'eval_precision@spa.rst.rststb': 0.1306729965552728, 'eval_recall@spa.rst.rststb': 0.1238950707085405, 'eval_loss@spa.rst.rststb': 2.4587881565093994, 'eval_runtime': 5.1383, 'eval_samples_per_second': 74.539, 'eval_steps_per_second': 2.335, 'epoch': 5.0}
{'train@spa.rst.rststb_loss': 2.1608691215515137, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.39285714285714285, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.09279886610877594, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.125272357642063, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.11612730872805559, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.1608693599700928, 'train@spa.rst.rststb_runtime': 27.2488, 'train@spa.rst.rststb_samples_per_second': 82.206, 'train@spa.rst.rststb_steps_per_second': 2.569, 'epoch': 6.0}
{'loss': 2.2546, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.394665002822876, 'eval_accuracy@spa.rst.rststb': 0.3968668407310705, 'eval_f1@spa.rst.rststb': 0.1078309493795571, 'eval_precision@spa.rst.rststb': 0.12220545011121833, 'eval_recall@spa.rst.rststb': 0.13326722070838584, 'eval_loss@spa.rst.rststb': 2.394665241241455, 'eval_runtime': 5.1007, 'eval_samples_per_second': 75.088, 'eval_steps_per_second': 2.353, 'epoch': 6.0}
{'train@spa.rst.rststb_loss': 2.0960466861724854, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.41517857142857145, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10608509622593244, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.13212173184012546, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13075697160410807, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.096046209335327, 'train@spa.rst.rststb_runtime': 27.3087, 'train@spa.rst.rststb_samples_per_second': 82.025, 'train@spa.rst.rststb_steps_per_second': 2.563, 'epoch': 7.0}
{'loss': 2.1836, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.3464064598083496, 'eval_accuracy@spa.rst.rststb': 0.4073107049608355, 'eval_f1@spa.rst.rststb': 0.12327472627322703, 'eval_precision@spa.rst.rststb': 0.1298268171507008, 'eval_recall@spa.rst.rststb': 0.1502204978011827, 'eval_loss@spa.rst.rststb': 2.3464066982269287, 'eval_runtime': 5.0936, 'eval_samples_per_second': 75.192, 'eval_steps_per_second': 2.356, 'epoch': 7.0}
{'train@spa.rst.rststb_loss': 2.0482804775238037, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4205357142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10917751254215628, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.1374595243660779, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13247507128053165, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0482802391052246, 'train@spa.rst.rststb_runtime': 27.2666, 'train@spa.rst.rststb_samples_per_second': 82.152, 'train@spa.rst.rststb_steps_per_second': 2.567, 'epoch': 8.0}
{'loss': 2.1337, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.309779167175293, 'eval_accuracy@spa.rst.rststb': 0.4073107049608355, 'eval_f1@spa.rst.rststb': 0.12418845721125858, 'eval_precision@spa.rst.rststb': 0.12888312465098875, 'eval_recall@spa.rst.rststb': 0.1502204978011827, 'eval_loss@spa.rst.rststb': 2.309779405593872, 'eval_runtime': 5.0603, 'eval_samples_per_second': 75.687, 'eval_steps_per_second': 2.371, 'epoch': 8.0}
{'train@spa.rst.rststb_loss': 2.017763137817383, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.42544642857142856, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.11088238687146589, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.12747728132501607, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13634743669748853, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.017763137817383, 'train@spa.rst.rststb_runtime': 27.2817, 'train@spa.rst.rststb_samples_per_second': 82.106, 'train@spa.rst.rststb_steps_per_second': 2.566, 'epoch': 9.0}
{'loss': 2.0874, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.284970760345459, 'eval_accuracy@spa.rst.rststb': 0.4177545691906005, 'eval_f1@spa.rst.rststb': 0.12456431953326931, 'eval_precision@spa.rst.rststb': 0.14521120701244924, 'eval_recall@spa.rst.rststb': 0.15405764476955447, 'eval_loss@spa.rst.rststb': 2.284970760345459, 'eval_runtime': 5.0572, 'eval_samples_per_second': 75.734, 'eval_steps_per_second': 2.373, 'epoch': 9.0}
{'train@spa.rst.rststb_loss': 1.9941271543502808, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.43214285714285716, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.11547076704913205, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.1337542010988416, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.1394944042592988, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.9941271543502808, 'train@spa.rst.rststb_runtime': 27.2768, 'train@spa.rst.rststb_samples_per_second': 82.121, 'train@spa.rst.rststb_steps_per_second': 2.566, 'epoch': 10.0}
{'loss': 2.0675, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.26833176612854, 'eval_accuracy@spa.rst.rststb': 0.42036553524804177, 'eval_f1@spa.rst.rststb': 0.1249612946821887, 'eval_precision@spa.rst.rststb': 0.1358915747202263, 'eval_recall@spa.rst.rststb': 0.1547065740362644, 'eval_loss@spa.rst.rststb': 2.26833176612854, 'eval_runtime': 5.1025, 'eval_samples_per_second': 75.061, 'eval_steps_per_second': 2.352, 'epoch': 10.0}
{'train@spa.rst.rststb_loss': 1.9814507961273193, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.43348214285714287, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.11552876215200682, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.13076855197798906, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13979792510106323, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.9814507961273193, 'train@spa.rst.rststb_runtime': 27.2689, 'train@spa.rst.rststb_samples_per_second': 82.145, 'train@spa.rst.rststb_steps_per_second': 2.567, 'epoch': 11.0}
{'loss': 2.0429, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.2580785751342773, 'eval_accuracy@spa.rst.rststb': 0.42036553524804177, 'eval_f1@spa.rst.rststb': 0.1260746455424081, 'eval_precision@spa.rst.rststb': 0.13962015761087762, 'eval_recall@spa.rst.rststb': 0.15604657547198023, 'eval_loss@spa.rst.rststb': 2.2580785751342773, 'eval_runtime': 5.0561, 'eval_samples_per_second': 75.75, 'eval_steps_per_second': 2.373, 'epoch': 11.0}
{'train@spa.rst.rststb_loss': 1.9769657850265503, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.43526785714285715, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.11652107490369314, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.13303631160988696, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.1408119279289318, 'train@spa.rst.rststb_loss@spa.rst.rststb': 1.9769660234451294, 'train@spa.rst.rststb_runtime': 27.2682, 'train@spa.rst.rststb_samples_per_second': 82.147, 'train@spa.rst.rststb_steps_per_second': 2.567, 'epoch': 12.0}
{'loss': 2.0318, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.2546498775482178, 'eval_accuracy@spa.rst.rststb': 0.42036553524804177, 'eval_f1@spa.rst.rststb': 0.12636233209841222, 'eval_precision@spa.rst.rststb': 0.1390532836594728, 'eval_recall@spa.rst.rststb': 0.15604657547198023, 'eval_loss@spa.rst.rststb': 2.2546496391296387, 'eval_runtime': 5.0653, 'eval_samples_per_second': 75.613, 'eval_steps_per_second': 2.369, 'epoch': 12.0}
{'train_runtime': 1072.8474, 'train_samples_per_second': 25.055, 'train_steps_per_second': 0.783, 'train_loss': 2.3614518665132067, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =      2.413
  train_runtime            = 0:19:18.79
  train_samples_per_second =     25.381
  train_steps_per_second   =      0.797
-------------------------------------------------------------------
Lang1:  zho.rst.sctb    Lang2:  spa.rst.rststb
Saving run to:  runs/full_shot/FullShot=v4_zho.rst.sctb_spa.rst.rststb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 439 examples
read 94 examples
read 159 examples
read 2240 examples
read 383 examples
read 426 examples
Total prediction labels:  30
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (zh): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=30, bias=True)
    )
  )
)
{'train@zho.rst.sctb_loss': 3.190847873687744, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019230769230769232, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.01282051282051282, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.190847873687744, 'train@zho.rst.sctb_runtime': 5.3912, 'train@zho.rst.sctb_samples_per_second': 81.429, 'train@zho.rst.sctb_steps_per_second': 2.597, 'epoch': 1.0}
{'loss': 3.3269, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.214956045150757, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 3.214956045150757, 'eval_runtime': 1.3915, 'eval_samples_per_second': 67.554, 'eval_steps_per_second': 2.156, 'epoch': 1.0}
{'train@zho.rst.sctb_loss': 2.989442825317383, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.989443063735962, 'train@zho.rst.sctb_runtime': 5.4333, 'train@zho.rst.sctb_samples_per_second': 80.797, 'train@zho.rst.sctb_steps_per_second': 2.577, 'epoch': 2.0}
{'loss': 3.1088, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.0233044624328613, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 3.0233054161071777, 'eval_runtime': 1.4001, 'eval_samples_per_second': 67.136, 'eval_steps_per_second': 2.143, 'epoch': 2.0}
{'train@zho.rst.sctb_loss': 2.80854868888855, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.808549165725708, 'train@zho.rst.sctb_runtime': 5.4154, 'train@zho.rst.sctb_samples_per_second': 81.065, 'train@zho.rst.sctb_steps_per_second': 2.585, 'epoch': 3.0}
{'loss': 2.9284, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.851719379425049, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.8517189025878906, 'eval_runtime': 1.3969, 'eval_samples_per_second': 67.292, 'eval_steps_per_second': 2.148, 'epoch': 3.0}
{'train@zho.rst.sctb_loss': 2.6583073139190674, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.6583077907562256, 'train@zho.rst.sctb_runtime': 5.4168, 'train@zho.rst.sctb_samples_per_second': 81.045, 'train@zho.rst.sctb_steps_per_second': 2.585, 'epoch': 4.0}
{'loss': 2.7593, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.711883544921875, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.711883783340454, 'eval_runtime': 1.3854, 'eval_samples_per_second': 67.852, 'eval_steps_per_second': 2.165, 'epoch': 4.0}
{'train@zho.rst.sctb_loss': 2.5414199829101562, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.5414199829101562, 'train@zho.rst.sctb_runtime': 5.4395, 'train@zho.rst.sctb_samples_per_second': 80.705, 'train@zho.rst.sctb_steps_per_second': 2.574, 'epoch': 5.0}
{'loss': 2.6399, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.6067557334899902, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.6067557334899902, 'eval_runtime': 1.3912, 'eval_samples_per_second': 67.565, 'eval_steps_per_second': 2.156, 'epoch': 5.0}
{'train@zho.rst.sctb_loss': 2.4513051509857178, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019230769230769232, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.01282051282051282, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.4513051509857178, 'train@zho.rst.sctb_runtime': 5.4533, 'train@zho.rst.sctb_samples_per_second': 80.502, 'train@zho.rst.sctb_steps_per_second': 2.567, 'epoch': 6.0}
{'loss': 2.5336, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.5304388999938965, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.5304386615753174, 'eval_runtime': 1.4425, 'eval_samples_per_second': 65.163, 'eval_steps_per_second': 2.08, 'epoch': 6.0}
{'train@zho.rst.sctb_loss': 2.3846592903137207, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.33712984054669703, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.02090185676392573, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.028323289613612194, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.039271255060728746, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.384659767150879, 'train@zho.rst.sctb_runtime': 5.4686, 'train@zho.rst.sctb_samples_per_second': 80.277, 'train@zho.rst.sctb_steps_per_second': 2.56, 'epoch': 7.0}
{'loss': 2.4584, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.4761552810668945, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.03185595567867036, 'eval_precision@zho.rst.sctb': 0.044050343249427915, 'eval_recall@zho.rst.sctb': 0.0540828173374613, 'eval_loss@zho.rst.sctb': 2.4761552810668945, 'eval_runtime': 1.3951, 'eval_samples_per_second': 67.377, 'eval_steps_per_second': 2.15, 'epoch': 7.0}
{'train@zho.rst.sctb_loss': 2.341344118118286, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.34851936218678814, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.025216958181679034, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.0387017354230469, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.041436969663357553, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.341343879699707, 'train@zho.rst.sctb_runtime': 5.459, 'train@zho.rst.sctb_samples_per_second': 80.417, 'train@zho.rst.sctb_steps_per_second': 2.565, 'epoch': 8.0}
{'loss': 2.4001, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.442500591278076, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.035909445745511324, 'eval_precision@zho.rst.sctb': 0.043859649122807015, 'eval_recall@zho.rst.sctb': 0.05553405572755418, 'eval_loss@zho.rst.sctb': 2.442500591278076, 'eval_runtime': 1.4018, 'eval_samples_per_second': 67.056, 'eval_steps_per_second': 2.14, 'epoch': 8.0}
{'train@zho.rst.sctb_loss': 2.310901641845703, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.37585421412300685, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.03246782226824, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.040127450346121495, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04629526925849925, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.310900926589966, 'train@zho.rst.sctb_runtime': 6.2694, 'train@zho.rst.sctb_samples_per_second': 70.023, 'train@zho.rst.sctb_steps_per_second': 2.233, 'epoch': 9.0}
{'loss': 2.3547, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.4189138412475586, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.034797738147020446, 'eval_precision@zho.rst.sctb': 0.038202247191011236, 'eval_recall@zho.rst.sctb': 0.05388931888544892, 'eval_loss@zho.rst.sctb': 2.4189140796661377, 'eval_runtime': 1.3867, 'eval_samples_per_second': 67.788, 'eval_steps_per_second': 2.163, 'epoch': 9.0}
{'train@zho.rst.sctb_loss': 2.2909576892852783, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.38496583143507973, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.0347095413967347, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.039115268508889724, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04805612556153291, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.2909581661224365, 'train@zho.rst.sctb_runtime': 5.4395, 'train@zho.rst.sctb_samples_per_second': 80.705, 'train@zho.rst.sctb_steps_per_second': 2.574, 'epoch': 10.0}
{'loss': 2.3298, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.4031896591186523, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.034424296034203154, 'eval_precision@zho.rst.sctb': 0.03258145363408521, 'eval_recall@zho.rst.sctb': 0.05388931888544892, 'eval_loss@zho.rst.sctb': 2.4031896591186523, 'eval_runtime': 1.4205, 'eval_samples_per_second': 66.176, 'eval_steps_per_second': 2.112, 'epoch': 10.0}
{'train@zho.rst.sctb_loss': 2.281216621398926, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.38724373576309795, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.0354920814479638, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03914742568376592, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04860240696578116, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.281216859817505, 'train@zho.rst.sctb_runtime': 5.4294, 'train@zho.rst.sctb_samples_per_second': 80.856, 'train@zho.rst.sctb_steps_per_second': 2.579, 'epoch': 11.0}
{'loss': 2.3126, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.396078109741211, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.03850133809099019, 'eval_precision@zho.rst.sctb': 0.03748470012239902, 'eval_recall@zho.rst.sctb': 0.05698529411764706, 'eval_loss@zho.rst.sctb': 2.39607834815979, 'eval_runtime': 1.4191, 'eval_samples_per_second': 66.24, 'eval_steps_per_second': 2.114, 'epoch': 11.0}
{'train@zho.rst.sctb_loss': 2.278062343597412, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3895216400911162, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.035828013261223805, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03839957510843587, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04900726526537629, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.278062582015991, 'train@zho.rst.sctb_runtime': 5.4809, 'train@zho.rst.sctb_samples_per_second': 80.096, 'train@zho.rst.sctb_steps_per_second': 2.554, 'epoch': 12.0}
{'loss': 2.2951, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.393848180770874, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.03850133809099019, 'eval_precision@zho.rst.sctb': 0.03748470012239902, 'eval_recall@zho.rst.sctb': 0.05698529411764706, 'eval_loss@zho.rst.sctb': 2.393848419189453, 'eval_runtime': 1.3925, 'eval_samples_per_second': 67.506, 'eval_steps_per_second': 2.154, 'epoch': 12.0}
{'train_runtime': 214.1864, 'train_samples_per_second': 24.595, 'train_steps_per_second': 0.784, 'train_loss': 2.6206349418276833, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.6206
  train_runtime            = 0:03:34.18
  train_samples_per_second =     24.595
  train_steps_per_second   =      0.784
{'train@spa.rst.rststb_loss': 2.7078192234039307, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.20669642857142856, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.014616473466625233, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.060823385958156846, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.037027397671648365, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.7078189849853516, 'train@spa.rst.rststb_runtime': 27.0653, 'train@spa.rst.rststb_samples_per_second': 82.763, 'train@spa.rst.rststb_steps_per_second': 2.586, 'epoch': 1.0}
{'loss': 3.078, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.7286014556884766, 'eval_accuracy@spa.rst.rststb': 0.206266318537859, 'eval_f1@spa.rst.rststb': 0.014869188782232262, 'eval_precision@spa.rst.rststb': 0.00896810080599387, 'eval_recall@spa.rst.rststb': 0.043478260869565216, 'eval_loss@spa.rst.rststb': 2.7286016941070557, 'eval_runtime': 4.8762, 'eval_samples_per_second': 78.545, 'eval_steps_per_second': 2.461, 'epoch': 1.0}
{'train@spa.rst.rststb_loss': 2.513122081756592, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.29151785714285716, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.044648966010147395, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.044079120112536375, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.060704689580977425, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.513122320175171, 'train@spa.rst.rststb_runtime': 27.0561, 'train@spa.rst.rststb_samples_per_second': 82.791, 'train@spa.rst.rststb_steps_per_second': 2.587, 'epoch': 2.0}
{'loss': 2.6344, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.610412120819092, 'eval_accuracy@spa.rst.rststb': 0.25326370757180156, 'eval_f1@spa.rst.rststb': 0.04248181486033149, 'eval_precision@spa.rst.rststb': 0.04861229888596221, 'eval_recall@spa.rst.rststb': 0.0610792537537459, 'eval_loss@spa.rst.rststb': 2.610412359237671, 'eval_runtime': 4.8991, 'eval_samples_per_second': 78.178, 'eval_steps_per_second': 2.449, 'epoch': 2.0}
{'train@spa.rst.rststb_loss': 2.403150796890259, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.3209821428571429, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.05200628209947562, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.06148524449206289, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.07217967272017055, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.403151035308838, 'train@spa.rst.rststb_runtime': 27.0304, 'train@spa.rst.rststb_samples_per_second': 82.87, 'train@spa.rst.rststb_steps_per_second': 2.59, 'epoch': 3.0}
{'loss': 2.4952, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.534620523452759, 'eval_accuracy@spa.rst.rststb': 0.2793733681462141, 'eval_f1@spa.rst.rststb': 0.051987809553772545, 'eval_precision@spa.rst.rststb': 0.08206167342105702, 'eval_recall@spa.rst.rststb': 0.07464489444791567, 'eval_loss@spa.rst.rststb': 2.534620523452759, 'eval_runtime': 4.8927, 'eval_samples_per_second': 78.28, 'eval_steps_per_second': 2.453, 'epoch': 3.0}
{'train@spa.rst.rststb_loss': 2.312485933303833, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.34151785714285715, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.07164303512672232, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.07662323901892773, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.08714792263230801, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.312485933303833, 'train@spa.rst.rststb_runtime': 27.0445, 'train@spa.rst.rststb_samples_per_second': 82.827, 'train@spa.rst.rststb_steps_per_second': 2.588, 'epoch': 4.0}
{'loss': 2.4093, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.46718168258667, 'eval_accuracy@spa.rst.rststb': 0.31070496083550914, 'eval_f1@spa.rst.rststb': 0.07490235243540945, 'eval_precision@spa.rst.rststb': 0.10452295732481229, 'eval_recall@spa.rst.rststb': 0.0913161652088783, 'eval_loss@spa.rst.rststb': 2.46718168258667, 'eval_runtime': 4.86, 'eval_samples_per_second': 78.806, 'eval_steps_per_second': 2.469, 'epoch': 4.0}
{'train@spa.rst.rststb_loss': 2.2336249351501465, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.3808035714285714, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.08889390823697808, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.0786402572928157, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.1110568016601897, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.2336249351501465, 'train@spa.rst.rststb_runtime': 27.0272, 'train@spa.rst.rststb_samples_per_second': 82.879, 'train@spa.rst.rststb_steps_per_second': 2.59, 'epoch': 5.0}
{'loss': 2.3221, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.4107038974761963, 'eval_accuracy@spa.rst.rststb': 0.34986945169712796, 'eval_f1@spa.rst.rststb': 0.10150857303664396, 'eval_precision@spa.rst.rststb': 0.09202379431445518, 'eval_recall@spa.rst.rststb': 0.12540736766744864, 'eval_loss@spa.rst.rststb': 2.4107038974761963, 'eval_runtime': 4.8529, 'eval_samples_per_second': 78.921, 'eval_steps_per_second': 2.473, 'epoch': 5.0}
{'train@spa.rst.rststb_loss': 2.171553373336792, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.39151785714285714, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.09403970672011153, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.09273748291574939, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.1183725731091912, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.171553373336792, 'train@spa.rst.rststb_runtime': 27.0673, 'train@spa.rst.rststb_samples_per_second': 82.757, 'train@spa.rst.rststb_steps_per_second': 2.586, 'epoch': 6.0}
{'loss': 2.2555, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.362046957015991, 'eval_accuracy@spa.rst.rststb': 0.3577023498694517, 'eval_f1@spa.rst.rststb': 0.10409527525878215, 'eval_precision@spa.rst.rststb': 0.09090127689871937, 'eval_recall@spa.rst.rststb': 0.13384916880407233, 'eval_loss@spa.rst.rststb': 2.362046957015991, 'eval_runtime': 4.8793, 'eval_samples_per_second': 78.495, 'eval_steps_per_second': 2.459, 'epoch': 6.0}
{'train@spa.rst.rststb_loss': 2.1191279888153076, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4125, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10354510929679664, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.10169901339598662, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13012385737123328, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.1191279888153076, 'train@spa.rst.rststb_runtime': 27.0823, 'train@spa.rst.rststb_samples_per_second': 82.711, 'train@spa.rst.rststb_steps_per_second': 2.585, 'epoch': 7.0}
{'loss': 2.2038, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.3246824741363525, 'eval_accuracy@spa.rst.rststb': 0.36553524804177545, 'eval_f1@spa.rst.rststb': 0.10215059053376806, 'eval_precision@spa.rst.rststb': 0.08721282917880271, 'eval_recall@spa.rst.rststb': 0.13794290115867425, 'eval_loss@spa.rst.rststb': 2.3246822357177734, 'eval_runtime': 4.93, 'eval_samples_per_second': 77.688, 'eval_steps_per_second': 2.434, 'epoch': 7.0}
{'train@spa.rst.rststb_loss': 2.079822540283203, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.41517857142857145, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.105727507486242, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.1390185507208216, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13155987572802547, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0798227787017822, 'train@spa.rst.rststb_runtime': 27.1307, 'train@spa.rst.rststb_samples_per_second': 82.563, 'train@spa.rst.rststb_steps_per_second': 2.58, 'epoch': 8.0}
{'loss': 2.1528, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.295898199081421, 'eval_accuracy@spa.rst.rststb': 0.37597911227154046, 'eval_f1@spa.rst.rststb': 0.10583390413063205, 'eval_precision@spa.rst.rststb': 0.09268752442770761, 'eval_recall@spa.rst.rststb': 0.1417816970156401, 'eval_loss@spa.rst.rststb': 2.295897960662842, 'eval_runtime': 4.904, 'eval_samples_per_second': 78.1, 'eval_steps_per_second': 2.447, 'epoch': 8.0}
{'train@spa.rst.rststb_loss': 2.050691843032837, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4191964285714286, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10679360896841265, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.1351638629354588, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13549920318812628, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.050691843032837, 'train@spa.rst.rststb_runtime': 27.075, 'train@spa.rst.rststb_samples_per_second': 82.733, 'train@spa.rst.rststb_steps_per_second': 2.585, 'epoch': 9.0}
{'loss': 2.1205, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.2739086151123047, 'eval_accuracy@spa.rst.rststb': 0.3785900783289817, 'eval_f1@spa.rst.rststb': 0.10486440595479808, 'eval_precision@spa.rst.rststb': 0.08910751902645785, 'eval_recall@spa.rst.rststb': 0.1435316351150636, 'eval_loss@spa.rst.rststb': 2.2739083766937256, 'eval_runtime': 4.86, 'eval_samples_per_second': 78.806, 'eval_steps_per_second': 2.469, 'epoch': 9.0}
{'train@spa.rst.rststb_loss': 2.029991865158081, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4232142857142857, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.1087414015899147, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.11758022676263662, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13615438768443786, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.029991865158081, 'train@spa.rst.rststb_runtime': 27.0549, 'train@spa.rst.rststb_samples_per_second': 82.795, 'train@spa.rst.rststb_steps_per_second': 2.587, 'epoch': 10.0}
{'loss': 2.0903, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.259408712387085, 'eval_accuracy@spa.rst.rststb': 0.3838120104438642, 'eval_f1@spa.rst.rststb': 0.10665682352734329, 'eval_precision@spa.rst.rststb': 0.09155032443075921, 'eval_recall@spa.rst.rststb': 0.14473092211429966, 'eval_loss@spa.rst.rststb': 2.259408950805664, 'eval_runtime': 4.8792, 'eval_samples_per_second': 78.496, 'eval_steps_per_second': 2.459, 'epoch': 10.0}
{'train@spa.rst.rststb_loss': 2.0180509090423584, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.425, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10946789185603564, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.11790968499349992, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13691578398407778, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0180509090423584, 'train@spa.rst.rststb_runtime': 27.0772, 'train@spa.rst.rststb_samples_per_second': 82.727, 'train@spa.rst.rststb_steps_per_second': 2.585, 'epoch': 11.0}
{'loss': 2.0768, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.2488532066345215, 'eval_accuracy@spa.rst.rststb': 0.38642297650130547, 'eval_f1@spa.rst.rststb': 0.10751477407008184, 'eval_precision@spa.rst.rststb': 0.09259217668000952, 'eval_recall@spa.rst.rststb': 0.1453798513810096, 'eval_loss@spa.rst.rststb': 2.2488529682159424, 'eval_runtime': 4.8634, 'eval_samples_per_second': 78.751, 'eval_steps_per_second': 2.467, 'epoch': 11.0}
{'train@spa.rst.rststb_loss': 2.014207601547241, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.42589285714285713, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10944883602482498, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.11808107947359785, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13699187618872144, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.014207601547241, 'train@spa.rst.rststb_runtime': 27.0479, 'train@spa.rst.rststb_samples_per_second': 82.816, 'train@spa.rst.rststb_steps_per_second': 2.588, 'epoch': 12.0}
{'loss': 2.0653, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.2457966804504395, 'eval_accuracy@spa.rst.rststb': 0.38903394255874674, 'eval_f1@spa.rst.rststb': 0.1082505835559536, 'eval_precision@spa.rst.rststb': 0.09324046968890744, 'eval_recall@spa.rst.rststb': 0.14602878064771954, 'eval_loss@spa.rst.rststb': 2.2457964420318604, 'eval_runtime': 4.8874, 'eval_samples_per_second': 78.364, 'eval_steps_per_second': 2.455, 'epoch': 12.0}
{'train_runtime': 1067.1097, 'train_samples_per_second': 25.19, 'train_steps_per_second': 0.787, 'train_loss': 2.3253346216110957, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.6206
  train_runtime            = 0:03:34.18
  train_samples_per_second =     24.595
  train_steps_per_second   =      0.784

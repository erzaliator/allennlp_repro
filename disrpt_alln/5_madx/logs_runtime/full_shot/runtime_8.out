-------------------------------------------------------------------
Lang1:  deu.rst.pcc    Lang2:  por.rst.cstn
Saving run to:  runs/full_shot/FullShot=v4_deu.rst.pcc_por.rst.cstn_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2164 examples
read 241 examples
read 260 examples
read 4148 examples
read 573 examples
read 272 examples
Total prediction labels:  42
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (pt): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (pt): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=42, bias=True)
    )
  )
)
{'train@deu.rst.pcc_loss': 3.2224459648132324, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.11414048059149723, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.025409493764911865, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.03933641261377003, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.048482217828848526, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.2224462032318115, 'train@deu.rst.pcc_runtime': 26.7103, 'train@deu.rst.pcc_samples_per_second': 81.017, 'train@deu.rst.pcc_steps_per_second': 2.546, 'epoch': 1.0}
{'loss': 3.5046, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.2518255710601807, 'eval_accuracy@deu.rst.pcc': 0.11618257261410789, 'eval_f1@deu.rst.pcc': 0.023980380377839543, 'eval_precision@deu.rst.pcc': 0.06701923076923076, 'eval_recall@deu.rst.pcc': 0.0479306573056573, 'eval_loss@deu.rst.pcc': 3.2518255710601807, 'eval_runtime': 3.2857, 'eval_samples_per_second': 73.348, 'eval_steps_per_second': 2.435, 'epoch': 1.0}
{'train@deu.rst.pcc_loss': 2.9857518672943115, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.12476894639556377, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.028735446769349325, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.08873953909014046, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.051758154848002426, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.9857513904571533, 'train@deu.rst.pcc_runtime': 26.572, 'train@deu.rst.pcc_samples_per_second': 81.439, 'train@deu.rst.pcc_steps_per_second': 2.559, 'epoch': 2.0}
{'loss': 3.1099, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.0277609825134277, 'eval_accuracy@deu.rst.pcc': 0.11618257261410789, 'eval_f1@deu.rst.pcc': 0.017740299823633156, 'eval_precision@deu.rst.pcc': 0.03376327603675366, 'eval_recall@deu.rst.pcc': 0.04421041921041922, 'eval_loss@deu.rst.pcc': 3.027761220932007, 'eval_runtime': 3.3229, 'eval_samples_per_second': 72.526, 'eval_steps_per_second': 2.408, 'epoch': 2.0}
{'train@deu.rst.pcc_loss': 2.90110445022583, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.14695009242144177, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.04679231722291441, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.05659651580481109, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.06769770408626204, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.901104211807251, 'train@deu.rst.pcc_runtime': 26.6297, 'train@deu.rst.pcc_samples_per_second': 81.263, 'train@deu.rst.pcc_steps_per_second': 2.554, 'epoch': 3.0}
{'loss': 2.9786, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.957491397857666, 'eval_accuracy@deu.rst.pcc': 0.12448132780082988, 'eval_f1@deu.rst.pcc': 0.02656900913869441, 'eval_precision@deu.rst.pcc': 0.057188055653289875, 'eval_recall@deu.rst.pcc': 0.05825193325193326, 'eval_loss@deu.rst.pcc': 2.957491397857666, 'eval_runtime': 3.3134, 'eval_samples_per_second': 72.735, 'eval_steps_per_second': 2.414, 'epoch': 3.0}
{'train@deu.rst.pcc_loss': 2.846290349960327, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.1866913123844732, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.06516836829878053, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.07131586151550708, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.10184990186847272, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.846290349960327, 'train@deu.rst.pcc_runtime': 26.6446, 'train@deu.rst.pcc_samples_per_second': 81.217, 'train@deu.rst.pcc_steps_per_second': 2.552, 'epoch': 4.0}
{'loss': 2.9112, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.9144699573516846, 'eval_accuracy@deu.rst.pcc': 0.17012448132780084, 'eval_f1@deu.rst.pcc': 0.05894731184238553, 'eval_precision@deu.rst.pcc': 0.0829889223018994, 'eval_recall@deu.rst.pcc': 0.10177553927553928, 'eval_loss@deu.rst.pcc': 2.9144699573516846, 'eval_runtime': 3.3264, 'eval_samples_per_second': 72.45, 'eval_steps_per_second': 2.405, 'epoch': 4.0}
{'train@deu.rst.pcc_loss': 2.805413007736206, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.19824399260628467, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.06694179421869441, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.08180767990308166, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.11236857984697994, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.805413246154785, 'train@deu.rst.pcc_runtime': 26.616, 'train@deu.rst.pcc_samples_per_second': 81.304, 'train@deu.rst.pcc_steps_per_second': 2.555, 'epoch': 5.0}
{'loss': 2.8596, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.8813071250915527, 'eval_accuracy@deu.rst.pcc': 0.17012448132780084, 'eval_f1@deu.rst.pcc': 0.05532780539825479, 'eval_precision@deu.rst.pcc': 0.07759746973456651, 'eval_recall@deu.rst.pcc': 0.11417633292633293, 'eval_loss@deu.rst.pcc': 2.881307363510132, 'eval_runtime': 3.3006, 'eval_samples_per_second': 73.017, 'eval_steps_per_second': 2.424, 'epoch': 5.0}
{'train@deu.rst.pcc_loss': 2.7722575664520264, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.1963955637707948, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.0656169887116416, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.0696737783358828, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.11415233510364042, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7722573280334473, 'train@deu.rst.pcc_runtime': 26.6474, 'train@deu.rst.pcc_samples_per_second': 81.209, 'train@deu.rst.pcc_steps_per_second': 2.552, 'epoch': 6.0}
{'loss': 2.8236, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.8525850772857666, 'eval_accuracy@deu.rst.pcc': 0.17842323651452283, 'eval_f1@deu.rst.pcc': 0.06440705854175653, 'eval_precision@deu.rst.pcc': 0.0909525641357593, 'eval_recall@deu.rst.pcc': 0.12224900644018293, 'eval_loss@deu.rst.pcc': 2.8525853157043457, 'eval_runtime': 3.3302, 'eval_samples_per_second': 72.368, 'eval_steps_per_second': 2.402, 'epoch': 6.0}
{'train@deu.rst.pcc_loss': 2.747750759124756, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2000924214417745, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.06717999258006899, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.07192939835136382, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.11603860410460667, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7477505207061768, 'train@deu.rst.pcc_runtime': 26.6254, 'train@deu.rst.pcc_samples_per_second': 81.276, 'train@deu.rst.pcc_steps_per_second': 2.554, 'epoch': 7.0}
{'loss': 2.7981, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.8338708877563477, 'eval_accuracy@deu.rst.pcc': 0.1825726141078838, 'eval_f1@deu.rst.pcc': 0.06437206785363299, 'eval_precision@deu.rst.pcc': 0.09143545966915533, 'eval_recall@deu.rst.pcc': 0.12284678828796476, 'eval_loss@deu.rst.pcc': 2.8338706493377686, 'eval_runtime': 3.3166, 'eval_samples_per_second': 72.665, 'eval_steps_per_second': 2.412, 'epoch': 7.0}
{'train@deu.rst.pcc_loss': 2.728861093521118, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.19916820702402957, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.06715248344602229, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.10967736899858221, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.11566884817000922, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7288613319396973, 'train@deu.rst.pcc_runtime': 26.583, 'train@deu.rst.pcc_samples_per_second': 81.405, 'train@deu.rst.pcc_steps_per_second': 2.558, 'epoch': 8.0}
{'loss': 2.7731, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.820420980453491, 'eval_accuracy@deu.rst.pcc': 0.17842323651452283, 'eval_f1@deu.rst.pcc': 0.06318409377899069, 'eval_precision@deu.rst.pcc': 0.09113488717823394, 'eval_recall@deu.rst.pcc': 0.12231260025377673, 'eval_loss@deu.rst.pcc': 2.8204212188720703, 'eval_runtime': 3.3511, 'eval_samples_per_second': 71.917, 'eval_steps_per_second': 2.387, 'epoch': 8.0}
{'train@deu.rst.pcc_loss': 2.7136714458465576, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.20055452865064696, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.0690638708254386, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.11096930682020903, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.1169054674821802, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7136712074279785, 'train@deu.rst.pcc_runtime': 26.6101, 'train@deu.rst.pcc_samples_per_second': 81.322, 'train@deu.rst.pcc_steps_per_second': 2.555, 'epoch': 9.0}
{'loss': 2.761, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.8120973110198975, 'eval_accuracy@deu.rst.pcc': 0.1908713692946058, 'eval_f1@deu.rst.pcc': 0.06785399256052227, 'eval_precision@deu.rst.pcc': 0.0925078697872448, 'eval_recall@deu.rst.pcc': 0.12843032512150157, 'eval_loss@deu.rst.pcc': 2.8120973110198975, 'eval_runtime': 3.3024, 'eval_samples_per_second': 72.977, 'eval_steps_per_second': 2.422, 'epoch': 9.0}
{'train@deu.rst.pcc_loss': 2.70353364944458, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.20055452865064696, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.07003692736286768, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.09016070415880904, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.1172459329609933, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.70353364944458, 'train@deu.rst.pcc_runtime': 26.6219, 'train@deu.rst.pcc_samples_per_second': 81.286, 'train@deu.rst.pcc_steps_per_second': 2.554, 'epoch': 10.0}
{'loss': 2.7484, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.801863193511963, 'eval_accuracy@deu.rst.pcc': 0.18672199170124482, 'eval_f1@deu.rst.pcc': 0.06573349437299263, 'eval_precision@deu.rst.pcc': 0.09081705331705331, 'eval_recall@deu.rst.pcc': 0.12611551030668677, 'eval_loss@deu.rst.pcc': 2.8018627166748047, 'eval_runtime': 3.3163, 'eval_samples_per_second': 72.672, 'eval_steps_per_second': 2.412, 'epoch': 10.0}
{'train@deu.rst.pcc_loss': 2.6973843574523926, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.20471349353049909, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.07154548117105265, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.08744572556587146, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.11907134501727452, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6973841190338135, 'train@deu.rst.pcc_runtime': 26.6064, 'train@deu.rst.pcc_samples_per_second': 81.334, 'train@deu.rst.pcc_steps_per_second': 2.556, 'epoch': 11.0}
{'loss': 2.7335, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.7972609996795654, 'eval_accuracy@deu.rst.pcc': 0.18672199170124482, 'eval_f1@deu.rst.pcc': 0.0662478256248731, 'eval_precision@deu.rst.pcc': 0.09124162425504014, 'eval_recall@deu.rst.pcc': 0.12522519691637338, 'eval_loss@deu.rst.pcc': 2.7972609996795654, 'eval_runtime': 3.3034, 'eval_samples_per_second': 72.956, 'eval_steps_per_second': 2.422, 'epoch': 11.0}
{'train@deu.rst.pcc_loss': 2.695305109024048, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.20194085027726433, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.07035571172091368, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.08469762181103496, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.118020794600297, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.695305347442627, 'train@deu.rst.pcc_runtime': 26.5865, 'train@deu.rst.pcc_samples_per_second': 81.395, 'train@deu.rst.pcc_steps_per_second': 2.558, 'epoch': 12.0}
{'loss': 2.7248, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.7951595783233643, 'eval_accuracy@deu.rst.pcc': 0.18672199170124482, 'eval_f1@deu.rst.pcc': 0.06625496656373694, 'eval_precision@deu.rst.pcc': 0.091137102014295, 'eval_recall@deu.rst.pcc': 0.12611551030668677, 'eval_loss@deu.rst.pcc': 2.795159339904785, 'eval_runtime': 3.3226, 'eval_samples_per_second': 72.533, 'eval_steps_per_second': 2.408, 'epoch': 12.0}
{'train_runtime': 1024.8414, 'train_samples_per_second': 25.339, 'train_steps_per_second': 0.796, 'train_loss': 2.8938726911357806, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.8939
  train_runtime            = 0:17:04.84
  train_samples_per_second =     25.339
  train_steps_per_second   =      0.796
{'train@por.rst.cstn_loss': 2.3702268600463867, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.35053037608486015, 'train@por.rst.cstn_f1@por.rst.cstn': 0.047538479171570946, 'train@por.rst.cstn_precision@por.rst.cstn': 0.08104060691189346, 'train@por.rst.cstn_recall@por.rst.cstn': 0.05468902692430017, 'train@por.rst.cstn_loss@por.rst.cstn': 2.3702268600463867, 'train@por.rst.cstn_runtime': 50.6302, 'train@por.rst.cstn_samples_per_second': 81.927, 'train@por.rst.cstn_steps_per_second': 2.568, 'epoch': 1.0}
{'loss': 2.7888, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.4711990356445312, 'eval_accuracy@por.rst.cstn': 0.3054101221640489, 'eval_f1@por.rst.cstn': 0.0524236886531932, 'eval_precision@por.rst.cstn': 0.1113530602686328, 'eval_recall@por.rst.cstn': 0.06496953396786974, 'eval_loss@por.rst.cstn': 2.4711990356445312, 'eval_runtime': 7.3104, 'eval_samples_per_second': 78.381, 'eval_steps_per_second': 2.462, 'epoch': 1.0}
{'train@por.rst.cstn_loss': 2.1007883548736572, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.43804243008678884, 'train@por.rst.cstn_f1@por.rst.cstn': 0.06886207450386089, 'train@por.rst.cstn_precision@por.rst.cstn': 0.07069879243159227, 'train@por.rst.cstn_recall@por.rst.cstn': 0.08214874239278241, 'train@por.rst.cstn_loss@por.rst.cstn': 2.100788116455078, 'train@por.rst.cstn_runtime': 50.7104, 'train@por.rst.cstn_samples_per_second': 81.798, 'train@por.rst.cstn_steps_per_second': 2.564, 'epoch': 2.0}
{'loss': 2.2826, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.2414920330047607, 'eval_accuracy@por.rst.cstn': 0.3717277486910995, 'eval_f1@por.rst.cstn': 0.09542703457979766, 'eval_precision@por.rst.cstn': 0.0936534935340668, 'eval_recall@por.rst.cstn': 0.11897715002562101, 'eval_loss@por.rst.cstn': 2.2414917945861816, 'eval_runtime': 7.3594, 'eval_samples_per_second': 77.86, 'eval_steps_per_second': 2.446, 'epoch': 2.0}
{'train@por.rst.cstn_loss': 1.9247170686721802, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.47565091610414656, 'train@por.rst.cstn_f1@por.rst.cstn': 0.07673684347652912, 'train@por.rst.cstn_precision@por.rst.cstn': 0.07264601185312336, 'train@por.rst.cstn_recall@por.rst.cstn': 0.09198939151508001, 'train@por.rst.cstn_loss@por.rst.cstn': 1.9247171878814697, 'train@por.rst.cstn_runtime': 50.699, 'train@por.rst.cstn_samples_per_second': 81.816, 'train@por.rst.cstn_steps_per_second': 2.564, 'epoch': 3.0}
{'loss': 2.0733, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.076159715652466, 'eval_accuracy@por.rst.cstn': 0.39965095986038396, 'eval_f1@por.rst.cstn': 0.10058800386794248, 'eval_precision@por.rst.cstn': 0.09240213191826095, 'eval_recall@por.rst.cstn': 0.13121228331338586, 'eval_loss@por.rst.cstn': 2.0761594772338867, 'eval_runtime': 7.3193, 'eval_samples_per_second': 78.286, 'eval_steps_per_second': 2.459, 'epoch': 3.0}
{'train@por.rst.cstn_loss': 1.8151999711990356, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5069913211186113, 'train@por.rst.cstn_f1@por.rst.cstn': 0.08822723186984588, 'train@por.rst.cstn_precision@por.rst.cstn': 0.10376631295314427, 'train@por.rst.cstn_recall@por.rst.cstn': 0.10069834238585276, 'train@por.rst.cstn_loss@por.rst.cstn': 1.8152002096176147, 'train@por.rst.cstn_runtime': 50.6706, 'train@por.rst.cstn_samples_per_second': 81.862, 'train@por.rst.cstn_steps_per_second': 2.566, 'epoch': 4.0}
{'loss': 1.9367, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.9706119298934937, 'eval_accuracy@por.rst.cstn': 0.42757417102966844, 'eval_f1@por.rst.cstn': 0.11386254925529884, 'eval_precision@por.rst.cstn': 0.11981658115294712, 'eval_recall@por.rst.cstn': 0.14410332634796957, 'eval_loss@por.rst.cstn': 1.9706121683120728, 'eval_runtime': 7.3333, 'eval_samples_per_second': 78.136, 'eval_steps_per_second': 2.455, 'epoch': 4.0}
{'train@por.rst.cstn_loss': 1.7289698123931885, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5262777242044359, 'train@por.rst.cstn_f1@por.rst.cstn': 0.10507798022386357, 'train@por.rst.cstn_precision@por.rst.cstn': 0.12593895045859627, 'train@por.rst.cstn_recall@por.rst.cstn': 0.11915750276897066, 'train@por.rst.cstn_loss@por.rst.cstn': 1.728969693183899, 'train@por.rst.cstn_runtime': 50.7507, 'train@por.rst.cstn_samples_per_second': 81.733, 'train@por.rst.cstn_steps_per_second': 2.562, 'epoch': 5.0}
{'loss': 1.8352, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.8753745555877686, 'eval_accuracy@por.rst.cstn': 0.45200698080279234, 'eval_f1@por.rst.cstn': 0.14847546303668555, 'eval_precision@por.rst.cstn': 0.17129787063655746, 'eval_recall@por.rst.cstn': 0.17333987291861225, 'eval_loss@por.rst.cstn': 1.875374674797058, 'eval_runtime': 7.3648, 'eval_samples_per_second': 77.803, 'eval_steps_per_second': 2.444, 'epoch': 5.0}
{'train@por.rst.cstn_loss': 1.6686196327209473, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5453230472516876, 'train@por.rst.cstn_f1@por.rst.cstn': 0.11195387725851305, 'train@por.rst.cstn_precision@por.rst.cstn': 0.12858429177352032, 'train@por.rst.cstn_recall@por.rst.cstn': 0.12757258744460034, 'train@por.rst.cstn_loss@por.rst.cstn': 1.6686197519302368, 'train@por.rst.cstn_runtime': 50.6952, 'train@por.rst.cstn_samples_per_second': 81.822, 'train@por.rst.cstn_steps_per_second': 2.564, 'epoch': 6.0}
{'loss': 1.7625, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.8135426044464111, 'eval_accuracy@por.rst.cstn': 0.46596858638743455, 'eval_f1@por.rst.cstn': 0.15890191917762556, 'eval_precision@por.rst.cstn': 0.17788138005866097, 'eval_recall@por.rst.cstn': 0.1833079749919775, 'eval_loss@por.rst.cstn': 1.8135426044464111, 'eval_runtime': 7.33, 'eval_samples_per_second': 78.172, 'eval_steps_per_second': 2.456, 'epoch': 6.0}
{'train@por.rst.cstn_loss': 1.6255781650543213, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5561716489874639, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1218270080092858, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13955436757091383, 'train@por.rst.cstn_recall@por.rst.cstn': 0.13473914366593082, 'train@por.rst.cstn_loss@por.rst.cstn': 1.6255781650543213, 'train@por.rst.cstn_runtime': 50.6549, 'train@por.rst.cstn_samples_per_second': 81.887, 'train@por.rst.cstn_steps_per_second': 2.566, 'epoch': 7.0}
{'loss': 1.7073, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.7705755233764648, 'eval_accuracy@por.rst.cstn': 0.47469458987783597, 'eval_f1@por.rst.cstn': 0.16900528829339548, 'eval_precision@por.rst.cstn': 0.1657204302882436, 'eval_recall@por.rst.cstn': 0.19245246005179437, 'eval_loss@por.rst.cstn': 1.7705755233764648, 'eval_runtime': 7.3389, 'eval_samples_per_second': 78.078, 'eval_steps_per_second': 2.453, 'epoch': 7.0}
{'train@por.rst.cstn_loss': 1.5963705778121948, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5631629701060752, 'train@por.rst.cstn_f1@por.rst.cstn': 0.12649338247169933, 'train@por.rst.cstn_precision@por.rst.cstn': 0.14074555834665847, 'train@por.rst.cstn_recall@por.rst.cstn': 0.13924400810062498, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5963703393936157, 'train@por.rst.cstn_runtime': 50.7399, 'train@por.rst.cstn_samples_per_second': 81.75, 'train@por.rst.cstn_steps_per_second': 2.562, 'epoch': 8.0}
{'loss': 1.6775, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.7419989109039307, 'eval_accuracy@por.rst.cstn': 0.48342059336823734, 'eval_f1@por.rst.cstn': 0.17395393793354336, 'eval_precision@por.rst.cstn': 0.16748804378845306, 'eval_recall@por.rst.cstn': 0.19700113410847758, 'eval_loss@por.rst.cstn': 1.7419987916946411, 'eval_runtime': 7.3217, 'eval_samples_per_second': 78.261, 'eval_steps_per_second': 2.458, 'epoch': 8.0}
{'train@por.rst.cstn_loss': 1.577857494354248, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5665380906460945, 'train@por.rst.cstn_f1@por.rst.cstn': 0.12882194001394465, 'train@por.rst.cstn_precision@por.rst.cstn': 0.1429618932201718, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14085980370812312, 'train@por.rst.cstn_loss@por.rst.cstn': 1.577857494354248, 'train@por.rst.cstn_runtime': 50.6397, 'train@por.rst.cstn_samples_per_second': 81.912, 'train@por.rst.cstn_steps_per_second': 2.567, 'epoch': 9.0}
{'loss': 1.6493, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.7237533330917358, 'eval_accuracy@por.rst.cstn': 0.4869109947643979, 'eval_f1@por.rst.cstn': 0.17699334017959922, 'eval_precision@por.rst.cstn': 0.1727063256455548, 'eval_recall@por.rst.cstn': 0.197761929377282, 'eval_loss@por.rst.cstn': 1.7237533330917358, 'eval_runtime': 7.3428, 'eval_samples_per_second': 78.035, 'eval_steps_per_second': 2.451, 'epoch': 9.0}
{'train@por.rst.cstn_loss': 1.5601251125335693, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5754580520732884, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1356110035425617, 'train@por.rst.cstn_precision@por.rst.cstn': 0.14752210047125094, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14779045641253433, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5601249933242798, 'train@por.rst.cstn_runtime': 50.6126, 'train@por.rst.cstn_samples_per_second': 81.956, 'train@por.rst.cstn_steps_per_second': 2.569, 'epoch': 10.0}
{'loss': 1.6256, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.7029039859771729, 'eval_accuracy@por.rst.cstn': 0.493891797556719, 'eval_f1@por.rst.cstn': 0.17892161346726987, 'eval_precision@por.rst.cstn': 0.18941975665761052, 'eval_recall@por.rst.cstn': 0.2036350645748399, 'eval_loss@por.rst.cstn': 1.7029038667678833, 'eval_runtime': 7.336, 'eval_samples_per_second': 78.108, 'eval_steps_per_second': 2.454, 'epoch': 10.0}
{'train@por.rst.cstn_loss': 1.552450180053711, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.577145612343298, 'train@por.rst.cstn_f1@por.rst.cstn': 0.13686981819605992, 'train@por.rst.cstn_precision@por.rst.cstn': 0.14945955343890285, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14896467257346752, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5524500608444214, 'train@por.rst.cstn_runtime': 50.6083, 'train@por.rst.cstn_samples_per_second': 81.963, 'train@por.rst.cstn_steps_per_second': 2.569, 'epoch': 11.0}
{'loss': 1.6157, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.6966631412506104, 'eval_accuracy@por.rst.cstn': 0.4956369982547993, 'eval_f1@por.rst.cstn': 0.18317246458288386, 'eval_precision@por.rst.cstn': 0.19333915149984107, 'eval_recall@por.rst.cstn': 0.20742294336271866, 'eval_loss@por.rst.cstn': 1.6966631412506104, 'eval_runtime': 7.3221, 'eval_samples_per_second': 78.256, 'eval_steps_per_second': 2.458, 'epoch': 11.0}
{'train@por.rst.cstn_loss': 1.5498484373092651, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5761812921890067, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1369467428702923, 'train@por.rst.cstn_precision@por.rst.cstn': 0.14873301168002145, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14861808648946367, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5498483180999756, 'train@por.rst.cstn_runtime': 50.6747, 'train@por.rst.cstn_samples_per_second': 81.855, 'train@por.rst.cstn_steps_per_second': 2.565, 'epoch': 12.0}
{'loss': 1.6103, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.6935622692108154, 'eval_accuracy@por.rst.cstn': 0.49912739965095987, 'eval_f1@por.rst.cstn': 0.1855615876172686, 'eval_precision@por.rst.cstn': 0.1974807301999734, 'eval_recall@por.rst.cstn': 0.20798759610128448, 'eval_loss@por.rst.cstn': 1.6935622692108154, 'eval_runtime': 7.3307, 'eval_samples_per_second': 78.164, 'eval_steps_per_second': 2.455, 'epoch': 12.0}
{'train_runtime': 1975.1238, 'train_samples_per_second': 25.201, 'train_steps_per_second': 0.79, 'train_loss': 1.8803944612160708, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.8939
  train_runtime            = 0:17:04.84
  train_samples_per_second =     25.339
  train_steps_per_second   =      0.796
-------------------------------------------------------------------
Lang1:  eng.pdtb.pdtb    Lang2:  por.rst.cstn
Saving run to:  runs/full_shot/FullShot=v4_eng.pdtb.pdtb_por.rst.cstn_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 43920 examples
read 1674 examples
read 2257 examples
read 4148 examples
read 573 examples
read 272 examples
Total prediction labels:  55
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (pt): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (pt): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=55, bias=True)
    )
  )
)
{'train@eng.pdtb.pdtb_loss': 1.2877060174942017, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.5927595628415301, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.2574919126613206, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.32151183649889786, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.25413523536836796, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.2877060174942017, 'train@eng.pdtb.pdtb_runtime': 524.5686, 'train@eng.pdtb.pdtb_samples_per_second': 83.726, 'train@eng.pdtb.pdtb_steps_per_second': 2.617, 'epoch': 1.0}
{'loss': 1.8919, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.21375572681427, 'eval_accuracy@eng.pdtb.pdtb': 0.6272401433691757, 'eval_f1@eng.pdtb.pdtb': 0.30398689497821485, 'eval_precision@eng.pdtb.pdtb': 0.33257573023527237, 'eval_recall@eng.pdtb.pdtb': 0.3052263593057684, 'eval_loss@eng.pdtb.pdtb': 1.21375572681427, 'eval_runtime': 20.4707, 'eval_samples_per_second': 81.775, 'eval_steps_per_second': 2.589, 'epoch': 1.0}
{'train@eng.pdtb.pdtb_loss': 1.109703779220581, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6371584699453552, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.3397669804812706, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.4592046218981524, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.3312318462617417, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.109703779220581, 'train@eng.pdtb.pdtb_runtime': 524.3242, 'train@eng.pdtb.pdtb_samples_per_second': 83.765, 'train@eng.pdtb.pdtb_steps_per_second': 2.619, 'epoch': 2.0}
{'loss': 1.2382, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.050174355506897, 'eval_accuracy@eng.pdtb.pdtb': 0.6600955794504182, 'eval_f1@eng.pdtb.pdtb': 0.3904851021699319, 'eval_precision@eng.pdtb.pdtb': 0.43729924731678543, 'eval_recall@eng.pdtb.pdtb': 0.38399352760696515, 'eval_loss@eng.pdtb.pdtb': 1.050174355506897, 'eval_runtime': 20.4708, 'eval_samples_per_second': 81.775, 'eval_steps_per_second': 2.589, 'epoch': 2.0}
{'train@eng.pdtb.pdtb_loss': 1.0578832626342773, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6511384335154827, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.41587417745800714, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.4681326197543168, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.3963438768746049, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.057883381843567, 'train@eng.pdtb.pdtb_runtime': 524.4825, 'train@eng.pdtb.pdtb_samples_per_second': 83.74, 'train@eng.pdtb.pdtb_steps_per_second': 2.618, 'epoch': 3.0}
{'loss': 1.1336, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.0090259313583374, 'eval_accuracy@eng.pdtb.pdtb': 0.6726403823178017, 'eval_f1@eng.pdtb.pdtb': 0.45157415641018356, 'eval_precision@eng.pdtb.pdtb': 0.5029457758095177, 'eval_recall@eng.pdtb.pdtb': 0.4335096195375742, 'eval_loss@eng.pdtb.pdtb': 1.0090259313583374, 'eval_runtime': 20.4539, 'eval_samples_per_second': 81.843, 'eval_steps_per_second': 2.591, 'epoch': 3.0}
{'train@eng.pdtb.pdtb_loss': 1.0070831775665283, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6651867030965392, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.44251027318256336, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.4743493630562785, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4272695155033002, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.0070830583572388, 'train@eng.pdtb.pdtb_runtime': 524.5439, 'train@eng.pdtb.pdtb_samples_per_second': 83.73, 'train@eng.pdtb.pdtb_steps_per_second': 2.618, 'epoch': 4.0}
{'loss': 1.0817, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 0.961732804775238, 'eval_accuracy@eng.pdtb.pdtb': 0.6881720430107527, 'eval_f1@eng.pdtb.pdtb': 0.5000839814137774, 'eval_precision@eng.pdtb.pdtb': 0.5561305811971534, 'eval_recall@eng.pdtb.pdtb': 0.4744051051573955, 'eval_loss@eng.pdtb.pdtb': 0.9617326855659485, 'eval_runtime': 20.4266, 'eval_samples_per_second': 81.952, 'eval_steps_per_second': 2.595, 'epoch': 4.0}
{'train@eng.pdtb.pdtb_loss': 0.9827064871788025, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.672563752276867, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4515134667363569, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.47558461126080237, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.443597405652187, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9827064871788025, 'train@eng.pdtb.pdtb_runtime': 524.4218, 'train@eng.pdtb.pdtb_samples_per_second': 83.749, 'train@eng.pdtb.pdtb_steps_per_second': 2.618, 'epoch': 5.0}
{'loss': 1.0527, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 0.9441391825675964, 'eval_accuracy@eng.pdtb.pdtb': 0.6953405017921147, 'eval_f1@eng.pdtb.pdtb': 0.5118675492380163, 'eval_precision@eng.pdtb.pdtb': 0.5637814381866123, 'eval_recall@eng.pdtb.pdtb': 0.48969822090175547, 'eval_loss@eng.pdtb.pdtb': 0.9441391825675964, 'eval_runtime': 20.4716, 'eval_samples_per_second': 81.772, 'eval_steps_per_second': 2.589, 'epoch': 5.0}
{'train@eng.pdtb.pdtb_loss': 0.9632285237312317, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6778688524590164, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.45930661369121534, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.47859224320837923, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.45350195732197546, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9632285237312317, 'train@eng.pdtb.pdtb_runtime': 524.3577, 'train@eng.pdtb.pdtb_samples_per_second': 83.76, 'train@eng.pdtb.pdtb_steps_per_second': 2.618, 'epoch': 6.0}
{'loss': 1.0273, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 0.9354938864707947, 'eval_accuracy@eng.pdtb.pdtb': 0.6941457586618877, 'eval_f1@eng.pdtb.pdtb': 0.5262112286155729, 'eval_precision@eng.pdtb.pdtb': 0.5672789189915429, 'eval_recall@eng.pdtb.pdtb': 0.5051927109562777, 'eval_loss@eng.pdtb.pdtb': 0.9354937672615051, 'eval_runtime': 20.4246, 'eval_samples_per_second': 81.96, 'eval_steps_per_second': 2.595, 'epoch': 6.0}
{'train@eng.pdtb.pdtb_loss': 0.9507918953895569, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6795309653916212, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.45998630542933056, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.4871260121111774, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.44942080122447836, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9507918953895569, 'train@eng.pdtb.pdtb_runtime': 524.3316, 'train@eng.pdtb.pdtb_samples_per_second': 83.764, 'train@eng.pdtb.pdtb_steps_per_second': 2.619, 'epoch': 7.0}
{'loss': 1.0121, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 0.9201653599739075, 'eval_accuracy@eng.pdtb.pdtb': 0.6983273596176822, 'eval_f1@eng.pdtb.pdtb': 0.5285207964076994, 'eval_precision@eng.pdtb.pdtb': 0.5833685140082857, 'eval_recall@eng.pdtb.pdtb': 0.5020448568374845, 'eval_loss@eng.pdtb.pdtb': 0.9201653003692627, 'eval_runtime': 20.5057, 'eval_samples_per_second': 81.636, 'eval_steps_per_second': 2.585, 'epoch': 7.0}
{'train@eng.pdtb.pdtb_loss': 0.9392545819282532, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6831056466302368, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.46557066562604704, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5246291047294546, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4615996117842447, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9392545819282532, 'train@eng.pdtb.pdtb_runtime': 524.3465, 'train@eng.pdtb.pdtb_samples_per_second': 83.761, 'train@eng.pdtb.pdtb_steps_per_second': 2.618, 'epoch': 8.0}
{'loss': 1.0, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 0.9159882068634033, 'eval_accuracy@eng.pdtb.pdtb': 0.6929510155316607, 'eval_f1@eng.pdtb.pdtb': 0.5193937256854382, 'eval_precision@eng.pdtb.pdtb': 0.5470640155430462, 'eval_recall@eng.pdtb.pdtb': 0.505410729471339, 'eval_loss@eng.pdtb.pdtb': 0.9159882664680481, 'eval_runtime': 20.4691, 'eval_samples_per_second': 81.782, 'eval_steps_per_second': 2.589, 'epoch': 8.0}
{'train@eng.pdtb.pdtb_loss': 0.9304777383804321, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6859972677595628, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.47047367144668745, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5306512803037706, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.46431756615425884, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9304776191711426, 'train@eng.pdtb.pdtb_runtime': 524.9597, 'train@eng.pdtb.pdtb_samples_per_second': 83.664, 'train@eng.pdtb.pdtb_steps_per_second': 2.615, 'epoch': 9.0}
{'loss': 0.9873, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 0.9132755398750305, 'eval_accuracy@eng.pdtb.pdtb': 0.6965352449223416, 'eval_f1@eng.pdtb.pdtb': 0.5354877720369353, 'eval_precision@eng.pdtb.pdtb': 0.5704761967563637, 'eval_recall@eng.pdtb.pdtb': 0.5174845595662194, 'eval_loss@eng.pdtb.pdtb': 0.9132755398750305, 'eval_runtime': 20.5008, 'eval_samples_per_second': 81.655, 'eval_steps_per_second': 2.585, 'epoch': 9.0}
{'train@eng.pdtb.pdtb_loss': 0.9268913865089417, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6871584699453552, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.47183969297342665, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5264951252055541, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4669759760690903, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9268915057182312, 'train@eng.pdtb.pdtb_runtime': 524.6712, 'train@eng.pdtb.pdtb_samples_per_second': 83.71, 'train@eng.pdtb.pdtb_steps_per_second': 2.617, 'epoch': 10.0}
{'loss': 0.984, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 0.9077959060668945, 'eval_accuracy@eng.pdtb.pdtb': 0.6959378733572282, 'eval_f1@eng.pdtb.pdtb': 0.5365588828401953, 'eval_precision@eng.pdtb.pdtb': 0.5656788063985679, 'eval_recall@eng.pdtb.pdtb': 0.5251303110594372, 'eval_loss@eng.pdtb.pdtb': 0.9077960848808289, 'eval_runtime': 20.4616, 'eval_samples_per_second': 81.812, 'eval_steps_per_second': 2.59, 'epoch': 10.0}
{'train@eng.pdtb.pdtb_loss': 0.9227622747421265, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6870901639344262, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4733720761109576, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5219541774525859, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4703691884829193, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9227622747421265, 'train@eng.pdtb.pdtb_runtime': 524.7111, 'train@eng.pdtb.pdtb_samples_per_second': 83.703, 'train@eng.pdtb.pdtb_steps_per_second': 2.617, 'epoch': 11.0}
{'loss': 0.9782, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.9075678586959839, 'eval_accuracy@eng.pdtb.pdtb': 0.6995221027479092, 'eval_f1@eng.pdtb.pdtb': 0.5445407811473053, 'eval_precision@eng.pdtb.pdtb': 0.5843594797764332, 'eval_recall@eng.pdtb.pdtb': 0.5271804685959752, 'eval_loss@eng.pdtb.pdtb': 0.9075677394866943, 'eval_runtime': 20.4641, 'eval_samples_per_second': 81.802, 'eval_steps_per_second': 2.59, 'epoch': 11.0}
{'train@eng.pdtb.pdtb_loss': 0.921258807182312, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6884790528233151, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4747791428045891, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.532004930986506, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.46954275333675993, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9212589263916016, 'train@eng.pdtb.pdtb_runtime': 524.7764, 'train@eng.pdtb.pdtb_samples_per_second': 83.693, 'train@eng.pdtb.pdtb_steps_per_second': 2.616, 'epoch': 12.0}
{'loss': 0.9724, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.9058918356895447, 'eval_accuracy@eng.pdtb.pdtb': 0.6989247311827957, 'eval_f1@eng.pdtb.pdtb': 0.5397681233693266, 'eval_precision@eng.pdtb.pdtb': 0.5825199631078151, 'eval_recall@eng.pdtb.pdtb': 0.520962444841921, 'eval_loss@eng.pdtb.pdtb': 0.9058918952941895, 'eval_runtime': 20.5313, 'eval_samples_per_second': 81.534, 'eval_steps_per_second': 2.581, 'epoch': 12.0}
{'train_runtime': 19837.8753, 'train_samples_per_second': 26.567, 'train_steps_per_second': 0.831, 'train_loss': 1.1132830503815208, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.1133
  train_runtime            = 5:30:37.87
  train_samples_per_second =     26.567
  train_steps_per_second   =      0.831
{'train@por.rst.cstn_loss': 2.2337138652801514, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.42381870781099323, 'train@por.rst.cstn_f1@por.rst.cstn': 0.0659408315882287, 'train@por.rst.cstn_precision@por.rst.cstn': 0.06476175273130742, 'train@por.rst.cstn_recall@por.rst.cstn': 0.07631614964106107, 'train@por.rst.cstn_loss@por.rst.cstn': 2.2337138652801514, 'train@por.rst.cstn_runtime': 50.7342, 'train@por.rst.cstn_samples_per_second': 81.759, 'train@por.rst.cstn_steps_per_second': 2.562, 'epoch': 1.0}
{'loss': 3.051, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.4040613174438477, 'eval_accuracy@por.rst.cstn': 0.3699825479930192, 'eval_f1@por.rst.cstn': 0.07708753100113652, 'eval_precision@por.rst.cstn': 0.06317343036052488, 'eval_recall@por.rst.cstn': 0.10438157466969616, 'eval_loss@por.rst.cstn': 2.4040608406066895, 'eval_runtime': 7.4559, 'eval_samples_per_second': 76.851, 'eval_steps_per_second': 2.414, 'epoch': 1.0}
{'train@por.rst.cstn_loss': 1.8444088697433472, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5318225650916104, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1101774166463486, 'train@por.rst.cstn_precision@por.rst.cstn': 0.10452222006357817, 'train@por.rst.cstn_recall@por.rst.cstn': 0.12195817912622597, 'train@por.rst.cstn_loss@por.rst.cstn': 1.8444087505340576, 'train@por.rst.cstn_runtime': 50.7881, 'train@por.rst.cstn_samples_per_second': 81.673, 'train@por.rst.cstn_steps_per_second': 2.56, 'epoch': 2.0}
{'loss': 2.0746, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.0008533000946045, 'eval_accuracy@por.rst.cstn': 0.4642233856893543, 'eval_f1@por.rst.cstn': 0.14623651307674673, 'eval_precision@por.rst.cstn': 0.13335131204601636, 'eval_recall@por.rst.cstn': 0.17460113332799002, 'eval_loss@por.rst.cstn': 2.0008533000946045, 'eval_runtime': 7.4635, 'eval_samples_per_second': 76.773, 'eval_steps_per_second': 2.412, 'epoch': 2.0}
{'train@por.rst.cstn_loss': 1.67256760597229, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5631629701060752, 'train@por.rst.cstn_f1@por.rst.cstn': 0.13834502553155084, 'train@por.rst.cstn_precision@por.rst.cstn': 0.18107358309097515, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14576169199085515, 'train@por.rst.cstn_loss@por.rst.cstn': 1.67256760597229, 'train@por.rst.cstn_runtime': 50.7984, 'train@por.rst.cstn_samples_per_second': 81.656, 'train@por.rst.cstn_steps_per_second': 2.559, 'epoch': 3.0}
{'loss': 1.8135, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.8065136671066284, 'eval_accuracy@por.rst.cstn': 0.49214659685863876, 'eval_f1@por.rst.cstn': 0.18631958584985167, 'eval_precision@por.rst.cstn': 0.1868899506094628, 'eval_recall@por.rst.cstn': 0.20802436907336014, 'eval_loss@por.rst.cstn': 1.8065139055252075, 'eval_runtime': 7.4654, 'eval_samples_per_second': 76.754, 'eval_steps_per_second': 2.411, 'epoch': 3.0}
{'train@por.rst.cstn_loss': 1.5853891372680664, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5822082931533269, 'train@por.rst.cstn_f1@por.rst.cstn': 0.14941630289886443, 'train@por.rst.cstn_precision@por.rst.cstn': 0.19582602975690183, 'train@por.rst.cstn_recall@por.rst.cstn': 0.15708704247348446, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5853890180587769, 'train@por.rst.cstn_runtime': 52.0737, 'train@por.rst.cstn_samples_per_second': 79.656, 'train@por.rst.cstn_steps_per_second': 2.496, 'epoch': 4.0}
{'loss': 1.6925, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.7165372371673584, 'eval_accuracy@por.rst.cstn': 0.5200698080279232, 'eval_f1@por.rst.cstn': 0.20583889674262823, 'eval_precision@por.rst.cstn': 0.20263967678198702, 'eval_recall@por.rst.cstn': 0.228041136047689, 'eval_loss@por.rst.cstn': 1.716537356376648, 'eval_runtime': 7.448, 'eval_samples_per_second': 76.933, 'eval_steps_per_second': 2.417, 'epoch': 4.0}
{'train@por.rst.cstn_loss': 1.5257407426834106, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5906460945033751, 'train@por.rst.cstn_f1@por.rst.cstn': 0.16010726964672733, 'train@por.rst.cstn_precision@por.rst.cstn': 0.23200661973530662, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1675860453381116, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5257407426834106, 'train@por.rst.cstn_runtime': 50.756, 'train@por.rst.cstn_samples_per_second': 81.724, 'train@por.rst.cstn_steps_per_second': 2.561, 'epoch': 5.0}
{'loss': 1.6188, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.6644593477249146, 'eval_accuracy@por.rst.cstn': 0.518324607329843, 'eval_f1@por.rst.cstn': 0.21756750670241595, 'eval_precision@por.rst.cstn': 0.21928641935757687, 'eval_recall@por.rst.cstn': 0.23945047971467776, 'eval_loss@por.rst.cstn': 1.664459228515625, 'eval_runtime': 7.4442, 'eval_samples_per_second': 76.973, 'eval_steps_per_second': 2.418, 'epoch': 5.0}
{'train@por.rst.cstn_loss': 1.4833457469940186, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.6036644165863067, 'train@por.rst.cstn_f1@por.rst.cstn': 0.17488460507124876, 'train@por.rst.cstn_precision@por.rst.cstn': 0.2537525737242692, 'train@por.rst.cstn_recall@por.rst.cstn': 0.18002149565246012, 'train@por.rst.cstn_loss@por.rst.cstn': 1.4833455085754395, 'train@por.rst.cstn_runtime': 50.8232, 'train@por.rst.cstn_samples_per_second': 81.616, 'train@por.rst.cstn_steps_per_second': 2.558, 'epoch': 6.0}
{'loss': 1.5624, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.6228817701339722, 'eval_accuracy@por.rst.cstn': 0.5340314136125655, 'eval_f1@por.rst.cstn': 0.22242692774441766, 'eval_precision@por.rst.cstn': 0.22814692608184473, 'eval_recall@por.rst.cstn': 0.2512667013083073, 'eval_loss@por.rst.cstn': 1.6228816509246826, 'eval_runtime': 7.4598, 'eval_samples_per_second': 76.812, 'eval_steps_per_second': 2.413, 'epoch': 6.0}
{'train@por.rst.cstn_loss': 1.453505039215088, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.6113789778206364, 'train@por.rst.cstn_f1@por.rst.cstn': 0.18832836854232468, 'train@por.rst.cstn_precision@por.rst.cstn': 0.2824244247346763, 'train@por.rst.cstn_recall@por.rst.cstn': 0.19187365343929497, 'train@por.rst.cstn_loss@por.rst.cstn': 1.4535049200057983, 'train@por.rst.cstn_runtime': 50.7571, 'train@por.rst.cstn_samples_per_second': 81.723, 'train@por.rst.cstn_steps_per_second': 2.561, 'epoch': 7.0}
{'loss': 1.5423, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.5956825017929077, 'eval_accuracy@por.rst.cstn': 0.5322862129144852, 'eval_f1@por.rst.cstn': 0.22965003594236083, 'eval_precision@por.rst.cstn': 0.23067450034551887, 'eval_recall@por.rst.cstn': 0.25695751783197784, 'eval_loss@por.rst.cstn': 1.5956826210021973, 'eval_runtime': 7.4672, 'eval_samples_per_second': 76.735, 'eval_steps_per_second': 2.411, 'epoch': 7.0}
{'train@por.rst.cstn_loss': 1.4295363426208496, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.6149951783992286, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1942554535704239, 'train@por.rst.cstn_precision@por.rst.cstn': 0.27733122217909056, 'train@por.rst.cstn_recall@por.rst.cstn': 0.19655365546650977, 'train@por.rst.cstn_loss@por.rst.cstn': 1.4295364618301392, 'train@por.rst.cstn_runtime': 50.746, 'train@por.rst.cstn_samples_per_second': 81.74, 'train@por.rst.cstn_steps_per_second': 2.562, 'epoch': 8.0}
{'loss': 1.5185, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.5771976709365845, 'eval_accuracy@por.rst.cstn': 0.5357766143106457, 'eval_f1@por.rst.cstn': 0.2327144149073654, 'eval_precision@por.rst.cstn': 0.2403442702573719, 'eval_recall@por.rst.cstn': 0.26059677377516555, 'eval_loss@por.rst.cstn': 1.577197790145874, 'eval_runtime': 8.5049, 'eval_samples_per_second': 67.373, 'eval_steps_per_second': 2.116, 'epoch': 8.0}
{'train@por.rst.cstn_loss': 1.413692831993103, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.6157184185149469, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1946802618171976, 'train@por.rst.cstn_precision@por.rst.cstn': 0.26354892163843013, 'train@por.rst.cstn_recall@por.rst.cstn': 0.19626024928931987, 'train@por.rst.cstn_loss@por.rst.cstn': 1.4136929512023926, 'train@por.rst.cstn_runtime': 50.8413, 'train@por.rst.cstn_samples_per_second': 81.587, 'train@por.rst.cstn_steps_per_second': 2.557, 'epoch': 9.0}
{'loss': 1.4971, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.5634199380874634, 'eval_accuracy@por.rst.cstn': 0.5392670157068062, 'eval_f1@por.rst.cstn': 0.23149823212663065, 'eval_precision@por.rst.cstn': 0.23879057128783157, 'eval_recall@por.rst.cstn': 0.25832102586544636, 'eval_loss@por.rst.cstn': 1.5634198188781738, 'eval_runtime': 7.4555, 'eval_samples_per_second': 76.856, 'eval_steps_per_second': 2.414, 'epoch': 9.0}
{'train@por.rst.cstn_loss': 1.4033780097961426, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.6248794599807136, 'train@por.rst.cstn_f1@por.rst.cstn': 0.20152727022556557, 'train@por.rst.cstn_precision@por.rst.cstn': 0.2586204188230353, 'train@por.rst.cstn_recall@por.rst.cstn': 0.2037284927775854, 'train@por.rst.cstn_loss@por.rst.cstn': 1.4033781290054321, 'train@por.rst.cstn_runtime': 50.8257, 'train@por.rst.cstn_samples_per_second': 81.612, 'train@por.rst.cstn_steps_per_second': 2.558, 'epoch': 10.0}
{'loss': 1.4676, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.5534231662750244, 'eval_accuracy@por.rst.cstn': 0.5445026178010471, 'eval_f1@por.rst.cstn': 0.23737970183759868, 'eval_precision@por.rst.cstn': 0.23574976235171652, 'eval_recall@por.rst.cstn': 0.26860811470984747, 'eval_loss@por.rst.cstn': 1.5534230470657349, 'eval_runtime': 7.4819, 'eval_samples_per_second': 76.584, 'eval_steps_per_second': 2.406, 'epoch': 10.0}
{'train@por.rst.cstn_loss': 1.3964396715164185, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.6248794599807136, 'train@por.rst.cstn_f1@por.rst.cstn': 0.20295371900216563, 'train@por.rst.cstn_precision@por.rst.cstn': 0.2814844996014533, 'train@por.rst.cstn_recall@por.rst.cstn': 0.2048030287183421, 'train@por.rst.cstn_loss@por.rst.cstn': 1.3964396715164185, 'train@por.rst.cstn_runtime': 50.7103, 'train@por.rst.cstn_samples_per_second': 81.798, 'train@por.rst.cstn_steps_per_second': 2.564, 'epoch': 11.0}
{'loss': 1.4718, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.5495365858078003, 'eval_accuracy@por.rst.cstn': 0.5462478184991274, 'eval_f1@por.rst.cstn': 0.23776718882476633, 'eval_precision@por.rst.cstn': 0.2387919057252333, 'eval_recall@por.rst.cstn': 0.267910141185787, 'eval_loss@por.rst.cstn': 1.5495368242263794, 'eval_runtime': 7.4691, 'eval_samples_per_second': 76.716, 'eval_steps_per_second': 2.41, 'epoch': 11.0}
{'train@por.rst.cstn_loss': 1.393892765045166, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.6251205400192864, 'train@por.rst.cstn_f1@por.rst.cstn': 0.20394113076722017, 'train@por.rst.cstn_precision@por.rst.cstn': 0.280809710881328, 'train@por.rst.cstn_recall@por.rst.cstn': 0.20566985067709878, 'train@por.rst.cstn_loss@por.rst.cstn': 1.393892765045166, 'train@por.rst.cstn_runtime': 50.8593, 'train@por.rst.cstn_samples_per_second': 81.558, 'train@por.rst.cstn_steps_per_second': 2.556, 'epoch': 12.0}
{'loss': 1.4542, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.5464357137680054, 'eval_accuracy@por.rst.cstn': 0.5427574171029669, 'eval_f1@por.rst.cstn': 0.23695472679429896, 'eval_precision@por.rst.cstn': 0.23517146372672168, 'eval_recall@por.rst.cstn': 0.26695320338674394, 'eval_loss@por.rst.cstn': 1.546435832977295, 'eval_runtime': 7.461, 'eval_samples_per_second': 76.799, 'eval_steps_per_second': 2.413, 'epoch': 12.0}
{'train_runtime': 1982.1139, 'train_samples_per_second': 25.113, 'train_steps_per_second': 0.787, 'train_loss': 1.730355463272486, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.1133
  train_runtime            = 5:30:37.87
  train_samples_per_second =     26.567
  train_steps_per_second   =      0.831
-------------------------------------------------------------------
Lang1:  eng.rst.gum    Lang2:  por.rst.cstn
Saving run to:  runs/full_shot/FullShot=v4_eng.rst.gum_por.rst.cstn_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 13897 examples
read 2149 examples
read 2091 examples
read 4148 examples
read 573 examples
read 272 examples
Total prediction labels:  37
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (pt): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (pt): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=37, bias=True)
    )
  )
)
{'train@eng.rst.gum_loss': 2.5519211292266846, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.23724544865798375, 'train@eng.rst.gum_f1@eng.rst.gum': 0.03037027780637336, 'train@eng.rst.gum_precision@eng.rst.gum': 0.07191074279975922, 'train@eng.rst.gum_recall@eng.rst.gum': 0.05127701063679632, 'train@eng.rst.gum_loss@eng.rst.gum': 2.5519211292266846, 'train@eng.rst.gum_runtime': 166.1734, 'train@eng.rst.gum_samples_per_second': 83.629, 'train@eng.rst.gum_steps_per_second': 2.618, 'epoch': 1.0}
{'loss': 2.8373, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.632505416870117, 'eval_accuracy@eng.rst.gum': 0.23127035830618892, 'eval_f1@eng.rst.gum': 0.02988852989950391, 'eval_precision@eng.rst.gum': 0.06124855636560124, 'eval_recall@eng.rst.gum': 0.051863164258175366, 'eval_loss@eng.rst.gum': 2.6325056552886963, 'eval_runtime': 25.9987, 'eval_samples_per_second': 82.658, 'eval_steps_per_second': 2.616, 'epoch': 1.0}
{'train@eng.rst.gum_loss': 2.107334613800049, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.39756782039289057, 'train@eng.rst.gum_f1@eng.rst.gum': 0.15875207846723488, 'train@eng.rst.gum_precision@eng.rst.gum': 0.21070850961854323, 'train@eng.rst.gum_recall@eng.rst.gum': 0.17028715975899097, 'train@eng.rst.gum_loss@eng.rst.gum': 2.107334613800049, 'train@eng.rst.gum_runtime': 166.1225, 'train@eng.rst.gum_samples_per_second': 83.655, 'train@eng.rst.gum_steps_per_second': 2.619, 'epoch': 2.0}
{'loss': 2.3935, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.212467670440674, 'eval_accuracy@eng.rst.gum': 0.36389018147975805, 'eval_f1@eng.rst.gum': 0.14527922438232313, 'eval_precision@eng.rst.gum': 0.1922769087165272, 'eval_recall@eng.rst.gum': 0.16125793642070071, 'eval_loss@eng.rst.gum': 2.212467670440674, 'eval_runtime': 26.0246, 'eval_samples_per_second': 82.576, 'eval_steps_per_second': 2.613, 'epoch': 2.0}
{'train@eng.rst.gum_loss': 1.813036322593689, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.47787292221342736, 'train@eng.rst.gum_f1@eng.rst.gum': 0.2504746449648195, 'train@eng.rst.gum_precision@eng.rst.gum': 0.3537075933458074, 'train@eng.rst.gum_recall@eng.rst.gum': 0.2701311153506329, 'train@eng.rst.gum_loss@eng.rst.gum': 1.813036322593689, 'train@eng.rst.gum_runtime': 166.301, 'train@eng.rst.gum_samples_per_second': 83.565, 'train@eng.rst.gum_steps_per_second': 2.616, 'epoch': 3.0}
{'loss': 2.0233, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.9477900266647339, 'eval_accuracy@eng.rst.gum': 0.44532340623545835, 'eval_f1@eng.rst.gum': 0.24302812144629837, 'eval_precision@eng.rst.gum': 0.2556461064048604, 'eval_recall@eng.rst.gum': 0.26509879381058865, 'eval_loss@eng.rst.gum': 1.9477897882461548, 'eval_runtime': 25.9969, 'eval_samples_per_second': 82.664, 'eval_steps_per_second': 2.616, 'epoch': 3.0}
{'train@eng.rst.gum_loss': 1.6808016300201416, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5037777937684392, 'train@eng.rst.gum_f1@eng.rst.gum': 0.286381387714291, 'train@eng.rst.gum_precision@eng.rst.gum': 0.4324404172174897, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3021173383580405, 'train@eng.rst.gum_loss@eng.rst.gum': 1.6808016300201416, 'train@eng.rst.gum_runtime': 166.2835, 'train@eng.rst.gum_samples_per_second': 83.574, 'train@eng.rst.gum_steps_per_second': 2.616, 'epoch': 4.0}
{'loss': 1.8208, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.8467953205108643, 'eval_accuracy@eng.rst.gum': 0.4592833876221498, 'eval_f1@eng.rst.gum': 0.26852216127954454, 'eval_precision@eng.rst.gum': 0.31840054737842655, 'eval_recall@eng.rst.gum': 0.2894121607893236, 'eval_loss@eng.rst.gum': 1.8467955589294434, 'eval_runtime': 26.0524, 'eval_samples_per_second': 82.488, 'eval_steps_per_second': 2.61, 'epoch': 4.0}
{'train@eng.rst.gum_loss': 1.6044832468032837, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5212635820680722, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3293458125736333, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5252433836860737, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3396117427557703, 'train@eng.rst.gum_loss@eng.rst.gum': 1.6044832468032837, 'train@eng.rst.gum_runtime': 166.1013, 'train@eng.rst.gum_samples_per_second': 83.666, 'train@eng.rst.gum_steps_per_second': 2.619, 'epoch': 5.0}
{'loss': 1.7172, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.7896097898483276, 'eval_accuracy@eng.rst.gum': 0.47510469986040016, 'eval_f1@eng.rst.gum': 0.2918475043356951, 'eval_precision@eng.rst.gum': 0.32433157671195006, 'eval_recall@eng.rst.gum': 0.3118910092633668, 'eval_loss@eng.rst.gum': 1.7896097898483276, 'eval_runtime': 25.9808, 'eval_samples_per_second': 82.715, 'eval_steps_per_second': 2.617, 'epoch': 5.0}
{'train@eng.rst.gum_loss': 1.5508313179016113, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5385334964380801, 'train@eng.rst.gum_f1@eng.rst.gum': 0.37354984937659064, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5339397182020107, 'train@eng.rst.gum_recall@eng.rst.gum': 0.37582498340600756, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5508313179016113, 'train@eng.rst.gum_runtime': 166.1579, 'train@eng.rst.gum_samples_per_second': 83.637, 'train@eng.rst.gum_steps_per_second': 2.618, 'epoch': 6.0}
{'loss': 1.6578, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.7487720251083374, 'eval_accuracy@eng.rst.gum': 0.4960446719404374, 'eval_f1@eng.rst.gum': 0.3471557880129276, 'eval_precision@eng.rst.gum': 0.4261720331538873, 'eval_recall@eng.rst.gum': 0.35508451839892036, 'eval_loss@eng.rst.gum': 1.748772144317627, 'eval_runtime': 25.9735, 'eval_samples_per_second': 82.738, 'eval_steps_per_second': 2.618, 'epoch': 6.0}
{'train@eng.rst.gum_loss': 1.514410138130188, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5458732100453335, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3860458198866348, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5330364745018344, 'train@eng.rst.gum_recall@eng.rst.gum': 0.39005677709720293, 'train@eng.rst.gum_loss@eng.rst.gum': 1.514410138130188, 'train@eng.rst.gum_runtime': 166.1195, 'train@eng.rst.gum_samples_per_second': 83.657, 'train@eng.rst.gum_steps_per_second': 2.619, 'epoch': 7.0}
{'loss': 1.6086, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.7283782958984375, 'eval_accuracy@eng.rst.gum': 0.5006979990693345, 'eval_f1@eng.rst.gum': 0.35712911951467957, 'eval_precision@eng.rst.gum': 0.4103604694562672, 'eval_recall@eng.rst.gum': 0.3687764403843808, 'eval_loss@eng.rst.gum': 1.728378415107727, 'eval_runtime': 25.9992, 'eval_samples_per_second': 82.656, 'eval_steps_per_second': 2.615, 'epoch': 7.0}
{'train@eng.rst.gum_loss': 1.488613486289978, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5538605454414622, 'train@eng.rst.gum_f1@eng.rst.gum': 0.40007470571439063, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5300393862838032, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3997118448017049, 'train@eng.rst.gum_loss@eng.rst.gum': 1.488613486289978, 'train@eng.rst.gum_runtime': 166.0229, 'train@eng.rst.gum_samples_per_second': 83.705, 'train@eng.rst.gum_steps_per_second': 2.62, 'epoch': 8.0}
{'loss': 1.5794, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.709259033203125, 'eval_accuracy@eng.rst.gum': 0.5025593299208935, 'eval_f1@eng.rst.gum': 0.36661165781092026, 'eval_precision@eng.rst.gum': 0.41093204654741994, 'eval_recall@eng.rst.gum': 0.3779546175444567, 'eval_loss@eng.rst.gum': 1.7092591524124146, 'eval_runtime': 25.9975, 'eval_samples_per_second': 82.662, 'eval_steps_per_second': 2.616, 'epoch': 8.0}
{'train@eng.rst.gum_loss': 1.4679769277572632, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5623515866733827, 'train@eng.rst.gum_f1@eng.rst.gum': 0.41610065650949457, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5258971050495501, 'train@eng.rst.gum_recall@eng.rst.gum': 0.41697570083739427, 'train@eng.rst.gum_loss@eng.rst.gum': 1.467976689338684, 'train@eng.rst.gum_runtime': 166.1401, 'train@eng.rst.gum_samples_per_second': 83.646, 'train@eng.rst.gum_steps_per_second': 2.618, 'epoch': 9.0}
{'loss': 1.554, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.6926177740097046, 'eval_accuracy@eng.rst.gum': 0.5034899953466728, 'eval_f1@eng.rst.gum': 0.3720413181678291, 'eval_precision@eng.rst.gum': 0.4046761566743525, 'eval_recall@eng.rst.gum': 0.3876456280726589, 'eval_loss@eng.rst.gum': 1.692617654800415, 'eval_runtime': 25.9834, 'eval_samples_per_second': 82.707, 'eval_steps_per_second': 2.617, 'epoch': 9.0}
{'train@eng.rst.gum_loss': 1.453089714050293, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5648701158523423, 'train@eng.rst.gum_f1@eng.rst.gum': 0.42177746418938, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5323172913326311, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4182436579818707, 'train@eng.rst.gum_loss@eng.rst.gum': 1.453089714050293, 'train@eng.rst.gum_runtime': 165.9484, 'train@eng.rst.gum_samples_per_second': 83.743, 'train@eng.rst.gum_steps_per_second': 2.621, 'epoch': 10.0}
{'loss': 1.5386, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.68399178981781, 'eval_accuracy@eng.rst.gum': 0.5109353187529083, 'eval_f1@eng.rst.gum': 0.3846730362171746, 'eval_precision@eng.rst.gum': 0.4357234233962425, 'eval_recall@eng.rst.gum': 0.3922477697787528, 'eval_loss@eng.rst.gum': 1.6839916706085205, 'eval_runtime': 25.9659, 'eval_samples_per_second': 82.762, 'eval_steps_per_second': 2.619, 'epoch': 10.0}
{'train@eng.rst.gum_loss': 1.4461034536361694, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5671727711016766, 'train@eng.rst.gum_f1@eng.rst.gum': 0.42571747611730165, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5291537819205626, 'train@eng.rst.gum_recall@eng.rst.gum': 0.42533026256669176, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4461033344268799, 'train@eng.rst.gum_runtime': 166.0855, 'train@eng.rst.gum_samples_per_second': 83.674, 'train@eng.rst.gum_steps_per_second': 2.619, 'epoch': 11.0}
{'loss': 1.5303, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.6775543689727783, 'eval_accuracy@eng.rst.gum': 0.5141926477431363, 'eval_f1@eng.rst.gum': 0.38754431965773756, 'eval_precision@eng.rst.gum': 0.42375541148970536, 'eval_recall@eng.rst.gum': 0.40079159527233366, 'eval_loss@eng.rst.gum': 1.6775543689727783, 'eval_runtime': 26.0175, 'eval_samples_per_second': 82.598, 'eval_steps_per_second': 2.614, 'epoch': 11.0}
{'train@eng.rst.gum_loss': 1.442338228225708, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5673886450313017, 'train@eng.rst.gum_f1@eng.rst.gum': 0.42636769117767415, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5322355664938714, 'train@eng.rst.gum_recall@eng.rst.gum': 0.42448575068439526, 'train@eng.rst.gum_loss@eng.rst.gum': 1.442338228225708, 'train@eng.rst.gum_runtime': 166.2406, 'train@eng.rst.gum_samples_per_second': 83.596, 'train@eng.rst.gum_steps_per_second': 2.617, 'epoch': 12.0}
{'loss': 1.518, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.6757172346115112, 'eval_accuracy@eng.rst.gum': 0.5118659841786878, 'eval_f1@eng.rst.gum': 0.38539928521133876, 'eval_precision@eng.rst.gum': 0.42654936582407027, 'eval_recall@eng.rst.gum': 0.397443604755734, 'eval_loss@eng.rst.gum': 1.6757173538208008, 'eval_runtime': 26.0121, 'eval_samples_per_second': 82.615, 'eval_steps_per_second': 2.614, 'epoch': 12.0}
{'train_runtime': 6507.772, 'train_samples_per_second': 25.625, 'train_steps_per_second': 0.802, 'train_loss': 1.8149125607077645, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.8149
  train_runtime            = 1:48:27.77
  train_samples_per_second =     25.625
  train_steps_per_second   =      0.802
{'train@por.rst.cstn_loss': 2.0716712474823, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.4840887174541948, 'train@por.rst.cstn_f1@por.rst.cstn': 0.08097056396264805, 'train@por.rst.cstn_precision@por.rst.cstn': 0.07551912644436969, 'train@por.rst.cstn_recall@por.rst.cstn': 0.09183506502282152, 'train@por.rst.cstn_loss@por.rst.cstn': 2.0716712474823, 'train@por.rst.cstn_runtime': 50.6814, 'train@por.rst.cstn_samples_per_second': 81.845, 'train@por.rst.cstn_steps_per_second': 2.565, 'epoch': 1.0}
{'loss': 2.6746, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.316734790802002, 'eval_accuracy@por.rst.cstn': 0.4031413612565445, 'eval_f1@por.rst.cstn': 0.10780402871991218, 'eval_precision@por.rst.cstn': 0.10387789161374067, 'eval_recall@por.rst.cstn': 0.12441490449083542, 'eval_loss@por.rst.cstn': 2.316734790802002, 'eval_runtime': 7.3348, 'eval_samples_per_second': 78.121, 'eval_steps_per_second': 2.454, 'epoch': 1.0}
{'train@por.rst.cstn_loss': 1.7660318613052368, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5279652844744455, 'train@por.rst.cstn_f1@por.rst.cstn': 0.08980944486854468, 'train@por.rst.cstn_precision@por.rst.cstn': 0.08115304973035414, 'train@por.rst.cstn_recall@por.rst.cstn': 0.10371419495986295, 'train@por.rst.cstn_loss@por.rst.cstn': 1.7660316228866577, 'train@por.rst.cstn_runtime': 50.584, 'train@por.rst.cstn_samples_per_second': 82.002, 'train@por.rst.cstn_steps_per_second': 2.57, 'epoch': 2.0}
{'loss': 1.9531, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.9692001342773438, 'eval_accuracy@por.rst.cstn': 0.4258289703315881, 'eval_f1@por.rst.cstn': 0.11918790628417715, 'eval_precision@por.rst.cstn': 0.10894366351375022, 'eval_recall@por.rst.cstn': 0.14420969779821455, 'eval_loss@por.rst.cstn': 1.9692001342773438, 'eval_runtime': 7.3302, 'eval_samples_per_second': 78.17, 'eval_steps_per_second': 2.456, 'epoch': 2.0}
{'train@por.rst.cstn_loss': 1.6241408586502075, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.563404050144648, 'train@por.rst.cstn_f1@por.rst.cstn': 0.12654466751023163, 'train@por.rst.cstn_precision@por.rst.cstn': 0.16479612182023, 'train@por.rst.cstn_recall@por.rst.cstn': 0.13387836834440164, 'train@por.rst.cstn_loss@por.rst.cstn': 1.624140739440918, 'train@por.rst.cstn_runtime': 50.5851, 'train@por.rst.cstn_samples_per_second': 82.0, 'train@por.rst.cstn_steps_per_second': 2.57, 'epoch': 3.0}
{'loss': 1.749, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.7909742593765259, 'eval_accuracy@por.rst.cstn': 0.45898778359511344, 'eval_f1@por.rst.cstn': 0.15898713233446793, 'eval_precision@por.rst.cstn': 0.2112504353883664, 'eval_recall@por.rst.cstn': 0.174312334165153, 'eval_loss@por.rst.cstn': 1.7909741401672363, 'eval_runtime': 7.2949, 'eval_samples_per_second': 78.548, 'eval_steps_per_second': 2.467, 'epoch': 3.0}
{'train@por.rst.cstn_loss': 1.5410517454147339, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.587029893924783, 'train@por.rst.cstn_f1@por.rst.cstn': 0.14920511151553228, 'train@por.rst.cstn_precision@por.rst.cstn': 0.17271607596645655, 'train@por.rst.cstn_recall@por.rst.cstn': 0.15316341351229024, 'train@por.rst.cstn_loss@por.rst.cstn': 1.541051983833313, 'train@por.rst.cstn_runtime': 50.5691, 'train@por.rst.cstn_samples_per_second': 82.026, 'train@por.rst.cstn_steps_per_second': 2.571, 'epoch': 4.0}
{'loss': 1.6412, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.7104969024658203, 'eval_accuracy@por.rst.cstn': 0.4851657940663176, 'eval_f1@por.rst.cstn': 0.17992049678938118, 'eval_precision@por.rst.cstn': 0.1903731758117047, 'eval_recall@por.rst.cstn': 0.19642765585661362, 'eval_loss@por.rst.cstn': 1.7104967832565308, 'eval_runtime': 7.2967, 'eval_samples_per_second': 78.528, 'eval_steps_per_second': 2.467, 'epoch': 4.0}
{'train@por.rst.cstn_loss': 1.485171914100647, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.6072806171648988, 'train@por.rst.cstn_f1@por.rst.cstn': 0.16864864034551216, 'train@por.rst.cstn_precision@por.rst.cstn': 0.22189166037507235, 'train@por.rst.cstn_recall@por.rst.cstn': 0.17150871931582573, 'train@por.rst.cstn_loss@por.rst.cstn': 1.4851717948913574, 'train@por.rst.cstn_runtime': 50.6389, 'train@por.rst.cstn_samples_per_second': 81.913, 'train@por.rst.cstn_steps_per_second': 2.567, 'epoch': 5.0}
{'loss': 1.5706, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.6623088121414185, 'eval_accuracy@por.rst.cstn': 0.5095986038394416, 'eval_f1@por.rst.cstn': 0.20980298728769642, 'eval_precision@por.rst.cstn': 0.21815309445816775, 'eval_recall@por.rst.cstn': 0.22252010956244364, 'eval_loss@por.rst.cstn': 1.6623088121414185, 'eval_runtime': 7.3231, 'eval_samples_per_second': 78.245, 'eval_steps_per_second': 2.458, 'epoch': 5.0}
{'train@por.rst.cstn_loss': 1.445172905921936, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.6186113789778206, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1759314778768058, 'train@por.rst.cstn_precision@por.rst.cstn': 0.23053237356401557, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1784664309058457, 'train@por.rst.cstn_loss@por.rst.cstn': 1.445172905921936, 'train@por.rst.cstn_runtime': 50.6374, 'train@por.rst.cstn_samples_per_second': 81.916, 'train@por.rst.cstn_steps_per_second': 2.567, 'epoch': 6.0}
{'loss': 1.5194, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.6199924945831299, 'eval_accuracy@por.rst.cstn': 0.5287958115183246, 'eval_f1@por.rst.cstn': 0.21986050514655966, 'eval_precision@por.rst.cstn': 0.224984893411819, 'eval_recall@por.rst.cstn': 0.23100065110674642, 'eval_loss@por.rst.cstn': 1.6199924945831299, 'eval_runtime': 7.3073, 'eval_samples_per_second': 78.415, 'eval_steps_per_second': 2.463, 'epoch': 6.0}
{'train@por.rst.cstn_loss': 1.4185229539871216, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.6227097396335584, 'train@por.rst.cstn_f1@por.rst.cstn': 0.18537228459868985, 'train@por.rst.cstn_precision@por.rst.cstn': 0.2278331085248485, 'train@por.rst.cstn_recall@por.rst.cstn': 0.187844140113417, 'train@por.rst.cstn_loss@por.rst.cstn': 1.4185229539871216, 'train@por.rst.cstn_runtime': 50.6228, 'train@por.rst.cstn_samples_per_second': 81.939, 'train@por.rst.cstn_steps_per_second': 2.568, 'epoch': 7.0}
{'loss': 1.4937, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.5986499786376953, 'eval_accuracy@por.rst.cstn': 0.537521815008726, 'eval_f1@por.rst.cstn': 0.22431508560535363, 'eval_precision@por.rst.cstn': 0.22527186146920072, 'eval_recall@por.rst.cstn': 0.2383794706553184, 'eval_loss@por.rst.cstn': 1.5986499786376953, 'eval_runtime': 7.2986, 'eval_samples_per_second': 78.508, 'eval_steps_per_second': 2.466, 'epoch': 7.0}
{'train@por.rst.cstn_loss': 1.4009137153625488, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.626808100289296, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1975778855809952, 'train@por.rst.cstn_precision@por.rst.cstn': 0.23040995735327166, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1969973047103255, 'train@por.rst.cstn_loss@por.rst.cstn': 1.4009134769439697, 'train@por.rst.cstn_runtime': 50.6549, 'train@por.rst.cstn_samples_per_second': 81.887, 'train@por.rst.cstn_steps_per_second': 2.566, 'epoch': 8.0}
{'loss': 1.4727, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.5917432308197021, 'eval_accuracy@por.rst.cstn': 0.5340314136125655, 'eval_f1@por.rst.cstn': 0.22564430745252792, 'eval_precision@por.rst.cstn': 0.22789465211511234, 'eval_recall@por.rst.cstn': 0.23893485510398343, 'eval_loss@por.rst.cstn': 1.5917431116104126, 'eval_runtime': 7.3128, 'eval_samples_per_second': 78.355, 'eval_steps_per_second': 2.461, 'epoch': 8.0}
{'train@por.rst.cstn_loss': 1.3880788087844849, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.6294599807135969, 'train@por.rst.cstn_f1@por.rst.cstn': 0.20204026346121945, 'train@por.rst.cstn_precision@por.rst.cstn': 0.23105814329053057, 'train@por.rst.cstn_recall@por.rst.cstn': 0.20212117454919887, 'train@por.rst.cstn_loss@por.rst.cstn': 1.388079047203064, 'train@por.rst.cstn_runtime': 50.6145, 'train@por.rst.cstn_samples_per_second': 81.953, 'train@por.rst.cstn_steps_per_second': 2.568, 'epoch': 9.0}
{'loss': 1.4663, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.5798155069351196, 'eval_accuracy@por.rst.cstn': 0.5392670157068062, 'eval_f1@por.rst.cstn': 0.22781905705525976, 'eval_precision@por.rst.cstn': 0.22987764094171903, 'eval_recall@por.rst.cstn': 0.2419766365548558, 'eval_loss@por.rst.cstn': 1.5798155069351196, 'eval_runtime': 7.3193, 'eval_samples_per_second': 78.286, 'eval_steps_per_second': 2.459, 'epoch': 9.0}
{'train@por.rst.cstn_loss': 1.3804121017456055, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.6323529411764706, 'train@por.rst.cstn_f1@por.rst.cstn': 0.2024230898966036, 'train@por.rst.cstn_precision@por.rst.cstn': 0.22369014927184694, 'train@por.rst.cstn_recall@por.rst.cstn': 0.20455016946988439, 'train@por.rst.cstn_loss@por.rst.cstn': 1.380411982536316, 'train@por.rst.cstn_runtime': 50.6429, 'train@por.rst.cstn_samples_per_second': 81.907, 'train@por.rst.cstn_steps_per_second': 2.567, 'epoch': 10.0}
{'loss': 1.4378, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.5713750123977661, 'eval_accuracy@por.rst.cstn': 0.5410122164048866, 'eval_f1@por.rst.cstn': 0.2303074723140088, 'eval_precision@por.rst.cstn': 0.2292179334007047, 'eval_recall@por.rst.cstn': 0.24670132556808239, 'eval_loss@por.rst.cstn': 1.5713750123977661, 'eval_runtime': 7.3164, 'eval_samples_per_second': 78.318, 'eval_steps_per_second': 2.46, 'epoch': 10.0}
{'train@por.rst.cstn_loss': 1.374036192893982, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.6337994214079075, 'train@por.rst.cstn_f1@por.rst.cstn': 0.20443148542250972, 'train@por.rst.cstn_precision@por.rst.cstn': 0.22543512476446959, 'train@por.rst.cstn_recall@por.rst.cstn': 0.20652181547076143, 'train@por.rst.cstn_loss@por.rst.cstn': 1.374036192893982, 'train@por.rst.cstn_runtime': 50.637, 'train@por.rst.cstn_samples_per_second': 81.916, 'train@por.rst.cstn_steps_per_second': 2.567, 'epoch': 11.0}
{'loss': 1.4337, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.5693639516830444, 'eval_accuracy@por.rst.cstn': 0.5427574171029669, 'eval_f1@por.rst.cstn': 0.2310786378758344, 'eval_precision@por.rst.cstn': 0.23093113369699989, 'eval_recall@por.rst.cstn': 0.24729940418779614, 'eval_loss@por.rst.cstn': 1.5693641901016235, 'eval_runtime': 7.2818, 'eval_samples_per_second': 78.689, 'eval_steps_per_second': 2.472, 'epoch': 11.0}
{'train@por.rst.cstn_loss': 1.3722381591796875, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.6340405014464803, 'train@por.rst.cstn_f1@por.rst.cstn': 0.20447261830566232, 'train@por.rst.cstn_precision@por.rst.cstn': 0.22509672477164516, 'train@por.rst.cstn_recall@por.rst.cstn': 0.2066349754317579, 'train@por.rst.cstn_loss@por.rst.cstn': 1.3722381591796875, 'train@por.rst.cstn_runtime': 50.63, 'train@por.rst.cstn_samples_per_second': 81.928, 'train@por.rst.cstn_steps_per_second': 2.568, 'epoch': 12.0}
{'loss': 1.4245, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.5666818618774414, 'eval_accuracy@por.rst.cstn': 0.5445026178010471, 'eval_f1@por.rst.cstn': 0.23179767653956881, 'eval_precision@por.rst.cstn': 0.23109306008216218, 'eval_recall@por.rst.cstn': 0.24785399313951426, 'eval_loss@por.rst.cstn': 1.5666816234588623, 'eval_runtime': 7.3268, 'eval_samples_per_second': 78.206, 'eval_steps_per_second': 2.457, 'epoch': 12.0}
{'train_runtime': 1974.4613, 'train_samples_per_second': 25.21, 'train_steps_per_second': 0.79, 'train_loss': 1.653060541397486, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.8149
  train_runtime            = 1:48:27.77
  train_samples_per_second =     25.625
  train_steps_per_second   =      0.802
-------------------------------------------------------------------
Lang1:  eng.rst.rstdt    Lang2:  por.rst.cstn
Saving run to:  runs/full_shot/FullShot=v4_eng.rst.rstdt_por.rst.cstn_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 16002 examples
read 1621 examples
read 2155 examples
read 4148 examples
read 573 examples
read 272 examples
Total prediction labels:  39
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (pt): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (pt): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=39, bias=True)
    )
  )
)
{'train@eng.rst.rstdt_loss': 1.7659262418746948, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.5075615548056492, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.08365581595784033, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.08712771100780309, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.11053391084136194, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.7659263610839844, 'train@eng.rst.rstdt_runtime': 191.0977, 'train@eng.rst.rstdt_samples_per_second': 83.737, 'train@eng.rst.rstdt_steps_per_second': 2.622, 'epoch': 1.0}
{'loss': 2.2182, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.7424323558807373, 'eval_accuracy@eng.rst.rstdt': 0.5231338679827268, 'eval_f1@eng.rst.rstdt': 0.08766914486577286, 'eval_precision@eng.rst.rstdt': 0.09580203449014903, 'eval_recall@eng.rst.rstdt': 0.11121711560654554, 'eval_loss@eng.rst.rstdt': 1.7424323558807373, 'eval_runtime': 19.7326, 'eval_samples_per_second': 82.148, 'eval_steps_per_second': 2.585, 'epoch': 1.0}
{'train@eng.rst.rstdt_loss': 1.4414879083633423, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.590801149856268, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.19241599630150666, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.23151384890108598, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.2027603278201902, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.4414880275726318, 'train@eng.rst.rstdt_runtime': 191.3769, 'train@eng.rst.rstdt_samples_per_second': 83.615, 'train@eng.rst.rstdt_steps_per_second': 2.618, 'epoch': 2.0}
{'loss': 1.6302, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.442488670349121, 'eval_accuracy@eng.rst.rstdt': 0.6070326958667489, 'eval_f1@eng.rst.rstdt': 0.20108025444321254, 'eval_precision@eng.rst.rstdt': 0.2100458597002558, 'eval_recall@eng.rst.rstdt': 0.21041752338033667, 'eval_loss@eng.rst.rstdt': 1.442488670349121, 'eval_runtime': 19.7293, 'eval_samples_per_second': 82.162, 'eval_steps_per_second': 2.585, 'epoch': 2.0}
{'train@eng.rst.rstdt_loss': 1.3224633932113647, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6271716035495563, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.2566907942040065, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.4377337963441872, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.25540616161024965, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.3224635124206543, 'train@eng.rst.rstdt_runtime': 191.1874, 'train@eng.rst.rstdt_samples_per_second': 83.698, 'train@eng.rst.rstdt_steps_per_second': 2.62, 'epoch': 3.0}
{'loss': 1.4372, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.3524281978607178, 'eval_accuracy@eng.rst.rstdt': 0.621221468229488, 'eval_f1@eng.rst.rstdt': 0.23661259746620356, 'eval_precision@eng.rst.rstdt': 0.29318746393160544, 'eval_recall@eng.rst.rstdt': 0.2432451885669195, 'eval_loss@eng.rst.rstdt': 1.3524280786514282, 'eval_runtime': 19.711, 'eval_samples_per_second': 82.238, 'eval_steps_per_second': 2.587, 'epoch': 3.0}
{'train@eng.rst.rstdt_loss': 1.2528462409973145, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6459817522809649, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3343238566543543, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.46221801378447364, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.31432876709965163, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.2528462409973145, 'train@eng.rst.rstdt_runtime': 191.2603, 'train@eng.rst.rstdt_samples_per_second': 83.666, 'train@eng.rst.rstdt_steps_per_second': 2.619, 'epoch': 4.0}
{'loss': 1.3382, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.3107044696807861, 'eval_accuracy@eng.rst.rstdt': 0.6329426280074029, 'eval_f1@eng.rst.rstdt': 0.33571035317119446, 'eval_precision@eng.rst.rstdt': 0.4274792023095719, 'eval_recall@eng.rst.rstdt': 0.32288280907162, 'eval_loss@eng.rst.rstdt': 1.3107045888900757, 'eval_runtime': 19.7533, 'eval_samples_per_second': 82.062, 'eval_steps_per_second': 2.582, 'epoch': 4.0}
{'train@eng.rst.rstdt_loss': 1.2008848190307617, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6561679790026247, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3530762768512823, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.4603736614417644, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.33098032657820375, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.2008846998214722, 'train@eng.rst.rstdt_runtime': 191.1887, 'train@eng.rst.rstdt_samples_per_second': 83.697, 'train@eng.rst.rstdt_steps_per_second': 2.62, 'epoch': 5.0}
{'loss': 1.2773, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.2655788660049438, 'eval_accuracy@eng.rst.rstdt': 0.6403454657618753, 'eval_f1@eng.rst.rstdt': 0.32835691629723285, 'eval_precision@eng.rst.rstdt': 0.4086886185069716, 'eval_recall@eng.rst.rstdt': 0.3233994488999543, 'eval_loss@eng.rst.rstdt': 1.2655788660049438, 'eval_runtime': 19.7238, 'eval_samples_per_second': 82.185, 'eval_steps_per_second': 2.586, 'epoch': 5.0}
{'train@eng.rst.rstdt_loss': 1.1660574674606323, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6628546431696039, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3658882082919538, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5375742664692371, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.34090376895147034, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1660574674606323, 'train@eng.rst.rstdt_runtime': 190.9825, 'train@eng.rst.rstdt_samples_per_second': 83.788, 'train@eng.rst.rstdt_steps_per_second': 2.623, 'epoch': 6.0}
{'loss': 1.2354, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.237707257270813, 'eval_accuracy@eng.rst.rstdt': 0.6458975940777298, 'eval_f1@eng.rst.rstdt': 0.34058085793665027, 'eval_precision@eng.rst.rstdt': 0.41792941151562113, 'eval_recall@eng.rst.rstdt': 0.33261912136035254, 'eval_loss@eng.rst.rstdt': 1.2377071380615234, 'eval_runtime': 19.7349, 'eval_samples_per_second': 82.139, 'eval_steps_per_second': 2.584, 'epoch': 6.0}
{'train@eng.rst.rstdt_loss': 1.145596981048584, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6659792525934258, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3720700436464061, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5941206758514215, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.34654921813108197, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.145596981048584, 'train@eng.rst.rstdt_runtime': 191.2339, 'train@eng.rst.rstdt_samples_per_second': 83.678, 'train@eng.rst.rstdt_steps_per_second': 2.62, 'epoch': 7.0}
{'loss': 1.2107, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.2219399213790894, 'eval_accuracy@eng.rst.rstdt': 0.649599012954966, 'eval_f1@eng.rst.rstdt': 0.34646694854064414, 'eval_precision@eng.rst.rstdt': 0.4280170868168818, 'eval_recall@eng.rst.rstdt': 0.3379535459593178, 'eval_loss@eng.rst.rstdt': 1.2219399213790894, 'eval_runtime': 19.6713, 'eval_samples_per_second': 82.404, 'eval_steps_per_second': 2.593, 'epoch': 7.0}
{'train@eng.rst.rstdt_loss': 1.1283599138259888, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6674790651168604, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3894451727147044, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5765968781786093, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.36126289568509634, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1283599138259888, 'train@eng.rst.rstdt_runtime': 191.0414, 'train@eng.rst.rstdt_samples_per_second': 83.762, 'train@eng.rst.rstdt_steps_per_second': 2.622, 'epoch': 8.0}
{'loss': 1.1876, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.2168294191360474, 'eval_accuracy@eng.rst.rstdt': 0.6440468846391116, 'eval_f1@eng.rst.rstdt': 0.34343854960461917, 'eval_precision@eng.rst.rstdt': 0.40281991586370297, 'eval_recall@eng.rst.rstdt': 0.34039536965970085, 'eval_loss@eng.rst.rstdt': 1.2168292999267578, 'eval_runtime': 19.7223, 'eval_samples_per_second': 82.191, 'eval_steps_per_second': 2.586, 'epoch': 8.0}
{'train@eng.rst.rstdt_loss': 1.1162161827087402, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6698537682789651, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3945825549332341, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5809313785561382, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3637831316472465, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1162161827087402, 'train@eng.rst.rstdt_runtime': 191.3553, 'train@eng.rst.rstdt_samples_per_second': 83.625, 'train@eng.rst.rstdt_steps_per_second': 2.618, 'epoch': 9.0}
{'loss': 1.1714, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.2062517404556274, 'eval_accuracy@eng.rst.rstdt': 0.6477483035163479, 'eval_f1@eng.rst.rstdt': 0.3447285320060168, 'eval_precision@eng.rst.rstdt': 0.40450589272763754, 'eval_recall@eng.rst.rstdt': 0.3393517155382033, 'eval_loss@eng.rst.rstdt': 1.2062517404556274, 'eval_runtime': 19.7416, 'eval_samples_per_second': 82.111, 'eval_steps_per_second': 2.583, 'epoch': 9.0}
{'train@eng.rst.rstdt_loss': 1.1124842166900635, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6688538932633421, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.41037363121421466, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5600854852435252, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3782127561150323, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1124842166900635, 'train@eng.rst.rstdt_runtime': 191.2984, 'train@eng.rst.rstdt_samples_per_second': 83.649, 'train@eng.rst.rstdt_steps_per_second': 2.619, 'epoch': 10.0}
{'loss': 1.1643, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.2134982347488403, 'eval_accuracy@eng.rst.rstdt': 0.6384947563232573, 'eval_f1@eng.rst.rstdt': 0.3542214319100254, 'eval_precision@eng.rst.rstdt': 0.4588269616052947, 'eval_recall@eng.rst.rstdt': 0.34783286403057734, 'eval_loss@eng.rst.rstdt': 1.2134982347488403, 'eval_runtime': 19.7519, 'eval_samples_per_second': 82.068, 'eval_steps_per_second': 2.582, 'epoch': 10.0}
{'train@eng.rst.rstdt_loss': 1.1043422222137451, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6714160729908761, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4062987163194099, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5675419122484896, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3734589055017726, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1043421030044556, 'train@eng.rst.rstdt_runtime': 191.2418, 'train@eng.rst.rstdt_samples_per_second': 83.674, 'train@eng.rst.rstdt_steps_per_second': 2.62, 'epoch': 11.0}
{'loss': 1.1595, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.2033098936080933, 'eval_accuracy@eng.rst.rstdt': 0.6440468846391116, 'eval_f1@eng.rst.rstdt': 0.35323397028525505, 'eval_precision@eng.rst.rstdt': 0.4615424002362894, 'eval_recall@eng.rst.rstdt': 0.34525165662467977, 'eval_loss@eng.rst.rstdt': 1.2033098936080933, 'eval_runtime': 19.7062, 'eval_samples_per_second': 82.259, 'eval_steps_per_second': 2.588, 'epoch': 11.0}
{'train@eng.rst.rstdt_loss': 1.1027071475982666, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.671666041744782, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4067875524864979, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5671807374972682, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3736386170761353, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1027072668075562, 'train@eng.rst.rstdt_runtime': 191.2919, 'train@eng.rst.rstdt_samples_per_second': 83.652, 'train@eng.rst.rstdt_steps_per_second': 2.619, 'epoch': 12.0}
{'loss': 1.1533, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.2010691165924072, 'eval_accuracy@eng.rst.rstdt': 0.6446637877853177, 'eval_f1@eng.rst.rstdt': 0.3535954611216883, 'eval_precision@eng.rst.rstdt': 0.4620792984561217, 'eval_recall@eng.rst.rstdt': 0.3453420153488147, 'eval_loss@eng.rst.rstdt': 1.2010692358016968, 'eval_runtime': 19.6803, 'eval_samples_per_second': 82.366, 'eval_steps_per_second': 2.591, 'epoch': 12.0}
{'train_runtime': 7368.9347, 'train_samples_per_second': 26.059, 'train_steps_per_second': 0.816, 'train_loss': 1.3486047720639451, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.3486
  train_runtime            = 2:02:48.93
  train_samples_per_second =     26.059
  train_steps_per_second   =      0.816
{'train@por.rst.cstn_loss': 2.26865816116333, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.42213114754098363, 'train@por.rst.cstn_f1@por.rst.cstn': 0.06417165685683368, 'train@por.rst.cstn_precision@por.rst.cstn': 0.08626186326455121, 'train@por.rst.cstn_recall@por.rst.cstn': 0.07075899195403793, 'train@por.rst.cstn_loss@por.rst.cstn': 2.26865816116333, 'train@por.rst.cstn_runtime': 50.6902, 'train@por.rst.cstn_samples_per_second': 81.83, 'train@por.rst.cstn_steps_per_second': 2.565, 'epoch': 1.0}
{'loss': 3.1984, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.493218183517456, 'eval_accuracy@por.rst.cstn': 0.39267015706806285, 'eval_f1@por.rst.cstn': 0.08247690623971592, 'eval_precision@por.rst.cstn': 0.1184314756645868, 'eval_recall@por.rst.cstn': 0.09537691108679071, 'eval_loss@por.rst.cstn': 2.493218421936035, 'eval_runtime': 7.3379, 'eval_samples_per_second': 78.088, 'eval_steps_per_second': 2.453, 'epoch': 1.0}
{'train@por.rst.cstn_loss': 1.8367944955825806, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5433944069431051, 'train@por.rst.cstn_f1@por.rst.cstn': 0.11361414709868463, 'train@por.rst.cstn_precision@por.rst.cstn': 0.11868033551964005, 'train@por.rst.cstn_recall@por.rst.cstn': 0.12096710547383555, 'train@por.rst.cstn_loss@por.rst.cstn': 1.8367946147918701, 'train@por.rst.cstn_runtime': 50.6256, 'train@por.rst.cstn_samples_per_second': 81.935, 'train@por.rst.cstn_steps_per_second': 2.568, 'epoch': 2.0}
{'loss': 2.0853, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.043889284133911, 'eval_accuracy@por.rst.cstn': 0.45898778359511344, 'eval_f1@por.rst.cstn': 0.14044396710348198, 'eval_precision@por.rst.cstn': 0.15204044712978707, 'eval_recall@por.rst.cstn': 0.15893995098303126, 'eval_loss@por.rst.cstn': 2.043889045715332, 'eval_runtime': 7.319, 'eval_samples_per_second': 78.289, 'eval_steps_per_second': 2.459, 'epoch': 2.0}
{'train@por.rst.cstn_loss': 1.6381973028182983, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5764223722275795, 'train@por.rst.cstn_f1@por.rst.cstn': 0.14284375206030903, 'train@por.rst.cstn_precision@por.rst.cstn': 0.1501908845733115, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14712726889877636, 'train@por.rst.cstn_loss@por.rst.cstn': 1.6381971836090088, 'train@por.rst.cstn_runtime': 50.6254, 'train@por.rst.cstn_samples_per_second': 81.935, 'train@por.rst.cstn_steps_per_second': 2.568, 'epoch': 3.0}
{'loss': 1.7939, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.8039367198944092, 'eval_accuracy@por.rst.cstn': 0.49912739965095987, 'eval_f1@por.rst.cstn': 0.1922284733294003, 'eval_precision@por.rst.cstn': 0.20493122232442, 'eval_recall@por.rst.cstn': 0.20256334257010353, 'eval_loss@por.rst.cstn': 1.8039367198944092, 'eval_runtime': 7.3212, 'eval_samples_per_second': 78.266, 'eval_steps_per_second': 2.459, 'epoch': 3.0}
{'train@por.rst.cstn_loss': 1.5394765138626099, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5904050144648023, 'train@por.rst.cstn_f1@por.rst.cstn': 0.15400060421471912, 'train@por.rst.cstn_precision@por.rst.cstn': 0.16797306929270936, 'train@por.rst.cstn_recall@por.rst.cstn': 0.15991156622194289, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5394765138626099, 'train@por.rst.cstn_runtime': 50.6098, 'train@por.rst.cstn_samples_per_second': 81.96, 'train@por.rst.cstn_steps_per_second': 2.569, 'epoch': 4.0}
{'loss': 1.6548, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.6958991289138794, 'eval_accuracy@por.rst.cstn': 0.5148342059336823, 'eval_f1@por.rst.cstn': 0.20728132239938485, 'eval_precision@por.rst.cstn': 0.22478641637724273, 'eval_recall@por.rst.cstn': 0.22077660539539049, 'eval_loss@por.rst.cstn': 1.6958991289138794, 'eval_runtime': 8.3367, 'eval_samples_per_second': 68.732, 'eval_steps_per_second': 2.159, 'epoch': 4.0}
{'train@por.rst.cstn_loss': 1.4776068925857544, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.6099324975891997, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1699425297413269, 'train@por.rst.cstn_precision@por.rst.cstn': 0.1737726312408193, 'train@por.rst.cstn_recall@por.rst.cstn': 0.17434837876988607, 'train@por.rst.cstn_loss@por.rst.cstn': 1.477607011795044, 'train@por.rst.cstn_runtime': 50.6003, 'train@por.rst.cstn_samples_per_second': 81.976, 'train@por.rst.cstn_steps_per_second': 2.569, 'epoch': 5.0}
{'loss': 1.578, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.650354266166687, 'eval_accuracy@por.rst.cstn': 0.5392670157068062, 'eval_f1@por.rst.cstn': 0.23378558557250625, 'eval_precision@por.rst.cstn': 0.24395378734770432, 'eval_recall@por.rst.cstn': 0.24405160778938606, 'eval_loss@por.rst.cstn': 1.6503545045852661, 'eval_runtime': 7.3395, 'eval_samples_per_second': 78.071, 'eval_steps_per_second': 2.452, 'epoch': 5.0}
{'train@por.rst.cstn_loss': 1.4357596635818481, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.6210221793635486, 'train@por.rst.cstn_f1@por.rst.cstn': 0.18351778471305713, 'train@por.rst.cstn_precision@por.rst.cstn': 0.2131622603154414, 'train@por.rst.cstn_recall@por.rst.cstn': 0.18519586616283332, 'train@por.rst.cstn_loss@por.rst.cstn': 1.4357597827911377, 'train@por.rst.cstn_runtime': 50.6335, 'train@por.rst.cstn_samples_per_second': 81.922, 'train@por.rst.cstn_steps_per_second': 2.567, 'epoch': 6.0}
{'loss': 1.5163, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.606135368347168, 'eval_accuracy@por.rst.cstn': 0.5479930191972077, 'eval_f1@por.rst.cstn': 0.2369491459703023, 'eval_precision@por.rst.cstn': 0.24218872424232699, 'eval_recall@por.rst.cstn': 0.2477345747888706, 'eval_loss@por.rst.cstn': 1.606135368347168, 'eval_runtime': 7.3249, 'eval_samples_per_second': 78.227, 'eval_steps_per_second': 2.457, 'epoch': 6.0}
{'train@por.rst.cstn_loss': 1.4103087186813354, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.6270491803278688, 'train@por.rst.cstn_f1@por.rst.cstn': 0.19011141831432177, 'train@por.rst.cstn_precision@por.rst.cstn': 0.21059578894216718, 'train@por.rst.cstn_recall@por.rst.cstn': 0.19207812238387845, 'train@por.rst.cstn_loss@por.rst.cstn': 1.4103089570999146, 'train@por.rst.cstn_runtime': 50.705, 'train@por.rst.cstn_samples_per_second': 81.807, 'train@por.rst.cstn_steps_per_second': 2.564, 'epoch': 7.0}
{'loss': 1.4855, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.5883899927139282, 'eval_accuracy@por.rst.cstn': 0.5445026178010471, 'eval_f1@por.rst.cstn': 0.23930299222879692, 'eval_precision@por.rst.cstn': 0.27483210240010525, 'eval_recall@por.rst.cstn': 0.25146950424930453, 'eval_loss@por.rst.cstn': 1.5883898735046387, 'eval_runtime': 7.3424, 'eval_samples_per_second': 78.04, 'eval_steps_per_second': 2.452, 'epoch': 7.0}
{'train@por.rst.cstn_loss': 1.392336368560791, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.630665380906461, 'train@por.rst.cstn_f1@por.rst.cstn': 0.19536113814014147, 'train@por.rst.cstn_precision@por.rst.cstn': 0.21591623339441823, 'train@por.rst.cstn_recall@por.rst.cstn': 0.19677056174585184, 'train@por.rst.cstn_loss@por.rst.cstn': 1.3923362493515015, 'train@por.rst.cstn_runtime': 50.7447, 'train@por.rst.cstn_samples_per_second': 81.743, 'train@por.rst.cstn_steps_per_second': 2.562, 'epoch': 8.0}
{'loss': 1.4637, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.5852097272872925, 'eval_accuracy@por.rst.cstn': 0.5532286212914486, 'eval_f1@por.rst.cstn': 0.2431080251679506, 'eval_precision@por.rst.cstn': 0.27891815040751217, 'eval_recall@por.rst.cstn': 0.25563071876000937, 'eval_loss@por.rst.cstn': 1.585209608078003, 'eval_runtime': 7.3333, 'eval_samples_per_second': 78.137, 'eval_steps_per_second': 2.455, 'epoch': 8.0}
{'train@por.rst.cstn_loss': 1.3767553567886353, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.6345226615236258, 'train@por.rst.cstn_f1@por.rst.cstn': 0.19892503300806183, 'train@por.rst.cstn_precision@por.rst.cstn': 0.2202331111372754, 'train@por.rst.cstn_recall@por.rst.cstn': 0.19969587918816636, 'train@por.rst.cstn_loss@por.rst.cstn': 1.3767553567886353, 'train@por.rst.cstn_runtime': 50.6505, 'train@por.rst.cstn_samples_per_second': 81.895, 'train@por.rst.cstn_steps_per_second': 2.567, 'epoch': 9.0}
{'loss': 1.4512, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.5692898035049438, 'eval_accuracy@por.rst.cstn': 0.5532286212914486, 'eval_f1@por.rst.cstn': 0.24381000400079447, 'eval_precision@por.rst.cstn': 0.27859871032317146, 'eval_recall@por.rst.cstn': 0.25582686129024795, 'eval_loss@por.rst.cstn': 1.5692899227142334, 'eval_runtime': 7.3466, 'eval_samples_per_second': 77.995, 'eval_steps_per_second': 2.45, 'epoch': 9.0}
{'train@por.rst.cstn_loss': 1.3682372570037842, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.6371745419479267, 'train@por.rst.cstn_f1@por.rst.cstn': 0.2018577132934458, 'train@por.rst.cstn_precision@por.rst.cstn': 0.2523818175087341, 'train@por.rst.cstn_recall@por.rst.cstn': 0.20450142799790222, 'train@por.rst.cstn_loss@por.rst.cstn': 1.368237018585205, 'train@por.rst.cstn_runtime': 50.6704, 'train@por.rst.cstn_samples_per_second': 81.862, 'train@por.rst.cstn_steps_per_second': 2.566, 'epoch': 10.0}
{'loss': 1.421, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.5565814971923828, 'eval_accuracy@por.rst.cstn': 0.5567190226876091, 'eval_f1@por.rst.cstn': 0.24224094658739836, 'eval_precision@por.rst.cstn': 0.23405464799960407, 'eval_recall@por.rst.cstn': 0.26488938389812117, 'eval_loss@por.rst.cstn': 1.5565814971923828, 'eval_runtime': 7.3463, 'eval_samples_per_second': 77.998, 'eval_steps_per_second': 2.45, 'epoch': 10.0}
{'train@por.rst.cstn_loss': 1.3616435527801514, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.6378977820636451, 'train@por.rst.cstn_f1@por.rst.cstn': 0.2027010210236619, 'train@por.rst.cstn_precision@por.rst.cstn': 0.25738513571311045, 'train@por.rst.cstn_recall@por.rst.cstn': 0.20484954230634375, 'train@por.rst.cstn_loss@por.rst.cstn': 1.3616435527801514, 'train@por.rst.cstn_runtime': 50.6705, 'train@por.rst.cstn_samples_per_second': 81.862, 'train@por.rst.cstn_steps_per_second': 2.566, 'epoch': 11.0}
{'loss': 1.4247, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.5552923679351807, 'eval_accuracy@por.rst.cstn': 0.5549738219895288, 'eval_f1@por.rst.cstn': 0.24219361175982942, 'eval_precision@por.rst.cstn': 0.23433554687348, 'eval_recall@por.rst.cstn': 0.2642967349203047, 'eval_loss@por.rst.cstn': 1.5552923679351807, 'eval_runtime': 7.3387, 'eval_samples_per_second': 78.079, 'eval_steps_per_second': 2.453, 'epoch': 11.0}
{'train@por.rst.cstn_loss': 1.3599773645401, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.6381388621022179, 'train@por.rst.cstn_f1@por.rst.cstn': 0.20138436624402267, 'train@por.rst.cstn_precision@por.rst.cstn': 0.25301814743395634, 'train@por.rst.cstn_recall@por.rst.cstn': 0.20407498610124608, 'train@por.rst.cstn_loss@por.rst.cstn': 1.3599773645401, 'train@por.rst.cstn_runtime': 50.7157, 'train@por.rst.cstn_samples_per_second': 81.789, 'train@por.rst.cstn_steps_per_second': 2.563, 'epoch': 12.0}
{'loss': 1.4151, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.5541032552719116, 'eval_accuracy@por.rst.cstn': 0.5549738219895288, 'eval_f1@por.rst.cstn': 0.2416742449426718, 'eval_precision@por.rst.cstn': 0.233575522804409, 'eval_recall@por.rst.cstn': 0.26437285497250135, 'eval_loss@por.rst.cstn': 1.5541032552719116, 'eval_runtime': 7.3275, 'eval_samples_per_second': 78.199, 'eval_steps_per_second': 2.457, 'epoch': 12.0}
{'train_runtime': 1976.3306, 'train_samples_per_second': 25.186, 'train_steps_per_second': 0.789, 'train_loss': 1.7073326502090846, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.3486
  train_runtime            = 2:02:48.93
  train_samples_per_second =     26.059
  train_steps_per_second   =      0.816
-------------------------------------------------------------------
Lang1:  eng.sdrt.stac    Lang2:  por.rst.cstn
Saving run to:  runs/full_shot/FullShot=v4_eng.sdrt.stac_por.rst.cstn_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 9580 examples
read 1145 examples
read 1510 examples
read 4148 examples
read 573 examples
read 272 examples
Total prediction labels:  48
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (pt): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (pt): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=48, bias=True)
    )
  )
)
{'train@eng.sdrt.stac_loss': 2.1145317554473877, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.3488517745302714, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.06454757141655733, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.07109055728270657, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.10898469021453158, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 2.1145317554473877, 'train@eng.sdrt.stac_runtime': 114.5756, 'train@eng.sdrt.stac_samples_per_second': 83.613, 'train@eng.sdrt.stac_steps_per_second': 2.618, 'epoch': 1.0}
{'loss': 2.6241, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.0721230506896973, 'eval_accuracy@eng.sdrt.stac': 0.3563318777292576, 'eval_f1@eng.sdrt.stac': 0.06471117026244513, 'eval_precision@eng.sdrt.stac': 0.048529200057584335, 'eval_recall@eng.sdrt.stac': 0.11118012114973475, 'eval_loss@eng.sdrt.stac': 2.072122812271118, 'eval_runtime': 14.0847, 'eval_samples_per_second': 81.294, 'eval_steps_per_second': 2.556, 'epoch': 1.0}
{'train@eng.sdrt.stac_loss': 1.883926272392273, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.418580375782881, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.12255022636503365, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.15139346897016262, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.17144937021569417, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.883926272392273, 'train@eng.sdrt.stac_runtime': 114.6171, 'train@eng.sdrt.stac_samples_per_second': 83.583, 'train@eng.sdrt.stac_steps_per_second': 2.617, 'epoch': 2.0}
{'loss': 2.0293, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.8440862894058228, 'eval_accuracy@eng.sdrt.stac': 0.4131004366812227, 'eval_f1@eng.sdrt.stac': 0.1168866843135761, 'eval_precision@eng.sdrt.stac': 0.1362206500234608, 'eval_recall@eng.sdrt.stac': 0.16899322009147433, 'eval_loss@eng.sdrt.stac': 1.8440864086151123, 'eval_runtime': 14.1304, 'eval_samples_per_second': 81.031, 'eval_steps_per_second': 2.548, 'epoch': 2.0}
{'train@eng.sdrt.stac_loss': 1.7881805896759033, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.44008350730688933, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.1491472894388094, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.13748106206124255, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.18833961494938994, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.7881805896759033, 'train@eng.sdrt.stac_runtime': 114.6675, 'train@eng.sdrt.stac_samples_per_second': 83.546, 'train@eng.sdrt.stac_steps_per_second': 2.616, 'epoch': 3.0}
{'loss': 1.8714, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.7458072900772095, 'eval_accuracy@eng.sdrt.stac': 0.4480349344978166, 'eval_f1@eng.sdrt.stac': 0.14320033948731592, 'eval_precision@eng.sdrt.stac': 0.1354801169292991, 'eval_recall@eng.sdrt.stac': 0.1862674605064174, 'eval_loss@eng.sdrt.stac': 1.745807409286499, 'eval_runtime': 14.1396, 'eval_samples_per_second': 80.978, 'eval_steps_per_second': 2.546, 'epoch': 3.0}
{'train@eng.sdrt.stac_loss': 1.720407247543335, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4552192066805846, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.16896158472851772, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.23979929512051162, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.20038245723063633, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.720407247543335, 'train@eng.sdrt.stac_runtime': 114.6764, 'train@eng.sdrt.stac_samples_per_second': 83.539, 'train@eng.sdrt.stac_steps_per_second': 2.616, 'epoch': 4.0}
{'loss': 1.7926, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.6772301197052002, 'eval_accuracy@eng.sdrt.stac': 0.45327510917030567, 'eval_f1@eng.sdrt.stac': 0.15846374119969475, 'eval_precision@eng.sdrt.stac': 0.21404250494460397, 'eval_recall@eng.sdrt.stac': 0.19414740324108937, 'eval_loss@eng.sdrt.stac': 1.6772301197052002, 'eval_runtime': 14.1141, 'eval_samples_per_second': 81.125, 'eval_steps_per_second': 2.551, 'epoch': 4.0}
{'train@eng.sdrt.stac_loss': 1.6699455976486206, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4767223382045929, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.20203951667743733, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.21418233702806772, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2248018152280276, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.6699455976486206, 'train@eng.sdrt.stac_runtime': 114.4928, 'train@eng.sdrt.stac_samples_per_second': 83.673, 'train@eng.sdrt.stac_steps_per_second': 2.62, 'epoch': 5.0}
{'loss': 1.7385, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.6295640468597412, 'eval_accuracy@eng.sdrt.stac': 0.47685589519650656, 'eval_f1@eng.sdrt.stac': 0.18882437673903468, 'eval_precision@eng.sdrt.stac': 0.21087837250378488, 'eval_recall@eng.sdrt.stac': 0.2154333685095331, 'eval_loss@eng.sdrt.stac': 1.6295640468597412, 'eval_runtime': 14.0885, 'eval_samples_per_second': 81.272, 'eval_steps_per_second': 2.555, 'epoch': 5.0}
{'train@eng.sdrt.stac_loss': 1.6289786100387573, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4824634655532359, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.20331506682266662, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.24338528781230054, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2262546863919626, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.6289786100387573, 'train@eng.sdrt.stac_runtime': 114.4184, 'train@eng.sdrt.stac_samples_per_second': 83.728, 'train@eng.sdrt.stac_steps_per_second': 2.622, 'epoch': 6.0}
{'loss': 1.6903, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.6041696071624756, 'eval_accuracy@eng.sdrt.stac': 0.4733624454148472, 'eval_f1@eng.sdrt.stac': 0.18578740467727206, 'eval_precision@eng.sdrt.stac': 0.2126085764229967, 'eval_recall@eng.sdrt.stac': 0.21188324217793597, 'eval_loss@eng.sdrt.stac': 1.6041696071624756, 'eval_runtime': 14.1067, 'eval_samples_per_second': 81.167, 'eval_steps_per_second': 2.552, 'epoch': 6.0}
{'train@eng.sdrt.stac_loss': 1.6052507162094116, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.49551148225469727, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2253163599632667, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.23941193570313113, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.24562666863735885, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.605250597000122, 'train@eng.sdrt.stac_runtime': 114.7017, 'train@eng.sdrt.stac_samples_per_second': 83.521, 'train@eng.sdrt.stac_steps_per_second': 2.615, 'epoch': 7.0}
{'loss': 1.6594, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.5820363759994507, 'eval_accuracy@eng.sdrt.stac': 0.48995633187772925, 'eval_f1@eng.sdrt.stac': 0.21076560961648735, 'eval_precision@eng.sdrt.stac': 0.24051175529202856, 'eval_recall@eng.sdrt.stac': 0.23092100578208236, 'eval_loss@eng.sdrt.stac': 1.5820364952087402, 'eval_runtime': 14.0911, 'eval_samples_per_second': 81.257, 'eval_steps_per_second': 2.555, 'epoch': 7.0}
{'train@eng.sdrt.stac_loss': 1.5742337703704834, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5028183716075156, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.23206147193795754, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.24268315147764769, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.25438501724676066, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5742337703704834, 'train@eng.sdrt.stac_runtime': 114.628, 'train@eng.sdrt.stac_samples_per_second': 83.575, 'train@eng.sdrt.stac_steps_per_second': 2.617, 'epoch': 8.0}
{'loss': 1.6345, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.5500434637069702, 'eval_accuracy@eng.sdrt.stac': 0.5056768558951965, 'eval_f1@eng.sdrt.stac': 0.2193256615368722, 'eval_precision@eng.sdrt.stac': 0.219736049756248, 'eval_recall@eng.sdrt.stac': 0.24189017387975764, 'eval_loss@eng.sdrt.stac': 1.5500434637069702, 'eval_runtime': 14.1273, 'eval_samples_per_second': 81.049, 'eval_steps_per_second': 2.548, 'epoch': 8.0}
{'train@eng.sdrt.stac_loss': 1.5564799308776855, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.508455114822547, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.24011970032986657, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.3344952119942539, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.26226912541091796, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.556480050086975, 'train@eng.sdrt.stac_runtime': 114.717, 'train@eng.sdrt.stac_samples_per_second': 83.51, 'train@eng.sdrt.stac_steps_per_second': 2.615, 'epoch': 9.0}
{'loss': 1.6123, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.5357637405395508, 'eval_accuracy@eng.sdrt.stac': 0.5152838427947598, 'eval_f1@eng.sdrt.stac': 0.23573715466843004, 'eval_precision@eng.sdrt.stac': 0.27315485550635915, 'eval_recall@eng.sdrt.stac': 0.2543559391722184, 'eval_loss@eng.sdrt.stac': 1.5357639789581299, 'eval_runtime': 14.0811, 'eval_samples_per_second': 81.315, 'eval_steps_per_second': 2.557, 'epoch': 9.0}
{'train@eng.sdrt.stac_loss': 1.5411300659179688, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5092901878914405, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.24320925141625693, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.31856638675178794, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2648483372202902, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5411299467086792, 'train@eng.sdrt.stac_runtime': 114.4519, 'train@eng.sdrt.stac_samples_per_second': 83.703, 'train@eng.sdrt.stac_steps_per_second': 2.621, 'epoch': 10.0}
{'loss': 1.5953, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.5231248140335083, 'eval_accuracy@eng.sdrt.stac': 0.519650655021834, 'eval_f1@eng.sdrt.stac': 0.2396603706888884, 'eval_precision@eng.sdrt.stac': 0.30619250127110265, 'eval_recall@eng.sdrt.stac': 0.2578091988524314, 'eval_loss@eng.sdrt.stac': 1.5231249332427979, 'eval_runtime': 14.0887, 'eval_samples_per_second': 81.271, 'eval_steps_per_second': 2.555, 'epoch': 10.0}
{'train@eng.sdrt.stac_loss': 1.5333114862442017, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.512839248434238, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.25278715188981027, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.32056128037997567, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2718184874398075, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5333114862442017, 'train@eng.sdrt.stac_runtime': 114.5342, 'train@eng.sdrt.stac_samples_per_second': 83.643, 'train@eng.sdrt.stac_steps_per_second': 2.619, 'epoch': 11.0}
{'loss': 1.5854, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.5170903205871582, 'eval_accuracy@eng.sdrt.stac': 0.5187772925764192, 'eval_f1@eng.sdrt.stac': 0.23939324112090432, 'eval_precision@eng.sdrt.stac': 0.3014995883430002, 'eval_recall@eng.sdrt.stac': 0.2581795041594792, 'eval_loss@eng.sdrt.stac': 1.5170904397964478, 'eval_runtime': 14.106, 'eval_samples_per_second': 81.171, 'eval_steps_per_second': 2.552, 'epoch': 11.0}
{'train@eng.sdrt.stac_loss': 1.5313187837600708, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5131524008350731, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2561484281236481, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.32086679028265613, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.27399929468558465, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.53131902217865, 'train@eng.sdrt.stac_runtime': 114.6776, 'train@eng.sdrt.stac_samples_per_second': 83.539, 'train@eng.sdrt.stac_steps_per_second': 2.616, 'epoch': 12.0}
{'loss': 1.5809, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.5157005786895752, 'eval_accuracy@eng.sdrt.stac': 0.519650655021834, 'eval_f1@eng.sdrt.stac': 0.23865090328954197, 'eval_precision@eng.sdrt.stac': 0.3004044055297518, 'eval_recall@eng.sdrt.stac': 0.2575357021181147, 'eval_loss@eng.sdrt.stac': 1.5157006978988647, 'eval_runtime': 14.0937, 'eval_samples_per_second': 81.242, 'eval_steps_per_second': 2.554, 'epoch': 12.0}
{'train_runtime': 4439.4054, 'train_samples_per_second': 25.895, 'train_steps_per_second': 0.811, 'train_loss': 1.7845021989610461, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.7845
  train_runtime            = 1:13:59.40
  train_samples_per_second =     25.895
  train_steps_per_second   =      0.811
{'train@por.rst.cstn_loss': 2.34450101852417, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.3471552555448409, 'train@por.rst.cstn_f1@por.rst.cstn': 0.04087622341245356, 'train@por.rst.cstn_precision@por.rst.cstn': 0.059587271546768086, 'train@por.rst.cstn_recall@por.rst.cstn': 0.05571838704694573, 'train@por.rst.cstn_loss@por.rst.cstn': 2.34450101852417, 'train@por.rst.cstn_runtime': 50.7055, 'train@por.rst.cstn_samples_per_second': 81.806, 'train@por.rst.cstn_steps_per_second': 2.564, 'epoch': 1.0}
{'loss': 3.1905, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.453859567642212, 'eval_accuracy@por.rst.cstn': 0.31762652705061084, 'eval_f1@por.rst.cstn': 0.054227241529918264, 'eval_precision@por.rst.cstn': 0.04933558454729854, 'eval_recall@por.rst.cstn': 0.07657033348732954, 'eval_loss@por.rst.cstn': 2.453859806060791, 'eval_runtime': 7.4197, 'eval_samples_per_second': 77.226, 'eval_steps_per_second': 2.426, 'epoch': 1.0}
{'train@por.rst.cstn_loss': 2.083850145339966, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.43201542912246865, 'train@por.rst.cstn_f1@por.rst.cstn': 0.06865215124861057, 'train@por.rst.cstn_precision@por.rst.cstn': 0.08173827731689401, 'train@por.rst.cstn_recall@por.rst.cstn': 0.07647010927044584, 'train@por.rst.cstn_loss@por.rst.cstn': 2.0838499069213867, 'train@por.rst.cstn_runtime': 50.822, 'train@por.rst.cstn_samples_per_second': 81.618, 'train@por.rst.cstn_steps_per_second': 2.558, 'epoch': 2.0}
{'loss': 2.2514, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.220456123352051, 'eval_accuracy@por.rst.cstn': 0.36649214659685864, 'eval_f1@por.rst.cstn': 0.08735853683697643, 'eval_precision@por.rst.cstn': 0.11491519819504416, 'eval_recall@por.rst.cstn': 0.09925516720295167, 'eval_loss@por.rst.cstn': 2.22045636177063, 'eval_runtime': 7.4222, 'eval_samples_per_second': 77.201, 'eval_steps_per_second': 2.425, 'epoch': 2.0}
{'train@por.rst.cstn_loss': 1.910483717918396, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5, 'train@por.rst.cstn_f1@por.rst.cstn': 0.0854214105813067, 'train@por.rst.cstn_precision@por.rst.cstn': 0.07984676583950615, 'train@por.rst.cstn_recall@por.rst.cstn': 0.09657536791133195, 'train@por.rst.cstn_loss@por.rst.cstn': 1.9104835987091064, 'train@por.rst.cstn_runtime': 50.7656, 'train@por.rst.cstn_samples_per_second': 81.709, 'train@por.rst.cstn_steps_per_second': 2.561, 'epoch': 3.0}
{'loss': 2.055, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.0364208221435547, 'eval_accuracy@por.rst.cstn': 0.41012216404886565, 'eval_f1@por.rst.cstn': 0.1125059113416642, 'eval_precision@por.rst.cstn': 0.10208066615691248, 'eval_recall@por.rst.cstn': 0.13381860148034477, 'eval_loss@por.rst.cstn': 2.036421060562134, 'eval_runtime': 7.4317, 'eval_samples_per_second': 77.102, 'eval_steps_per_second': 2.422, 'epoch': 3.0}
{'train@por.rst.cstn_loss': 1.7873402833938599, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5248312439729991, 'train@por.rst.cstn_f1@por.rst.cstn': 0.10047304744565991, 'train@por.rst.cstn_precision@por.rst.cstn': 0.12276881395156591, 'train@por.rst.cstn_recall@por.rst.cstn': 0.10965734725821596, 'train@por.rst.cstn_loss@por.rst.cstn': 1.7873402833938599, 'train@por.rst.cstn_runtime': 50.7338, 'train@por.rst.cstn_samples_per_second': 81.76, 'train@por.rst.cstn_steps_per_second': 2.562, 'epoch': 4.0}
{'loss': 1.9057, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.9243475198745728, 'eval_accuracy@por.rst.cstn': 0.4293193717277487, 'eval_f1@por.rst.cstn': 0.13060783121148314, 'eval_precision@por.rst.cstn': 0.18098051332968323, 'eval_recall@por.rst.cstn': 0.15215503875270883, 'eval_loss@por.rst.cstn': 1.9243472814559937, 'eval_runtime': 7.3933, 'eval_samples_per_second': 77.503, 'eval_steps_per_second': 2.435, 'epoch': 4.0}
{'train@por.rst.cstn_loss': 1.6883485317230225, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5540019286403086, 'train@por.rst.cstn_f1@por.rst.cstn': 0.12151074941911909, 'train@por.rst.cstn_precision@por.rst.cstn': 0.12182690699106351, 'train@por.rst.cstn_recall@por.rst.cstn': 0.13168552330136923, 'train@por.rst.cstn_loss@por.rst.cstn': 1.688348650932312, 'train@por.rst.cstn_runtime': 50.7203, 'train@por.rst.cstn_samples_per_second': 81.782, 'train@por.rst.cstn_steps_per_second': 2.563, 'epoch': 5.0}
{'loss': 1.8036, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.8466460704803467, 'eval_accuracy@por.rst.cstn': 0.45549738219895286, 'eval_f1@por.rst.cstn': 0.15748457544016922, 'eval_precision@por.rst.cstn': 0.15665678820323065, 'eval_recall@por.rst.cstn': 0.17869247919175105, 'eval_loss@por.rst.cstn': 1.8466461896896362, 'eval_runtime': 8.3125, 'eval_samples_per_second': 68.932, 'eval_steps_per_second': 2.165, 'epoch': 5.0}
{'train@por.rst.cstn_loss': 1.6362024545669556, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5643683702989393, 'train@por.rst.cstn_f1@por.rst.cstn': 0.12447248671857063, 'train@por.rst.cstn_precision@por.rst.cstn': 0.1227793489127341, 'train@por.rst.cstn_recall@por.rst.cstn': 0.13547573609146377, 'train@por.rst.cstn_loss@por.rst.cstn': 1.636202335357666, 'train@por.rst.cstn_runtime': 50.7788, 'train@por.rst.cstn_samples_per_second': 81.688, 'train@por.rst.cstn_steps_per_second': 2.56, 'epoch': 6.0}
{'loss': 1.731, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.7971868515014648, 'eval_accuracy@por.rst.cstn': 0.4642233856893543, 'eval_f1@por.rst.cstn': 0.1653663735006928, 'eval_precision@por.rst.cstn': 0.16249456358613423, 'eval_recall@por.rst.cstn': 0.18461373820225493, 'eval_loss@por.rst.cstn': 1.7971868515014648, 'eval_runtime': 7.3906, 'eval_samples_per_second': 77.531, 'eval_steps_per_second': 2.436, 'epoch': 6.0}
{'train@por.rst.cstn_loss': 1.595444917678833, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.567261330761813, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1267902810721993, 'train@por.rst.cstn_precision@por.rst.cstn': 0.12484038032739185, 'train@por.rst.cstn_recall@por.rst.cstn': 0.13978895209525677, 'train@por.rst.cstn_loss@por.rst.cstn': 1.595444917678833, 'train@por.rst.cstn_runtime': 50.8356, 'train@por.rst.cstn_samples_per_second': 81.596, 'train@por.rst.cstn_steps_per_second': 2.557, 'epoch': 7.0}
{'loss': 1.6831, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.7603440284729004, 'eval_accuracy@por.rst.cstn': 0.4694589877835951, 'eval_f1@por.rst.cstn': 0.1670500459136823, 'eval_precision@por.rst.cstn': 0.15795882546366105, 'eval_recall@por.rst.cstn': 0.19114678689777503, 'eval_loss@por.rst.cstn': 1.7603440284729004, 'eval_runtime': 7.4215, 'eval_samples_per_second': 77.208, 'eval_steps_per_second': 2.425, 'epoch': 7.0}
{'train@por.rst.cstn_loss': 1.5701544284820557, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5740115718418515, 'train@por.rst.cstn_f1@por.rst.cstn': 0.13001245084582103, 'train@por.rst.cstn_precision@por.rst.cstn': 0.14272930119441796, 'train@por.rst.cstn_recall@por.rst.cstn': 0.142274736357693, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5701543092727661, 'train@por.rst.cstn_runtime': 50.8361, 'train@por.rst.cstn_samples_per_second': 81.596, 'train@por.rst.cstn_steps_per_second': 2.557, 'epoch': 8.0}
{'loss': 1.6501, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.7426108121871948, 'eval_accuracy@por.rst.cstn': 0.47643979057591623, 'eval_f1@por.rst.cstn': 0.17072441278197675, 'eval_precision@por.rst.cstn': 0.16243229219673772, 'eval_recall@por.rst.cstn': 0.1928641072077727, 'eval_loss@por.rst.cstn': 1.7426106929779053, 'eval_runtime': 7.3722, 'eval_samples_per_second': 77.724, 'eval_steps_per_second': 2.442, 'epoch': 8.0}
{'train@por.rst.cstn_loss': 1.5526100397109985, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5752169720347156, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1322719528118767, 'train@por.rst.cstn_precision@por.rst.cstn': 0.14816669056949416, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1437082872274877, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5526098012924194, 'train@por.rst.cstn_runtime': 50.7628, 'train@por.rst.cstn_samples_per_second': 81.713, 'train@por.rst.cstn_steps_per_second': 2.561, 'epoch': 9.0}
{'loss': 1.6223, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.7224425077438354, 'eval_accuracy@por.rst.cstn': 0.47643979057591623, 'eval_f1@por.rst.cstn': 0.17165739757004142, 'eval_precision@por.rst.cstn': 0.163579913945713, 'eval_recall@por.rst.cstn': 0.19306024973801134, 'eval_loss@por.rst.cstn': 1.7224425077438354, 'eval_runtime': 7.4316, 'eval_samples_per_second': 77.104, 'eval_steps_per_second': 2.422, 'epoch': 9.0}
{'train@por.rst.cstn_loss': 1.5381544828414917, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5802796528447445, 'train@por.rst.cstn_f1@por.rst.cstn': 0.13623807648430375, 'train@por.rst.cstn_precision@por.rst.cstn': 0.14940813297473327, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14738266744708406, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5381544828414917, 'train@por.rst.cstn_runtime': 50.7794, 'train@por.rst.cstn_samples_per_second': 81.687, 'train@por.rst.cstn_steps_per_second': 2.56, 'epoch': 10.0}
{'loss': 1.5974, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.7084364891052246, 'eval_accuracy@por.rst.cstn': 0.4799301919720768, 'eval_f1@por.rst.cstn': 0.17829014656885678, 'eval_precision@por.rst.cstn': 0.1954928848914223, 'eval_recall@por.rst.cstn': 0.1986558840807853, 'eval_loss@por.rst.cstn': 1.7084364891052246, 'eval_runtime': 7.3914, 'eval_samples_per_second': 77.523, 'eval_steps_per_second': 2.435, 'epoch': 10.0}
{'train@por.rst.cstn_loss': 1.5290117263793945, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.584619093539055, 'train@por.rst.cstn_f1@por.rst.cstn': 0.13922672285211793, 'train@por.rst.cstn_precision@por.rst.cstn': 0.18369483716575571, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1496010317973166, 'train@por.rst.cstn_loss@por.rst.cstn': 1.529011607170105, 'train@por.rst.cstn_runtime': 50.7825, 'train@por.rst.cstn_samples_per_second': 81.682, 'train@por.rst.cstn_steps_per_second': 2.56, 'epoch': 11.0}
{'loss': 1.603, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.7013587951660156, 'eval_accuracy@por.rst.cstn': 0.4886561954624782, 'eval_f1@por.rst.cstn': 0.18308538805939253, 'eval_precision@por.rst.cstn': 0.2086792833433446, 'eval_recall@por.rst.cstn': 0.20359160492559703, 'eval_loss@por.rst.cstn': 1.7013585567474365, 'eval_runtime': 7.3831, 'eval_samples_per_second': 77.609, 'eval_steps_per_second': 2.438, 'epoch': 11.0}
{'train@por.rst.cstn_loss': 1.5267601013183594, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5851012536162006, 'train@por.rst.cstn_f1@por.rst.cstn': 0.13944034615748507, 'train@por.rst.cstn_precision@por.rst.cstn': 0.18324982354609992, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14972591743459976, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5267601013183594, 'train@por.rst.cstn_runtime': 50.7322, 'train@por.rst.cstn_samples_per_second': 81.763, 'train@por.rst.cstn_steps_per_second': 2.562, 'epoch': 12.0}
{'loss': 1.588, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.698931097984314, 'eval_accuracy@por.rst.cstn': 0.493891797556719, 'eval_f1@por.rst.cstn': 0.1907885031545602, 'eval_precision@por.rst.cstn': 0.22224628295731874, 'eval_recall@por.rst.cstn': 0.20841254156471545, 'eval_loss@por.rst.cstn': 1.6989309787750244, 'eval_runtime': 7.4044, 'eval_samples_per_second': 77.386, 'eval_steps_per_second': 2.431, 'epoch': 12.0}
{'train_runtime': 1978.5679, 'train_samples_per_second': 25.158, 'train_steps_per_second': 0.788, 'train_loss': 1.890090580475636, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.7845
  train_runtime            = 1:13:59.40
  train_samples_per_second =     25.895
  train_steps_per_second   =      0.811
-------------------------------------------------------------------
Lang1:  fas.rst.prstc    Lang2:  por.rst.cstn
Saving run to:  runs/full_shot/FullShot=v4_fas.rst.prstc_por.rst.cstn_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 4100 examples
read 499 examples
read 592 examples
read 4148 examples
read 573 examples
read 272 examples
Total prediction labels:  39
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (ar): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (pt): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (pt): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=39, bias=True)
    )
  )
)
{'train@fas.rst.prstc_loss': 2.4383513927459717, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.23780487804878048, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.02260214430599826, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.013988522238163558, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.058823529411764705, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.4383513927459717, 'train@fas.rst.prstc_runtime': 49.1998, 'train@fas.rst.prstc_samples_per_second': 83.334, 'train@fas.rst.prstc_steps_per_second': 2.622, 'epoch': 1.0}
{'loss': 2.905, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.3636252880096436, 'eval_accuracy@fas.rst.prstc': 0.24448897795591182, 'eval_f1@fas.rst.prstc': 0.026194310252281264, 'eval_precision@fas.rst.prstc': 0.016299265197060788, 'eval_recall@fas.rst.prstc': 0.06666666666666667, 'eval_loss@fas.rst.prstc': 2.3636255264282227, 'eval_runtime': 6.3094, 'eval_samples_per_second': 79.088, 'eval_steps_per_second': 2.536, 'epoch': 1.0}
{'train@fas.rst.prstc_loss': 2.3658816814422607, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.26170731707317074, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04139203919044426, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.030024566317724127, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.06800188269524941, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3658816814422607, 'train@fas.rst.prstc_runtime': 49.2469, 'train@fas.rst.prstc_samples_per_second': 83.254, 'train@fas.rst.prstc_steps_per_second': 2.619, 'epoch': 2.0}
{'loss': 2.4205, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.2804012298583984, 'eval_accuracy@fas.rst.prstc': 0.28857715430861725, 'eval_f1@fas.rst.prstc': 0.05128327753865774, 'eval_precision@fas.rst.prstc': 0.03780054061744202, 'eval_recall@fas.rst.prstc': 0.08045141363744358, 'eval_loss@fas.rst.prstc': 2.2804009914398193, 'eval_runtime': 6.3207, 'eval_samples_per_second': 78.946, 'eval_steps_per_second': 2.531, 'epoch': 2.0}
{'train@fas.rst.prstc_loss': 2.345088243484497, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.24829268292682927, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.029922310519012285, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.030818953635812343, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.0619789694382936, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.345088481903076, 'train@fas.rst.prstc_runtime': 49.246, 'train@fas.rst.prstc_samples_per_second': 83.255, 'train@fas.rst.prstc_steps_per_second': 2.619, 'epoch': 3.0}
{'loss': 2.3689, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.2601232528686523, 'eval_accuracy@fas.rst.prstc': 0.250501002004008, 'eval_f1@fas.rst.prstc': 0.03232636963086214, 'eval_precision@fas.rst.prstc': 0.03656228253305498, 'eval_recall@fas.rst.prstc': 0.0685055832739368, 'eval_loss@fas.rst.prstc': 2.2601234912872314, 'eval_runtime': 6.3114, 'eval_samples_per_second': 79.063, 'eval_steps_per_second': 2.535, 'epoch': 3.0}
{'train@fas.rst.prstc_loss': 2.3293561935424805, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.23780487804878048, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.02260214430599826, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.013988522238163558, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.058823529411764705, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3293559551239014, 'train@fas.rst.prstc_runtime': 49.2412, 'train@fas.rst.prstc_samples_per_second': 83.264, 'train@fas.rst.prstc_steps_per_second': 2.62, 'epoch': 4.0}
{'loss': 2.3556, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.249789237976074, 'eval_accuracy@fas.rst.prstc': 0.24448897795591182, 'eval_f1@fas.rst.prstc': 0.026194310252281264, 'eval_precision@fas.rst.prstc': 0.016299265197060788, 'eval_recall@fas.rst.prstc': 0.06666666666666667, 'eval_loss@fas.rst.prstc': 2.2497894763946533, 'eval_runtime': 6.3174, 'eval_samples_per_second': 78.989, 'eval_steps_per_second': 2.533, 'epoch': 4.0}
{'train@fas.rst.prstc_loss': 2.316427707672119, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2704878048780488, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.03910904872555539, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.0326596304907835, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.06860070815014495, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.316427707672119, 'train@fas.rst.prstc_runtime': 49.218, 'train@fas.rst.prstc_samples_per_second': 83.303, 'train@fas.rst.prstc_steps_per_second': 2.621, 'epoch': 5.0}
{'loss': 2.3372, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.235642671585083, 'eval_accuracy@fas.rst.prstc': 0.26452905811623245, 'eval_f1@fas.rst.prstc': 0.04067146282973621, 'eval_precision@fas.rst.prstc': 0.03597305919886565, 'eval_recall@fas.rst.prstc': 0.07272986457590877, 'eval_loss@fas.rst.prstc': 2.235642671585083, 'eval_runtime': 6.3494, 'eval_samples_per_second': 78.591, 'eval_steps_per_second': 2.52, 'epoch': 5.0}
{'train@fas.rst.prstc_loss': 2.30131196975708, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2451219512195122, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.02729104347008314, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.0340753649577179, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.060955467839072346, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.30131196975708, 'train@fas.rst.prstc_runtime': 49.2691, 'train@fas.rst.prstc_samples_per_second': 83.216, 'train@fas.rst.prstc_steps_per_second': 2.618, 'epoch': 6.0}
{'loss': 2.3226, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.2264328002929688, 'eval_accuracy@fas.rst.prstc': 0.24649298597194388, 'eval_f1@fas.rst.prstc': 0.02848667312121735, 'eval_precision@fas.rst.prstc': 0.033095723014256624, 'eval_recall@fas.rst.prstc': 0.06727963886909004, 'eval_loss@fas.rst.prstc': 2.2264328002929688, 'eval_runtime': 6.3464, 'eval_samples_per_second': 78.627, 'eval_steps_per_second': 2.521, 'epoch': 6.0}
{'train@fas.rst.prstc_loss': 2.277310848236084, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2873170731707317, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.0436941862562652, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03372439740227301, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07359156210220041, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.277310609817505, 'train@fas.rst.prstc_runtime': 49.7288, 'train@fas.rst.prstc_samples_per_second': 82.447, 'train@fas.rst.prstc_steps_per_second': 2.594, 'epoch': 7.0}
{'loss': 2.3095, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.1960766315460205, 'eval_accuracy@fas.rst.prstc': 0.2865731462925852, 'eval_f1@fas.rst.prstc': 0.04810918865130465, 'eval_precision@fas.rst.prstc': 0.03924699772801039, 'eval_recall@fas.rst.prstc': 0.07920646234259919, 'eval_loss@fas.rst.prstc': 2.1960763931274414, 'eval_runtime': 6.2898, 'eval_samples_per_second': 79.335, 'eval_steps_per_second': 2.544, 'epoch': 7.0}
{'train@fas.rst.prstc_loss': 2.2415714263916016, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2990243902439024, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04740444636291934, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03425819747478897, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07777542440871602, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.2415711879730225, 'train@fas.rst.prstc_runtime': 49.2413, 'train@fas.rst.prstc_samples_per_second': 83.263, 'train@fas.rst.prstc_steps_per_second': 2.62, 'epoch': 8.0}
{'loss': 2.2896, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.1638970375061035, 'eval_accuracy@fas.rst.prstc': 0.31462925851703405, 'eval_f1@fas.rst.prstc': 0.055819249670382355, 'eval_precision@fas.rst.prstc': 0.04117032392894462, 'eval_recall@fas.rst.prstc': 0.08768828700403897, 'eval_loss@fas.rst.prstc': 2.1638967990875244, 'eval_runtime': 6.2988, 'eval_samples_per_second': 79.221, 'eval_steps_per_second': 2.54, 'epoch': 8.0}
{'train@fas.rst.prstc_loss': 2.2082128524780273, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3048780487804878, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.049742357347678706, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.0361986743468416, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08016174063733512, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.2082128524780273, 'train@fas.rst.prstc_runtime': 49.1636, 'train@fas.rst.prstc_samples_per_second': 83.395, 'train@fas.rst.prstc_steps_per_second': 2.624, 'epoch': 9.0}
{'loss': 2.2544, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.1235439777374268, 'eval_accuracy@fas.rst.prstc': 0.3286573146292585, 'eval_f1@fas.rst.prstc': 0.060251294740048894, 'eval_precision@fas.rst.prstc': 0.045143859419768825, 'eval_recall@fas.rst.prstc': 0.09207887859349013, 'eval_loss@fas.rst.prstc': 2.1235437393188477, 'eval_runtime': 6.3106, 'eval_samples_per_second': 79.073, 'eval_steps_per_second': 2.535, 'epoch': 9.0}
{'train@fas.rst.prstc_loss': 2.1878483295440674, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.30853658536585366, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.05079780796042255, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.037309513297048416, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08147149749152252, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1878483295440674, 'train@fas.rst.prstc_runtime': 49.2139, 'train@fas.rst.prstc_samples_per_second': 83.31, 'train@fas.rst.prstc_steps_per_second': 2.621, 'epoch': 10.0}
{'loss': 2.2363, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.0986168384552, 'eval_accuracy@fas.rst.prstc': 0.3286573146292585, 'eval_f1@fas.rst.prstc': 0.06070726719537133, 'eval_precision@fas.rst.prstc': 0.04593331117757837, 'eval_recall@fas.rst.prstc': 0.09214540270848183, 'eval_loss@fas.rst.prstc': 2.0986170768737793, 'eval_runtime': 6.3196, 'eval_samples_per_second': 78.961, 'eval_steps_per_second': 2.532, 'epoch': 10.0}
{'train@fas.rst.prstc_loss': 2.1788570880889893, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3124390243902439, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.051646359765856784, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03814176548393213, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.0826575954986468, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.17885684967041, 'train@fas.rst.prstc_runtime': 49.3274, 'train@fas.rst.prstc_samples_per_second': 83.118, 'train@fas.rst.prstc_steps_per_second': 2.615, 'epoch': 11.0}
{'loss': 2.2176, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.0879719257354736, 'eval_accuracy@fas.rst.prstc': 0.3346693386773547, 'eval_f1@fas.rst.prstc': 0.06198717948717948, 'eval_precision@fas.rst.prstc': 0.047126861080349455, 'eval_recall@fas.rst.prstc': 0.09388453314326443, 'eval_loss@fas.rst.prstc': 2.0879716873168945, 'eval_runtime': 6.283, 'eval_samples_per_second': 79.421, 'eval_steps_per_second': 2.547, 'epoch': 11.0}
{'train@fas.rst.prstc_loss': 2.178140640258789, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3131707317073171, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.05173860262438197, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.038182644672018116, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08284779050736497, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.17814040184021, 'train@fas.rst.prstc_runtime': 49.2641, 'train@fas.rst.prstc_samples_per_second': 83.225, 'train@fas.rst.prstc_steps_per_second': 2.619, 'epoch': 12.0}
{'loss': 2.2059, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.0870907306671143, 'eval_accuracy@fas.rst.prstc': 0.3346693386773547, 'eval_f1@fas.rst.prstc': 0.06198717948717948, 'eval_precision@fas.rst.prstc': 0.047126861080349455, 'eval_recall@fas.rst.prstc': 0.09388453314326443, 'eval_loss@fas.rst.prstc': 2.0870907306671143, 'eval_runtime': 6.3341, 'eval_samples_per_second': 78.78, 'eval_steps_per_second': 2.526, 'epoch': 12.0}
{'train_runtime': 1908.1618, 'train_samples_per_second': 25.784, 'train_steps_per_second': 0.811, 'train_loss': 2.351932318635689, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.3519
  train_runtime            = 0:31:48.16
  train_samples_per_second =     25.784
  train_steps_per_second   =      0.811
{'train@por.rst.cstn_loss': 2.406068801879883, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.3201542912246866, 'train@por.rst.cstn_f1@por.rst.cstn': 0.03449897792845096, 'train@por.rst.cstn_precision@por.rst.cstn': 0.04462179762623682, 'train@por.rst.cstn_recall@por.rst.cstn': 0.04625777326307355, 'train@por.rst.cstn_loss@por.rst.cstn': 2.406069278717041, 'train@por.rst.cstn_runtime': 50.7005, 'train@por.rst.cstn_samples_per_second': 81.814, 'train@por.rst.cstn_steps_per_second': 2.564, 'epoch': 1.0}
{'loss': 2.9564, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.5125691890716553, 'eval_accuracy@por.rst.cstn': 0.2949389179755672, 'eval_f1@por.rst.cstn': 0.04025900756507244, 'eval_precision@por.rst.cstn': 0.04251727233693743, 'eval_recall@por.rst.cstn': 0.059759081498211936, 'eval_loss@por.rst.cstn': 2.5125694274902344, 'eval_runtime': 7.3171, 'eval_samples_per_second': 78.31, 'eval_steps_per_second': 2.46, 'epoch': 1.0}
{'train@por.rst.cstn_loss': 2.187767505645752, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.3700578592092575, 'train@por.rst.cstn_f1@por.rst.cstn': 0.05187176852213319, 'train@por.rst.cstn_precision@por.rst.cstn': 0.07735735729916166, 'train@por.rst.cstn_recall@por.rst.cstn': 0.06250208561152551, 'train@por.rst.cstn_loss@por.rst.cstn': 2.187767744064331, 'train@por.rst.cstn_runtime': 50.6643, 'train@por.rst.cstn_samples_per_second': 81.872, 'train@por.rst.cstn_steps_per_second': 2.566, 'epoch': 2.0}
{'loss': 2.3284, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.3265509605407715, 'eval_accuracy@por.rst.cstn': 0.3158813263525305, 'eval_f1@por.rst.cstn': 0.06067839286728041, 'eval_precision@por.rst.cstn': 0.09203876399868248, 'eval_recall@por.rst.cstn': 0.08398869523791513, 'eval_loss@por.rst.cstn': 2.3265509605407715, 'eval_runtime': 7.35, 'eval_samples_per_second': 77.959, 'eval_steps_per_second': 2.449, 'epoch': 2.0}
{'train@por.rst.cstn_loss': 2.027993679046631, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.4609450337512054, 'train@por.rst.cstn_f1@por.rst.cstn': 0.0814691499817799, 'train@por.rst.cstn_precision@por.rst.cstn': 0.09356125164598439, 'train@por.rst.cstn_recall@por.rst.cstn': 0.09007342129794477, 'train@por.rst.cstn_loss@por.rst.cstn': 2.027993679046631, 'train@por.rst.cstn_runtime': 50.703, 'train@por.rst.cstn_samples_per_second': 81.81, 'train@por.rst.cstn_steps_per_second': 2.564, 'epoch': 3.0}
{'loss': 2.1536, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.163217067718506, 'eval_accuracy@por.rst.cstn': 0.3961605584642234, 'eval_f1@por.rst.cstn': 0.10619113912734077, 'eval_precision@por.rst.cstn': 0.11224842409052935, 'eval_recall@por.rst.cstn': 0.12920089930179382, 'eval_loss@por.rst.cstn': 2.163217067718506, 'eval_runtime': 7.3125, 'eval_samples_per_second': 78.359, 'eval_steps_per_second': 2.462, 'epoch': 3.0}
{'train@por.rst.cstn_loss': 1.8776469230651855, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5209739633558341, 'train@por.rst.cstn_f1@por.rst.cstn': 0.09398972582904827, 'train@por.rst.cstn_precision@por.rst.cstn': 0.08990274136609969, 'train@por.rst.cstn_recall@por.rst.cstn': 0.10551733586237892, 'train@por.rst.cstn_loss@por.rst.cstn': 1.877647042274475, 'train@por.rst.cstn_runtime': 50.6902, 'train@por.rst.cstn_samples_per_second': 81.83, 'train@por.rst.cstn_steps_per_second': 2.565, 'epoch': 4.0}
{'loss': 2.0094, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.0225110054016113, 'eval_accuracy@por.rst.cstn': 0.4328097731239092, 'eval_f1@por.rst.cstn': 0.11955764633399739, 'eval_precision@por.rst.cstn': 0.11225072534082398, 'eval_recall@por.rst.cstn': 0.145417779987158, 'eval_loss@por.rst.cstn': 2.0225107669830322, 'eval_runtime': 7.2965, 'eval_samples_per_second': 78.531, 'eval_steps_per_second': 2.467, 'epoch': 4.0}
{'train@por.rst.cstn_loss': 1.7756147384643555, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5364030858244937, 'train@por.rst.cstn_f1@por.rst.cstn': 0.10884486419951944, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13889003704620329, 'train@por.rst.cstn_recall@por.rst.cstn': 0.11721365804957908, 'train@por.rst.cstn_loss@por.rst.cstn': 1.775614619255066, 'train@por.rst.cstn_runtime': 50.6322, 'train@por.rst.cstn_samples_per_second': 81.924, 'train@por.rst.cstn_steps_per_second': 2.568, 'epoch': 5.0}
{'loss': 1.8892, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.9320696592330933, 'eval_accuracy@por.rst.cstn': 0.4607329842931937, 'eval_f1@por.rst.cstn': 0.1417665114435532, 'eval_precision@por.rst.cstn': 0.1549193456668436, 'eval_recall@por.rst.cstn': 0.16117872459925717, 'eval_loss@por.rst.cstn': 1.9320693016052246, 'eval_runtime': 7.3323, 'eval_samples_per_second': 78.148, 'eval_steps_per_second': 2.455, 'epoch': 5.0}
{'train@por.rst.cstn_loss': 1.7007964849472046, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5581002892960463, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1221073122959041, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13805836085017623, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1295265826397822, 'train@por.rst.cstn_loss@por.rst.cstn': 1.700796365737915, 'train@por.rst.cstn_runtime': 50.588, 'train@por.rst.cstn_samples_per_second': 81.996, 'train@por.rst.cstn_steps_per_second': 2.57, 'epoch': 6.0}
{'loss': 1.7976, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.8674885034561157, 'eval_accuracy@por.rst.cstn': 0.48342059336823734, 'eval_f1@por.rst.cstn': 0.15727872997361067, 'eval_precision@por.rst.cstn': 0.15719722580777348, 'eval_recall@por.rst.cstn': 0.17574403730010138, 'eval_loss@por.rst.cstn': 1.8674885034561157, 'eval_runtime': 7.3204, 'eval_samples_per_second': 78.274, 'eval_steps_per_second': 2.459, 'epoch': 6.0}
{'train@por.rst.cstn_loss': 1.6466008424758911, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5699132111861138, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1300654904876978, 'train@por.rst.cstn_precision@por.rst.cstn': 0.15511976650898715, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1409937305837081, 'train@por.rst.cstn_loss@por.rst.cstn': 1.6466008424758911, 'train@por.rst.cstn_runtime': 50.6851, 'train@por.rst.cstn_samples_per_second': 81.839, 'train@por.rst.cstn_steps_per_second': 2.565, 'epoch': 7.0}
{'loss': 1.7368, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.8161970376968384, 'eval_accuracy@por.rst.cstn': 0.48342059336823734, 'eval_f1@por.rst.cstn': 0.16518822198937674, 'eval_precision@por.rst.cstn': 0.16614250088865193, 'eval_recall@por.rst.cstn': 0.18438529174175108, 'eval_loss@por.rst.cstn': 1.8161967992782593, 'eval_runtime': 7.3064, 'eval_samples_per_second': 78.424, 'eval_steps_per_second': 2.464, 'epoch': 7.0}
{'train@por.rst.cstn_loss': 1.612929105758667, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5754580520732884, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1333053098250358, 'train@por.rst.cstn_precision@por.rst.cstn': 0.15306064383388135, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14288306910175086, 'train@por.rst.cstn_loss@por.rst.cstn': 1.612929105758667, 'train@por.rst.cstn_runtime': 50.6206, 'train@por.rst.cstn_samples_per_second': 81.943, 'train@por.rst.cstn_steps_per_second': 2.568, 'epoch': 8.0}
{'loss': 1.689, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.797115683555603, 'eval_accuracy@por.rst.cstn': 0.5008726003490401, 'eval_f1@por.rst.cstn': 0.17649633513025573, 'eval_precision@por.rst.cstn': 0.1796213689329148, 'eval_recall@por.rst.cstn': 0.19329656111172652, 'eval_loss@por.rst.cstn': 1.7971159219741821, 'eval_runtime': 7.3147, 'eval_samples_per_second': 78.335, 'eval_steps_per_second': 2.461, 'epoch': 8.0}
{'train@por.rst.cstn_loss': 1.5855436325073242, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5826904532304725, 'train@por.rst.cstn_f1@por.rst.cstn': 0.13611659967914824, 'train@por.rst.cstn_precision@por.rst.cstn': 0.146387406063752, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14618007347611478, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5855436325073242, 'train@por.rst.cstn_runtime': 50.6013, 'train@por.rst.cstn_samples_per_second': 81.974, 'train@por.rst.cstn_steps_per_second': 2.569, 'epoch': 9.0}
{'loss': 1.6621, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.7698779106140137, 'eval_accuracy@por.rst.cstn': 0.5043630017452007, 'eval_f1@por.rst.cstn': 0.1842538397312502, 'eval_precision@por.rst.cstn': 0.2236917563027027, 'eval_recall@por.rst.cstn': 0.19756290879912683, 'eval_loss@por.rst.cstn': 1.7698783874511719, 'eval_runtime': 10.1233, 'eval_samples_per_second': 56.602, 'eval_steps_per_second': 1.778, 'epoch': 9.0}
{'train@por.rst.cstn_loss': 1.5702896118164062, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.587029893924783, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1402904993562406, 'train@por.rst.cstn_precision@por.rst.cstn': 0.1441793148490561, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1513546217851832, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5702897310256958, 'train@por.rst.cstn_runtime': 50.6601, 'train@por.rst.cstn_samples_per_second': 81.879, 'train@por.rst.cstn_steps_per_second': 2.566, 'epoch': 10.0}
{'loss': 1.6313, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.754498839378357, 'eval_accuracy@por.rst.cstn': 0.506108202443281, 'eval_f1@por.rst.cstn': 0.19036260250538053, 'eval_precision@por.rst.cstn': 0.22312391007777607, 'eval_recall@por.rst.cstn': 0.20556761183711467, 'eval_loss@por.rst.cstn': 1.7544989585876465, 'eval_runtime': 7.3425, 'eval_samples_per_second': 78.039, 'eval_steps_per_second': 2.451, 'epoch': 10.0}
{'train@por.rst.cstn_loss': 1.5596529245376587, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5872709739633558, 'train@por.rst.cstn_f1@por.rst.cstn': 0.14093734963999133, 'train@por.rst.cstn_precision@por.rst.cstn': 0.14382380326097535, 'train@por.rst.cstn_recall@por.rst.cstn': 0.15194862310684454, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5596530437469482, 'train@por.rst.cstn_runtime': 50.7179, 'train@por.rst.cstn_samples_per_second': 81.786, 'train@por.rst.cstn_steps_per_second': 2.563, 'epoch': 11.0}
{'loss': 1.6214, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.745710015296936, 'eval_accuracy@por.rst.cstn': 0.5078534031413613, 'eval_f1@por.rst.cstn': 0.19210079905513233, 'eval_precision@por.rst.cstn': 0.20177857346749314, 'eval_recall@por.rst.cstn': 0.2092377720862223, 'eval_loss@por.rst.cstn': 1.745710015296936, 'eval_runtime': 7.341, 'eval_samples_per_second': 78.054, 'eval_steps_per_second': 2.452, 'epoch': 11.0}
{'train@por.rst.cstn_loss': 1.5567489862442017, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5896817743490839, 'train@por.rst.cstn_f1@por.rst.cstn': 0.14142104059050647, 'train@por.rst.cstn_precision@por.rst.cstn': 0.1441391974181462, 'train@por.rst.cstn_recall@por.rst.cstn': 0.15237549382543342, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5567492246627808, 'train@por.rst.cstn_runtime': 50.7165, 'train@por.rst.cstn_samples_per_second': 81.788, 'train@por.rst.cstn_steps_per_second': 2.563, 'epoch': 12.0}
{'loss': 1.6125, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.7428679466247559, 'eval_accuracy@por.rst.cstn': 0.506108202443281, 'eval_f1@por.rst.cstn': 0.19169969205397813, 'eval_precision@por.rst.cstn': 0.20137220207974768, 'eval_recall@por.rst.cstn': 0.20875930318670075, 'eval_loss@por.rst.cstn': 1.7428679466247559, 'eval_runtime': 7.325, 'eval_samples_per_second': 78.226, 'eval_steps_per_second': 2.457, 'epoch': 12.0}
{'train_runtime': 1978.1899, 'train_samples_per_second': 25.162, 'train_steps_per_second': 0.789, 'train_loss': 1.9239837548671626, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.3519
  train_runtime            = 0:31:48.16
  train_samples_per_second =     25.784
  train_steps_per_second   =      0.811
-------------------------------------------------------------------
Lang1:  fra.sdrt.annodis    Lang2:  por.rst.cstn
Saving run to:  runs/full_shot/FullShot=v4_fra.sdrt.annodis_por.rst.cstn_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2185 examples
read 528 examples
read 625 examples
read 4148 examples
read 573 examples
read 272 examples
Total prediction labels:  45
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (fr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (pt): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (pt): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=45, bias=True)
    )
  )
)
{'train@fra.sdrt.annodis_loss': 2.780571937561035, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.2320366132723112, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.03818294408438205, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.03143172861969084, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.06528725596925587, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.780571699142456, 'train@fra.sdrt.annodis_runtime': 26.8505, 'train@fra.sdrt.annodis_samples_per_second': 81.376, 'train@fra.sdrt.annodis_steps_per_second': 2.57, 'epoch': 1.0}
{'loss': 3.2854, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.7954092025756836, 'eval_accuracy@fra.sdrt.annodis': 0.23863636363636365, 'eval_f1@fra.sdrt.annodis': 0.03951404312700406, 'eval_precision@fra.sdrt.annodis': 0.056485565827450936, 'eval_recall@fra.sdrt.annodis': 0.06463299575102968, 'eval_loss@fra.sdrt.annodis': 2.7954089641571045, 'eval_runtime': 6.8003, 'eval_samples_per_second': 77.643, 'eval_steps_per_second': 2.5, 'epoch': 1.0}
{'train@fra.sdrt.annodis_loss': 2.43218731880188, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.2645308924485126, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.05564045876038832, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.04535278971455035, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.07834651807819025, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.432187557220459, 'train@fra.sdrt.annodis_runtime': 26.9476, 'train@fra.sdrt.annodis_samples_per_second': 81.083, 'train@fra.sdrt.annodis_steps_per_second': 2.561, 'epoch': 2.0}
{'loss': 2.5926, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.4384982585906982, 'eval_accuracy@fra.sdrt.annodis': 0.25757575757575757, 'eval_f1@fra.sdrt.annodis': 0.054351765248232625, 'eval_precision@fra.sdrt.annodis': 0.04476188906865774, 'eval_recall@fra.sdrt.annodis': 0.07372966597611803, 'eval_loss@fra.sdrt.annodis': 2.4384982585906982, 'eval_runtime': 6.8429, 'eval_samples_per_second': 77.161, 'eval_steps_per_second': 2.484, 'epoch': 2.0}
{'train@fra.sdrt.annodis_loss': 2.3426623344421387, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.26361556064073227, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.053366846080032844, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.05241203673798468, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.07590784234314144, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.3426623344421387, 'train@fra.sdrt.annodis_runtime': 26.9729, 'train@fra.sdrt.annodis_samples_per_second': 81.007, 'train@fra.sdrt.annodis_steps_per_second': 2.558, 'epoch': 3.0}
{'loss': 2.4116, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.349944591522217, 'eval_accuracy@fra.sdrt.annodis': 0.25, 'eval_f1@fra.sdrt.annodis': 0.05183890485831835, 'eval_precision@fra.sdrt.annodis': 0.05177391557759656, 'eval_recall@fra.sdrt.annodis': 0.07030429529564179, 'eval_loss@fra.sdrt.annodis': 2.3499443531036377, 'eval_runtime': 6.8148, 'eval_samples_per_second': 77.479, 'eval_steps_per_second': 2.495, 'epoch': 3.0}
{'train@fra.sdrt.annodis_loss': 2.288884162902832, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.2759725400457666, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.057831632786558104, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.10017571939298482, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.08027351642946112, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.288884162902832, 'train@fra.sdrt.annodis_runtime': 26.9352, 'train@fra.sdrt.annodis_samples_per_second': 81.121, 'train@fra.sdrt.annodis_steps_per_second': 2.562, 'epoch': 4.0}
{'loss': 2.3528, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.296436071395874, 'eval_accuracy@fra.sdrt.annodis': 0.26704545454545453, 'eval_f1@fra.sdrt.annodis': 0.05667037594053369, 'eval_precision@fra.sdrt.annodis': 0.05523865047016429, 'eval_recall@fra.sdrt.annodis': 0.07540059963043694, 'eval_loss@fra.sdrt.annodis': 2.296436071395874, 'eval_runtime': 6.8406, 'eval_samples_per_second': 77.186, 'eval_steps_per_second': 2.485, 'epoch': 4.0}
{'train@fra.sdrt.annodis_loss': 2.2465131282806396, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.32311212814645307, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.07327932184947866, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.09943351193859681, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.09823061376533647, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.2465133666992188, 'train@fra.sdrt.annodis_runtime': 26.9078, 'train@fra.sdrt.annodis_samples_per_second': 81.203, 'train@fra.sdrt.annodis_steps_per_second': 2.564, 'epoch': 5.0}
{'loss': 2.3026, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.2604379653930664, 'eval_accuracy@fra.sdrt.annodis': 0.2840909090909091, 'eval_f1@fra.sdrt.annodis': 0.061036168853506316, 'eval_precision@fra.sdrt.annodis': 0.04807692307692308, 'eval_recall@fra.sdrt.annodis': 0.08424173567821888, 'eval_loss@fra.sdrt.annodis': 2.2604379653930664, 'eval_runtime': 6.7987, 'eval_samples_per_second': 77.662, 'eval_steps_per_second': 2.5, 'epoch': 5.0}
{'train@fra.sdrt.annodis_loss': 2.208174705505371, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.3437070938215103, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.08263192288725267, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.09259659659747986, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.10688063304237655, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.208174467086792, 'train@fra.sdrt.annodis_runtime': 26.8908, 'train@fra.sdrt.annodis_samples_per_second': 81.255, 'train@fra.sdrt.annodis_steps_per_second': 2.566, 'epoch': 6.0}
{'loss': 2.2561, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.227287530899048, 'eval_accuracy@fra.sdrt.annodis': 0.30113636363636365, 'eval_f1@fra.sdrt.annodis': 0.06453259647808168, 'eval_precision@fra.sdrt.annodis': 0.05093009051030041, 'eval_recall@fra.sdrt.annodis': 0.09065607663738505, 'eval_loss@fra.sdrt.annodis': 2.227287530899048, 'eval_runtime': 6.7893, 'eval_samples_per_second': 77.77, 'eval_steps_per_second': 2.504, 'epoch': 6.0}
{'train@fra.sdrt.annodis_loss': 2.1728708744049072, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.35469107551487417, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.08858334105326766, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.08873479353741344, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1119555238824498, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.1728711128234863, 'train@fra.sdrt.annodis_runtime': 26.8911, 'train@fra.sdrt.annodis_samples_per_second': 81.254, 'train@fra.sdrt.annodis_steps_per_second': 2.566, 'epoch': 7.0}
{'loss': 2.2244, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.1974658966064453, 'eval_accuracy@fra.sdrt.annodis': 0.2916666666666667, 'eval_f1@fra.sdrt.annodis': 0.06270175259952154, 'eval_precision@fra.sdrt.annodis': 0.04901313019790613, 'eval_recall@fra.sdrt.annodis': 0.08744778007498673, 'eval_loss@fra.sdrt.annodis': 2.1974661350250244, 'eval_runtime': 6.8324, 'eval_samples_per_second': 77.278, 'eval_steps_per_second': 2.488, 'epoch': 7.0}
{'train@fra.sdrt.annodis_loss': 2.1411073207855225, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.36613272311212813, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.09274177298841055, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.08805775594320069, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.11640918934818739, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.1411073207855225, 'train@fra.sdrt.annodis_runtime': 26.9612, 'train@fra.sdrt.annodis_samples_per_second': 81.043, 'train@fra.sdrt.annodis_steps_per_second': 2.559, 'epoch': 8.0}
{'loss': 2.1995, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.1718499660491943, 'eval_accuracy@fra.sdrt.annodis': 0.30113636363636365, 'eval_f1@fra.sdrt.annodis': 0.06673653278306226, 'eval_precision@fra.sdrt.annodis': 0.06023927284279251, 'eval_recall@fra.sdrt.annodis': 0.09152996586214038, 'eval_loss@fra.sdrt.annodis': 2.1718497276306152, 'eval_runtime': 6.844, 'eval_samples_per_second': 77.148, 'eval_steps_per_second': 2.484, 'epoch': 8.0}
{'train@fra.sdrt.annodis_loss': 2.1160130500793457, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.37620137299771167, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.10421938749503425, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.13447698745766778, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1236302207523939, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.1160130500793457, 'train@fra.sdrt.annodis_runtime': 26.9188, 'train@fra.sdrt.annodis_samples_per_second': 81.17, 'train@fra.sdrt.annodis_steps_per_second': 2.563, 'epoch': 9.0}
{'loss': 2.1743, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.1512606143951416, 'eval_accuracy@fra.sdrt.annodis': 0.30492424242424243, 'eval_f1@fra.sdrt.annodis': 0.07289172923681303, 'eval_precision@fra.sdrt.annodis': 0.11593649868467074, 'eval_recall@fra.sdrt.annodis': 0.09483536031651779, 'eval_loss@fra.sdrt.annodis': 2.1512603759765625, 'eval_runtime': 6.8157, 'eval_samples_per_second': 77.468, 'eval_steps_per_second': 2.494, 'epoch': 9.0}
{'train@fra.sdrt.annodis_loss': 2.0971217155456543, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.3848970251716247, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.11336845495190155, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.13250046328246812, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.13037417330697287, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.0971219539642334, 'train@fra.sdrt.annodis_runtime': 26.914, 'train@fra.sdrt.annodis_samples_per_second': 81.185, 'train@fra.sdrt.annodis_steps_per_second': 2.564, 'epoch': 10.0}
{'loss': 2.1497, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.1369988918304443, 'eval_accuracy@fra.sdrt.annodis': 0.3106060606060606, 'eval_f1@fra.sdrt.annodis': 0.07450366393726647, 'eval_precision@fra.sdrt.annodis': 0.11344438537420992, 'eval_recall@fra.sdrt.annodis': 0.09705963374189887, 'eval_loss@fra.sdrt.annodis': 2.1369986534118652, 'eval_runtime': 6.8445, 'eval_samples_per_second': 77.143, 'eval_steps_per_second': 2.484, 'epoch': 10.0}
{'train@fra.sdrt.annodis_loss': 2.0853328704833984, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.3922196796338673, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.11815477382471773, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.13118803956737782, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.13483070384421988, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.0853331089019775, 'train@fra.sdrt.annodis_runtime': 26.9578, 'train@fra.sdrt.annodis_samples_per_second': 81.053, 'train@fra.sdrt.annodis_steps_per_second': 2.56, 'epoch': 11.0}
{'loss': 2.1243, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.1270978450775146, 'eval_accuracy@fra.sdrt.annodis': 0.3106060606060606, 'eval_f1@fra.sdrt.annodis': 0.07909523234812067, 'eval_precision@fra.sdrt.annodis': 0.10180991650008743, 'eval_recall@fra.sdrt.annodis': 0.09932660659710701, 'eval_loss@fra.sdrt.annodis': 2.1270978450775146, 'eval_runtime': 6.8264, 'eval_samples_per_second': 77.347, 'eval_steps_per_second': 2.49, 'epoch': 11.0}
{'train@fra.sdrt.annodis_loss': 2.0813536643981934, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.3917620137299771, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.11804081705724531, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12747309315483565, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.13500864102673024, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.0813534259796143, 'train@fra.sdrt.annodis_runtime': 26.8996, 'train@fra.sdrt.annodis_samples_per_second': 81.228, 'train@fra.sdrt.annodis_steps_per_second': 2.565, 'epoch': 12.0}
{'loss': 2.1267, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.1241953372955322, 'eval_accuracy@fra.sdrt.annodis': 0.3087121212121212, 'eval_f1@fra.sdrt.annodis': 0.0787018498096098, 'eval_precision@fra.sdrt.annodis': 0.10104280966238889, 'eval_recall@fra.sdrt.annodis': 0.09880739579752237, 'eval_loss@fra.sdrt.annodis': 2.1241953372955322, 'eval_runtime': 6.8681, 'eval_samples_per_second': 76.878, 'eval_steps_per_second': 2.475, 'epoch': 12.0}
{'train_runtime': 1077.0059, 'train_samples_per_second': 24.345, 'train_steps_per_second': 0.769, 'train_loss': 2.3499913054387926, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =       2.35
  train_runtime            = 0:17:57.00
  train_samples_per_second =     24.345
  train_steps_per_second   =      0.769
{'train@por.rst.cstn_loss': 2.4481661319732666, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.2938765670202507, 'train@por.rst.cstn_f1@por.rst.cstn': 0.023509616109223544, 'train@por.rst.cstn_precision@por.rst.cstn': 0.043204852579852585, 'train@por.rst.cstn_recall@por.rst.cstn': 0.036977790219194555, 'train@por.rst.cstn_loss@por.rst.cstn': 2.4481661319732666, 'train@por.rst.cstn_runtime': 50.6928, 'train@por.rst.cstn_samples_per_second': 81.826, 'train@por.rst.cstn_steps_per_second': 2.564, 'epoch': 1.0}
{'loss': 3.0601, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.5354084968566895, 'eval_accuracy@por.rst.cstn': 0.2862129144851658, 'eval_f1@por.rst.cstn': 0.027598142851220822, 'eval_precision@por.rst.cstn': 0.05829346092503987, 'eval_recall@por.rst.cstn': 0.04958677685950413, 'eval_loss@por.rst.cstn': 2.5354084968566895, 'eval_runtime': 7.3663, 'eval_samples_per_second': 77.787, 'eval_steps_per_second': 2.444, 'epoch': 1.0}
{'train@por.rst.cstn_loss': 2.058196544647217, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.4648023143683703, 'train@por.rst.cstn_f1@por.rst.cstn': 0.0750250787683236, 'train@por.rst.cstn_precision@por.rst.cstn': 0.07439857120530843, 'train@por.rst.cstn_recall@por.rst.cstn': 0.08669221189184413, 'train@por.rst.cstn_loss@por.rst.cstn': 2.058196544647217, 'train@por.rst.cstn_runtime': 50.7342, 'train@por.rst.cstn_samples_per_second': 81.759, 'train@por.rst.cstn_steps_per_second': 2.562, 'epoch': 2.0}
{'loss': 2.28, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.205064058303833, 'eval_accuracy@por.rst.cstn': 0.39790575916230364, 'eval_f1@por.rst.cstn': 0.10152370261969122, 'eval_precision@por.rst.cstn': 0.09763898610760192, 'eval_recall@por.rst.cstn': 0.12257314144145846, 'eval_loss@por.rst.cstn': 2.205064058303833, 'eval_runtime': 7.3795, 'eval_samples_per_second': 77.647, 'eval_steps_per_second': 2.439, 'epoch': 2.0}
{'train@por.rst.cstn_loss': 1.8601306676864624, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5096432015429122, 'train@por.rst.cstn_f1@por.rst.cstn': 0.09957866137150598, 'train@por.rst.cstn_precision@por.rst.cstn': 0.10028476570612978, 'train@por.rst.cstn_recall@por.rst.cstn': 0.11143148878629858, 'train@por.rst.cstn_loss@por.rst.cstn': 1.8601306676864624, 'train@por.rst.cstn_runtime': 50.7283, 'train@por.rst.cstn_samples_per_second': 81.769, 'train@por.rst.cstn_steps_per_second': 2.563, 'epoch': 3.0}
{'loss': 2.0183, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.016864061355591, 'eval_accuracy@por.rst.cstn': 0.418848167539267, 'eval_f1@por.rst.cstn': 0.11788356249091075, 'eval_precision@por.rst.cstn': 0.12927606719568982, 'eval_recall@por.rst.cstn': 0.1469872729494115, 'eval_loss@por.rst.cstn': 2.016864061355591, 'eval_runtime': 7.3732, 'eval_samples_per_second': 77.714, 'eval_steps_per_second': 2.441, 'epoch': 3.0}
{'train@por.rst.cstn_loss': 1.7419798374176025, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5402603664416586, 'train@por.rst.cstn_f1@por.rst.cstn': 0.10990118816278065, 'train@por.rst.cstn_precision@por.rst.cstn': 0.11174450241836888, 'train@por.rst.cstn_recall@por.rst.cstn': 0.12748071064698627, 'train@por.rst.cstn_loss@por.rst.cstn': 1.7419798374176025, 'train@por.rst.cstn_runtime': 50.7573, 'train@por.rst.cstn_samples_per_second': 81.722, 'train@por.rst.cstn_steps_per_second': 2.561, 'epoch': 4.0}
{'loss': 1.8653, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.9089921712875366, 'eval_accuracy@por.rst.cstn': 0.4642233856893543, 'eval_f1@por.rst.cstn': 0.14460179385936636, 'eval_precision@por.rst.cstn': 0.15077071832009972, 'eval_recall@por.rst.cstn': 0.17859379254220112, 'eval_loss@por.rst.cstn': 1.908992052078247, 'eval_runtime': 7.4095, 'eval_samples_per_second': 77.333, 'eval_steps_per_second': 2.429, 'epoch': 4.0}
{'train@por.rst.cstn_loss': 1.6649123430252075, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5511089681774349, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1175381764473097, 'train@por.rst.cstn_precision@por.rst.cstn': 0.12422252407185885, 'train@por.rst.cstn_recall@por.rst.cstn': 0.13291295396612204, 'train@por.rst.cstn_loss@por.rst.cstn': 1.664912462234497, 'train@por.rst.cstn_runtime': 50.724, 'train@por.rst.cstn_samples_per_second': 81.776, 'train@por.rst.cstn_steps_per_second': 2.563, 'epoch': 5.0}
{'loss': 1.7704, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.837674617767334, 'eval_accuracy@por.rst.cstn': 0.4694589877835951, 'eval_f1@por.rst.cstn': 0.1569111230004542, 'eval_precision@por.rst.cstn': 0.16988046596537693, 'eval_recall@por.rst.cstn': 0.186296807115405, 'eval_loss@por.rst.cstn': 1.8376743793487549, 'eval_runtime': 7.4032, 'eval_samples_per_second': 77.399, 'eval_steps_per_second': 2.431, 'epoch': 5.0}
{'train@por.rst.cstn_loss': 1.6084314584732056, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.569672131147541, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1274770156957429, 'train@por.rst.cstn_precision@por.rst.cstn': 0.12967247066721976, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14112033553046474, 'train@por.rst.cstn_loss@por.rst.cstn': 1.608431339263916, 'train@por.rst.cstn_runtime': 50.7113, 'train@por.rst.cstn_samples_per_second': 81.796, 'train@por.rst.cstn_steps_per_second': 2.564, 'epoch': 6.0}
{'loss': 1.6954, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.7827023267745972, 'eval_accuracy@por.rst.cstn': 0.49912739965095987, 'eval_f1@por.rst.cstn': 0.18052131942717198, 'eval_precision@por.rst.cstn': 0.22828382459487961, 'eval_recall@por.rst.cstn': 0.20142134291291772, 'eval_loss@por.rst.cstn': 1.7827023267745972, 'eval_runtime': 7.358, 'eval_samples_per_second': 77.875, 'eval_steps_per_second': 2.446, 'epoch': 6.0}
{'train@por.rst.cstn_loss': 1.5690312385559082, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5875120540019286, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1412915988234515, 'train@por.rst.cstn_precision@por.rst.cstn': 0.14928763138217416, 'train@por.rst.cstn_recall@por.rst.cstn': 0.15266875843140573, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5690311193466187, 'train@por.rst.cstn_runtime': 50.6958, 'train@por.rst.cstn_samples_per_second': 81.821, 'train@por.rst.cstn_steps_per_second': 2.564, 'epoch': 7.0}
{'loss': 1.649, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.7439765930175781, 'eval_accuracy@por.rst.cstn': 0.5078534031413613, 'eval_f1@por.rst.cstn': 0.19273978736188369, 'eval_precision@por.rst.cstn': 0.20249456425927015, 'eval_recall@por.rst.cstn': 0.21146879955908454, 'eval_loss@por.rst.cstn': 1.7439769506454468, 'eval_runtime': 7.3726, 'eval_samples_per_second': 77.72, 'eval_steps_per_second': 2.441, 'epoch': 7.0}
{'train@por.rst.cstn_loss': 1.545377492904663, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5918514946962391, 'train@por.rst.cstn_f1@por.rst.cstn': 0.14426246790497546, 'train@por.rst.cstn_precision@por.rst.cstn': 0.1491379174618938, 'train@por.rst.cstn_recall@por.rst.cstn': 0.15518146586709863, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5453773736953735, 'train@por.rst.cstn_runtime': 50.6696, 'train@por.rst.cstn_samples_per_second': 81.864, 'train@por.rst.cstn_steps_per_second': 2.566, 'epoch': 8.0}
{'loss': 1.6192, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.7259271144866943, 'eval_accuracy@por.rst.cstn': 0.5113438045375218, 'eval_f1@por.rst.cstn': 0.19613155928965192, 'eval_precision@por.rst.cstn': 0.2017806662567839, 'eval_recall@por.rst.cstn': 0.21379790955409847, 'eval_loss@por.rst.cstn': 1.7259272336959839, 'eval_runtime': 7.4151, 'eval_samples_per_second': 77.275, 'eval_steps_per_second': 2.427, 'epoch': 8.0}
{'train@por.rst.cstn_loss': 1.5263547897338867, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5949855351976856, 'train@por.rst.cstn_f1@por.rst.cstn': 0.14698226738704634, 'train@por.rst.cstn_precision@por.rst.cstn': 0.14886326758225726, 'train@por.rst.cstn_recall@por.rst.cstn': 0.15765104770863939, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5263549089431763, 'train@por.rst.cstn_runtime': 50.774, 'train@por.rst.cstn_samples_per_second': 81.695, 'train@por.rst.cstn_steps_per_second': 2.56, 'epoch': 9.0}
{'loss': 1.5968, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.7056701183319092, 'eval_accuracy@por.rst.cstn': 0.5165794066317626, 'eval_f1@por.rst.cstn': 0.20168674303719153, 'eval_precision@por.rst.cstn': 0.2025715436880411, 'eval_recall@por.rst.cstn': 0.2190111312536942, 'eval_loss@por.rst.cstn': 1.7056701183319092, 'eval_runtime': 7.372, 'eval_samples_per_second': 77.726, 'eval_steps_per_second': 2.442, 'epoch': 9.0}
{'train@por.rst.cstn_loss': 1.513179063796997, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.600771456123433, 'train@por.rst.cstn_f1@por.rst.cstn': 0.15055870739057026, 'train@por.rst.cstn_precision@por.rst.cstn': 0.14834252457595834, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1621363281973643, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5131789445877075, 'train@por.rst.cstn_runtime': 50.7922, 'train@por.rst.cstn_samples_per_second': 81.666, 'train@por.rst.cstn_steps_per_second': 2.559, 'epoch': 10.0}
{'loss': 1.5704, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.6929113864898682, 'eval_accuracy@por.rst.cstn': 0.5270506108202443, 'eval_f1@por.rst.cstn': 0.2076358597734126, 'eval_precision@por.rst.cstn': 0.20553258540787198, 'eval_recall@por.rst.cstn': 0.22636195456301553, 'eval_loss@por.rst.cstn': 1.6929112672805786, 'eval_runtime': 7.3958, 'eval_samples_per_second': 77.476, 'eval_steps_per_second': 2.434, 'epoch': 10.0}
{'train@por.rst.cstn_loss': 1.506266713142395, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.6024590163934426, 'train@por.rst.cstn_f1@por.rst.cstn': 0.15142067113887095, 'train@por.rst.cstn_precision@por.rst.cstn': 0.1492138652005798, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1628529722841811, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5062665939331055, 'train@por.rst.cstn_runtime': 50.7665, 'train@por.rst.cstn_samples_per_second': 81.707, 'train@por.rst.cstn_steps_per_second': 2.561, 'epoch': 11.0}
{'loss': 1.5699, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.6879867315292358, 'eval_accuracy@por.rst.cstn': 0.525305410122164, 'eval_f1@por.rst.cstn': 0.20788603985321671, 'eval_precision@por.rst.cstn': 0.20538263922572011, 'eval_recall@por.rst.cstn': 0.22682572553073907, 'eval_loss@por.rst.cstn': 1.6879867315292358, 'eval_runtime': 7.3964, 'eval_samples_per_second': 77.47, 'eval_steps_per_second': 2.434, 'epoch': 11.0}
{'train@por.rst.cstn_loss': 1.5038037300109863, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.6027000964320154, 'train@por.rst.cstn_f1@por.rst.cstn': 0.15127971928868117, 'train@por.rst.cstn_precision@por.rst.cstn': 0.1487469326012157, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1628676880095193, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5038037300109863, 'train@por.rst.cstn_runtime': 50.8123, 'train@por.rst.cstn_samples_per_second': 81.634, 'train@por.rst.cstn_steps_per_second': 2.558, 'epoch': 12.0}
{'loss': 1.5585, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.6850087642669678, 'eval_accuracy@por.rst.cstn': 0.5287958115183246, 'eval_f1@por.rst.cstn': 0.2089921461905132, 'eval_precision@por.rst.cstn': 0.2067369982671516, 'eval_recall@por.rst.cstn': 0.22762458082564177, 'eval_loss@por.rst.cstn': 1.6850087642669678, 'eval_runtime': 7.3859, 'eval_samples_per_second': 77.581, 'eval_steps_per_second': 2.437, 'epoch': 12.0}
{'train_runtime': 1975.9891, 'train_samples_per_second': 25.19, 'train_steps_per_second': 0.789, 'train_loss': 1.8544379209860777, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =       2.35
  train_runtime            = 0:17:57.00
  train_samples_per_second =     24.345
  train_steps_per_second   =      0.769
-------------------------------------------------------------------
Lang1:  nld.rst.nldt    Lang2:  por.rst.cstn
Saving run to:  runs/full_shot/FullShot=v4_nld.rst.nldt_por.rst.cstn_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 1608 examples
read 331 examples
read 326 examples
read 4148 examples
read 573 examples
read 272 examples
Total prediction labels:  42
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (pt): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (pt): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=42, bias=True)
    )
  )
)
{'train@nld.rst.nldt_loss': 3.263258218765259, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26430348258706465, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01306566650270536, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.00825948383084577, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03125, 'train@nld.rst.nldt_loss@nld.rst.nldt': 3.263258218765259, 'train@nld.rst.nldt_runtime': 19.8318, 'train@nld.rst.nldt_samples_per_second': 81.082, 'train@nld.rst.nldt_steps_per_second': 2.572, 'epoch': 1.0}
{'loss': 3.5289, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.2307369709014893, 'eval_accuracy@nld.rst.nldt': 0.27794561933534745, 'eval_f1@nld.rst.nldt': 0.016110673321075214, 'eval_precision@nld.rst.nldt': 0.010294282197605462, 'eval_recall@nld.rst.nldt': 0.037037037037037035, 'eval_loss@nld.rst.nldt': 3.2307369709014893, 'eval_runtime': 4.414, 'eval_samples_per_second': 74.989, 'eval_steps_per_second': 2.492, 'epoch': 1.0}
{'train@nld.rst.nldt_loss': 2.9085590839385986, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26430348258706465, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01306566650270536, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.00825948383084577, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03125, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.9085593223571777, 'train@nld.rst.nldt_runtime': 19.9439, 'train@nld.rst.nldt_samples_per_second': 80.626, 'train@nld.rst.nldt_steps_per_second': 2.557, 'epoch': 2.0}
{'loss': 3.0861, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.8449769020080566, 'eval_accuracy@nld.rst.nldt': 0.27794561933534745, 'eval_f1@nld.rst.nldt': 0.016110673321075214, 'eval_precision@nld.rst.nldt': 0.010294282197605462, 'eval_recall@nld.rst.nldt': 0.037037037037037035, 'eval_loss@nld.rst.nldt': 2.844977378845215, 'eval_runtime': 4.4302, 'eval_samples_per_second': 74.715, 'eval_steps_per_second': 2.483, 'epoch': 2.0}
{'train@nld.rst.nldt_loss': 2.7946200370788574, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.27300995024875624, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.020812021440891407, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.027252578981302386, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.0357125350140056, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.7946200370788574, 'train@nld.rst.nldt_runtime': 19.9541, 'train@nld.rst.nldt_samples_per_second': 80.585, 'train@nld.rst.nldt_steps_per_second': 2.556, 'epoch': 3.0}
{'loss': 2.8784, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.7308876514434814, 'eval_accuracy@nld.rst.nldt': 0.29607250755287007, 'eval_f1@nld.rst.nldt': 0.030320092820092823, 'eval_precision@nld.rst.nldt': 0.04256319811875367, 'eval_recall@nld.rst.nldt': 0.048383303938859486, 'eval_loss@nld.rst.nldt': 2.7308881282806396, 'eval_runtime': 4.4055, 'eval_samples_per_second': 75.133, 'eval_steps_per_second': 2.497, 'epoch': 3.0}
{'train@nld.rst.nldt_loss': 2.742593288421631, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.28544776119402987, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.028163823725081395, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.0288918652016458, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.042653478057889826, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.74259352684021, 'train@nld.rst.nldt_runtime': 19.923, 'train@nld.rst.nldt_samples_per_second': 80.711, 'train@nld.rst.nldt_steps_per_second': 2.56, 'epoch': 4.0}
{'loss': 2.7788, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.683922529220581, 'eval_accuracy@nld.rst.nldt': 0.3081570996978852, 'eval_f1@nld.rst.nldt': 0.0364760366941219, 'eval_precision@nld.rst.nldt': 0.04059312936124531, 'eval_recall@nld.rst.nldt': 0.05527055696137822, 'eval_loss@nld.rst.nldt': 2.683922290802002, 'eval_runtime': 4.4461, 'eval_samples_per_second': 74.447, 'eval_steps_per_second': 2.474, 'epoch': 4.0}
{'train@nld.rst.nldt_loss': 2.704882860183716, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2972636815920398, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.032048373824237864, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.027576874090247453, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.04934698879551821, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.704883098602295, 'train@nld.rst.nldt_runtime': 19.9351, 'train@nld.rst.nldt_samples_per_second': 80.662, 'train@nld.rst.nldt_steps_per_second': 2.558, 'epoch': 5.0}
{'loss': 2.7451, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.649362325668335, 'eval_accuracy@nld.rst.nldt': 0.311178247734139, 'eval_f1@nld.rst.nldt': 0.037782119702219855, 'eval_precision@nld.rst.nldt': 0.0360733774526878, 'eval_recall@nld.rst.nldt': 0.05898320680929376, 'eval_loss@nld.rst.nldt': 2.649362325668335, 'eval_runtime': 4.4425, 'eval_samples_per_second': 74.507, 'eval_steps_per_second': 2.476, 'epoch': 5.0}
{'train@nld.rst.nldt_loss': 2.6705329418182373, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3034825870646766, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03398201116398492, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.028014060317356385, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05270074696545285, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6705331802368164, 'train@nld.rst.nldt_runtime': 19.9484, 'train@nld.rst.nldt_samples_per_second': 80.608, 'train@nld.rst.nldt_steps_per_second': 2.557, 'epoch': 6.0}
{'loss': 2.7067, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.620460271835327, 'eval_accuracy@nld.rst.nldt': 0.3202416918429003, 'eval_f1@nld.rst.nldt': 0.0401916741539383, 'eval_precision@nld.rst.nldt': 0.0353041213256267, 'eval_recall@nld.rst.nldt': 0.06415663420494339, 'eval_loss@nld.rst.nldt': 2.6204605102539062, 'eval_runtime': 4.4274, 'eval_samples_per_second': 74.762, 'eval_steps_per_second': 2.485, 'epoch': 6.0}
{'train@nld.rst.nldt_loss': 2.6433141231536865, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30845771144278605, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03611646587025323, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.028111049032910312, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05672210550887022, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6433141231536865, 'train@nld.rst.nldt_runtime': 19.9158, 'train@nld.rst.nldt_samples_per_second': 80.74, 'train@nld.rst.nldt_steps_per_second': 2.561, 'epoch': 7.0}
{'loss': 2.685, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.5976600646972656, 'eval_accuracy@nld.rst.nldt': 0.32326283987915405, 'eval_f1@nld.rst.nldt': 0.04089567723366107, 'eval_precision@nld.rst.nldt': 0.03495747291367729, 'eval_recall@nld.rst.nldt': 0.06521483526314444, 'eval_loss@nld.rst.nldt': 2.5976600646972656, 'eval_runtime': 4.416, 'eval_samples_per_second': 74.955, 'eval_steps_per_second': 2.491, 'epoch': 7.0}
{'train@nld.rst.nldt_loss': 2.621551752090454, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31467661691542287, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03756311325906722, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.028311540422094092, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.06042483660130719, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.621551752090454, 'train@nld.rst.nldt_runtime': 19.9035, 'train@nld.rst.nldt_samples_per_second': 80.79, 'train@nld.rst.nldt_steps_per_second': 2.562, 'epoch': 8.0}
{'loss': 2.658, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.5804781913757324, 'eval_accuracy@nld.rst.nldt': 0.3323262839879154, 'eval_f1@nld.rst.nldt': 0.0433654943458865, 'eval_precision@nld.rst.nldt': 0.03656450700475858, 'eval_recall@nld.rst.nldt': 0.06938885054827083, 'eval_loss@nld.rst.nldt': 2.5804781913757324, 'eval_runtime': 4.4279, 'eval_samples_per_second': 74.753, 'eval_steps_per_second': 2.484, 'epoch': 8.0}
{'train@nld.rst.nldt_loss': 2.6080267429351807, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31902985074626866, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03875592643671063, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.028895118402568733, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.062102591036414564, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6080267429351807, 'train@nld.rst.nldt_runtime': 19.913, 'train@nld.rst.nldt_samples_per_second': 80.751, 'train@nld.rst.nldt_steps_per_second': 2.561, 'epoch': 9.0}
{'loss': 2.6445, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.569645643234253, 'eval_accuracy@nld.rst.nldt': 0.3323262839879154, 'eval_f1@nld.rst.nldt': 0.04354090207397109, 'eval_precision@nld.rst.nldt': 0.03674168586449288, 'eval_recall@nld.rst.nldt': 0.06938885054827083, 'eval_loss@nld.rst.nldt': 2.5696451663970947, 'eval_runtime': 4.4032, 'eval_samples_per_second': 75.173, 'eval_steps_per_second': 2.498, 'epoch': 9.0}
{'train@nld.rst.nldt_loss': 2.5984389781951904, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31902985074626866, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03884977187728543, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.029021223832313706, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.06204423436041083, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.5984389781951904, 'train@nld.rst.nldt_runtime': 19.8747, 'train@nld.rst.nldt_samples_per_second': 80.907, 'train@nld.rst.nldt_steps_per_second': 2.566, 'epoch': 10.0}
{'loss': 2.6285, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.561462163925171, 'eval_accuracy@nld.rst.nldt': 0.3323262839879154, 'eval_f1@nld.rst.nldt': 0.04354090207397109, 'eval_precision@nld.rst.nldt': 0.03674168586449288, 'eval_recall@nld.rst.nldt': 0.06938885054827083, 'eval_loss@nld.rst.nldt': 2.561462640762329, 'eval_runtime': 4.4153, 'eval_samples_per_second': 74.967, 'eval_steps_per_second': 2.491, 'epoch': 10.0}
{'train@nld.rst.nldt_loss': 2.5917744636535645, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31965174129353235, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03905465777261739, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.02900203962703963, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.06269957983193278, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.5917739868164062, 'train@nld.rst.nldt_runtime': 19.9305, 'train@nld.rst.nldt_samples_per_second': 80.68, 'train@nld.rst.nldt_steps_per_second': 2.559, 'epoch': 11.0}
{'loss': 2.6165, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.556359052658081, 'eval_accuracy@nld.rst.nldt': 0.3323262839879154, 'eval_f1@nld.rst.nldt': 0.04340741515747342, 'eval_precision@nld.rst.nldt': 0.03614763992122482, 'eval_recall@nld.rst.nldt': 0.06938885054827083, 'eval_loss@nld.rst.nldt': 2.556358814239502, 'eval_runtime': 4.4253, 'eval_samples_per_second': 74.797, 'eval_steps_per_second': 2.486, 'epoch': 11.0}
{'train@nld.rst.nldt_loss': 2.589061737060547, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31965174129353235, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.038974192552358025, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.028925674401056402, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.06269957983193278, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.589061737060547, 'train@nld.rst.nldt_runtime': 19.9103, 'train@nld.rst.nldt_samples_per_second': 80.762, 'train@nld.rst.nldt_steps_per_second': 2.561, 'epoch': 12.0}
{'loss': 2.6211, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.554649591445923, 'eval_accuracy@nld.rst.nldt': 0.3293051359516616, 'eval_f1@nld.rst.nldt': 0.042855933521858895, 'eval_precision@nld.rst.nldt': 0.035663295210327774, 'eval_recall@nld.rst.nldt': 0.06898627405873782, 'eval_loss@nld.rst.nldt': 2.554649829864502, 'eval_runtime': 4.3843, 'eval_samples_per_second': 75.496, 'eval_steps_per_second': 2.509, 'epoch': 12.0}
{'train_runtime': 787.5391, 'train_samples_per_second': 24.502, 'train_steps_per_second': 0.777, 'train_loss': 2.798124176224852, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.7981
  train_runtime            = 0:13:07.53
  train_samples_per_second =     24.502
  train_steps_per_second   =      0.777
{'train@por.rst.cstn_loss': 2.401395320892334, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.2888138862102218, 'train@por.rst.cstn_f1@por.rst.cstn': 0.02070790284233123, 'train@por.rst.cstn_precision@por.rst.cstn': 0.04002072665203609, 'train@por.rst.cstn_recall@por.rst.cstn': 0.035198252688172046, 'train@por.rst.cstn_loss@por.rst.cstn': 2.401395559310913, 'train@por.rst.cstn_runtime': 53.7886, 'train@por.rst.cstn_samples_per_second': 77.117, 'train@por.rst.cstn_steps_per_second': 2.417, 'epoch': 1.0}
{'loss': 2.8796, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.523402452468872, 'eval_accuracy@por.rst.cstn': 0.28097731239092494, 'eval_f1@por.rst.cstn': 0.019940549913301957, 'eval_precision@por.rst.cstn': 0.012771696017769315, 'eval_recall@por.rst.cstn': 0.045454545454545456, 'eval_loss@por.rst.cstn': 2.523402452468872, 'eval_runtime': 8.5562, 'eval_samples_per_second': 66.969, 'eval_steps_per_second': 2.104, 'epoch': 1.0}
{'train@por.rst.cstn_loss': 2.154980421066284, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.39729990356798456, 'train@por.rst.cstn_f1@por.rst.cstn': 0.06027856504843765, 'train@por.rst.cstn_precision@por.rst.cstn': 0.07707646706317679, 'train@por.rst.cstn_recall@por.rst.cstn': 0.06996315764571538, 'train@por.rst.cstn_loss@por.rst.cstn': 2.1549808979034424, 'train@por.rst.cstn_runtime': 50.7631, 'train@por.rst.cstn_samples_per_second': 81.713, 'train@por.rst.cstn_steps_per_second': 2.561, 'epoch': 2.0}
{'loss': 2.3234, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.2903122901916504, 'eval_accuracy@por.rst.cstn': 0.3333333333333333, 'eval_f1@por.rst.cstn': 0.07562727031707829, 'eval_precision@por.rst.cstn': 0.08098352713178293, 'eval_recall@por.rst.cstn': 0.09490808408012485, 'eval_loss@por.rst.cstn': 2.2903125286102295, 'eval_runtime': 7.3594, 'eval_samples_per_second': 77.86, 'eval_steps_per_second': 2.446, 'epoch': 2.0}
{'train@por.rst.cstn_loss': 1.9433753490447998, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.47444551591128253, 'train@por.rst.cstn_f1@por.rst.cstn': 0.07596447879259713, 'train@por.rst.cstn_precision@por.rst.cstn': 0.12505254403089472, 'train@por.rst.cstn_recall@por.rst.cstn': 0.09154054973464182, 'train@por.rst.cstn_loss@por.rst.cstn': 1.9433753490447998, 'train@por.rst.cstn_runtime': 50.7136, 'train@por.rst.cstn_samples_per_second': 81.793, 'train@por.rst.cstn_steps_per_second': 2.563, 'epoch': 3.0}
{'loss': 2.1088, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.096626043319702, 'eval_accuracy@por.rst.cstn': 0.39965095986038396, 'eval_f1@por.rst.cstn': 0.09780842013435218, 'eval_precision@por.rst.cstn': 0.09020025302072392, 'eval_recall@por.rst.cstn': 0.1313586022188061, 'eval_loss@por.rst.cstn': 2.096626043319702, 'eval_runtime': 7.3687, 'eval_samples_per_second': 77.762, 'eval_steps_per_second': 2.443, 'epoch': 3.0}
{'train@por.rst.cstn_loss': 1.798179030418396, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5229026036644165, 'train@por.rst.cstn_f1@por.rst.cstn': 0.10126981545651623, 'train@por.rst.cstn_precision@por.rst.cstn': 0.14339076753858876, 'train@por.rst.cstn_recall@por.rst.cstn': 0.11152339395121502, 'train@por.rst.cstn_loss@por.rst.cstn': 1.7981791496276855, 'train@por.rst.cstn_runtime': 50.6616, 'train@por.rst.cstn_samples_per_second': 81.877, 'train@por.rst.cstn_steps_per_second': 2.566, 'epoch': 4.0}
{'loss': 1.9393, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.9670066833496094, 'eval_accuracy@por.rst.cstn': 0.4328097731239092, 'eval_f1@por.rst.cstn': 0.1207042115723972, 'eval_precision@por.rst.cstn': 0.14056050695696862, 'eval_recall@por.rst.cstn': 0.15229380794182126, 'eval_loss@por.rst.cstn': 1.9670066833496094, 'eval_runtime': 7.3297, 'eval_samples_per_second': 78.175, 'eval_steps_per_second': 2.456, 'epoch': 4.0}
{'train@por.rst.cstn_loss': 1.7024275064468384, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5424300867888139, 'train@por.rst.cstn_f1@por.rst.cstn': 0.12031647362723162, 'train@por.rst.cstn_precision@por.rst.cstn': 0.1334859442532349, 'train@por.rst.cstn_recall@por.rst.cstn': 0.13029507962319958, 'train@por.rst.cstn_loss@por.rst.cstn': 1.702427625656128, 'train@por.rst.cstn_runtime': 50.6823, 'train@por.rst.cstn_samples_per_second': 81.843, 'train@por.rst.cstn_steps_per_second': 2.565, 'epoch': 5.0}
{'loss': 1.8147, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.877768635749817, 'eval_accuracy@por.rst.cstn': 0.45898778359511344, 'eval_f1@por.rst.cstn': 0.1567463100441515, 'eval_precision@por.rst.cstn': 0.1783290363033838, 'eval_recall@por.rst.cstn': 0.17779632936695552, 'eval_loss@por.rst.cstn': 1.8777683973312378, 'eval_runtime': 7.3572, 'eval_samples_per_second': 77.882, 'eval_steps_per_second': 2.447, 'epoch': 5.0}
{'train@por.rst.cstn_loss': 1.6363582611083984, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5653326904532304, 'train@por.rst.cstn_f1@por.rst.cstn': 0.12795392569503247, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13128175533030764, 'train@por.rst.cstn_recall@por.rst.cstn': 0.13923693672040305, 'train@por.rst.cstn_loss@por.rst.cstn': 1.6363582611083984, 'train@por.rst.cstn_runtime': 50.6862, 'train@por.rst.cstn_samples_per_second': 81.837, 'train@por.rst.cstn_steps_per_second': 2.565, 'epoch': 6.0}
{'loss': 1.7358, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.8157590627670288, 'eval_accuracy@por.rst.cstn': 0.4781849912739965, 'eval_f1@por.rst.cstn': 0.1716605514063818, 'eval_precision@por.rst.cstn': 0.17207358140227286, 'eval_recall@por.rst.cstn': 0.19213067403206782, 'eval_loss@por.rst.cstn': 1.8157590627670288, 'eval_runtime': 7.3671, 'eval_samples_per_second': 77.778, 'eval_steps_per_second': 2.443, 'epoch': 6.0}
{'train@por.rst.cstn_loss': 1.5903890132904053, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5781099324975892, 'train@por.rst.cstn_f1@por.rst.cstn': 0.13422522598155312, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13153336524290613, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14571361859901977, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5903890132904053, 'train@por.rst.cstn_runtime': 50.9211, 'train@por.rst.cstn_samples_per_second': 81.459, 'train@por.rst.cstn_steps_per_second': 2.553, 'epoch': 7.0}
{'loss': 1.6764, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.7744615077972412, 'eval_accuracy@por.rst.cstn': 0.49214659685863876, 'eval_f1@por.rst.cstn': 0.18681394819989236, 'eval_precision@por.rst.cstn': 0.22374122708347308, 'eval_recall@por.rst.cstn': 0.20164750794821523, 'eval_loss@por.rst.cstn': 1.7744616270065308, 'eval_runtime': 7.3451, 'eval_samples_per_second': 78.011, 'eval_steps_per_second': 2.451, 'epoch': 7.0}
{'train@por.rst.cstn_loss': 1.563297152519226, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5819672131147541, 'train@por.rst.cstn_f1@por.rst.cstn': 0.13736302099127706, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13406902898763273, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14828898185878595, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5632972717285156, 'train@por.rst.cstn_runtime': 50.7423, 'train@por.rst.cstn_samples_per_second': 81.746, 'train@por.rst.cstn_steps_per_second': 2.562, 'epoch': 8.0}
{'loss': 1.6428, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.7520583868026733, 'eval_accuracy@por.rst.cstn': 0.5026178010471204, 'eval_f1@por.rst.cstn': 0.19390170408284077, 'eval_precision@por.rst.cstn': 0.23072073892097283, 'eval_recall@por.rst.cstn': 0.20706652320704746, 'eval_loss@por.rst.cstn': 1.7520583868026733, 'eval_runtime': 7.3556, 'eval_samples_per_second': 77.9, 'eval_steps_per_second': 2.447, 'epoch': 8.0}
{'train@por.rst.cstn_loss': 1.544758677482605, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5867888138862102, 'train@por.rst.cstn_f1@por.rst.cstn': 0.14017813833256842, 'train@por.rst.cstn_precision@por.rst.cstn': 0.16714829619205956, 'train@por.rst.cstn_recall@por.rst.cstn': 0.15044635621412386, 'train@por.rst.cstn_loss@por.rst.cstn': 1.544758677482605, 'train@por.rst.cstn_runtime': 50.6602, 'train@por.rst.cstn_samples_per_second': 81.879, 'train@por.rst.cstn_steps_per_second': 2.566, 'epoch': 9.0}
{'loss': 1.619, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.7346876859664917, 'eval_accuracy@por.rst.cstn': 0.5095986038394416, 'eval_f1@por.rst.cstn': 0.19634933705592905, 'eval_precision@por.rst.cstn': 0.23345240217983598, 'eval_recall@por.rst.cstn': 0.2088223163009932, 'eval_loss@por.rst.cstn': 1.7346876859664917, 'eval_runtime': 7.3708, 'eval_samples_per_second': 77.739, 'eval_steps_per_second': 2.442, 'epoch': 9.0}
{'train@por.rst.cstn_loss': 1.5257110595703125, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.595708775313404, 'train@por.rst.cstn_f1@por.rst.cstn': 0.14563266385663093, 'train@por.rst.cstn_precision@por.rst.cstn': 0.15133387344473953, 'train@por.rst.cstn_recall@por.rst.cstn': 0.15674631722279914, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5257110595703125, 'train@por.rst.cstn_runtime': 50.655, 'train@por.rst.cstn_samples_per_second': 81.887, 'train@por.rst.cstn_steps_per_second': 2.566, 'epoch': 10.0}
{'loss': 1.5906, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.7159925699234009, 'eval_accuracy@por.rst.cstn': 0.5113438045375218, 'eval_f1@por.rst.cstn': 0.19754093595176656, 'eval_precision@por.rst.cstn': 0.20692193166927328, 'eval_recall@por.rst.cstn': 0.2134513618157263, 'eval_loss@por.rst.cstn': 1.7159925699234009, 'eval_runtime': 7.3403, 'eval_samples_per_second': 78.063, 'eval_steps_per_second': 2.452, 'epoch': 10.0}
{'train@por.rst.cstn_loss': 1.518671989440918, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5971552555448408, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1467460085736984, 'train@por.rst.cstn_precision@por.rst.cstn': 0.15212783560371826, 'train@por.rst.cstn_recall@por.rst.cstn': 0.15749547942878755, 'train@por.rst.cstn_loss@por.rst.cstn': 1.518671989440918, 'train@por.rst.cstn_runtime': 50.6848, 'train@por.rst.cstn_samples_per_second': 81.839, 'train@por.rst.cstn_steps_per_second': 2.565, 'epoch': 11.0}
{'loss': 1.579, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.711183786392212, 'eval_accuracy@por.rst.cstn': 0.5130890052356021, 'eval_f1@por.rst.cstn': 0.1978480652252345, 'eval_precision@por.rst.cstn': 0.20807680665500738, 'eval_recall@por.rst.cstn': 0.2136575681328126, 'eval_loss@por.rst.cstn': 1.711183786392212, 'eval_runtime': 7.3172, 'eval_samples_per_second': 78.308, 'eval_steps_per_second': 2.46, 'epoch': 11.0}
{'train@por.rst.cstn_loss': 1.5159441232681274, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5971552555448408, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1469173040634228, 'train@por.rst.cstn_precision@por.rst.cstn': 0.15101044174714434, 'train@por.rst.cstn_recall@por.rst.cstn': 0.15768292337041034, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5159441232681274, 'train@por.rst.cstn_runtime': 50.7548, 'train@por.rst.cstn_samples_per_second': 81.726, 'train@por.rst.cstn_steps_per_second': 2.561, 'epoch': 12.0}
{'loss': 1.5764, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.7082380056381226, 'eval_accuracy@por.rst.cstn': 0.5130890052356021, 'eval_f1@por.rst.cstn': 0.19671090959459755, 'eval_precision@por.rst.cstn': 0.2075553041721259, 'eval_recall@por.rst.cstn': 0.21168690455927816, 'eval_loss@por.rst.cstn': 1.7082382440567017, 'eval_runtime': 7.3261, 'eval_samples_per_second': 78.214, 'eval_steps_per_second': 2.457, 'epoch': 12.0}
{'train_runtime': 1980.8955, 'train_samples_per_second': 25.128, 'train_steps_per_second': 0.788, 'train_loss': 1.8738175612229568, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.7981
  train_runtime            = 0:13:07.53
  train_samples_per_second =     24.502
  train_steps_per_second   =      0.777
-------------------------------------------------------------------
Lang1:  rus.rst.rrt    Lang2:  por.rst.cstn
Saving run to:  runs/full_shot/FullShot=v4_rus.rst.rrt_por.rst.cstn_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 28822 examples
read 2855 examples
read 2843 examples
read 4148 examples
read 573 examples
read 272 examples
Total prediction labels:  37
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (ru): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (pt): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (pt): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=37, bias=True)
    )
  )
)
{'train@rus.rst.rrt_loss': 1.73322331905365, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.4964263409895219, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.17888995387138015, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.23262524035434007, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.19657069555113021, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.7332230806350708, 'train@rus.rst.rrt_runtime': 350.4753, 'train@rus.rst.rrt_samples_per_second': 82.237, 'train@rus.rst.rrt_steps_per_second': 2.571, 'epoch': 1.0}
{'loss': 2.1888, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.7718523740768433, 'eval_accuracy@rus.rst.rrt': 0.4763572679509632, 'eval_f1@rus.rst.rrt': 0.1986861334186945, 'eval_precision@rus.rst.rrt': 0.2104816698058781, 'eval_recall@rus.rst.rrt': 0.21670885984155858, 'eval_loss@rus.rst.rrt': 1.7718523740768433, 'eval_runtime': 35.025, 'eval_samples_per_second': 81.513, 'eval_steps_per_second': 2.57, 'epoch': 1.0}
{'train@rus.rst.rrt_loss': 1.505377173423767, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.543265561029769, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.2290722717712793, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.26912276178443056, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.2344686703901213, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.505377173423767, 'train@rus.rst.rrt_runtime': 350.6089, 'train@rus.rst.rrt_samples_per_second': 82.206, 'train@rus.rst.rrt_steps_per_second': 2.57, 'epoch': 2.0}
{'loss': 1.6443, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.5738037824630737, 'eval_accuracy@rus.rst.rrt': 0.5229422066549912, 'eval_f1@rus.rst.rrt': 0.25467744666575143, 'eval_precision@rus.rst.rrt': 0.2953400016266719, 'eval_recall@rus.rst.rrt': 0.261680753620096, 'eval_loss@rus.rst.rrt': 1.5738037824630737, 'eval_runtime': 35.0534, 'eval_samples_per_second': 81.447, 'eval_steps_per_second': 2.568, 'epoch': 2.0}
{'train@rus.rst.rrt_loss': 1.427473783493042, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5651238637152175, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.2759857790851458, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4256186107163082, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.27259762937169996, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.4274736642837524, 'train@rus.rst.rrt_runtime': 350.6232, 'train@rus.rst.rrt_samples_per_second': 82.202, 'train@rus.rst.rrt_steps_per_second': 2.57, 'epoch': 3.0}
{'loss': 1.5138, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.5028127431869507, 'eval_accuracy@rus.rst.rrt': 0.5401050788091069, 'eval_f1@rus.rst.rrt': 0.3017330815218176, 'eval_precision@rus.rst.rrt': 0.38710764544013093, 'eval_recall@rus.rst.rrt': 0.300363877390942, 'eval_loss@rus.rst.rrt': 1.5028127431869507, 'eval_runtime': 35.0851, 'eval_samples_per_second': 81.373, 'eval_steps_per_second': 2.565, 'epoch': 3.0}
{'train@rus.rst.rrt_loss': 1.3727980852127075, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5824023315522865, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.31260010600686233, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.42873149099271457, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.2960425823732275, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3727978467941284, 'train@rus.rst.rrt_runtime': 350.6606, 'train@rus.rst.rrt_samples_per_second': 82.193, 'train@rus.rst.rrt_steps_per_second': 2.569, 'epoch': 4.0}
{'loss': 1.4507, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.4522311687469482, 'eval_accuracy@rus.rst.rrt': 0.5520140105078809, 'eval_f1@rus.rst.rrt': 0.3462434464210781, 'eval_precision@rus.rst.rrt': 0.4595837049396121, 'eval_recall@rus.rst.rrt': 0.3308076173245016, 'eval_loss@rus.rst.rrt': 1.4522311687469482, 'eval_runtime': 35.1449, 'eval_samples_per_second': 81.235, 'eval_steps_per_second': 2.561, 'epoch': 4.0}
{'train@rus.rst.rrt_loss': 1.3389040231704712, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.589063909513566, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.32895033666773243, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4432446540827779, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3101841213153816, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3389040231704712, 'train@rus.rst.rrt_runtime': 350.5845, 'train@rus.rst.rrt_samples_per_second': 82.211, 'train@rus.rst.rrt_steps_per_second': 2.57, 'epoch': 5.0}
{'loss': 1.4113, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.427871584892273, 'eval_accuracy@rus.rst.rrt': 0.5590192644483363, 'eval_f1@rus.rst.rrt': 0.3706220397112645, 'eval_precision@rus.rst.rrt': 0.4584368078850061, 'eval_recall@rus.rst.rrt': 0.3545009210026159, 'eval_loss@rus.rst.rrt': 1.4278714656829834, 'eval_runtime': 35.0899, 'eval_samples_per_second': 81.362, 'eval_steps_per_second': 2.565, 'epoch': 5.0}
{'train@rus.rst.rrt_loss': 1.3151823282241821, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5969051419054888, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.34108616216198945, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4402039399504148, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3221057657378721, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3151825666427612, 'train@rus.rst.rrt_runtime': 350.6956, 'train@rus.rst.rrt_samples_per_second': 82.185, 'train@rus.rst.rrt_steps_per_second': 2.569, 'epoch': 6.0}
{'loss': 1.383, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.407012939453125, 'eval_accuracy@rus.rst.rrt': 0.5681260945709282, 'eval_f1@rus.rst.rrt': 0.38877313112926865, 'eval_precision@rus.rst.rrt': 0.47359559881296637, 'eval_recall@rus.rst.rrt': 0.3738870103493013, 'eval_loss@rus.rst.rrt': 1.4070130586624146, 'eval_runtime': 35.0988, 'eval_samples_per_second': 81.342, 'eval_steps_per_second': 2.564, 'epoch': 6.0}
{'train@rus.rst.rrt_loss': 1.30045485496521, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.599854277982097, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.34992116245074634, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.45643333291155064, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3312335999470457, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3004547357559204, 'train@rus.rst.rrt_runtime': 350.8934, 'train@rus.rst.rrt_samples_per_second': 82.139, 'train@rus.rst.rrt_steps_per_second': 2.568, 'epoch': 7.0}
{'loss': 1.3586, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.4000273942947388, 'eval_accuracy@rus.rst.rrt': 0.5688266199649737, 'eval_f1@rus.rst.rrt': 0.39921241134282187, 'eval_precision@rus.rst.rrt': 0.4696733256959854, 'eval_recall@rus.rst.rrt': 0.38428516522341055, 'eval_loss@rus.rst.rrt': 1.4000275135040283, 'eval_runtime': 35.0685, 'eval_samples_per_second': 81.412, 'eval_steps_per_second': 2.566, 'epoch': 7.0}
{'train@rus.rst.rrt_loss': 1.286301612854004, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.605093331482895, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36110131258457295, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4524838495846455, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34377155197306153, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2863017320632935, 'train@rus.rst.rrt_runtime': 351.0559, 'train@rus.rst.rrt_samples_per_second': 82.101, 'train@rus.rst.rrt_steps_per_second': 2.567, 'epoch': 8.0}
{'loss': 1.3422, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.3875333070755005, 'eval_accuracy@rus.rst.rrt': 0.5779334500875657, 'eval_f1@rus.rst.rrt': 0.41360316719181556, 'eval_precision@rus.rst.rrt': 0.4717729012729271, 'eval_recall@rus.rst.rrt': 0.3994491292500168, 'eval_loss@rus.rst.rrt': 1.3875333070755005, 'eval_runtime': 35.0924, 'eval_samples_per_second': 81.357, 'eval_steps_per_second': 2.565, 'epoch': 8.0}
{'train@rus.rst.rrt_loss': 1.2742277383804321, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6069669002845048, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3627890518501411, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4553349919430682, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34455178652169394, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2742277383804321, 'train@rus.rst.rrt_runtime': 350.668, 'train@rus.rst.rrt_samples_per_second': 82.192, 'train@rus.rst.rrt_steps_per_second': 2.569, 'epoch': 9.0}
{'loss': 1.3343, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.3800833225250244, 'eval_accuracy@rus.rst.rrt': 0.5775831873905429, 'eval_f1@rus.rst.rrt': 0.41500854050511043, 'eval_precision@rus.rst.rrt': 0.47755817327007116, 'eval_recall@rus.rst.rrt': 0.3988540690934711, 'eval_loss@rus.rst.rrt': 1.3800833225250244, 'eval_runtime': 35.0563, 'eval_samples_per_second': 81.44, 'eval_steps_per_second': 2.567, 'epoch': 9.0}
{'train@rus.rst.rrt_loss': 1.2673008441925049, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.609187426271598, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36652306501152054, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.47147322239304723, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3450250287167689, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2673008441925049, 'train@rus.rst.rrt_runtime': 350.5215, 'train@rus.rst.rrt_samples_per_second': 82.226, 'train@rus.rst.rrt_steps_per_second': 2.57, 'epoch': 10.0}
{'loss': 1.3249, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.3764373064041138, 'eval_accuracy@rus.rst.rrt': 0.5786339754816112, 'eval_f1@rus.rst.rrt': 0.4154032422026524, 'eval_precision@rus.rst.rrt': 0.4771253263512814, 'eval_recall@rus.rst.rrt': 0.3984776601409909, 'eval_loss@rus.rst.rrt': 1.3764373064041138, 'eval_runtime': 35.0867, 'eval_samples_per_second': 81.37, 'eval_steps_per_second': 2.565, 'epoch': 10.0}
{'train@rus.rst.rrt_loss': 1.2634729146957397, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6098466449240164, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36840020297031173, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4667330704349029, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3491891022913865, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2634727954864502, 'train@rus.rst.rrt_runtime': 350.6362, 'train@rus.rst.rrt_samples_per_second': 82.199, 'train@rus.rst.rrt_steps_per_second': 2.57, 'epoch': 11.0}
{'loss': 1.3143, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.3759373426437378, 'eval_accuracy@rus.rst.rrt': 0.5726795096322241, 'eval_f1@rus.rst.rrt': 0.4126568936727628, 'eval_precision@rus.rst.rrt': 0.46993728839224624, 'eval_recall@rus.rst.rrt': 0.39819528732267645, 'eval_loss@rus.rst.rrt': 1.3759373426437378, 'eval_runtime': 35.0568, 'eval_samples_per_second': 81.439, 'eval_steps_per_second': 2.567, 'epoch': 11.0}
{'train@rus.rst.rrt_loss': 1.2621098756790161, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6108528207619179, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3689358056803929, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4667759793921423, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3499853540662541, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2621098756790161, 'train@rus.rst.rrt_runtime': 350.6499, 'train@rus.rst.rrt_samples_per_second': 82.196, 'train@rus.rst.rrt_steps_per_second': 2.57, 'epoch': 12.0}
{'loss': 1.3109, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.3742039203643799, 'eval_accuracy@rus.rst.rrt': 0.5786339754816112, 'eval_f1@rus.rst.rrt': 0.41642934867163356, 'eval_precision@rus.rst.rrt': 0.47808385400452313, 'eval_recall@rus.rst.rrt': 0.40163558859984855, 'eval_loss@rus.rst.rrt': 1.3742038011550903, 'eval_runtime': 35.035, 'eval_samples_per_second': 81.49, 'eval_steps_per_second': 2.569, 'epoch': 12.0}
{'train_runtime': 13504.4872, 'train_samples_per_second': 25.611, 'train_steps_per_second': 0.801, 'train_loss': 1.4647540940296373, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.4648
  train_runtime            = 3:45:04.48
  train_samples_per_second =     25.611
  train_steps_per_second   =      0.801
{'train@por.rst.cstn_loss': 2.153289794921875, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.43683702989392476, 'train@por.rst.cstn_f1@por.rst.cstn': 0.06565104958240081, 'train@por.rst.cstn_precision@por.rst.cstn': 0.0734835319003804, 'train@por.rst.cstn_recall@por.rst.cstn': 0.0764685564897851, 'train@por.rst.cstn_loss@por.rst.cstn': 2.153289794921875, 'train@por.rst.cstn_runtime': 53.5043, 'train@por.rst.cstn_samples_per_second': 77.527, 'train@por.rst.cstn_steps_per_second': 2.43, 'epoch': 1.0}
{'loss': 2.8404, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.332791805267334, 'eval_accuracy@por.rst.cstn': 0.3769633507853403, 'eval_f1@por.rst.cstn': 0.09415428689948407, 'eval_precision@por.rst.cstn': 0.10002790048825855, 'eval_recall@por.rst.cstn': 0.11739299290266629, 'eval_loss@por.rst.cstn': 2.332791566848755, 'eval_runtime': 7.3048, 'eval_samples_per_second': 78.442, 'eval_steps_per_second': 2.464, 'epoch': 1.0}
{'train@por.rst.cstn_loss': 1.75607430934906, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5376084860173578, 'train@por.rst.cstn_f1@por.rst.cstn': 0.11314959608457954, 'train@por.rst.cstn_precision@por.rst.cstn': 0.11548013252705469, 'train@por.rst.cstn_recall@por.rst.cstn': 0.12575902938018224, 'train@por.rst.cstn_loss@por.rst.cstn': 1.7560744285583496, 'train@por.rst.cstn_runtime': 50.7109, 'train@por.rst.cstn_samples_per_second': 81.797, 'train@por.rst.cstn_steps_per_second': 2.564, 'epoch': 2.0}
{'loss': 1.9856, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.9118622541427612, 'eval_accuracy@por.rst.cstn': 0.4467713787085515, 'eval_f1@por.rst.cstn': 0.14983105330680876, 'eval_precision@por.rst.cstn': 0.1412417889321299, 'eval_recall@por.rst.cstn': 0.1726620870057525, 'eval_loss@por.rst.cstn': 1.9118622541427612, 'eval_runtime': 7.2961, 'eval_samples_per_second': 78.535, 'eval_steps_per_second': 2.467, 'epoch': 2.0}
{'train@por.rst.cstn_loss': 1.604561448097229, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5658148505303761, 'train@por.rst.cstn_f1@por.rst.cstn': 0.135081124445031, 'train@por.rst.cstn_precision@por.rst.cstn': 0.14902788109517412, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14666075471560389, 'train@por.rst.cstn_loss@por.rst.cstn': 1.6045615673065186, 'train@por.rst.cstn_runtime': 50.6739, 'train@por.rst.cstn_samples_per_second': 81.857, 'train@por.rst.cstn_steps_per_second': 2.565, 'epoch': 3.0}
{'loss': 1.7434, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.727132797241211, 'eval_accuracy@por.rst.cstn': 0.4956369982547993, 'eval_f1@por.rst.cstn': 0.20218365490061932, 'eval_precision@por.rst.cstn': 0.20904446741110883, 'eval_recall@por.rst.cstn': 0.2213911080251833, 'eval_loss@por.rst.cstn': 1.7271325588226318, 'eval_runtime': 7.3314, 'eval_samples_per_second': 78.157, 'eval_steps_per_second': 2.455, 'epoch': 3.0}
{'train@por.rst.cstn_loss': 1.521858811378479, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5841369334619093, 'train@por.rst.cstn_f1@por.rst.cstn': 0.15575357786427654, 'train@por.rst.cstn_precision@por.rst.cstn': 0.20506208962666841, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1614752699454248, 'train@por.rst.cstn_loss@por.rst.cstn': 1.521858811378479, 'train@por.rst.cstn_runtime': 50.6517, 'train@por.rst.cstn_samples_per_second': 81.893, 'train@por.rst.cstn_steps_per_second': 2.567, 'epoch': 4.0}
{'loss': 1.6226, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.6457774639129639, 'eval_accuracy@por.rst.cstn': 0.506108202443281, 'eval_f1@por.rst.cstn': 0.21930488334514633, 'eval_precision@por.rst.cstn': 0.2222525314744969, 'eval_recall@por.rst.cstn': 0.2351187401475584, 'eval_loss@por.rst.cstn': 1.6457772254943848, 'eval_runtime': 7.3136, 'eval_samples_per_second': 78.347, 'eval_steps_per_second': 2.461, 'epoch': 4.0}
{'train@por.rst.cstn_loss': 1.4654706716537476, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.6065573770491803, 'train@por.rst.cstn_f1@por.rst.cstn': 0.17574219116384432, 'train@por.rst.cstn_precision@por.rst.cstn': 0.21838968958594795, 'train@por.rst.cstn_recall@por.rst.cstn': 0.17951390723090344, 'train@por.rst.cstn_loss@por.rst.cstn': 1.4654706716537476, 'train@por.rst.cstn_runtime': 50.6404, 'train@por.rst.cstn_samples_per_second': 81.911, 'train@por.rst.cstn_steps_per_second': 2.567, 'epoch': 5.0}
{'loss': 1.5556, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.6005939245224, 'eval_accuracy@por.rst.cstn': 0.5357766143106457, 'eval_f1@por.rst.cstn': 0.23389921012468912, 'eval_precision@por.rst.cstn': 0.23003521120290218, 'eval_recall@por.rst.cstn': 0.25620094395027393, 'eval_loss@por.rst.cstn': 1.6005936861038208, 'eval_runtime': 7.3136, 'eval_samples_per_second': 78.347, 'eval_steps_per_second': 2.461, 'epoch': 5.0}
{'train@por.rst.cstn_loss': 1.4278348684310913, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.6178881388621023, 'train@por.rst.cstn_f1@por.rst.cstn': 0.18926438045281063, 'train@por.rst.cstn_precision@por.rst.cstn': 0.22705688101061033, 'train@por.rst.cstn_recall@por.rst.cstn': 0.18978762707305322, 'train@por.rst.cstn_loss@por.rst.cstn': 1.4278349876403809, 'train@por.rst.cstn_runtime': 50.5582, 'train@por.rst.cstn_samples_per_second': 82.044, 'train@por.rst.cstn_steps_per_second': 2.571, 'epoch': 6.0}
{'loss': 1.5059, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.5650596618652344, 'eval_accuracy@por.rst.cstn': 0.5357766143106457, 'eval_f1@por.rst.cstn': 0.23565274109447998, 'eval_precision@por.rst.cstn': 0.23314712960099324, 'eval_recall@por.rst.cstn': 0.25732462841595966, 'eval_loss@por.rst.cstn': 1.5650593042373657, 'eval_runtime': 7.3047, 'eval_samples_per_second': 78.443, 'eval_steps_per_second': 2.464, 'epoch': 6.0}
{'train@por.rst.cstn_loss': 1.3989191055297852, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.6301832208293153, 'train@por.rst.cstn_f1@por.rst.cstn': 0.20169261861519794, 'train@por.rst.cstn_precision@por.rst.cstn': 0.2432921239148043, 'train@por.rst.cstn_recall@por.rst.cstn': 0.2042270435097302, 'train@por.rst.cstn_loss@por.rst.cstn': 1.3989191055297852, 'train@por.rst.cstn_runtime': 50.591, 'train@por.rst.cstn_samples_per_second': 81.991, 'train@por.rst.cstn_steps_per_second': 2.57, 'epoch': 7.0}
{'loss': 1.4792, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.5394376516342163, 'eval_accuracy@por.rst.cstn': 0.5445026178010471, 'eval_f1@por.rst.cstn': 0.24026173651637386, 'eval_precision@por.rst.cstn': 0.23262212048097214, 'eval_recall@por.rst.cstn': 0.26722870048978425, 'eval_loss@por.rst.cstn': 1.5394377708435059, 'eval_runtime': 7.2883, 'eval_samples_per_second': 78.619, 'eval_steps_per_second': 2.47, 'epoch': 7.0}
{'train@por.rst.cstn_loss': 1.377047061920166, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.6337994214079075, 'train@por.rst.cstn_f1@por.rst.cstn': 0.20744291012697685, 'train@por.rst.cstn_precision@por.rst.cstn': 0.23390630601622445, 'train@por.rst.cstn_recall@por.rst.cstn': 0.20934074039903305, 'train@por.rst.cstn_loss@por.rst.cstn': 1.377046823501587, 'train@por.rst.cstn_runtime': 50.6625, 'train@por.rst.cstn_samples_per_second': 81.875, 'train@por.rst.cstn_steps_per_second': 2.566, 'epoch': 8.0}
{'loss': 1.4582, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.528682827949524, 'eval_accuracy@por.rst.cstn': 0.5392670157068062, 'eval_f1@por.rst.cstn': 0.23853918881718422, 'eval_precision@por.rst.cstn': 0.23090687754746658, 'eval_recall@por.rst.cstn': 0.26548297118268604, 'eval_loss@por.rst.cstn': 1.528682827949524, 'eval_runtime': 7.3157, 'eval_samples_per_second': 78.324, 'eval_steps_per_second': 2.46, 'epoch': 8.0}
{'train@por.rst.cstn_loss': 1.365929365158081, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.6342815814850531, 'train@por.rst.cstn_f1@por.rst.cstn': 0.2092868316437015, 'train@por.rst.cstn_precision@por.rst.cstn': 0.24900521975318823, 'train@por.rst.cstn_recall@por.rst.cstn': 0.21013788405770245, 'train@por.rst.cstn_loss@por.rst.cstn': 1.3659294843673706, 'train@por.rst.cstn_runtime': 50.6513, 'train@por.rst.cstn_samples_per_second': 81.893, 'train@por.rst.cstn_steps_per_second': 2.567, 'epoch': 9.0}
{'loss': 1.4353, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.522099256515503, 'eval_accuracy@por.rst.cstn': 0.5427574171029669, 'eval_f1@por.rst.cstn': 0.24033317326811857, 'eval_precision@por.rst.cstn': 0.23494268623308381, 'eval_recall@por.rst.cstn': 0.2671479462926373, 'eval_loss@por.rst.cstn': 1.5220993757247925, 'eval_runtime': 7.3111, 'eval_samples_per_second': 78.374, 'eval_steps_per_second': 2.462, 'epoch': 9.0}
{'train@por.rst.cstn_loss': 1.3545926809310913, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.6395853423336548, 'train@por.rst.cstn_f1@por.rst.cstn': 0.2125013164340292, 'train@por.rst.cstn_precision@por.rst.cstn': 0.23446245092510296, 'train@por.rst.cstn_recall@por.rst.cstn': 0.21562041106209845, 'train@por.rst.cstn_loss@por.rst.cstn': 1.3545928001403809, 'train@por.rst.cstn_runtime': 50.7026, 'train@por.rst.cstn_samples_per_second': 81.81, 'train@por.rst.cstn_steps_per_second': 2.564, 'epoch': 10.0}
{'loss': 1.4205, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.5092366933822632, 'eval_accuracy@por.rst.cstn': 0.5497382198952879, 'eval_f1@por.rst.cstn': 0.23552199800321322, 'eval_precision@por.rst.cstn': 0.26855638915931845, 'eval_recall@por.rst.cstn': 0.2584349509378918, 'eval_loss@por.rst.cstn': 1.5092365741729736, 'eval_runtime': 7.311, 'eval_samples_per_second': 78.375, 'eval_steps_per_second': 2.462, 'epoch': 10.0}
{'train@por.rst.cstn_loss': 1.349507212638855, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.6391031822565092, 'train@por.rst.cstn_f1@por.rst.cstn': 0.21388336361121663, 'train@por.rst.cstn_precision@por.rst.cstn': 0.2436890174529035, 'train@por.rst.cstn_recall@por.rst.cstn': 0.21624290434284837, 'train@por.rst.cstn_loss@por.rst.cstn': 1.3495073318481445, 'train@por.rst.cstn_runtime': 50.6554, 'train@por.rst.cstn_samples_per_second': 81.887, 'train@por.rst.cstn_steps_per_second': 2.566, 'epoch': 11.0}
{'loss': 1.4087, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.5094462633132935, 'eval_accuracy@por.rst.cstn': 0.5479930191972077, 'eval_f1@por.rst.cstn': 0.23466830288653856, 'eval_precision@por.rst.cstn': 0.2680062816383856, 'eval_recall@por.rst.cstn': 0.25771685995515053, 'eval_loss@por.rst.cstn': 1.509446382522583, 'eval_runtime': 7.3203, 'eval_samples_per_second': 78.276, 'eval_steps_per_second': 2.459, 'epoch': 11.0}
{'train@por.rst.cstn_loss': 1.348004698753357, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.639344262295082, 'train@por.rst.cstn_f1@por.rst.cstn': 0.2140741390517613, 'train@por.rst.cstn_precision@por.rst.cstn': 0.243759981616068, 'train@por.rst.cstn_recall@por.rst.cstn': 0.21648315509772253, 'train@por.rst.cstn_loss@por.rst.cstn': 1.348004698753357, 'train@por.rst.cstn_runtime': 50.685, 'train@por.rst.cstn_samples_per_second': 81.839, 'train@por.rst.cstn_steps_per_second': 2.565, 'epoch': 12.0}
{'loss': 1.4048, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.5074864625930786, 'eval_accuracy@por.rst.cstn': 0.5514834205933682, 'eval_f1@por.rst.cstn': 0.23604593408873797, 'eval_precision@por.rst.cstn': 0.26916757033052696, 'eval_recall@por.rst.cstn': 0.2594186605699947, 'eval_loss@por.rst.cstn': 1.5074865818023682, 'eval_runtime': 7.3252, 'eval_samples_per_second': 78.223, 'eval_steps_per_second': 2.457, 'epoch': 12.0}
{'train_runtime': 1976.3613, 'train_samples_per_second': 25.186, 'train_steps_per_second': 0.789, 'train_loss': 1.6550174810947516, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.4648
  train_runtime            = 3:45:04.48
  train_samples_per_second =     25.611
  train_steps_per_second   =      0.801
-------------------------------------------------------------------
Lang1:  spa.rst.rststb    Lang2:  por.rst.cstn
Saving run to:  runs/full_shot/FullShot=v4_spa.rst.rststb_por.rst.cstn_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2240 examples
read 383 examples
read 426 examples
read 4148 examples
read 573 examples
read 272 examples
Total prediction labels:  41
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (pt): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (pt): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=41, bias=True)
    )
  )
)
{'train@spa.rst.rststb_loss': 2.916705846786499, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.26026785714285716, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.03660035000460532, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.04748020185231223, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.05197143947877874, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.916705846786499, 'train@spa.rst.rststb_runtime': 27.3744, 'train@spa.rst.rststb_samples_per_second': 81.828, 'train@spa.rst.rststb_steps_per_second': 2.557, 'epoch': 1.0}
{'loss': 3.3528, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.9456865787506104, 'eval_accuracy@spa.rst.rststb': 0.2349869451697128, 'eval_f1@spa.rst.rststb': 0.032202427977792526, 'eval_precision@spa.rst.rststb': 0.04822587253194817, 'eval_recall@spa.rst.rststb': 0.052805405603194314, 'eval_loss@spa.rst.rststb': 2.9456863403320312, 'eval_runtime': 4.9985, 'eval_samples_per_second': 76.623, 'eval_steps_per_second': 2.401, 'epoch': 1.0}
{'train@spa.rst.rststb_loss': 2.589705467224121, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.28080357142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.042236498546864606, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.04834906851390368, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.057032519880477514, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.589705228805542, 'train@spa.rst.rststb_runtime': 27.5323, 'train@spa.rst.rststb_samples_per_second': 81.359, 'train@spa.rst.rststb_steps_per_second': 2.542, 'epoch': 2.0}
{'loss': 2.7515, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.6887309551239014, 'eval_accuracy@spa.rst.rststb': 0.24804177545691905, 'eval_f1@spa.rst.rststb': 0.03767499922997621, 'eval_precision@spa.rst.rststb': 0.05459649681617644, 'eval_recall@spa.rst.rststb': 0.05759247242050616, 'eval_loss@spa.rst.rststb': 2.688730478286743, 'eval_runtime': 5.0504, 'eval_samples_per_second': 75.835, 'eval_steps_per_second': 2.376, 'epoch': 2.0}
{'train@spa.rst.rststb_loss': 2.4621951580047607, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.32723214285714286, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.0497565668965689, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.040802957397003334, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.07073182710365535, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.4621951580047607, 'train@spa.rst.rststb_runtime': 27.5581, 'train@spa.rst.rststb_samples_per_second': 81.283, 'train@spa.rst.rststb_steps_per_second': 2.54, 'epoch': 3.0}
{'loss': 2.5666, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.602119207382202, 'eval_accuracy@spa.rst.rststb': 0.2924281984334204, 'eval_f1@spa.rst.rststb': 0.05427508811823606, 'eval_precision@spa.rst.rststb': 0.08691297614726168, 'eval_recall@spa.rst.rststb': 0.07530633184491028, 'eval_loss@spa.rst.rststb': 2.602119207382202, 'eval_runtime': 5.045, 'eval_samples_per_second': 75.917, 'eval_steps_per_second': 2.379, 'epoch': 3.0}
{'train@spa.rst.rststb_loss': 2.3607983589172363, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.340625, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.062003169255814825, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.057731503846458435, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.07978092556569408, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.3607983589172363, 'train@spa.rst.rststb_runtime': 27.5464, 'train@spa.rst.rststb_samples_per_second': 81.317, 'train@spa.rst.rststb_steps_per_second': 2.541, 'epoch': 4.0}
{'loss': 2.4573, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.529491662979126, 'eval_accuracy@spa.rst.rststb': 0.3263707571801567, 'eval_f1@spa.rst.rststb': 0.07170049816530427, 'eval_precision@spa.rst.rststb': 0.06959148101929778, 'eval_recall@spa.rst.rststb': 0.09146982318891075, 'eval_loss@spa.rst.rststb': 2.529491662979126, 'eval_runtime': 5.0315, 'eval_samples_per_second': 76.12, 'eval_steps_per_second': 2.385, 'epoch': 4.0}
{'train@spa.rst.rststb_loss': 2.274036169052124, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.37589285714285714, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.07985366023353413, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.0877597693330197, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.10019759710846594, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.274036169052124, 'train@spa.rst.rststb_runtime': 27.4992, 'train@spa.rst.rststb_samples_per_second': 81.457, 'train@spa.rst.rststb_steps_per_second': 2.546, 'epoch': 5.0}
{'loss': 2.3653, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.469407320022583, 'eval_accuracy@spa.rst.rststb': 0.35509138381201044, 'eval_f1@spa.rst.rststb': 0.10286448014131819, 'eval_precision@spa.rst.rststb': 0.10495054453760191, 'eval_recall@spa.rst.rststb': 0.12150988494333684, 'eval_loss@spa.rst.rststb': 2.469406843185425, 'eval_runtime': 5.0118, 'eval_samples_per_second': 76.42, 'eval_steps_per_second': 2.394, 'epoch': 5.0}
{'train@spa.rst.rststb_loss': 2.202706813812256, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.39375, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.08861967575022699, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.0865471202816301, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.11170489297632429, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.2027065753936768, 'train@spa.rst.rststb_runtime': 27.5057, 'train@spa.rst.rststb_samples_per_second': 81.438, 'train@spa.rst.rststb_steps_per_second': 2.545, 'epoch': 6.0}
{'loss': 2.29, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.4175658226013184, 'eval_accuracy@spa.rst.rststb': 0.36553524804177545, 'eval_f1@spa.rst.rststb': 0.10341102270268386, 'eval_precision@spa.rst.rststb': 0.10475773340104746, 'eval_recall@spa.rst.rststb': 0.12830091958493703, 'eval_loss@spa.rst.rststb': 2.41756534576416, 'eval_runtime': 5.0424, 'eval_samples_per_second': 75.955, 'eval_steps_per_second': 2.38, 'epoch': 6.0}
{'train@spa.rst.rststb_loss': 2.1431567668914795, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.409375, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.09638953188388864, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.08676723806786056, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.12208013590773634, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.1431567668914795, 'train@spa.rst.rststb_runtime': 27.5105, 'train@spa.rst.rststb_samples_per_second': 81.423, 'train@spa.rst.rststb_steps_per_second': 2.544, 'epoch': 7.0}
{'loss': 2.2292, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.374405860900879, 'eval_accuracy@spa.rst.rststb': 0.37597911227154046, 'eval_f1@spa.rst.rststb': 0.10505880930541082, 'eval_precision@spa.rst.rststb': 0.0946384938558427, 'eval_recall@spa.rst.rststb': 0.13692556878388865, 'eval_loss@spa.rst.rststb': 2.374406099319458, 'eval_runtime': 5.0304, 'eval_samples_per_second': 76.137, 'eval_steps_per_second': 2.385, 'epoch': 7.0}
{'train@spa.rst.rststb_loss': 2.0970237255096436, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.41875, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10014733386127736, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.14066103520868115, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.12691052299224967, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0970237255096436, 'train@spa.rst.rststb_runtime': 27.5076, 'train@spa.rst.rststb_samples_per_second': 81.432, 'train@spa.rst.rststb_steps_per_second': 2.545, 'epoch': 8.0}
{'loss': 2.1728, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.33833384513855, 'eval_accuracy@spa.rst.rststb': 0.391644908616188, 'eval_f1@spa.rst.rststb': 0.11199871267909574, 'eval_precision@spa.rst.rststb': 0.10188844971453667, 'eval_recall@spa.rst.rststb': 0.14584563921773033, 'eval_loss@spa.rst.rststb': 2.33833384513855, 'eval_runtime': 5.0429, 'eval_samples_per_second': 75.948, 'eval_steps_per_second': 2.38, 'epoch': 8.0}
{'train@spa.rst.rststb_loss': 2.063340187072754, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4232142857142857, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10273021368368575, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.13048582641063747, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.131164021899004, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.063340187072754, 'train@spa.rst.rststb_runtime': 27.5294, 'train@spa.rst.rststb_samples_per_second': 81.367, 'train@spa.rst.rststb_steps_per_second': 2.543, 'epoch': 9.0}
{'loss': 2.1398, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.311295986175537, 'eval_accuracy@spa.rst.rststb': 0.3968668407310705, 'eval_f1@spa.rst.rststb': 0.11170468310408978, 'eval_precision@spa.rst.rststb': 0.09726075402807435, 'eval_recall@spa.rst.rststb': 0.14787699691366551, 'eval_loss@spa.rst.rststb': 2.311295986175537, 'eval_runtime': 5.0277, 'eval_samples_per_second': 76.179, 'eval_steps_per_second': 2.387, 'epoch': 9.0}
{'train@spa.rst.rststb_loss': 2.040040969848633, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.42723214285714284, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10438244543737064, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.13186451831806517, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13260195718822862, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.040040969848633, 'train@spa.rst.rststb_runtime': 27.5901, 'train@spa.rst.rststb_samples_per_second': 81.189, 'train@spa.rst.rststb_steps_per_second': 2.537, 'epoch': 10.0}
{'loss': 2.1043, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.2954258918762207, 'eval_accuracy@spa.rst.rststb': 0.39947780678851175, 'eval_f1@spa.rst.rststb': 0.11461172811397968, 'eval_precision@spa.rst.rststb': 0.10168617614269788, 'eval_recall@spa.rst.rststb': 0.14852592618037547, 'eval_loss@spa.rst.rststb': 2.2954258918762207, 'eval_runtime': 5.0331, 'eval_samples_per_second': 76.096, 'eval_steps_per_second': 2.384, 'epoch': 10.0}
{'train@spa.rst.rststb_loss': 2.026245594024658, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.42857142857142855, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10514953578683746, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.13347122572921274, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13346639168919816, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0262458324432373, 'train@spa.rst.rststb_runtime': 27.5362, 'train@spa.rst.rststb_samples_per_second': 81.347, 'train@spa.rst.rststb_steps_per_second': 2.542, 'epoch': 11.0}
{'loss': 2.0797, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.283123731613159, 'eval_accuracy@spa.rst.rststb': 0.39947780678851175, 'eval_f1@spa.rst.rststb': 0.11385172290944102, 'eval_precision@spa.rst.rststb': 0.100136191431527, 'eval_recall@spa.rst.rststb': 0.14862449771455924, 'eval_loss@spa.rst.rststb': 2.28312349319458, 'eval_runtime': 5.0195, 'eval_samples_per_second': 76.303, 'eval_steps_per_second': 2.391, 'epoch': 11.0}
{'train@spa.rst.rststb_loss': 2.0219924449920654, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.43125, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10630993728295712, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.13463276376846617, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13453146144618208, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0219924449920654, 'train@spa.rst.rststb_runtime': 27.5565, 'train@spa.rst.rststb_samples_per_second': 81.287, 'train@spa.rst.rststb_steps_per_second': 2.54, 'epoch': 12.0}
{'loss': 2.0775, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.2797768115997314, 'eval_accuracy@spa.rst.rststb': 0.402088772845953, 'eval_f1@spa.rst.rststb': 0.11379179128100943, 'eval_precision@spa.rst.rststb': 0.10012241662674269, 'eval_recall@spa.rst.rststb': 0.14917485544708536, 'eval_loss@spa.rst.rststb': 2.2797768115997314, 'eval_runtime': 5.0522, 'eval_samples_per_second': 75.809, 'eval_steps_per_second': 2.375, 'epoch': 12.0}
{'train_runtime': 1078.8886, 'train_samples_per_second': 24.915, 'train_steps_per_second': 0.779, 'train_loss': 2.382230231875465, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.3822
  train_runtime            = 0:17:58.88
  train_samples_per_second =     24.915
  train_steps_per_second   =      0.779
{'train@por.rst.cstn_loss': 2.351616859436035, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.3085824493731919, 'train@por.rst.cstn_f1@por.rst.cstn': 0.030289930247059796, 'train@por.rst.cstn_precision@por.rst.cstn': 0.08484370544625036, 'train@por.rst.cstn_recall@por.rst.cstn': 0.04078040020876451, 'train@por.rst.cstn_loss@por.rst.cstn': 2.351616859436035, 'train@por.rst.cstn_runtime': 50.6215, 'train@por.rst.cstn_samples_per_second': 81.941, 'train@por.rst.cstn_steps_per_second': 2.568, 'epoch': 1.0}
{'loss': 2.8073, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.472508430480957, 'eval_accuracy@por.rst.cstn': 0.2879581151832461, 'eval_f1@por.rst.cstn': 0.031096600120169147, 'eval_precision@por.rst.cstn': 0.04924775928297056, 'eval_recall@por.rst.cstn': 0.051948051948051945, 'eval_loss@por.rst.cstn': 2.472508668899536, 'eval_runtime': 7.3451, 'eval_samples_per_second': 78.011, 'eval_steps_per_second': 2.451, 'epoch': 1.0}
{'train@por.rst.cstn_loss': 2.055574893951416, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.4474445515911283, 'train@por.rst.cstn_f1@por.rst.cstn': 0.07128453360627134, 'train@por.rst.cstn_precision@por.rst.cstn': 0.07833564065553714, 'train@por.rst.cstn_recall@por.rst.cstn': 0.08315635589457451, 'train@por.rst.cstn_loss@por.rst.cstn': 2.055574893951416, 'train@por.rst.cstn_runtime': 50.6664, 'train@por.rst.cstn_samples_per_second': 81.869, 'train@por.rst.cstn_steps_per_second': 2.566, 'epoch': 2.0}
{'loss': 2.2627, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.2032294273376465, 'eval_accuracy@por.rst.cstn': 0.38394415357766143, 'eval_f1@por.rst.cstn': 0.09497870026284781, 'eval_precision@por.rst.cstn': 0.09840596064654661, 'eval_recall@por.rst.cstn': 0.12461624776998192, 'eval_loss@por.rst.cstn': 2.2032291889190674, 'eval_runtime': 7.3376, 'eval_samples_per_second': 78.091, 'eval_steps_per_second': 2.453, 'epoch': 2.0}
{'train@por.rst.cstn_loss': 1.847312569618225, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5149469623915139, 'train@por.rst.cstn_f1@por.rst.cstn': 0.10415504578962329, 'train@por.rst.cstn_precision@por.rst.cstn': 0.11416463030719279, 'train@por.rst.cstn_recall@por.rst.cstn': 0.11563518366277095, 'train@por.rst.cstn_loss@por.rst.cstn': 1.847312331199646, 'train@por.rst.cstn_runtime': 50.6578, 'train@por.rst.cstn_samples_per_second': 81.883, 'train@por.rst.cstn_steps_per_second': 2.566, 'epoch': 3.0}
{'loss': 2.0185, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.9978752136230469, 'eval_accuracy@por.rst.cstn': 0.4223385689354276, 'eval_f1@por.rst.cstn': 0.12425388597665563, 'eval_precision@por.rst.cstn': 0.12657127844080285, 'eval_recall@por.rst.cstn': 0.1517918118448595, 'eval_loss@por.rst.cstn': 1.9978752136230469, 'eval_runtime': 7.3164, 'eval_samples_per_second': 78.317, 'eval_steps_per_second': 2.46, 'epoch': 3.0}
{'train@por.rst.cstn_loss': 1.736041784286499, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5511089681774349, 'train@por.rst.cstn_f1@por.rst.cstn': 0.11523922000972744, 'train@por.rst.cstn_precision@por.rst.cstn': 0.11247502071970752, 'train@por.rst.cstn_recall@por.rst.cstn': 0.13091073697792383, 'train@por.rst.cstn_loss@por.rst.cstn': 1.736041784286499, 'train@por.rst.cstn_runtime': 50.6718, 'train@por.rst.cstn_samples_per_second': 81.86, 'train@por.rst.cstn_steps_per_second': 2.566, 'epoch': 4.0}
{'loss': 1.8618, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.8886322975158691, 'eval_accuracy@por.rst.cstn': 0.4572425828970332, 'eval_f1@por.rst.cstn': 0.1532351633477067, 'eval_precision@por.rst.cstn': 0.14153594611130496, 'eval_recall@por.rst.cstn': 0.18113930772553613, 'eval_loss@por.rst.cstn': 1.8886324167251587, 'eval_runtime': 7.3469, 'eval_samples_per_second': 77.992, 'eval_steps_per_second': 2.45, 'epoch': 4.0}
{'train@por.rst.cstn_loss': 1.6622151136398315, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5537608486017358, 'train@por.rst.cstn_f1@por.rst.cstn': 0.11973898126138516, 'train@por.rst.cstn_precision@por.rst.cstn': 0.11299417160708863, 'train@por.rst.cstn_recall@por.rst.cstn': 0.13480535537866692, 'train@por.rst.cstn_loss@por.rst.cstn': 1.662215232849121, 'train@por.rst.cstn_runtime': 50.6643, 'train@por.rst.cstn_samples_per_second': 81.872, 'train@por.rst.cstn_steps_per_second': 2.566, 'epoch': 5.0}
{'loss': 1.7636, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.8079227209091187, 'eval_accuracy@por.rst.cstn': 0.4781849912739965, 'eval_f1@por.rst.cstn': 0.1656559273426107, 'eval_precision@por.rst.cstn': 0.15315245183666237, 'eval_recall@por.rst.cstn': 0.19354754415083103, 'eval_loss@por.rst.cstn': 1.8079227209091187, 'eval_runtime': 7.3354, 'eval_samples_per_second': 78.115, 'eval_steps_per_second': 2.454, 'epoch': 5.0}
{'train@por.rst.cstn_loss': 1.6120768785476685, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5687078109932497, 'train@por.rst.cstn_f1@por.rst.cstn': 0.12600789190953082, 'train@por.rst.cstn_precision@por.rst.cstn': 0.1163841473468312, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1415765644212412, 'train@por.rst.cstn_loss@por.rst.cstn': 1.6120768785476685, 'train@por.rst.cstn_runtime': 50.7177, 'train@por.rst.cstn_samples_per_second': 81.786, 'train@por.rst.cstn_steps_per_second': 2.563, 'epoch': 6.0}
{'loss': 1.6998, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.7520544528961182, 'eval_accuracy@por.rst.cstn': 0.4869109947643979, 'eval_f1@por.rst.cstn': 0.1715777566726661, 'eval_precision@por.rst.cstn': 0.15762503592278934, 'eval_recall@por.rst.cstn': 0.19714786577174762, 'eval_loss@por.rst.cstn': 1.7520544528961182, 'eval_runtime': 7.3347, 'eval_samples_per_second': 78.122, 'eval_steps_per_second': 2.454, 'epoch': 6.0}
{'train@por.rst.cstn_loss': 1.5752278566360474, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5754580520732884, 'train@por.rst.cstn_f1@por.rst.cstn': 0.12969531367930792, 'train@por.rst.cstn_precision@por.rst.cstn': 0.1181779927540938, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14611663692044866, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5752277374267578, 'train@por.rst.cstn_runtime': 50.6748, 'train@por.rst.cstn_samples_per_second': 81.855, 'train@por.rst.cstn_steps_per_second': 2.565, 'epoch': 7.0}
{'loss': 1.6511, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.7159696817398071, 'eval_accuracy@por.rst.cstn': 0.4973821989528796, 'eval_f1@por.rst.cstn': 0.18046344390765753, 'eval_precision@por.rst.cstn': 0.16481717047121178, 'eval_recall@por.rst.cstn': 0.20763340108565823, 'eval_loss@por.rst.cstn': 1.7159696817398071, 'eval_runtime': 7.3421, 'eval_samples_per_second': 78.043, 'eval_steps_per_second': 2.452, 'epoch': 7.0}
{'train@por.rst.cstn_loss': 1.5507630109786987, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5769045323047252, 'train@por.rst.cstn_f1@por.rst.cstn': 0.13124381056166434, 'train@por.rst.cstn_precision@por.rst.cstn': 0.15020287112728206, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14728844277113404, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5507631301879883, 'train@por.rst.cstn_runtime': 50.7176, 'train@por.rst.cstn_samples_per_second': 81.786, 'train@por.rst.cstn_steps_per_second': 2.563, 'epoch': 8.0}
{'loss': 1.6273, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.6938536167144775, 'eval_accuracy@por.rst.cstn': 0.5008726003490401, 'eval_f1@por.rst.cstn': 0.18222842503734038, 'eval_precision@por.rst.cstn': 0.16776703487150257, 'eval_recall@por.rst.cstn': 0.20800191129398538, 'eval_loss@por.rst.cstn': 1.693853735923767, 'eval_runtime': 7.3577, 'eval_samples_per_second': 77.878, 'eval_steps_per_second': 2.446, 'epoch': 8.0}
{'train@por.rst.cstn_loss': 1.53253173828125, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5805207328833173, 'train@por.rst.cstn_f1@por.rst.cstn': 0.13281791369691082, 'train@por.rst.cstn_precision@por.rst.cstn': 0.1369189354500972, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1487820802813873, 'train@por.rst.cstn_loss@por.rst.cstn': 1.53253173828125, 'train@por.rst.cstn_runtime': 50.6855, 'train@por.rst.cstn_samples_per_second': 81.838, 'train@por.rst.cstn_steps_per_second': 2.565, 'epoch': 9.0}
{'loss': 1.6048, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.6748780012130737, 'eval_accuracy@por.rst.cstn': 0.5113438045375218, 'eval_f1@por.rst.cstn': 0.1937577573752214, 'eval_precision@por.rst.cstn': 0.1957846540220415, 'eval_recall@por.rst.cstn': 0.21614232160830874, 'eval_loss@por.rst.cstn': 1.6748781204223633, 'eval_runtime': 7.3549, 'eval_samples_per_second': 77.907, 'eval_steps_per_second': 2.447, 'epoch': 9.0}
{'train@por.rst.cstn_loss': 1.5186092853546143, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5843780135004821, 'train@por.rst.cstn_f1@por.rst.cstn': 0.13619145268898655, 'train@por.rst.cstn_precision@por.rst.cstn': 0.14785947865859855, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1517246843803739, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5186094045639038, 'train@por.rst.cstn_runtime': 50.6677, 'train@por.rst.cstn_samples_per_second': 81.867, 'train@por.rst.cstn_steps_per_second': 2.566, 'epoch': 10.0}
{'loss': 1.5754, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.661267638206482, 'eval_accuracy@por.rst.cstn': 0.5078534031413613, 'eval_f1@por.rst.cstn': 0.19117740200563135, 'eval_precision@por.rst.cstn': 0.20530300221439282, 'eval_recall@por.rst.cstn': 0.21738059891013917, 'eval_loss@por.rst.cstn': 1.661267638206482, 'eval_runtime': 7.3373, 'eval_samples_per_second': 78.094, 'eval_steps_per_second': 2.453, 'epoch': 10.0}
{'train@por.rst.cstn_loss': 1.511122465133667, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5855834136933462, 'train@por.rst.cstn_f1@por.rst.cstn': 0.13763498257654527, 'train@por.rst.cstn_precision@por.rst.cstn': 0.14999395405628024, 'train@por.rst.cstn_recall@por.rst.cstn': 0.15249301324916997, 'train@por.rst.cstn_loss@por.rst.cstn': 1.511122465133667, 'train@por.rst.cstn_runtime': 50.6509, 'train@por.rst.cstn_samples_per_second': 81.894, 'train@por.rst.cstn_steps_per_second': 2.567, 'epoch': 11.0}
{'loss': 1.5746, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.656053900718689, 'eval_accuracy@por.rst.cstn': 0.5078534031413613, 'eval_f1@por.rst.cstn': 0.18991180938377666, 'eval_precision@por.rst.cstn': 0.19828115224907253, 'eval_recall@por.rst.cstn': 0.21474538769369225, 'eval_loss@por.rst.cstn': 1.6560540199279785, 'eval_runtime': 7.3347, 'eval_samples_per_second': 78.122, 'eval_steps_per_second': 2.454, 'epoch': 11.0}
{'train@por.rst.cstn_loss': 1.5085941553115845, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5860655737704918, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1377305533863958, 'train@por.rst.cstn_precision@por.rst.cstn': 0.14939455383513334, 'train@por.rst.cstn_recall@por.rst.cstn': 0.15238621114937198, 'train@por.rst.cstn_loss@por.rst.cstn': 1.508594036102295, 'train@por.rst.cstn_runtime': 50.6893, 'train@por.rst.cstn_samples_per_second': 81.832, 'train@por.rst.cstn_steps_per_second': 2.565, 'epoch': 12.0}
{'loss': 1.5691, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.6527334451675415, 'eval_accuracy@por.rst.cstn': 0.5113438045375218, 'eval_f1@por.rst.cstn': 0.19158659111160323, 'eval_precision@por.rst.cstn': 0.20049809702159474, 'eval_recall@por.rst.cstn': 0.21531004043225802, 'eval_loss@por.rst.cstn': 1.6527332067489624, 'eval_runtime': 7.3492, 'eval_samples_per_second': 77.968, 'eval_steps_per_second': 2.449, 'epoch': 12.0}
{'train_runtime': 1973.6041, 'train_samples_per_second': 25.221, 'train_steps_per_second': 0.79, 'train_loss': 1.8346622956104768, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.3822
  train_runtime            = 0:17:58.88
  train_samples_per_second =     24.915
  train_steps_per_second   =      0.779
-------------------------------------------------------------------
Lang1:  spa.rst.sctb    Lang2:  por.rst.cstn
Saving run to:  runs/full_shot/FullShot=v4_spa.rst.sctb_por.rst.cstn_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 439 examples
read 94 examples
read 159 examples
read 4148 examples
read 573 examples
read 272 examples
Total prediction labels:  38
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (pt): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (pt): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=38, bias=True)
    )
  )
)
{'train@spa.rst.sctb_loss': 3.3774592876434326, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3462414578587699, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02370857685911143, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.045134640522875816, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.041362779156327543, 'train@spa.rst.sctb_loss@spa.rst.sctb': 3.3774592876434326, 'train@spa.rst.sctb_runtime': 5.6072, 'train@spa.rst.sctb_samples_per_second': 78.293, 'train@spa.rst.sctb_steps_per_second': 2.497, 'epoch': 1.0}
{'loss': 3.5408, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.358518123626709, 'eval_accuracy@spa.rst.sctb': 0.3723404255319149, 'eval_f1@spa.rst.sctb': 0.0546218487394958, 'eval_precision@spa.rst.sctb': 0.10465116279069768, 'eval_recall@spa.rst.sctb': 0.0695906432748538, 'eval_loss@spa.rst.sctb': 3.358518600463867, 'eval_runtime': 1.4849, 'eval_samples_per_second': 63.304, 'eval_steps_per_second': 2.02, 'epoch': 1.0}
{'train@spa.rst.sctb_loss': 3.1277294158935547, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3530751708428246, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.025751710119108793, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.04064447040498442, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04390681003584229, 'train@spa.rst.sctb_loss@spa.rst.sctb': 3.127729654312134, 'train@spa.rst.sctb_runtime': 5.6701, 'train@spa.rst.sctb_samples_per_second': 77.424, 'train@spa.rst.sctb_steps_per_second': 2.469, 'epoch': 2.0}
{'loss': 3.2741, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.1104724407196045, 'eval_accuracy@spa.rst.sctb': 0.3617021276595745, 'eval_f1@spa.rst.sctb': 0.036656891495601175, 'eval_precision@spa.rst.sctb': 0.04093945270415859, 'eval_recall@spa.rst.sctb': 0.06191950464396284, 'eval_loss@spa.rst.sctb': 3.110471725463867, 'eval_runtime': 1.4852, 'eval_samples_per_second': 63.291, 'eval_steps_per_second': 2.02, 'epoch': 2.0}
{'train@spa.rst.sctb_loss': 2.886413335800171, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3553530751708428, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02642568292481638, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.035470335675253706, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.044354838709677415, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.886413335800171, 'train@spa.rst.sctb_runtime': 5.656, 'train@spa.rst.sctb_samples_per_second': 77.617, 'train@spa.rst.sctb_steps_per_second': 2.475, 'epoch': 3.0}
{'loss': 3.0443, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.87359356880188, 'eval_accuracy@spa.rst.sctb': 0.3617021276595745, 'eval_f1@spa.rst.sctb': 0.036656891495601175, 'eval_precision@spa.rst.sctb': 0.04093945270415859, 'eval_recall@spa.rst.sctb': 0.06191950464396284, 'eval_loss@spa.rst.sctb': 2.873594284057617, 'eval_runtime': 1.4858, 'eval_samples_per_second': 63.267, 'eval_steps_per_second': 2.019, 'epoch': 3.0}
{'train@spa.rst.sctb_loss': 2.6782538890838623, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.35079726651480636, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02481969799414019, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03305340223944875, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04345878136200717, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.6782538890838623, 'train@spa.rst.sctb_runtime': 5.6754, 'train@spa.rst.sctb_samples_per_second': 77.351, 'train@spa.rst.sctb_steps_per_second': 2.467, 'epoch': 4.0}
{'loss': 2.816, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.6736807823181152, 'eval_accuracy@spa.rst.sctb': 0.3617021276595745, 'eval_f1@spa.rst.sctb': 0.03669467787114846, 'eval_precision@spa.rst.sctb': 0.07969639468690703, 'eval_recall@spa.rst.sctb': 0.06191950464396284, 'eval_loss@spa.rst.sctb': 2.6736807823181152, 'eval_runtime': 1.4724, 'eval_samples_per_second': 63.84, 'eval_steps_per_second': 2.037, 'epoch': 4.0}
{'train@spa.rst.sctb_loss': 2.517395496368408, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.35079726651480636, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.024810996563573884, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.0382771164021164, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04345878136200717, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.5173957347869873, 'train@spa.rst.sctb_runtime': 5.7201, 'train@spa.rst.sctb_samples_per_second': 76.747, 'train@spa.rst.sctb_steps_per_second': 2.448, 'epoch': 5.0}
{'loss': 2.6332, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.5198206901550293, 'eval_accuracy@spa.rst.sctb': 0.3617021276595745, 'eval_f1@spa.rst.sctb': 0.03669467787114846, 'eval_precision@spa.rst.sctb': 0.07969639468690703, 'eval_recall@spa.rst.sctb': 0.06191950464396284, 'eval_loss@spa.rst.sctb': 2.5198206901550293, 'eval_runtime': 1.4946, 'eval_samples_per_second': 62.894, 'eval_steps_per_second': 2.007, 'epoch': 5.0}
{'train@spa.rst.sctb_loss': 2.40718150138855, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3439635535307517, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02219911357603337, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.02822375127420999, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.042114695340501794, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.4071812629699707, 'train@spa.rst.sctb_runtime': 5.7003, 'train@spa.rst.sctb_samples_per_second': 77.013, 'train@spa.rst.sctb_steps_per_second': 2.456, 'epoch': 6.0}
{'loss': 2.4988, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.417895793914795, 'eval_accuracy@spa.rst.sctb': 0.3617021276595745, 'eval_f1@spa.rst.sctb': 0.03669467787114846, 'eval_precision@spa.rst.sctb': 0.07969639468690703, 'eval_recall@spa.rst.sctb': 0.06191950464396284, 'eval_loss@spa.rst.sctb': 2.417896270751953, 'eval_runtime': 1.4923, 'eval_samples_per_second': 62.989, 'eval_steps_per_second': 2.01, 'epoch': 6.0}
{'train@spa.rst.sctb_loss': 2.3364875316619873, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3530751708428246, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.025632707834264926, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.033542197677711695, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04390681003584229, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.3364877700805664, 'train@spa.rst.sctb_runtime': 5.7134, 'train@spa.rst.sctb_samples_per_second': 76.837, 'train@spa.rst.sctb_steps_per_second': 2.45, 'epoch': 7.0}
{'loss': 2.4154, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.354234218597412, 'eval_accuracy@spa.rst.sctb': 0.3617021276595745, 'eval_f1@spa.rst.sctb': 0.0366610644257703, 'eval_precision@spa.rst.sctb': 0.05051150895140665, 'eval_recall@spa.rst.sctb': 0.06191950464396284, 'eval_loss@spa.rst.sctb': 2.354233503341675, 'eval_runtime': 1.4894, 'eval_samples_per_second': 63.114, 'eval_steps_per_second': 2.014, 'epoch': 7.0}
{'train@spa.rst.sctb_loss': 2.2950143814086914, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3553530751708428, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02642568292481638, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.035470335675253706, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.044354838709677415, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.2950146198272705, 'train@spa.rst.sctb_runtime': 5.7061, 'train@spa.rst.sctb_samples_per_second': 76.935, 'train@spa.rst.sctb_steps_per_second': 2.453, 'epoch': 8.0}
{'loss': 2.35, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.3180389404296875, 'eval_accuracy@spa.rst.sctb': 0.3723404255319149, 'eval_f1@spa.rst.sctb': 0.042004485078488876, 'eval_precision@spa.rst.sctb': 0.06054729584141348, 'eval_recall@spa.rst.sctb': 0.06501547987616098, 'eval_loss@spa.rst.sctb': 2.3180391788482666, 'eval_runtime': 1.4836, 'eval_samples_per_second': 63.358, 'eval_steps_per_second': 2.022, 'epoch': 8.0}
{'train@spa.rst.sctb_loss': 2.2676446437835693, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3553530751708428, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02697134677811006, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.028145665285953053, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.044525089605734765, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.2676446437835693, 'train@spa.rst.sctb_runtime': 5.6972, 'train@spa.rst.sctb_samples_per_second': 77.056, 'train@spa.rst.sctb_steps_per_second': 2.457, 'epoch': 9.0}
{'loss': 2.3227, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.2940900325775146, 'eval_accuracy@spa.rst.sctb': 0.3829787234042553, 'eval_f1@spa.rst.sctb': 0.04652844744455159, 'eval_precision@spa.rst.sctb': 0.05710508922670192, 'eval_recall@spa.rst.sctb': 0.06811145510835914, 'eval_loss@spa.rst.sctb': 2.2940900325775146, 'eval_runtime': 1.4889, 'eval_samples_per_second': 63.133, 'eval_steps_per_second': 2.015, 'epoch': 9.0}
{'train@spa.rst.sctb_loss': 2.249965190887451, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.36674259681093396, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.030397558472424785, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03296257675819719, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04676523297491039, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.249965190887451, 'train@spa.rst.sctb_runtime': 5.6967, 'train@spa.rst.sctb_samples_per_second': 77.062, 'train@spa.rst.sctb_steps_per_second': 2.458, 'epoch': 10.0}
{'loss': 2.2975, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.279482126235962, 'eval_accuracy@spa.rst.sctb': 0.39361702127659576, 'eval_f1@spa.rst.sctb': 0.05090909090909091, 'eval_precision@spa.rst.sctb': 0.06127450980392156, 'eval_recall@spa.rst.sctb': 0.07120743034055728, 'eval_loss@spa.rst.sctb': 2.279482364654541, 'eval_runtime': 1.4899, 'eval_samples_per_second': 63.094, 'eval_steps_per_second': 2.014, 'epoch': 10.0}
{'train@spa.rst.sctb_loss': 2.2404401302337646, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3735763097949886, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03225276921320087, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.034230855351545006, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04810931899641577, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.240440607070923, 'train@spa.rst.sctb_runtime': 5.6774, 'train@spa.rst.sctb_samples_per_second': 77.324, 'train@spa.rst.sctb_steps_per_second': 2.466, 'epoch': 11.0}
{'loss': 2.2885, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.2712740898132324, 'eval_accuracy@spa.rst.sctb': 0.4148936170212766, 'eval_f1@spa.rst.sctb': 0.05811138014527845, 'eval_precision@spa.rst.sctb': 0.06205305651672434, 'eval_recall@spa.rst.sctb': 0.07739938080495357, 'eval_loss@spa.rst.sctb': 2.2712740898132324, 'eval_runtime': 1.4718, 'eval_samples_per_second': 63.87, 'eval_steps_per_second': 2.038, 'epoch': 11.0}
{'train@spa.rst.sctb_loss': 2.2373135089874268, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.37585421412300685, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.032871060036414365, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.034937061244250785, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04855734767025089, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.2373135089874268, 'train@spa.rst.sctb_runtime': 5.6971, 'train@spa.rst.sctb_samples_per_second': 77.057, 'train@spa.rst.sctb_steps_per_second': 2.457, 'epoch': 12.0}
{'loss': 2.284, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.2688708305358887, 'eval_accuracy@spa.rst.sctb': 0.4148936170212766, 'eval_f1@spa.rst.sctb': 0.05811138014527845, 'eval_precision@spa.rst.sctb': 0.06205305651672434, 'eval_recall@spa.rst.sctb': 0.07739938080495357, 'eval_loss@spa.rst.sctb': 2.2688710689544678, 'eval_runtime': 1.4787, 'eval_samples_per_second': 63.569, 'eval_steps_per_second': 2.029, 'epoch': 12.0}
{'train_runtime': 220.7463, 'train_samples_per_second': 23.864, 'train_steps_per_second': 0.761, 'train_loss': 2.647113641103109, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.6471
  train_runtime            = 0:03:40.74
  train_samples_per_second =     23.864
  train_steps_per_second   =      0.761
{'train@por.rst.cstn_loss': 2.4194858074188232, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.2774831243972999, 'train@por.rst.cstn_f1@por.rst.cstn': 0.013575674655595393, 'train@por.rst.cstn_precision@por.rst.cstn': 0.008671347637415621, 'train@por.rst.cstn_recall@por.rst.cstn': 0.03125, 'train@por.rst.cstn_loss@por.rst.cstn': 2.419485569000244, 'train@por.rst.cstn_runtime': 50.6671, 'train@por.rst.cstn_samples_per_second': 81.868, 'train@por.rst.cstn_steps_per_second': 2.566, 'epoch': 1.0}
{'loss': 2.8916, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.4838504791259766, 'eval_accuracy@por.rst.cstn': 0.28097731239092494, 'eval_f1@por.rst.cstn': 0.019940549913301957, 'eval_precision@por.rst.cstn': 0.012771696017769315, 'eval_recall@por.rst.cstn': 0.045454545454545456, 'eval_loss@por.rst.cstn': 2.4838504791259766, 'eval_runtime': 7.3147, 'eval_samples_per_second': 78.336, 'eval_steps_per_second': 2.461, 'epoch': 1.0}
{'train@por.rst.cstn_loss': 2.2032532691955566, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.3799421407907425, 'train@por.rst.cstn_f1@por.rst.cstn': 0.05682899479448891, 'train@por.rst.cstn_precision@por.rst.cstn': 0.09256394476075955, 'train@por.rst.cstn_recall@por.rst.cstn': 0.06413061337420489, 'train@por.rst.cstn_loss@por.rst.cstn': 2.2032532691955566, 'train@por.rst.cstn_runtime': 50.661, 'train@por.rst.cstn_samples_per_second': 81.878, 'train@por.rst.cstn_steps_per_second': 2.566, 'epoch': 2.0}
{'loss': 2.356, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.3084094524383545, 'eval_accuracy@por.rst.cstn': 0.3298429319371728, 'eval_f1@por.rst.cstn': 0.07266544731692343, 'eval_precision@por.rst.cstn': 0.1206201717251441, 'eval_recall@por.rst.cstn': 0.08459889392903747, 'eval_loss@por.rst.cstn': 2.3084094524383545, 'eval_runtime': 7.3151, 'eval_samples_per_second': 78.331, 'eval_steps_per_second': 2.461, 'epoch': 2.0}
{'train@por.rst.cstn_loss': 1.9763219356536865, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.4643201542912247, 'train@por.rst.cstn_f1@por.rst.cstn': 0.07263518800602128, 'train@por.rst.cstn_precision@por.rst.cstn': 0.07522994167136568, 'train@por.rst.cstn_recall@por.rst.cstn': 0.08919988299250656, 'train@por.rst.cstn_loss@por.rst.cstn': 1.9763219356536865, 'train@por.rst.cstn_runtime': 50.6129, 'train@por.rst.cstn_samples_per_second': 81.955, 'train@por.rst.cstn_steps_per_second': 2.569, 'epoch': 3.0}
{'loss': 2.1494, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.099026918411255, 'eval_accuracy@por.rst.cstn': 0.3856893542757417, 'eval_f1@por.rst.cstn': 0.09691221005592353, 'eval_precision@por.rst.cstn': 0.10001818442994916, 'eval_recall@por.rst.cstn': 0.12743435574203205, 'eval_loss@por.rst.cstn': 2.099026679992676, 'eval_runtime': 7.3072, 'eval_samples_per_second': 78.415, 'eval_steps_per_second': 2.463, 'epoch': 3.0}
{'train@por.rst.cstn_loss': 1.8103493452072144, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.532304725168756, 'train@por.rst.cstn_f1@por.rst.cstn': 0.10511699585484076, 'train@por.rst.cstn_precision@por.rst.cstn': 0.10251206332556244, 'train@por.rst.cstn_recall@por.rst.cstn': 0.11687353853441217, 'train@por.rst.cstn_loss@por.rst.cstn': 1.8103493452072144, 'train@por.rst.cstn_runtime': 50.6465, 'train@por.rst.cstn_samples_per_second': 81.901, 'train@por.rst.cstn_steps_per_second': 2.567, 'epoch': 4.0}
{'loss': 1.956, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.954546332359314, 'eval_accuracy@por.rst.cstn': 0.43106457242582896, 'eval_f1@por.rst.cstn': 0.1267671092475729, 'eval_precision@por.rst.cstn': 0.12563772192927566, 'eval_recall@por.rst.cstn': 0.1525475339444551, 'eval_loss@por.rst.cstn': 1.954546332359314, 'eval_runtime': 7.3265, 'eval_samples_per_second': 78.209, 'eval_steps_per_second': 2.457, 'epoch': 4.0}
{'train@por.rst.cstn_loss': 1.7088412046432495, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5380906460945034, 'train@por.rst.cstn_f1@por.rst.cstn': 0.11175720878283979, 'train@por.rst.cstn_precision@por.rst.cstn': 0.11726981579479408, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1265780134533799, 'train@por.rst.cstn_loss@por.rst.cstn': 1.7088412046432495, 'train@por.rst.cstn_runtime': 50.5876, 'train@por.rst.cstn_samples_per_second': 81.996, 'train@por.rst.cstn_steps_per_second': 2.57, 'epoch': 5.0}
{'loss': 1.8209, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.853276014328003, 'eval_accuracy@por.rst.cstn': 0.45549738219895286, 'eval_f1@por.rst.cstn': 0.15715013466496142, 'eval_precision@por.rst.cstn': 0.16024346137992473, 'eval_recall@por.rst.cstn': 0.17995869269532677, 'eval_loss@por.rst.cstn': 1.853276014328003, 'eval_runtime': 7.3161, 'eval_samples_per_second': 78.321, 'eval_steps_per_second': 2.46, 'epoch': 5.0}
{'train@por.rst.cstn_loss': 1.6418671607971191, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5614754098360656, 'train@por.rst.cstn_f1@por.rst.cstn': 0.12199862065733161, 'train@por.rst.cstn_precision@por.rst.cstn': 0.1275428116724867, 'train@por.rst.cstn_recall@por.rst.cstn': 0.13555962815687161, 'train@por.rst.cstn_loss@por.rst.cstn': 1.6418670415878296, 'train@por.rst.cstn_runtime': 50.7361, 'train@por.rst.cstn_samples_per_second': 81.756, 'train@por.rst.cstn_steps_per_second': 2.562, 'epoch': 6.0}
{'loss': 1.7396, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.7815406322479248, 'eval_accuracy@por.rst.cstn': 0.4816753926701571, 'eval_f1@por.rst.cstn': 0.16843044736868718, 'eval_precision@por.rst.cstn': 0.16251678186067325, 'eval_recall@por.rst.cstn': 0.19125265553391205, 'eval_loss@por.rst.cstn': 1.7815406322479248, 'eval_runtime': 7.3, 'eval_samples_per_second': 78.494, 'eval_steps_per_second': 2.466, 'epoch': 6.0}
{'train@por.rst.cstn_loss': 1.5963166952133179, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5754580520732884, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1307587088745633, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13543255934139217, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14315906012472418, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5963164567947388, 'train@por.rst.cstn_runtime': 50.6876, 'train@por.rst.cstn_samples_per_second': 81.835, 'train@por.rst.cstn_steps_per_second': 2.565, 'epoch': 7.0}
{'loss': 1.6822, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.7339227199554443, 'eval_accuracy@por.rst.cstn': 0.49214659685863876, 'eval_f1@por.rst.cstn': 0.17688764247739497, 'eval_precision@por.rst.cstn': 0.18447800163342684, 'eval_recall@por.rst.cstn': 0.1985891936692852, 'eval_loss@por.rst.cstn': 1.7339224815368652, 'eval_runtime': 7.3118, 'eval_samples_per_second': 78.366, 'eval_steps_per_second': 2.462, 'epoch': 7.0}
{'train@por.rst.cstn_loss': 1.5673905611038208, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5831726133076182, 'train@por.rst.cstn_f1@por.rst.cstn': 0.13606941497632302, 'train@por.rst.cstn_precision@por.rst.cstn': 0.1398857241330685, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1474628385591088, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5673905611038208, 'train@por.rst.cstn_runtime': 50.7394, 'train@por.rst.cstn_samples_per_second': 81.751, 'train@por.rst.cstn_steps_per_second': 2.562, 'epoch': 8.0}
{'loss': 1.6457, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.7089579105377197, 'eval_accuracy@por.rst.cstn': 0.493891797556719, 'eval_f1@por.rst.cstn': 0.17758283850845935, 'eval_precision@por.rst.cstn': 0.17890791174109796, 'eval_recall@por.rst.cstn': 0.19906766256880673, 'eval_loss@por.rst.cstn': 1.7089577913284302, 'eval_runtime': 7.3114, 'eval_samples_per_second': 78.371, 'eval_steps_per_second': 2.462, 'epoch': 8.0}
{'train@por.rst.cstn_loss': 1.5480300188064575, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5863066538090647, 'train@por.rst.cstn_f1@por.rst.cstn': 0.13879916802605852, 'train@por.rst.cstn_precision@por.rst.cstn': 0.14154312632412816, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14926234735858873, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5480300188064575, 'train@por.rst.cstn_runtime': 50.6691, 'train@por.rst.cstn_samples_per_second': 81.865, 'train@por.rst.cstn_steps_per_second': 2.566, 'epoch': 9.0}
{'loss': 1.6229, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.6920839548110962, 'eval_accuracy@por.rst.cstn': 0.49912739965095987, 'eval_f1@por.rst.cstn': 0.1815579966896161, 'eval_precision@por.rst.cstn': 0.18356639276068892, 'eval_recall@por.rst.cstn': 0.20061724934566624, 'eval_loss@por.rst.cstn': 1.6920838356018066, 'eval_runtime': 7.3456, 'eval_samples_per_second': 78.006, 'eval_steps_per_second': 2.45, 'epoch': 9.0}
{'train@por.rst.cstn_loss': 1.5322176218032837, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5937801350048216, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1446642113332522, 'train@por.rst.cstn_precision@por.rst.cstn': 0.15431470882866183, 'train@por.rst.cstn_recall@por.rst.cstn': 0.15499337890790577, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5322175025939941, 'train@por.rst.cstn_runtime': 50.7049, 'train@por.rst.cstn_samples_per_second': 81.807, 'train@por.rst.cstn_steps_per_second': 2.564, 'epoch': 10.0}
{'loss': 1.5936, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.672173261642456, 'eval_accuracy@por.rst.cstn': 0.5113438045375218, 'eval_f1@por.rst.cstn': 0.19420407687841956, 'eval_precision@por.rst.cstn': 0.20354212552861603, 'eval_recall@por.rst.cstn': 0.21150203407068394, 'eval_loss@por.rst.cstn': 1.672173261642456, 'eval_runtime': 7.3173, 'eval_samples_per_second': 78.308, 'eval_steps_per_second': 2.46, 'epoch': 10.0}
{'train@por.rst.cstn_loss': 1.524272084236145, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5952266152362584, 'train@por.rst.cstn_f1@por.rst.cstn': 0.14569043942325932, 'train@por.rst.cstn_precision@por.rst.cstn': 0.155339807055335, 'train@por.rst.cstn_recall@por.rst.cstn': 0.15573811535230334, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5242722034454346, 'train@por.rst.cstn_runtime': 50.6667, 'train@por.rst.cstn_samples_per_second': 81.868, 'train@por.rst.cstn_steps_per_second': 2.566, 'epoch': 11.0}
{'loss': 1.5887, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.6665912866592407, 'eval_accuracy@por.rst.cstn': 0.5165794066317626, 'eval_f1@por.rst.cstn': 0.19662915568445333, 'eval_precision@por.rst.cstn': 0.20266871425303298, 'eval_recall@por.rst.cstn': 0.21332931307187603, 'eval_loss@por.rst.cstn': 1.6665911674499512, 'eval_runtime': 7.3174, 'eval_samples_per_second': 78.307, 'eval_steps_per_second': 2.46, 'epoch': 11.0}
{'train@por.rst.cstn_loss': 1.521709680557251, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5964320154291225, 'train@por.rst.cstn_f1@por.rst.cstn': 0.14663758363162477, 'train@por.rst.cstn_precision@por.rst.cstn': 0.1551626385014499, 'train@por.rst.cstn_recall@por.rst.cstn': 0.15638351623027275, 'train@por.rst.cstn_loss@por.rst.cstn': 1.521709680557251, 'train@por.rst.cstn_runtime': 50.7233, 'train@por.rst.cstn_samples_per_second': 81.777, 'train@por.rst.cstn_steps_per_second': 2.563, 'epoch': 12.0}
{'loss': 1.5791, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.6637203693389893, 'eval_accuracy@por.rst.cstn': 0.5165794066317626, 'eval_f1@por.rst.cstn': 0.19595534597751918, 'eval_precision@por.rst.cstn': 0.20224190196267652, 'eval_recall@por.rst.cstn': 0.21234901317853264, 'eval_loss@por.rst.cstn': 1.6637202501296997, 'eval_runtime': 7.3202, 'eval_samples_per_second': 78.277, 'eval_steps_per_second': 2.459, 'epoch': 12.0}
{'train_runtime': 1974.5491, 'train_samples_per_second': 25.209, 'train_steps_per_second': 0.79, 'train_loss': 1.8854835119002904, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.6471
  train_runtime            = 0:03:40.74
  train_samples_per_second =     23.864
  train_steps_per_second   =      0.761
-------------------------------------------------------------------
Lang1:  tur.pdtb.tdb    Lang2:  por.rst.cstn
Saving run to:  runs/full_shot/FullShot=v4_tur.pdtb.tdb_por.rst.cstn_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2451 examples
read 312 examples
read 422 examples
read 4148 examples
read 573 examples
read 272 examples
Total prediction labels:  55
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (tr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (pt): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (tr): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (pt): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=55, bias=True)
    )
  )
)
{'train@tur.pdtb.tdb_loss': 3.056942939758301, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01741967515426626, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010891738953044898, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 3.056942939758301, 'train@tur.pdtb.tdb_runtime': 30.1436, 'train@tur.pdtb.tdb_samples_per_second': 81.311, 'train@tur.pdtb.tdb_steps_per_second': 2.554, 'epoch': 1.0}
{'loss': 3.5749, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.999582052230835, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.999582052230835, 'eval_runtime': 4.2819, 'eval_samples_per_second': 72.864, 'eval_steps_per_second': 2.335, 'epoch': 1.0}
{'train@tur.pdtb.tdb_loss': 2.4795026779174805, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01741967515426626, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010891738953044898, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.4795029163360596, 'train@tur.pdtb.tdb_runtime': 30.1494, 'train@tur.pdtb.tdb_samples_per_second': 81.295, 'train@tur.pdtb.tdb_steps_per_second': 2.554, 'epoch': 2.0}
{'loss': 2.6907, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.373141050338745, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.373141050338745, 'eval_runtime': 4.324, 'eval_samples_per_second': 72.155, 'eval_steps_per_second': 2.313, 'epoch': 2.0}
{'train@tur.pdtb.tdb_loss': 2.4012482166290283, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.251733986128111, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.018488635520422715, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.05438334754191532, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.04401064773735581, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.4012482166290283, 'train@tur.pdtb.tdb_runtime': 30.2174, 'train@tur.pdtb.tdb_samples_per_second': 81.112, 'train@tur.pdtb.tdb_steps_per_second': 2.548, 'epoch': 3.0}
{'loss': 2.4652, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.3245527744293213, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.324552536010742, 'eval_runtime': 4.299, 'eval_samples_per_second': 72.575, 'eval_steps_per_second': 2.326, 'epoch': 3.0}
{'train@tur.pdtb.tdb_loss': 2.34112811088562, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.27213382292941657, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.03462308118501641, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.053997446186161646, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.05410340791181222, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.341128349304199, 'train@tur.pdtb.tdb_runtime': 30.2012, 'train@tur.pdtb.tdb_samples_per_second': 81.156, 'train@tur.pdtb.tdb_steps_per_second': 2.55, 'epoch': 4.0}
{'loss': 2.3988, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.2897725105285645, 'eval_accuracy@tur.pdtb.tdb': 0.2692307692307692, 'eval_f1@tur.pdtb.tdb': 0.023545110192837462, 'eval_precision@tur.pdtb.tdb': 0.021473874962247057, 'eval_recall@tur.pdtb.tdb': 0.04758069712003092, 'eval_loss@tur.pdtb.tdb': 2.2897725105285645, 'eval_runtime': 4.3162, 'eval_samples_per_second': 72.285, 'eval_steps_per_second': 2.317, 'epoch': 4.0}
{'train@tur.pdtb.tdb_loss': 2.295478105545044, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.28886168910648713, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.04392664553510762, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.0679244252332708, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.0638332808883421, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.295478105545044, 'train@tur.pdtb.tdb_runtime': 30.2098, 'train@tur.pdtb.tdb_samples_per_second': 81.133, 'train@tur.pdtb.tdb_steps_per_second': 2.549, 'epoch': 5.0}
{'loss': 2.3535, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.256706476211548, 'eval_accuracy@tur.pdtb.tdb': 0.27564102564102566, 'eval_f1@tur.pdtb.tdb': 0.03244008670759558, 'eval_precision@tur.pdtb.tdb': 0.031087169025457943, 'eval_recall@tur.pdtb.tdb': 0.05289160205885932, 'eval_loss@tur.pdtb.tdb': 2.2567062377929688, 'eval_runtime': 4.2977, 'eval_samples_per_second': 72.596, 'eval_steps_per_second': 2.327, 'epoch': 5.0}
{'train@tur.pdtb.tdb_loss': 2.2543835639953613, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3068135454916361, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.05183364656283248, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.06489273266027315, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.07178500952219426, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.2543835639953613, 'train@tur.pdtb.tdb_runtime': 31.5897, 'train@tur.pdtb.tdb_samples_per_second': 77.589, 'train@tur.pdtb.tdb_steps_per_second': 2.438, 'epoch': 6.0}
{'loss': 2.3169, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.2299678325653076, 'eval_accuracy@tur.pdtb.tdb': 0.28205128205128205, 'eval_f1@tur.pdtb.tdb': 0.03583446172044014, 'eval_precision@tur.pdtb.tdb': 0.033368589014273635, 'eval_recall@tur.pdtb.tdb': 0.05549112671720751, 'eval_loss@tur.pdtb.tdb': 2.2299678325653076, 'eval_runtime': 4.3013, 'eval_samples_per_second': 72.537, 'eval_steps_per_second': 2.325, 'epoch': 6.0}
{'train@tur.pdtb.tdb_loss': 2.222283363342285, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3182374541003672, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.06870445343764593, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.09876796459366437, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.08349842139244004, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.222283363342285, 'train@tur.pdtb.tdb_runtime': 30.1913, 'train@tur.pdtb.tdb_samples_per_second': 81.182, 'train@tur.pdtb.tdb_steps_per_second': 2.55, 'epoch': 7.0}
{'loss': 2.2859, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.211427688598633, 'eval_accuracy@tur.pdtb.tdb': 0.28525641025641024, 'eval_f1@tur.pdtb.tdb': 0.062167721087262165, 'eval_precision@tur.pdtb.tdb': 0.08017986154632117, 'eval_recall@tur.pdtb.tdb': 0.07340055408800979, 'eval_loss@tur.pdtb.tdb': 2.211427927017212, 'eval_runtime': 4.2817, 'eval_samples_per_second': 72.868, 'eval_steps_per_second': 2.336, 'epoch': 7.0}
{'train@tur.pdtb.tdb_loss': 2.20589542388916, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3227254181966544, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.07661315365925081, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.09603632678203237, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.08864278048933175, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.20589542388916, 'train@tur.pdtb.tdb_runtime': 30.2507, 'train@tur.pdtb.tdb_samples_per_second': 81.023, 'train@tur.pdtb.tdb_steps_per_second': 2.545, 'epoch': 8.0}
{'loss': 2.2595, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.2001452445983887, 'eval_accuracy@tur.pdtb.tdb': 0.2948717948717949, 'eval_f1@tur.pdtb.tdb': 0.07644027949192413, 'eval_precision@tur.pdtb.tdb': 0.09054667040778151, 'eval_recall@tur.pdtb.tdb': 0.0886250185583067, 'eval_loss@tur.pdtb.tdb': 2.2001452445983887, 'eval_runtime': 4.3201, 'eval_samples_per_second': 72.221, 'eval_steps_per_second': 2.315, 'epoch': 8.0}
{'train@tur.pdtb.tdb_loss': 2.1804842948913574, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3333333333333333, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.0887908199768619, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.11887756137923362, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.10396703679667797, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1804842948913574, 'train@tur.pdtb.tdb_runtime': 30.2354, 'train@tur.pdtb.tdb_samples_per_second': 81.064, 'train@tur.pdtb.tdb_steps_per_second': 2.547, 'epoch': 9.0}
{'loss': 2.2313, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.181157350540161, 'eval_accuracy@tur.pdtb.tdb': 0.27884615384615385, 'eval_f1@tur.pdtb.tdb': 0.06616952884641564, 'eval_precision@tur.pdtb.tdb': 0.05607793089079032, 'eval_recall@tur.pdtb.tdb': 0.09131892990858263, 'eval_loss@tur.pdtb.tdb': 2.181157350540161, 'eval_runtime': 4.3049, 'eval_samples_per_second': 72.476, 'eval_steps_per_second': 2.323, 'epoch': 9.0}
{'train@tur.pdtb.tdb_loss': 2.1676337718963623, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.33700530395756834, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09135704270676084, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.109605142957938, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.10740352066472371, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1676337718963623, 'train@tur.pdtb.tdb_runtime': 30.2178, 'train@tur.pdtb.tdb_samples_per_second': 81.111, 'train@tur.pdtb.tdb_steps_per_second': 2.548, 'epoch': 10.0}
{'loss': 2.2223, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.171815872192383, 'eval_accuracy@tur.pdtb.tdb': 0.30128205128205127, 'eval_f1@tur.pdtb.tdb': 0.0791735474059208, 'eval_precision@tur.pdtb.tdb': 0.07477012694968065, 'eval_recall@tur.pdtb.tdb': 0.10548097995329377, 'eval_loss@tur.pdtb.tdb': 2.1718156337738037, 'eval_runtime': 4.2978, 'eval_samples_per_second': 72.595, 'eval_steps_per_second': 2.327, 'epoch': 10.0}
{'train@tur.pdtb.tdb_loss': 2.1626317501068115, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3398612811097511, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09285890228647227, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.09641927383056335, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.10945801905029202, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1626317501068115, 'train@tur.pdtb.tdb_runtime': 30.2315, 'train@tur.pdtb.tdb_samples_per_second': 81.075, 'train@tur.pdtb.tdb_steps_per_second': 2.547, 'epoch': 11.0}
{'loss': 2.2143, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.17006778717041, 'eval_accuracy@tur.pdtb.tdb': 0.30128205128205127, 'eval_f1@tur.pdtb.tdb': 0.07951738884473597, 'eval_precision@tur.pdtb.tdb': 0.07514964640783979, 'eval_recall@tur.pdtb.tdb': 0.1054067078201981, 'eval_loss@tur.pdtb.tdb': 2.1700680255889893, 'eval_runtime': 4.3042, 'eval_samples_per_second': 72.487, 'eval_steps_per_second': 2.323, 'epoch': 11.0}
{'train@tur.pdtb.tdb_loss': 2.1594302654266357, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.34149326805385555, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09399850062787722, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.11772095056610275, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11083361110786184, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1594302654266357, 'train@tur.pdtb.tdb_runtime': 30.3395, 'train@tur.pdtb.tdb_samples_per_second': 80.786, 'train@tur.pdtb.tdb_steps_per_second': 2.538, 'epoch': 12.0}
{'loss': 2.2027, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.167496681213379, 'eval_accuracy@tur.pdtb.tdb': 0.30128205128205127, 'eval_f1@tur.pdtb.tdb': 0.07951738884473597, 'eval_precision@tur.pdtb.tdb': 0.07514964640783979, 'eval_recall@tur.pdtb.tdb': 0.1054067078201981, 'eval_loss@tur.pdtb.tdb': 2.167496919631958, 'eval_runtime': 4.2941, 'eval_samples_per_second': 72.657, 'eval_steps_per_second': 2.329, 'epoch': 12.0}
{'train_runtime': 1170.03, 'train_samples_per_second': 25.138, 'train_steps_per_second': 0.79, 'train_loss': 2.434671112985322, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.4347
  train_runtime            = 0:19:30.02
  train_samples_per_second =     25.138
  train_steps_per_second   =       0.79
{'train@por.rst.cstn_loss': 2.450627088546753, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.2796528447444552, 'train@por.rst.cstn_f1@por.rst.cstn': 0.014711809136716785, 'train@por.rst.cstn_precision@por.rst.cstn': 0.06077563637828258, 'train@por.rst.cstn_recall@por.rst.cstn': 0.03181276995109646, 'train@por.rst.cstn_loss@por.rst.cstn': 2.450627326965332, 'train@por.rst.cstn_runtime': 50.8244, 'train@por.rst.cstn_samples_per_second': 81.614, 'train@por.rst.cstn_steps_per_second': 2.558, 'epoch': 1.0}
{'loss': 3.1302, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.546447992324829, 'eval_accuracy@por.rst.cstn': 0.2844677137870855, 'eval_f1@por.rst.cstn': 0.021869446535662525, 'eval_precision@por.rst.cstn': 0.058270975959242154, 'eval_recall@por.rst.cstn': 0.04641148325358852, 'eval_loss@por.rst.cstn': 2.54644775390625, 'eval_runtime': 7.4628, 'eval_samples_per_second': 76.78, 'eval_steps_per_second': 2.412, 'epoch': 1.0}
{'train@por.rst.cstn_loss': 2.1687169075012207, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.4206846673095468, 'train@por.rst.cstn_f1@por.rst.cstn': 0.06771698974375401, 'train@por.rst.cstn_precision@por.rst.cstn': 0.0799137311922665, 'train@por.rst.cstn_recall@por.rst.cstn': 0.07345436324881313, 'train@por.rst.cstn_loss@por.rst.cstn': 2.1687171459198, 'train@por.rst.cstn_runtime': 50.8273, 'train@por.rst.cstn_samples_per_second': 81.61, 'train@por.rst.cstn_steps_per_second': 2.558, 'epoch': 2.0}
{'loss': 2.3526, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.282062530517578, 'eval_accuracy@por.rst.cstn': 0.36649214659685864, 'eval_f1@por.rst.cstn': 0.09162160991088668, 'eval_precision@por.rst.cstn': 0.10673064719627913, 'eval_recall@por.rst.cstn': 0.10324952247669118, 'eval_loss@por.rst.cstn': 2.282062292098999, 'eval_runtime': 7.4702, 'eval_samples_per_second': 76.704, 'eval_steps_per_second': 2.41, 'epoch': 2.0}
{'train@por.rst.cstn_loss': 1.9265903234481812, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.4889103182256509, 'train@por.rst.cstn_f1@por.rst.cstn': 0.0786394301712904, 'train@por.rst.cstn_precision@por.rst.cstn': 0.07044182391116634, 'train@por.rst.cstn_recall@por.rst.cstn': 0.0942313573796241, 'train@por.rst.cstn_loss@por.rst.cstn': 1.9265904426574707, 'train@por.rst.cstn_runtime': 50.7711, 'train@por.rst.cstn_samples_per_second': 81.7, 'train@por.rst.cstn_steps_per_second': 2.561, 'epoch': 3.0}
{'loss': 2.1101, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.056671380996704, 'eval_accuracy@por.rst.cstn': 0.41535776614310643, 'eval_f1@por.rst.cstn': 0.10297198093808263, 'eval_precision@por.rst.cstn': 0.09350111804657259, 'eval_recall@por.rst.cstn': 0.13515816580617912, 'eval_loss@por.rst.cstn': 2.056671380996704, 'eval_runtime': 7.4546, 'eval_samples_per_second': 76.865, 'eval_steps_per_second': 2.415, 'epoch': 3.0}
{'train@por.rst.cstn_loss': 1.7747161388397217, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.53351012536162, 'train@por.rst.cstn_f1@por.rst.cstn': 0.10674546598873388, 'train@por.rst.cstn_precision@por.rst.cstn': 0.12350310157481474, 'train@por.rst.cstn_recall@por.rst.cstn': 0.12010914116546545, 'train@por.rst.cstn_loss@por.rst.cstn': 1.7747160196304321, 'train@por.rst.cstn_runtime': 50.7063, 'train@por.rst.cstn_samples_per_second': 81.804, 'train@por.rst.cstn_steps_per_second': 2.564, 'epoch': 4.0}
{'loss': 1.9174, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.9309322834014893, 'eval_accuracy@por.rst.cstn': 0.4293193717277487, 'eval_f1@por.rst.cstn': 0.13359276658478036, 'eval_precision@por.rst.cstn': 0.1721037488357675, 'eval_recall@por.rst.cstn': 0.1613264731477754, 'eval_loss@por.rst.cstn': 1.9309325218200684, 'eval_runtime': 7.4514, 'eval_samples_per_second': 76.898, 'eval_steps_per_second': 2.416, 'epoch': 4.0}
{'train@por.rst.cstn_loss': 1.687453031539917, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5407425265188043, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1140739242155549, 'train@por.rst.cstn_precision@por.rst.cstn': 0.12631741549338907, 'train@por.rst.cstn_recall@por.rst.cstn': 0.12953130563760543, 'train@por.rst.cstn_loss@por.rst.cstn': 1.6874529123306274, 'train@por.rst.cstn_runtime': 50.8, 'train@por.rst.cstn_samples_per_second': 81.654, 'train@por.rst.cstn_steps_per_second': 2.559, 'epoch': 5.0}
{'loss': 1.8018, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.8545676469802856, 'eval_accuracy@por.rst.cstn': 0.4572425828970332, 'eval_f1@por.rst.cstn': 0.1558206242951784, 'eval_precision@por.rst.cstn': 0.21708966895788134, 'eval_recall@por.rst.cstn': 0.18035602465860423, 'eval_loss@por.rst.cstn': 1.8545677661895752, 'eval_runtime': 7.4384, 'eval_samples_per_second': 77.033, 'eval_steps_per_second': 2.42, 'epoch': 5.0}
{'train@por.rst.cstn_loss': 1.6224415302276611, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5600289296046287, 'train@por.rst.cstn_f1@por.rst.cstn': 0.12374483044995561, 'train@por.rst.cstn_precision@por.rst.cstn': 0.14018223896087809, 'train@por.rst.cstn_recall@por.rst.cstn': 0.13700857848190995, 'train@por.rst.cstn_loss@por.rst.cstn': 1.6224415302276611, 'train@por.rst.cstn_runtime': 50.757, 'train@por.rst.cstn_samples_per_second': 81.723, 'train@por.rst.cstn_steps_per_second': 2.561, 'epoch': 6.0}
{'loss': 1.724, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.799189805984497, 'eval_accuracy@por.rst.cstn': 0.4694589877835951, 'eval_f1@por.rst.cstn': 0.16704880847395168, 'eval_precision@por.rst.cstn': 0.19220018791413357, 'eval_recall@por.rst.cstn': 0.18904661116799665, 'eval_loss@por.rst.cstn': 1.799189805984497, 'eval_runtime': 7.4379, 'eval_samples_per_second': 77.038, 'eval_steps_per_second': 2.42, 'epoch': 6.0}
{'train@por.rst.cstn_loss': 1.5788897275924683, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5761812921890067, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1396255968016216, 'train@por.rst.cstn_precision@por.rst.cstn': 0.1546162156750973, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14936812155472304, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5788898468017578, 'train@por.rst.cstn_runtime': 51.6402, 'train@por.rst.cstn_samples_per_second': 80.325, 'train@por.rst.cstn_steps_per_second': 2.517, 'epoch': 7.0}
{'loss': 1.6627, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.7593141794204712, 'eval_accuracy@por.rst.cstn': 0.4851657940663176, 'eval_f1@por.rst.cstn': 0.17982540372152372, 'eval_precision@por.rst.cstn': 0.18870228969394398, 'eval_recall@por.rst.cstn': 0.20077837500086304, 'eval_loss@por.rst.cstn': 1.7593141794204712, 'eval_runtime': 7.4462, 'eval_samples_per_second': 76.952, 'eval_steps_per_second': 2.417, 'epoch': 7.0}
{'train@por.rst.cstn_loss': 1.5535110235214233, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5788331726133076, 'train@por.rst.cstn_f1@por.rst.cstn': 0.14259022627788998, 'train@por.rst.cstn_precision@por.rst.cstn': 0.15500585374568987, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1514872703411808, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5535107851028442, 'train@por.rst.cstn_runtime': 50.8527, 'train@por.rst.cstn_samples_per_second': 81.569, 'train@por.rst.cstn_steps_per_second': 2.556, 'epoch': 8.0}
{'loss': 1.6371, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.7419793605804443, 'eval_accuracy@por.rst.cstn': 0.49214659685863876, 'eval_f1@por.rst.cstn': 0.18976902554754804, 'eval_precision@por.rst.cstn': 0.19300304506674343, 'eval_recall@por.rst.cstn': 0.21029095751115723, 'eval_loss@por.rst.cstn': 1.7419795989990234, 'eval_runtime': 7.4615, 'eval_samples_per_second': 76.795, 'eval_steps_per_second': 2.412, 'epoch': 8.0}
{'train@por.rst.cstn_loss': 1.533355474472046, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.587029893924783, 'train@por.rst.cstn_f1@por.rst.cstn': 0.14695610637778067, 'train@por.rst.cstn_precision@por.rst.cstn': 0.1566425537837768, 'train@por.rst.cstn_recall@por.rst.cstn': 0.15552591063959786, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5333555936813354, 'train@por.rst.cstn_runtime': 50.8571, 'train@por.rst.cstn_samples_per_second': 81.562, 'train@por.rst.cstn_steps_per_second': 2.556, 'epoch': 9.0}
{'loss': 1.611, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.7232273817062378, 'eval_accuracy@por.rst.cstn': 0.5008726003490401, 'eval_f1@por.rst.cstn': 0.1928622578492189, 'eval_precision@por.rst.cstn': 0.19524594095508585, 'eval_recall@por.rst.cstn': 0.21272136203486317, 'eval_loss@por.rst.cstn': 1.7232275009155273, 'eval_runtime': 7.4523, 'eval_samples_per_second': 76.889, 'eval_steps_per_second': 2.415, 'epoch': 9.0}
{'train@por.rst.cstn_loss': 1.5212465524673462, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5961909353905497, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1539261254094743, 'train@por.rst.cstn_precision@por.rst.cstn': 0.19031923106046317, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1625824810052649, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5212465524673462, 'train@por.rst.cstn_runtime': 50.7844, 'train@por.rst.cstn_samples_per_second': 81.679, 'train@por.rst.cstn_steps_per_second': 2.56, 'epoch': 10.0}
{'loss': 1.58, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.7127150297164917, 'eval_accuracy@por.rst.cstn': 0.49912739965095987, 'eval_f1@por.rst.cstn': 0.19875195954669866, 'eval_precision@por.rst.cstn': 0.1938953152460585, 'eval_recall@por.rst.cstn': 0.2225875954708907, 'eval_loss@por.rst.cstn': 1.7127150297164917, 'eval_runtime': 7.4829, 'eval_samples_per_second': 76.574, 'eval_steps_per_second': 2.405, 'epoch': 10.0}
{'train@por.rst.cstn_loss': 1.5133333206176758, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5954676952748312, 'train@por.rst.cstn_f1@por.rst.cstn': 0.15478712035258416, 'train@por.rst.cstn_precision@por.rst.cstn': 0.19156349838980039, 'train@por.rst.cstn_recall@por.rst.cstn': 0.16292199806473306, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5133333206176758, 'train@por.rst.cstn_runtime': 50.8542, 'train@por.rst.cstn_samples_per_second': 81.566, 'train@por.rst.cstn_steps_per_second': 2.556, 'epoch': 11.0}
{'loss': 1.5826, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.7078773975372314, 'eval_accuracy@por.rst.cstn': 0.5078534031413613, 'eval_f1@por.rst.cstn': 0.20160055021485065, 'eval_precision@por.rst.cstn': 0.19747003340998368, 'eval_recall@por.rst.cstn': 0.22501758723674678, 'eval_loss@por.rst.cstn': 1.707877516746521, 'eval_runtime': 7.4994, 'eval_samples_per_second': 76.407, 'eval_steps_per_second': 2.4, 'epoch': 11.0}
{'train@por.rst.cstn_loss': 1.5107372999191284, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.595708775313404, 'train@por.rst.cstn_f1@por.rst.cstn': 0.15527185708683766, 'train@por.rst.cstn_precision@por.rst.cstn': 0.1916191305305013, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1633419388138624, 'train@por.rst.cstn_loss@por.rst.cstn': 1.510737419128418, 'train@por.rst.cstn_runtime': 50.821, 'train@por.rst.cstn_samples_per_second': 81.62, 'train@por.rst.cstn_steps_per_second': 2.558, 'epoch': 12.0}
{'loss': 1.5677, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.7045683860778809, 'eval_accuracy@por.rst.cstn': 0.5078534031413613, 'eval_f1@por.rst.cstn': 0.20226068141749282, 'eval_precision@por.rst.cstn': 0.19770319286817575, 'eval_recall@por.rst.cstn': 0.22501758723674678, 'eval_loss@por.rst.cstn': 1.7045683860778809, 'eval_runtime': 7.4637, 'eval_samples_per_second': 76.772, 'eval_steps_per_second': 2.412, 'epoch': 12.0}
{'train_runtime': 1982.8678, 'train_samples_per_second': 25.103, 'train_steps_per_second': 0.787, 'train_loss': 1.8897775601118039, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.4347
  train_runtime            = 0:19:30.02
  train_samples_per_second =     25.138
  train_steps_per_second   =       0.79
-------------------------------------------------------------------
Lang1:  zho.rst.sctb    Lang2:  por.rst.cstn
Saving run to:  runs/full_shot/FullShot=v4_zho.rst.sctb_por.rst.cstn_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 439 examples
read 94 examples
read 159 examples
read 4148 examples
read 573 examples
read 272 examples
Total prediction labels:  38
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (zh): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (pt): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (pt): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=38, bias=True)
    )
  )
)
{'train@zho.rst.sctb_loss': 3.4055960178375244, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.4055960178375244, 'train@zho.rst.sctb_runtime': 5.5097, 'train@zho.rst.sctb_samples_per_second': 79.678, 'train@zho.rst.sctb_steps_per_second': 2.541, 'epoch': 1.0}
{'loss': 3.5381, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.4210546016693115, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 3.421055555343628, 'eval_runtime': 1.4686, 'eval_samples_per_second': 64.007, 'eval_steps_per_second': 2.043, 'epoch': 1.0}
{'train@zho.rst.sctb_loss': 3.2058050632476807, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.2058050632476807, 'train@zho.rst.sctb_runtime': 5.5307, 'train@zho.rst.sctb_samples_per_second': 79.375, 'train@zho.rst.sctb_steps_per_second': 2.531, 'epoch': 2.0}
{'loss': 3.3237, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.2322564125061035, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 3.2322568893432617, 'eval_runtime': 1.4692, 'eval_samples_per_second': 63.982, 'eval_steps_per_second': 2.042, 'epoch': 2.0}
{'train@zho.rst.sctb_loss': 3.0190017223358154, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.0190014839172363, 'train@zho.rst.sctb_runtime': 5.5787, 'train@zho.rst.sctb_samples_per_second': 78.692, 'train@zho.rst.sctb_steps_per_second': 2.51, 'epoch': 3.0}
{'loss': 3.143, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 3.05570650100708, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 3.0557069778442383, 'eval_runtime': 1.4519, 'eval_samples_per_second': 64.743, 'eval_steps_per_second': 2.066, 'epoch': 3.0}
{'train@zho.rst.sctb_loss': 2.8555099964141846, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.8555099964141846, 'train@zho.rst.sctb_runtime': 5.5809, 'train@zho.rst.sctb_samples_per_second': 78.661, 'train@zho.rst.sctb_steps_per_second': 2.509, 'epoch': 4.0}
{'loss': 2.9634, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.9025769233703613, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.9025769233703613, 'eval_runtime': 1.4726, 'eval_samples_per_second': 63.834, 'eval_steps_per_second': 2.037, 'epoch': 4.0}
{'train@zho.rst.sctb_loss': 2.7189228534698486, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.7189223766326904, 'train@zho.rst.sctb_runtime': 5.5795, 'train@zho.rst.sctb_samples_per_second': 78.681, 'train@zho.rst.sctb_steps_per_second': 2.509, 'epoch': 5.0}
{'loss': 2.8281, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.7769336700439453, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.7769343852996826, 'eval_runtime': 1.473, 'eval_samples_per_second': 63.816, 'eval_steps_per_second': 2.037, 'epoch': 5.0}
{'train@zho.rst.sctb_loss': 2.607862710952759, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.607862949371338, 'train@zho.rst.sctb_runtime': 5.6006, 'train@zho.rst.sctb_samples_per_second': 78.384, 'train@zho.rst.sctb_steps_per_second': 2.5, 'epoch': 6.0}
{'loss': 2.7071, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.6780202388763428, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.678020477294922, 'eval_runtime': 1.4781, 'eval_samples_per_second': 63.596, 'eval_steps_per_second': 2.03, 'epoch': 6.0}
{'train@zho.rst.sctb_loss': 2.524240732192993, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.524240493774414, 'train@zho.rst.sctb_runtime': 5.5951, 'train@zho.rst.sctb_samples_per_second': 78.461, 'train@zho.rst.sctb_steps_per_second': 2.502, 'epoch': 7.0}
{'loss': 2.6091, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.605212688446045, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.6052122116088867, 'eval_runtime': 1.4697, 'eval_samples_per_second': 63.961, 'eval_steps_per_second': 2.041, 'epoch': 7.0}
{'train@zho.rst.sctb_loss': 2.467524290084839, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.467524528503418, 'train@zho.rst.sctb_runtime': 5.5869, 'train@zho.rst.sctb_samples_per_second': 78.577, 'train@zho.rst.sctb_steps_per_second': 2.506, 'epoch': 8.0}
{'loss': 2.5345, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.5572454929351807, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.5572457313537598, 'eval_runtime': 1.4725, 'eval_samples_per_second': 63.838, 'eval_steps_per_second': 2.037, 'epoch': 8.0}
{'train@zho.rst.sctb_loss': 2.427791118621826, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.427790880203247, 'train@zho.rst.sctb_runtime': 5.577, 'train@zho.rst.sctb_samples_per_second': 78.717, 'train@zho.rst.sctb_steps_per_second': 2.51, 'epoch': 9.0}
{'loss': 2.4809, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.523223876953125, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.523224115371704, 'eval_runtime': 1.4789, 'eval_samples_per_second': 63.562, 'eval_steps_per_second': 2.029, 'epoch': 9.0}
{'train@zho.rst.sctb_loss': 2.401731014251709, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3416856492027335, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.02244329654169084, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03860958133475455, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04008097165991902, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.401731014251709, 'train@zho.rst.sctb_runtime': 5.5812, 'train@zho.rst.sctb_samples_per_second': 78.657, 'train@zho.rst.sctb_steps_per_second': 2.508, 'epoch': 10.0}
{'loss': 2.4532, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.500617742538452, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.500617742538452, 'eval_runtime': 1.464, 'eval_samples_per_second': 64.207, 'eval_steps_per_second': 2.049, 'epoch': 10.0}
{'train@zho.rst.sctb_loss': 2.3880927562713623, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.34851936218678814, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.024675378040762655, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.04297356390379646, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04129554655870445, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.3880927562713623, 'train@zho.rst.sctb_runtime': 5.5968, 'train@zho.rst.sctb_samples_per_second': 78.437, 'train@zho.rst.sctb_steps_per_second': 2.501, 'epoch': 11.0}
{'loss': 2.4219, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.489171028137207, 'eval_accuracy@zho.rst.sctb': 0.35106382978723405, 'eval_f1@zho.rst.sctb': 0.032795321637426905, 'eval_precision@zho.rst.sctb': 0.07074136955291455, 'eval_recall@zho.rst.sctb': 0.05572755417956656, 'eval_loss@zho.rst.sctb': 2.489171266555786, 'eval_runtime': 1.4951, 'eval_samples_per_second': 62.871, 'eval_steps_per_second': 2.007, 'epoch': 11.0}
{'train@zho.rst.sctb_loss': 2.3835864067077637, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.34851936218678814, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.024675378040762655, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.04297356390379646, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04129554655870445, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.3835866451263428, 'train@zho.rst.sctb_runtime': 5.5753, 'train@zho.rst.sctb_samples_per_second': 78.74, 'train@zho.rst.sctb_steps_per_second': 2.511, 'epoch': 12.0}
{'loss': 2.4093, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.4853644371032715, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.03185595567867036, 'eval_precision@zho.rst.sctb': 0.044050343249427915, 'eval_recall@zho.rst.sctb': 0.0540828173374613, 'eval_loss@zho.rst.sctb': 2.4853641986846924, 'eval_runtime': 1.4492, 'eval_samples_per_second': 64.863, 'eval_steps_per_second': 2.07, 'epoch': 12.0}
{'train_runtime': 216.6547, 'train_samples_per_second': 24.315, 'train_steps_per_second': 0.775, 'train_loss': 2.7843460582551502, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.7843
  train_runtime            = 0:03:36.65
  train_samples_per_second =     24.315
  train_steps_per_second   =      0.775
{'train@por.rst.cstn_loss': 2.4184913635253906, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.2796528447444552, 'train@por.rst.cstn_f1@por.rst.cstn': 0.01507514921929655, 'train@por.rst.cstn_precision@por.rst.cstn': 0.039940202947571876, 'train@por.rst.cstn_recall@por.rst.cstn': 0.032006048387096774, 'train@por.rst.cstn_loss@por.rst.cstn': 2.4184916019439697, 'train@por.rst.cstn_runtime': 50.605, 'train@por.rst.cstn_samples_per_second': 81.968, 'train@por.rst.cstn_steps_per_second': 2.569, 'epoch': 1.0}
{'loss': 2.9588, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.524078845977783, 'eval_accuracy@por.rst.cstn': 0.28097731239092494, 'eval_f1@por.rst.cstn': 0.019940549913301957, 'eval_precision@por.rst.cstn': 0.012771696017769315, 'eval_recall@por.rst.cstn': 0.045454545454545456, 'eval_loss@por.rst.cstn': 2.524078369140625, 'eval_runtime': 7.3167, 'eval_samples_per_second': 78.314, 'eval_steps_per_second': 2.46, 'epoch': 1.0}
{'train@por.rst.cstn_loss': 2.158797264099121, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.39657666345226616, 'train@por.rst.cstn_f1@por.rst.cstn': 0.06174444051162681, 'train@por.rst.cstn_precision@por.rst.cstn': 0.08692805508579159, 'train@por.rst.cstn_recall@por.rst.cstn': 0.06904116150985752, 'train@por.rst.cstn_loss@por.rst.cstn': 2.1587975025177, 'train@por.rst.cstn_runtime': 50.6804, 'train@por.rst.cstn_samples_per_second': 81.846, 'train@por.rst.cstn_steps_per_second': 2.565, 'epoch': 2.0}
{'loss': 2.3339, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.2720563411712646, 'eval_accuracy@por.rst.cstn': 0.34205933682373474, 'eval_f1@por.rst.cstn': 0.08100270559719351, 'eval_precision@por.rst.cstn': 0.11820966668875413, 'eval_recall@por.rst.cstn': 0.09381903375038386, 'eval_loss@por.rst.cstn': 2.2720563411712646, 'eval_runtime': 7.3022, 'eval_samples_per_second': 78.47, 'eval_steps_per_second': 2.465, 'epoch': 2.0}
{'train@por.rst.cstn_loss': 1.9316707849502563, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.47878495660559306, 'train@por.rst.cstn_f1@por.rst.cstn': 0.08007972200611674, 'train@por.rst.cstn_precision@por.rst.cstn': 0.12466078225976528, 'train@por.rst.cstn_recall@por.rst.cstn': 0.09395255623998036, 'train@por.rst.cstn_loss@por.rst.cstn': 1.9316710233688354, 'train@por.rst.cstn_runtime': 50.6599, 'train@por.rst.cstn_samples_per_second': 81.879, 'train@por.rst.cstn_steps_per_second': 2.566, 'epoch': 3.0}
{'loss': 2.105, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.0594279766082764, 'eval_accuracy@por.rst.cstn': 0.4013961605584642, 'eval_f1@por.rst.cstn': 0.10154724870559638, 'eval_precision@por.rst.cstn': 0.11669688958802152, 'eval_recall@por.rst.cstn': 0.13400686408799578, 'eval_loss@por.rst.cstn': 2.059427499771118, 'eval_runtime': 7.3197, 'eval_samples_per_second': 78.282, 'eval_steps_per_second': 2.459, 'epoch': 3.0}
{'train@por.rst.cstn_loss': 1.7756574153900146, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5467695274831244, 'train@por.rst.cstn_f1@por.rst.cstn': 0.11624042080881011, 'train@por.rst.cstn_precision@por.rst.cstn': 0.1361297883189733, 'train@por.rst.cstn_recall@por.rst.cstn': 0.12607826656940832, 'train@por.rst.cstn_loss@por.rst.cstn': 1.7756574153900146, 'train@por.rst.cstn_runtime': 50.6167, 'train@por.rst.cstn_samples_per_second': 81.949, 'train@por.rst.cstn_steps_per_second': 2.568, 'epoch': 4.0}
{'loss': 1.9207, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.9189074039459229, 'eval_accuracy@por.rst.cstn': 0.4642233856893543, 'eval_f1@por.rst.cstn': 0.1562453714106286, 'eval_precision@por.rst.cstn': 0.20052226237326948, 'eval_recall@por.rst.cstn': 0.17722191678141333, 'eval_loss@por.rst.cstn': 1.9189071655273438, 'eval_runtime': 7.3444, 'eval_samples_per_second': 78.018, 'eval_steps_per_second': 2.451, 'epoch': 4.0}
{'train@por.rst.cstn_loss': 1.6759406328201294, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5566538090646095, 'train@por.rst.cstn_f1@por.rst.cstn': 0.12632335553911733, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13538330321998998, 'train@por.rst.cstn_recall@por.rst.cstn': 0.13717112538284698, 'train@por.rst.cstn_loss@por.rst.cstn': 1.675940752029419, 'train@por.rst.cstn_runtime': 50.6527, 'train@por.rst.cstn_samples_per_second': 81.891, 'train@por.rst.cstn_steps_per_second': 2.566, 'epoch': 5.0}
{'loss': 1.7918, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.8219115734100342, 'eval_accuracy@por.rst.cstn': 0.4781849912739965, 'eval_f1@por.rst.cstn': 0.17124172957899353, 'eval_precision@por.rst.cstn': 0.17691780956589576, 'eval_recall@por.rst.cstn': 0.19058530864230885, 'eval_loss@por.rst.cstn': 1.8219118118286133, 'eval_runtime': 7.3402, 'eval_samples_per_second': 78.064, 'eval_steps_per_second': 2.452, 'epoch': 5.0}
{'train@por.rst.cstn_loss': 1.6124889850616455, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5790742526518804, 'train@por.rst.cstn_f1@por.rst.cstn': 0.13550059417373186, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13689014129333613, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14580450323352695, 'train@por.rst.cstn_loss@por.rst.cstn': 1.612489104270935, 'train@por.rst.cstn_runtime': 50.6403, 'train@por.rst.cstn_samples_per_second': 81.911, 'train@por.rst.cstn_steps_per_second': 2.567, 'epoch': 6.0}
{'loss': 1.7119, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.7620177268981934, 'eval_accuracy@por.rst.cstn': 0.518324607329843, 'eval_f1@por.rst.cstn': 0.18965928204928745, 'eval_precision@por.rst.cstn': 0.1893900402053581, 'eval_recall@por.rst.cstn': 0.20755223413066143, 'eval_loss@por.rst.cstn': 1.762018084526062, 'eval_runtime': 7.323, 'eval_samples_per_second': 78.246, 'eval_steps_per_second': 2.458, 'epoch': 6.0}
{'train@por.rst.cstn_loss': 1.5679341554641724, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5877531340405014, 'train@por.rst.cstn_f1@por.rst.cstn': 0.13955358666345488, 'train@por.rst.cstn_precision@por.rst.cstn': 0.1494401640665391, 'train@por.rst.cstn_recall@por.rst.cstn': 0.15112202724823715, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5679340362548828, 'train@por.rst.cstn_runtime': 50.6615, 'train@por.rst.cstn_samples_per_second': 81.877, 'train@por.rst.cstn_steps_per_second': 2.566, 'epoch': 7.0}
{'loss': 1.6528, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.7191624641418457, 'eval_accuracy@por.rst.cstn': 0.5165794066317626, 'eval_f1@por.rst.cstn': 0.18965885805213228, 'eval_precision@por.rst.cstn': 0.18333205840309078, 'eval_recall@por.rst.cstn': 0.2085603388937069, 'eval_loss@por.rst.cstn': 1.7191625833511353, 'eval_runtime': 7.3073, 'eval_samples_per_second': 78.414, 'eval_steps_per_second': 2.463, 'epoch': 7.0}
{'train@por.rst.cstn_loss': 1.5428872108459473, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5911282545805208, 'train@por.rst.cstn_f1@por.rst.cstn': 0.14161451707301997, 'train@por.rst.cstn_precision@por.rst.cstn': 0.14630649411374977, 'train@por.rst.cstn_recall@por.rst.cstn': 0.15264957031801069, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5428872108459473, 'train@por.rst.cstn_runtime': 50.6553, 'train@por.rst.cstn_samples_per_second': 81.887, 'train@por.rst.cstn_steps_per_second': 2.566, 'epoch': 8.0}
{'loss': 1.6192, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.7020726203918457, 'eval_accuracy@por.rst.cstn': 0.5218150087260035, 'eval_f1@por.rst.cstn': 0.19229365985683017, 'eval_precision@por.rst.cstn': 0.1879428512029233, 'eval_recall@por.rst.cstn': 0.2093692579754573, 'eval_loss@por.rst.cstn': 1.7020725011825562, 'eval_runtime': 7.3772, 'eval_samples_per_second': 77.672, 'eval_steps_per_second': 2.44, 'epoch': 8.0}
{'train@por.rst.cstn_loss': 1.5236680507659912, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5959498553519769, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1467294527000922, 'train@por.rst.cstn_precision@por.rst.cstn': 0.15712540948385928, 'train@por.rst.cstn_recall@por.rst.cstn': 0.15607105104222746, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5236681699752808, 'train@por.rst.cstn_runtime': 50.6346, 'train@por.rst.cstn_samples_per_second': 81.92, 'train@por.rst.cstn_steps_per_second': 2.567, 'epoch': 9.0}
{'loss': 1.5976, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.6854056119918823, 'eval_accuracy@por.rst.cstn': 0.5235602094240838, 'eval_f1@por.rst.cstn': 0.2019674376398596, 'eval_precision@por.rst.cstn': 0.21439879689374838, 'eval_recall@por.rst.cstn': 0.2151557965761211, 'eval_loss@por.rst.cstn': 1.6854058504104614, 'eval_runtime': 7.3005, 'eval_samples_per_second': 78.488, 'eval_steps_per_second': 2.466, 'epoch': 9.0}
{'train@por.rst.cstn_loss': 1.5066893100738525, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5983606557377049, 'train@por.rst.cstn_f1@por.rst.cstn': 0.15055082656644073, 'train@por.rst.cstn_precision@por.rst.cstn': 0.15343831556337334, 'train@por.rst.cstn_recall@por.rst.cstn': 0.16130333557591778, 'train@por.rst.cstn_loss@por.rst.cstn': 1.506689429283142, 'train@por.rst.cstn_runtime': 50.5698, 'train@por.rst.cstn_samples_per_second': 82.025, 'train@por.rst.cstn_steps_per_second': 2.571, 'epoch': 10.0}
{'loss': 1.5695, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.6653763055801392, 'eval_accuracy@por.rst.cstn': 0.5148342059336823, 'eval_f1@por.rst.cstn': 0.19801089476429723, 'eval_precision@por.rst.cstn': 0.2001981437948615, 'eval_recall@por.rst.cstn': 0.21417450981628222, 'eval_loss@por.rst.cstn': 1.6653764247894287, 'eval_runtime': 7.3191, 'eval_samples_per_second': 78.288, 'eval_steps_per_second': 2.459, 'epoch': 10.0}
{'train@por.rst.cstn_loss': 1.4998154640197754, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.6002892960462873, 'train@por.rst.cstn_f1@por.rst.cstn': 0.15154678996670667, 'train@por.rst.cstn_precision@por.rst.cstn': 0.15439549617257708, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1618014855894928, 'train@por.rst.cstn_loss@por.rst.cstn': 1.4998153448104858, 'train@por.rst.cstn_runtime': 50.6188, 'train@por.rst.cstn_samples_per_second': 81.946, 'train@por.rst.cstn_steps_per_second': 2.568, 'epoch': 11.0}
{'loss': 1.5626, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.6608539819717407, 'eval_accuracy@por.rst.cstn': 0.5218150087260035, 'eval_f1@por.rst.cstn': 0.20044594605718846, 'eval_precision@por.rst.cstn': 0.20321322933800462, 'eval_recall@por.rst.cstn': 0.21667640024723445, 'eval_loss@por.rst.cstn': 1.6608541011810303, 'eval_runtime': 7.3139, 'eval_samples_per_second': 78.344, 'eval_steps_per_second': 2.461, 'epoch': 11.0}
{'train@por.rst.cstn_loss': 1.497385859489441, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.6012536162005786, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1522673715409058, 'train@por.rst.cstn_precision@por.rst.cstn': 0.1552596670772522, 'train@por.rst.cstn_recall@por.rst.cstn': 0.16231531182961756, 'train@por.rst.cstn_loss@por.rst.cstn': 1.497385859489441, 'train@por.rst.cstn_runtime': 50.6335, 'train@por.rst.cstn_samples_per_second': 81.922, 'train@por.rst.cstn_steps_per_second': 2.567, 'epoch': 12.0}
{'loss': 1.5558, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.6583586931228638, 'eval_accuracy@por.rst.cstn': 0.5200698080279232, 'eval_f1@por.rst.cstn': 0.20021406897956126, 'eval_precision@por.rst.cstn': 0.20264842070491138, 'eval_recall@por.rst.cstn': 0.21619793134771292, 'eval_loss@por.rst.cstn': 1.6583586931228638, 'eval_runtime': 7.2968, 'eval_samples_per_second': 78.528, 'eval_steps_per_second': 2.467, 'epoch': 12.0}
{'train_runtime': 1973.3444, 'train_samples_per_second': 25.224, 'train_steps_per_second': 0.791, 'train_loss': 1.8649618588961088, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.7843
  train_runtime            = 0:03:36.65
  train_samples_per_second =     24.315
  train_steps_per_second   =      0.775

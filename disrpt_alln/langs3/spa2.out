0.3333333333333333
1.0
ASSIGN: 19
CustomBERTModel(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(31002, 768, padding_idx=1)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (encoder): CustomPooler2(
    (_dropout): Dropout(p=0.0, inplace=False)
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (relation_decoder): Linear(in_features=768, out_features=19, bias=True)
)
['events.out.tfevents.1668139619.bc01bc43a6e4.23306.0']
Epoch 1: Best val_acc: 0.4239
Epoch 1: Best val_acc: 0.4239
Epoch 1: train_loss: 2.3711 train_acc: 0.3471 | val_loss: 2.0782 val_acc: 0.4239
00:00:25.58
train_size: 103
Epoch 2: Best val_acc: 0.4348
Epoch 2: Best val_acc: 0.4348
Epoch 2: train_loss: 1.9470 train_acc: 0.4005 | val_loss: 2.0466 val_acc: 0.4348
00:00:25.26
train_size: 103
Epoch 3: Best val_acc: 0.4674
Epoch 3: Best val_acc: 0.4674
Epoch 3: train_loss: 1.8048 train_acc: 0.5267 | val_loss: 1.9164 val_acc: 0.4674
00:00:27.04
train_size: 103
Epoch 4: Best val_acc: 0.5217
Epoch 4: Best val_acc: 0.5217
Epoch 4: train_loss: 1.6822 train_acc: 0.5631 | val_loss: 1.8585 val_acc: 0.5217
00:00:24.73
train_size: 103
Epoch 5: Best val_acc: 0.5652
Epoch 5: Best val_acc: 0.5652
Epoch 5: train_loss: 1.5646 train_acc: 0.6286 | val_loss: 1.7336 val_acc: 0.5652
00:00:25.59
train_size: 103
Epoch 6: train_loss: 1.4585 train_acc: 0.6505 | val_loss: 1.7016 val_acc: 0.5326
00:00:22.66
train_size: 103
Epoch 7: Best val_acc: 0.5761
Epoch 7: Best val_acc: 0.5761
Epoch 7: train_loss: 1.3770 train_acc: 0.6772 | val_loss: 1.6394 val_acc: 0.5761
00:00:25.79
train_size: 103
Epoch 8: train_loss: 1.2903 train_acc: 0.6990 | val_loss: 1.6653 val_acc: 0.5435
00:00:25.99
train_size: 103
Epoch 9: Best val_acc: 0.5761
Epoch 9: train_loss: 1.2314 train_acc: 0.7160 | val_loss: 1.6598 val_acc: 0.5761
00:00:24.04
train_size: 103
Epoch 10: train_loss: 1.1644 train_acc: 0.7282 | val_loss: 1.5462 val_acc: 0.5543
00:00:22.76
train_size: 103
Epoch 11: Best val_acc: 0.6087
Epoch 11: Best val_acc: 0.6087
Epoch 11: train_loss: 1.1092 train_acc: 0.7209 | val_loss: 1.5077 val_acc: 0.6087
00:00:28.23
train_size: 103
Epoch 12: train_loss: 1.0516 train_acc: 0.7524 | val_loss: 1.4890 val_acc: 0.5870
00:00:26.88
train_size: 103
Epoch 13: Best val_acc: 0.6196
Epoch 13: Best val_acc: 0.6196
Epoch 13: train_loss: 0.9903 train_acc: 0.7621 | val_loss: 1.4767 val_acc: 0.6196
00:00:24.94
train_size: 103
Epoch 00014: reducing learning rate of group 0 to 7.0000e-07.
Epoch 14: train_loss: 0.9440 train_acc: 0.7670 | val_loss: 1.5068 val_acc: 0.5870
00:00:23.54
train_size: 103
Epoch 15: Best val_acc: 0.6196
Epoch 15: train_loss: 0.8988 train_acc: 0.7767 | val_loss: 1.4239 val_acc: 0.6196
00:00:26.52
train_size: 103
Epoch 16: train_loss: 0.8601 train_acc: 0.7913 | val_loss: 1.4216 val_acc: 0.6087
00:00:24.46
train_size: 103
Epoch 17: Best val_acc: 0.6196
Epoch 17: train_loss: 0.8245 train_acc: 0.7985 | val_loss: 1.4271 val_acc: 0.6196
00:00:27.62
train_size: 103
Epoch 18: Best val_acc: 0.6196
Epoch 18: train_loss: 0.8005 train_acc: 0.7985 | val_loss: 1.4040 val_acc: 0.6196
00:00:24.81
train_size: 103
Epoch 19: train_loss: 0.7714 train_acc: 0.8058 | val_loss: 1.3948 val_acc: 0.6087
00:00:24.89
train_size: 103
Epoch 20: train_loss: 0.7462 train_acc: 0.8155 | val_loss: 1.4238 val_acc: 0.6087
00:00:25.26
train_size: 103
Epoch 21: train_loss: 0.7115 train_acc: 0.8325 | val_loss: 1.3801 val_acc: 0.6087
00:00:23.10
train_size: 103
Epoch 22: train_loss: 0.6923 train_acc: 0.8325 | val_loss: 1.4187 val_acc: 0.5652
00:00:27.29
train_size: 103
Epoch 23: train_loss: 0.6631 train_acc: 0.8422 | val_loss: 1.4387 val_acc: 0.5761
00:00:23.08
train_size: 103
Epoch 24: train_loss: 0.6379 train_acc: 0.8544 | val_loss: 1.3602 val_acc: 0.6087
00:00:25.81
train_size: 103
Test_loss: 1.5774 test_acc: 0.5687
00:00:02.74
                precision    recall  f1-score   support

    antithesis       0.00      0.00      0.00         3
    background       1.00      0.50      0.67         4
  circumstance       0.00      0.00      0.00         3
     condition       0.00      0.00      0.00         1
   conjunction       0.00      0.00      0.00         1
      contrast       0.00      0.00      0.00         5
   disjunction       0.00      0.00      0.00         2
   elaboration       0.67      0.77      0.72        71
    enablement       0.00      0.00      0.00         1
      evidence       0.00      0.00      0.00         1
interpretation       0.00      0.00      0.00         3
          list       0.43      0.61      0.50        33
         means       0.00      0.00      0.00         2
    motivation       0.00      0.00      0.00         1
   preparation       0.59      1.00      0.74        10
       purpose       0.30      0.43      0.35         7
        result       0.00      0.00      0.00         5
      sequence       0.00      0.00      0.00         5
       summary       0.00      0.00      0.00         1

      accuracy                           0.57       159
     macro avg       0.16      0.17      0.16       159
  weighted avg       0.46      0.57      0.50       159

Test Loss: 1.577 |  Test Acc: 56.88%
Test_loss: 1.5862 test_acc: 0.5563
00:00:02.93
                precision    recall  f1-score   support

    antithesis       0.00      0.00      0.00         3
    background       1.00      0.25      0.40         4
  circumstance       0.00      0.00      0.00         3
     condition       0.00      0.00      0.00         1
   conjunction       0.00      0.00      0.00         1
      contrast       0.00      0.00      0.00         5
   disjunction       0.00      0.00      0.00         2
   elaboration       0.64      0.80      0.71        71
    enablement       0.00      0.00      0.00         1
      evidence       0.00      0.00      0.00         1
interpretation       0.00      0.00      0.00         3
          list       0.40      0.55      0.46        33
         means       0.00      0.00      0.00         2
    motivation       0.00      0.00      0.00         1
   preparation       0.50      1.00      0.67        10
       purpose       0.50      0.29      0.36         7
        result       0.00      0.00      0.00         5
      sequence       0.00      0.00      0.00         5
       summary       0.00      0.00      0.00         1

      accuracy                           0.55       159
     macro avg       0.16      0.15      0.14       159
  weighted avg       0.45      0.55      0.48       159

Latest Test Loss: 1.586 |  Latest Test Acc: 55.62%
Test_loss: 1.5569 test_acc: 0.5625
00:00:03.52
                precision    recall  f1-score   support

    antithesis       0.00      0.00      0.00         3
    background       1.00      0.50      0.67         4
  circumstance       0.00      0.00      0.00         3
     condition       0.00      0.00      0.00         1
   conjunction       0.00      0.00      0.00         1
      contrast       0.00      0.00      0.00         5
   disjunction       0.00      0.00      0.00         2
   elaboration       0.65      0.79      0.71        71
    enablement       0.00      0.00      0.00         1
      evidence       0.00      0.00      0.00         1
interpretation       0.00      0.00      0.00         3
          list       0.38      0.55      0.45        33
         means       0.00      0.00      0.00         2
    motivation       0.00      0.00      0.00         1
   preparation       0.53      1.00      0.69        10
       purpose       0.60      0.43      0.50         7
        result       0.00      0.00      0.00         5
      sequence       0.00      0.00      0.00         5
       summary       0.00      0.00      0.00         1

      accuracy                           0.56       159
     macro avg       0.17      0.17      0.16       159
  weighted avg       0.45      0.56      0.49       159

Best Test Loss: 1.557 |  Best Test Acc: 56.25%
Test_loss: 1.4011 test_acc: 0.6196
00:00:01.51
                precision    recall  f1-score   support

    antithesis       0.00      0.00      0.00         2
    background       0.75      0.60      0.67         5
  circumstance       0.00      0.00      0.00         2
     condition       0.00      0.00      0.00         1
   elaboration       0.57      0.88      0.69        33
      evidence       0.00      0.00      0.00         2
interpretation       0.00      0.00      0.00         1
          list       0.71      0.53      0.61        19
         means       0.00      0.00      0.00         2
    motivation       0.00      0.00      0.00         1
   preparation       0.58      1.00      0.73        11
       purpose       1.00      0.20      0.33         5
        result       0.00      0.00      0.00         5

      accuracy                           0.61        89
     macro avg       0.28      0.25      0.23        89
  weighted avg       0.53      0.61      0.53        89

Val Loss: 1.401 |  Val Acc: 61.96%

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seeding for comparing experiment in part 2\n",
    "import torch\n",
    "import json\n",
    "SEED = 1111\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('cuda:2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNLI Bert\n",
    "## Second Tutorial\n",
    "https://towardsdatascience.com/fine-tuning-pre-trained-transformer-models-for-sentence-entailment-d87caf9ec9db\n",
    "Check his Github code for complete notebook. I never referred to it. Medium was enough.\n",
    "BERT in keras-tf: https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define macros\n",
    "BERT_MODEL = 'bert-base-german-cased' #'dbmdz/bert-base-german-cased'\n",
    "# 'bert-base-multilingual-cased'\n",
    "# 'bert-base-uncased'\n",
    "\n",
    "batch_size = 4\n",
    "batches_per_epoch = 541"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://nlp.stanford.edu/projects/snli/snli_1.0.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# custom reader needed to handle quotechars\n",
    "def read_df_custom(file):\n",
    "    header = 'doc     unit1_toks      unit2_toks      unit1_txt       unit2_txt       s1_toks s2_toks unit1_sent      unit2_sent      dir     nuc_children    sat_children    genre   u1_discontinuous        u2_discontinuous       u1_issent        u2_issent       u1_length       u2_length       length_ratio    u1_speaker      u2_speaker      same_speaker    u1_func u1_pos  u1_depdir       u2_func u2_pos  u2_depdir       doclen  u1_position      u2_position     percent_distance        distance        lex_overlap_words       lex_overlap_length      unit1_case      unit2_case      label'\n",
    "    extracted_columns = ['unit1_txt', 'unit1_sent', 'unit2_txt', 'unit2_sent', 'dir', 'label', 'distance', 'u1_depdir', 'u2_depdir', 'u2_func', 'u1_position', 'u2_position', 'sat_children', 'nuc_children']\n",
    "    header = header.split()\n",
    "    df = pd.DataFrame(columns=extracted_columns)\n",
    "    file = open(file, 'r')\n",
    "\n",
    "    rows = []\n",
    "    count = 0 \n",
    "    for line in file:\n",
    "        line = line[:-1].split('\\t')\n",
    "        count+=1\n",
    "        if count ==1: continue\n",
    "        row = {}\n",
    "        for column in extracted_columns:\n",
    "            index = header.index(column)\n",
    "            row[column] = line[index]\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame.from_records(rows)])\n",
    "    return df\n",
    "\n",
    "# we only need specific columns\n",
    "# train_df = read_df_custom('../processed/nld.rst.nldt_train_enriched.rels')\n",
    "# test_df = read_df_custom('../processed/nld.rst.nldt_test_enriched.rels')\n",
    "# val_df = read_df_custom('../processed/nld.rst.nldt_dev_enriched.rels')\n",
    "# train_df = read_df_custom('../processed/fas.rst.prstc_train_enriched.rels')\n",
    "# test_df = read_df_custom('../processed/fas.rst.prstc_test_enriched.rels')\n",
    "# val_df = read_df_custom('../processed/fas.rst.prstc_dev_enriched.rels')\n",
    "train_df = read_df_custom('../processed/deu.rst.pcc_train_enriched.rels')\n",
    "test_df = read_df_custom('../processed/deu.rst.pcc_test_enriched.rels')\n",
    "val_df = read_df_custom('../processed/deu.rst.pcc_dev_enriched.rels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping any empty values\n",
    "train_df.dropna(inplace=True)\n",
    "val_df.dropna(inplace=True)\n",
    "test_df.dropna(inplace=True)\n",
    "\n",
    "# train_df = train_df[:500]\n",
    "# val_df = val_df[:50]\n",
    "# test_df = test_df[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare a dataset handler class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from multiprocessing.sharedctypes import Value\n",
    "import torch\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from sys import path\n",
    "path.append('/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/allennlp/data/data_loaders/')\n",
    "from allennlp.data import allennlp_collate#, DataLoader\n",
    "# from allennlp.data.data_loaders.simple_data_loader import SimpleDataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "class MNLIDataBert(Dataset):\n",
    "\n",
    "  def __init__(self, train_df, val_df, test_df):\n",
    "    self.num_labels = -1\n",
    "    self.train_df = train_df\n",
    "    self.val_df = val_df\n",
    "    self.test_df = test_df\n",
    "\n",
    "    self.tokenizer = BertTokenizer.from_pretrained(BERT_MODEL, do_lower_case=True) # Using a pre-trained BERT tokenizer to encode sentences\n",
    "    self.train_data = None\n",
    "    self.val_data = None\n",
    "    self.test_data = None\n",
    "    self.train_idx = None\n",
    "    self.val_idx = None\n",
    "    self.test_idx = None\n",
    "    self.init_data()\n",
    "\n",
    "  def init_data(self):\n",
    "    self.get_label_mapping()\n",
    "    self.train_data, self.train_idx = self.load_data(self.train_df)\n",
    "    self.val_data, self.val_idx = self.load_data(self.val_df)\n",
    "    self.test_data, self.test_idx = self.load_data(self.test_df)\n",
    "\n",
    "  def get_label_mapping(self):\n",
    "    labels = {}\n",
    "    labels_list = list(set(list(self.train_df['label'].unique()) + list(self.test_df['label'].unique()) + list(self.val_df['label'].unique())))\n",
    "    for i in range(len(labels_list)):\n",
    "        labels[labels_list[i]] = i\n",
    "    self.label_dict = labels# {'entailment': 0, 'contradiction': 1, 'neutral': 2}\n",
    "    # needed later for classification report object to generate precision and recall on test dataset\n",
    "    self.rev_label_dict = {self.label_dict[k]:k for k in self.label_dict.keys()} \n",
    "  \n",
    "  def add_directionality(self, premise, hypothesis, dir):\n",
    "    if dir == \"1<2\":\n",
    "        # hypothesis = 'left ' + hypothesis + ' {'\n",
    "        hypothesis = '< ' + hypothesis + ' {'\n",
    "        # hypothesis = '{ ' + hypothesis + ' {'\n",
    "    else:\n",
    "        # premise = '} ' + premise + ' right'\n",
    "        premise = '} ' + premise + ' >'\n",
    "        # premise = '} ' + premise + ' }'\n",
    "    return premise, hypothesis\n",
    "\n",
    "  def get_distance(self, d):\n",
    "    if d<-8: return -2\n",
    "    elif d>=-8 and d<-2: return -1\n",
    "    elif d>=-2 and d<0: return 0\n",
    "    elif d>=0 and d<2: return 1\n",
    "    elif d>=2 and d<8: return 2\n",
    "    elif d>=8: return 3\n",
    "\n",
    "  def get_dep(self, d):\n",
    "    if d=='ROOT': return 0\n",
    "    elif d=='RIGHT': return 1\n",
    "    elif d=='LEFT': return -1\n",
    "    else: raise ValueError()\n",
    "\n",
    "  def get_u2_func(self, u):\n",
    "    u2_dict = {'root':0, 'conj':1, 'advcl':2, 'acl':3, 'xcomp':4, 'obl':5, 'ccomp':6,\n",
    "       'parataxis':7, 'advmod':8, 'dep':9, 'csubj':10, 'nmod':11, 'punct':12, 'cc':13,\n",
    "       'appos':14, 'aux':15, 'obj':16, 'iobj':17, 'nsubj':18, 'nsubj:pass':19, 'csubj:pass':20}\n",
    "    return u2_dict[u]\n",
    "\n",
    "  def get_u_position(self, u):\n",
    "    if u>=0.0 and u<0.1: return -5\n",
    "    elif u>=0.1 and u<0.2: return -4\n",
    "    elif u>=0.2 and u<0.3: return -3\n",
    "    elif u<=0.3 and u<0.4: return -2\n",
    "    elif u<=0.4 and u<0.5: return -1\n",
    "    elif u<=0.5 and u<0.6: return 0\n",
    "    elif u<=0.6 and u<0.7: return 1\n",
    "    elif u<=0.7 and u<0.8: return 2\n",
    "    elif u<=0.8 and u<0.9: return 3\n",
    "    elif u<=0.9 and u<1.0: return 4\n",
    "    elif u<=1.0 and u<1e9: return 5\n",
    "\n",
    "  def get_feature(self, features):\n",
    "    distance = self.get_distance(int(features[0]))\n",
    "    u1_depdir = self.get_dep(features[1])\n",
    "    u2_depdir = self.get_dep(features[2])\n",
    "    u2_func = self.get_u2_func(features[3])\n",
    "    u1_position = self.get_u_position(float(features[4]))\n",
    "    u2_position = self.get_u_position(float(features[5]))\n",
    "    sat_children = int(features[6])\n",
    "    nuc_children = int(features[6])\n",
    "    return [distance, u1_depdir, u2_depdir, u2_func, u1_position, u2_position, sat_children, nuc_children]\n",
    "\n",
    "  def load_data2(self, df):\n",
    "    MAX_LEN = 256 # dont need to enforce this now because snli is a sanitized dataset where sentence lenghts are reasonable. otherwise the beert model doesn't have enough parameters to handle long length sentences\n",
    "    token_ids = []\n",
    "    mask_ids = []\n",
    "    seg_ids = []\n",
    "    y = []\n",
    "    idx = []\n",
    "    idx_map = {}\n",
    "    # self.reach = 'reach'\n",
    "\n",
    "    premise_list = df['unit1_txt'].to_list()\n",
    "    hypothesis_list = df['unit2_txt'].to_list()\n",
    "    label_list = df['label'].to_list()\n",
    "    dir_list = df['dir'].to_list()\n",
    "    \n",
    "    self.num_labels = max(self.num_labels, len(df['label'].unique()))\n",
    "\n",
    "    count=0\n",
    "    for (premise, hypothesis, label, dir) in zip(premise_list, hypothesis_list, label_list, dir_list):\n",
    "      print('old: ', premise, hypothesis)\n",
    "      premise, hypothesis = self.add_directionality(premise, hypothesis, dir)\n",
    "      print('new:', premise, hypothesis, '\\n')\n",
    "    \n",
    "\n",
    "  def load_data(self, df):\n",
    "    MAX_LEN = 256 # dont need to enforce this now because snli is a sanitized dataset where sentence lenghts are reasonable. otherwise the beert model doesn't have enough parameters to handle long length sentences\n",
    "    token_ids = []\n",
    "    mask_ids = []\n",
    "    seg_ids = []\n",
    "    y = []\n",
    "    feats = []\n",
    "    idx = []\n",
    "    idx_map = {}\n",
    "\n",
    "    self.num_labels = max(self.num_labels, len(df['label'].unique()))\n",
    "\n",
    "    count=0\n",
    "    # for (premise, hypothesis, label, dir, distance, u1_depdir) in zip(premise_list, hypothesis_list, label_list, dir_list, u1_depdir_list, feat_list):\n",
    "    for row in df.iterrows():\n",
    "      row = row[1]\n",
    "      premise = row['unit1_txt']\n",
    "      hypothesis = row['unit2_txt']\n",
    "      label = row['label']\n",
    "      dir = row['dir']\n",
    "\n",
    "      distance = row['distance']\n",
    "      u1_depdir = row['u1_depdir']\n",
    "      u2_depdir = row['u2_depdir']\n",
    "      u2_func = row['u2_func']\n",
    "      u1_position = row['u1_position']\n",
    "      u2_position = row['u2_position']\n",
    "      sat_children = row['sat_children']\n",
    "      nuc_children = row['nuc_children']\n",
    "      features = [distance, u1_depdir, u2_depdir, u2_func, u1_position, u2_position, sat_children, nuc_children]\n",
    "\n",
    "      premise, hypothesis = self.add_directionality(premise, hypothesis, dir)\n",
    "      premise_id = self.tokenizer.encode(premise, add_special_tokens = False, max_length=MAX_LEN, truncation=True)\n",
    "      # print(premise)\n",
    "      # print(premise_id)\n",
    "      # print(self.tokenizer.encode(\"< \"))\n",
    "      # print(self.tokenizer.decode(self.tokenizer.encode(\"< \")))\n",
    "      # print(self.tokenizer.encode(\"> \"))\n",
    "      # print(self.tokenizer.decode(self.tokenizer.encode(\"> \")))\n",
    "      # print(self.tokenizer.encode(\"UNK \"))\n",
    "      # print(self.tokenizer.encode(\"> this\"))\n",
    "      # print(self.tokenizer.encode(\"this .\"))\n",
    "      # print(self.tokenizer.encode(\"this } \"))\n",
    "      # print(self.tokenizer.encode(\"fffffff } \"))\n",
    "      hypothesis_id = self.tokenizer.encode(hypothesis, add_special_tokens = False, max_length=MAX_LEN, truncation=True)\n",
    "      pair_token_ids = [self.tokenizer.cls_token_id] + premise_id + [self.tokenizer.sep_token_id] + hypothesis_id + [self.tokenizer.sep_token_id]\n",
    "      # pair_token_ids = self.tokenizer.encode(premise, hypothesis, add_special_tokens = True, max_length=MAX_LEN*2, truncation=True)\n",
    "      premise_len = len(premise_id)\n",
    "      hypothesis_len = len(hypothesis_id)\n",
    "\n",
    "      segment_ids = torch.tensor([0] * (premise_len + 2) + [1] * (hypothesis_len + 1))  # sentence 0 and sentence 1\n",
    "      attention_mask_ids = torch.tensor([1] * (premise_len + hypothesis_len + 3))  # mask padded values\n",
    "\n",
    "      token_ids.append(torch.tensor(pair_token_ids))\n",
    "      seg_ids.append(segment_ids)\n",
    "      mask_ids.append(attention_mask_ids)\n",
    "      y.append(self.label_dict[label])\n",
    "      feats.append(self.get_feature(features))\n",
    "\n",
    "      idx_map[count] = [premise, hypothesis]\n",
    "      idx.append(count)\n",
    "      count+=1\n",
    "    \n",
    "    token_ids = pad_sequence(token_ids, batch_first=True)\n",
    "    mask_ids = pad_sequence(mask_ids, batch_first=True)\n",
    "    seg_ids = pad_sequence(seg_ids, batch_first=True)\n",
    "\n",
    "    y = torch.tensor(y)\n",
    "    idx = torch.tensor(idx)\n",
    "    feats = torch.tensor(feats)\n",
    "    dataset = TensorDataset(token_ids, mask_ids, seg_ids, feats, y, idx)\n",
    "    return dataset, idx_map\n",
    "\n",
    "  def get_data_loaders(self, batch_size=4, batches_per_epoch=402, shuffle=True): #1609 samples / 64:25=1600 / 402:4=1608\n",
    "    train_loader_torch = DataLoader(\n",
    "      self.train_data,\n",
    "      shuffle=shuffle,\n",
    "      batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    val_loader_torch = DataLoader(\n",
    "      self.val_data,\n",
    "      shuffle=shuffle,\n",
    "      batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    test_loader_torch = DataLoader(\n",
    "      self.test_data,\n",
    "      shuffle=False,#shuffle,\n",
    "      batch_size=batch_size,\n",
    "    )\n",
    "    \n",
    "    train_loader = LoaderWrapper(train_loader_torch, n_step=batches_per_epoch)\n",
    "    val_loader = LoaderWrapper(val_loader_torch, n_step=batches_per_epoch)\n",
    "    test_loader = LoaderWrapper(test_loader_torch, n_step=batches_per_epoch)\n",
    "\n",
    "    return train_loader, val_loader_torch, test_loader_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoaderWrapper:\n",
    "    def __init__(self, loader, n_step):\n",
    "        self.step = n_step\n",
    "        self.idx = 0\n",
    "        self.iter_loader = iter(loader)\n",
    "        self.loader = loader\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.step\n",
    "\n",
    "    def __next__(self):\n",
    "        # if reached number of steps desired, stop\n",
    "        if self.idx == self.step:\n",
    "            self.idx = 0\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            self.idx += 1\n",
    "        # while True\n",
    "        try:\n",
    "            return next(self.iter_loader)\n",
    "        except StopIteration:\n",
    "            # reinstate iter_loader, then continue\n",
    "            self.iter_loader = iter(self.loader)\n",
    "            return next(self.iter_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_dataset = MNLIDataBert(train_df, val_df, test_df)\n",
    "\n",
    "train_loader, val_loader, test_loader = mnli_dataset.get_data_loaders(batch_size=batch_size, batches_per_epoch=batches_per_epoch) #64X250\n",
    "label_dict = mnli_dataset.label_dict # required by custom func to calculate accuracy, bert model\n",
    "rev_label_dict = mnli_dataset.rev_label_dict # required by custom func to calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cause': 0, 'summary': 1, 'e-elaboration': 2, 'reason': 3, 'restatement': 4, 'disjunction': 5, 'evaluation-n': 6, 'means': 7, 'evaluation-s': 8, 'interpretation': 9, 'purpose': 10, 'antithesis': 11, 'sequence': 12, 'circumstance': 13, 'condition': 14, 'evidence': 15, 'contrast': 16, 'elaboration': 17, 'preparation': 18, 'list': 19, 'conjunction': 20, 'background': 21, 'joint': 22, 'solutionhood': 23, 'concession': 24, 'result': 25}\n"
     ]
    }
   ],
   "source": [
    "print(label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-german-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from torch import optim\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(BERT_MODEL, num_labels=len(label_dict)).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.6, mode='max', patience=2, min_lr=5e-7, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from CategoricalAccuracy import CategoricalAccuracy as CA\n",
    "import numpy as np\n",
    "\n",
    "ca = CA()\n",
    "\n",
    "x = torch.tensor(np.array([[[1,0,0], [1,0,0], [1,0,0]]]))\n",
    "y1 = torch.tensor(np.array([[0], [1], [1]]))\n",
    "y2 = torch.tensor(np.array([[0], [0], [0]]))\n",
    "\n",
    "ca(x,y1)\n",
    "print(ca.get_metric(reset=True))\n",
    "ca(x,y2)\n",
    "print(ca.get_metric(reset=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define evaulation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to evaluate model for train and test. And also use classification report for testing\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# helper function to calculate the batch accuracy\n",
    "def multi_acc(y_pred, y_test, allennlp=False):\n",
    "  if allennlp==False:\n",
    "    acc = (torch.log_softmax(y_pred, dim=1).argmax(dim=1) == y_test).sum().float() / float(y_test.size(0))\n",
    "    return acc\n",
    "\n",
    "def save_cm(y_true, y_pred, target_names, display_zeros=True):\n",
    "  confusion_matrix = pd.crosstab(y_true, y_pred)\n",
    "  print(confusion_matrix)\n",
    "  target_namesx = confusion_matrix.columns\n",
    "  target_namesy = confusion_matrix.index.values\n",
    "\n",
    "  fig, ax = plt.subplots(figsize=(10,10))\n",
    "  s = sns.heatmap(confusion_matrix, xticklabels=target_namesx, yticklabels=target_namesy, annot=True, fmt = '.5g')\n",
    "\n",
    "  plt.title('CM predicted v actual values')\n",
    "  plt.xlabel('Pred')\n",
    "  plt.ylabel('True')\n",
    "  plt.tight_layout()\n",
    "  # plt.show()\n",
    "  # plt.savefig(image_file+exp+'.png')\n",
    "  # print(image_file+exp+'.png')\n",
    "\n",
    "# freeze model weights and measure validation / test \n",
    "def evaluate_accuracy(model, optimizer, data_loader, rev_label_dict, label_dict, save_path, is_training=True):\n",
    "  model.eval()\n",
    "  total_val_acc  = 0\n",
    "  total_val_loss = 0\n",
    "  \n",
    "  #for classification report\n",
    "  y_true = []\n",
    "  y_pred = []\n",
    "  idx_list = []\n",
    "  premise_list = []\n",
    "  hypo_list = []\n",
    "  idx_map = mnli_dataset.val_idx if is_training else mnli_dataset.test_idx\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for batch_idx, (pair_token_ids, mask_ids, seg_ids, feat, y, idx) in enumerate(data_loader):      \n",
    "      optimizer.zero_grad()\n",
    "      pair_token_ids = pair_token_ids.to(device)\n",
    "      mask_ids = mask_ids.to(device)\n",
    "      seg_ids = seg_ids.to(device)\n",
    "      labels = y.to(device)\n",
    "      feat = feat.to(device)\n",
    "      \n",
    "      # loss, prediction = model(pair_token_ids, \n",
    "      #                       token_type_ids=seg_ids, \n",
    "      #                       attention_mask=mask_ids, \n",
    "      #                       labels=labels).values()\n",
    "      # acc = multi_acc(prediction, labels)\n",
    "\n",
    "      ############new code#####################\n",
    "\n",
    "      outputs = model(pair_token_ids, \n",
    "                            token_type_ids=seg_ids, \n",
    "                            attention_mask=mask_ids, \n",
    "                            feat=feat)\n",
    "      # probs = F.softmax(outputs, dim=1)\n",
    "      # max_idx = torch.max(outputs, 1).indices\n",
    "      # one_hot = F.one_hot(max_idx, outputs.shape[1])\n",
    "\n",
    "      criterion = nn.CrossEntropyLoss()\n",
    "      loss = criterion(outputs, labels)\n",
    "      acc = multi_acc(outputs, labels)\n",
    "      ########################################\n",
    "\n",
    "      total_val_loss += loss.item()\n",
    "      total_val_acc  += acc.item()\n",
    "\n",
    "      # log predictions for classification report\n",
    "      argmax_predictions = torch.argmax(outputs,dim=1).tolist()\n",
    "      labels_list = labels.tolist()\n",
    "      assert(len(labels_list)==len(argmax_predictions))\n",
    "      for p in argmax_predictions: y_pred.append(rev_label_dict[int(p)])\n",
    "      for l in labels_list: y_true.append(rev_label_dict[l])\n",
    "      for i in idx.tolist():\n",
    "        idx_list.append(i)\n",
    "        premise_list.append(idx_map[i][0])\n",
    "        hypo_list.append(idx_map[i][1])\n",
    "\n",
    "  val_acc  = total_val_acc/len(data_loader)\n",
    "  val_loss = total_val_loss/len(data_loader)\n",
    "  cr = classification_report(y_true, y_pred)\n",
    "\n",
    "  idx_json = {'idx': idx_list, 'gold_label': y_true, 'pred_label': y_pred, 'premise': premise_list, 'hypothesis': hypo_list}\n",
    "  # if not is_training: json.dump(idx_json, open(save_path, 'w', encoding='utf8'), ensure_ascii=False)\n",
    "\n",
    "  # if not is_training:\n",
    "  #   save_cm(y_true, y_pred, rev_label_dict)\n",
    "  \n",
    "  return val_acc, val_loss, cr, model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define bert custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASSIGN: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, AutoTokenizer\n",
    "import torch.nn as nn\n",
    "class CustomBERTModel(nn.Module):\n",
    "    #https://stackoverflow.com/questions/64156202/add-dense-layer-on-top-of-huggingface-bert-model\n",
    "    def __init__(self, num_labels):\n",
    "          super(CustomBERTModel, self).__init__()\n",
    "          self.num_classes = num_labels+1 # zero indexed classes\n",
    "          print('ASSIGN:', self.num_classes)\n",
    "          self.bert = BertModel.from_pretrained(BERT_MODEL)\n",
    "          ### New layers:\n",
    "          self.linear1 = nn.Linear(776, 512)\n",
    "          self.linear2 = nn.Linear(512, 256)\n",
    "          self.linear3 = nn.Linear(256, 128)\n",
    "          self.linear4 = nn.Linear(128, self.num_classes)\n",
    "          self.act1 = nn.ReLU() # can i use the same activation object everywhere?\n",
    "          self.act2 = nn.ReLU()\n",
    "          self.act3 = nn.ReLU()\n",
    "          self.drop = nn.Dropout(0.1) \n",
    "\n",
    "    def forward(self, pair_token_ids, token_type_ids, attention_mask, feat):\n",
    "        sequence_output, pooled_output = self.bert(input_ids=pair_token_ids, \n",
    "                        token_type_ids=token_type_ids, \n",
    "                        attention_mask=attention_mask).values()\n",
    "\n",
    "        feat_concat = torch.concat((sequence_output[:,0,:].view(-1,768), feat),-1)\n",
    "\n",
    "        # sequence_output has the following shape: (batch_size, sequence_length, 768)\n",
    "        linear1_output = self.linear1(feat_concat) ## extract the 1st token's embeddings\n",
    "        linear1_output = self.act1(linear1_output)\n",
    "        linear2_output = self.linear2(linear1_output)\n",
    "        linear2_output = self.act2(linear2_output)\n",
    "        linear3_output = self.linear3(linear2_output)\n",
    "        linear3_output = self.act3(linear3_output)\n",
    "        linear4_output = self.linear4(linear3_output)\n",
    "        # drop_output = self.drop(linear4_output)\n",
    "        return linear4_output# loss, outputs\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL)\n",
    "model = CustomBERTModel(mnli_dataset.num_labels) # You can pass the parameters if required to have more flexible model\n",
    "model.to(device) ## can be gpu\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertForSequenceClassification, AdamW\n",
    "# from torch import optim\n",
    "\n",
    "# # model = BertForSequenceClassification.from_pretrained(BERT_MODEL, num_labels=len(label_dict)).to(device)\n",
    "# optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.6, mode='max', patience=2, min_lr=5e-7, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define training regime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODIFIED\n",
    "import time\n",
    "import traceback\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Iterable, Dict, Any\n",
    "from EarlyStopperUtil import MetricTracker\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "EPOCHS = 100\n",
    "best_epoch = 'N'\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer, scheduler, rev_label_dict):  \n",
    "  EarlyStopper = MetricTracker(patience=12, metric_name='+accuracy')\n",
    "  best_acc = -1\n",
    "\n",
    "  for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_train_acc  = 0\n",
    "\n",
    "    # logging for scheduler\n",
    "    losses = []\n",
    "    accuracies= []\n",
    "\n",
    "    train_size = 0\n",
    "\n",
    "    for batch_idx, (pair_token_ids, mask_ids, seg_ids, feat, y, idx) in enumerate(train_loader):\n",
    "      train_size+=1\n",
    "      optimizer.zero_grad()\n",
    "      pair_token_ids = pair_token_ids.to(device)\n",
    "      mask_ids = mask_ids.to(device)\n",
    "      seg_ids = seg_ids.to(device)\n",
    "      labels = y.to(device)\n",
    "      feat = feat.to(device)\n",
    "\n",
    "      ############new code#####################\n",
    "\n",
    "      outputs = model(pair_token_ids, \n",
    "                            token_type_ids=seg_ids, \n",
    "                            attention_mask=mask_ids,\n",
    "                            feat=feat)\n",
    "      # outputs = F.log_softmax(outputs, dim=1) # log prob\n",
    "      # outputs = np.argmax(prob, axis=1) # preds\n",
    "      # https://stackoverflow.com/questions/43672047/convert-probability-vector-into-target-vector-in-python\n",
    "      # https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "      criterion = nn.CrossEntropyLoss()\n",
    "      loss = criterion(outputs, labels)\n",
    "      loss.backward()\n",
    "      acc = multi_acc(outputs, labels)\n",
    "      optimizer.step()\n",
    "      ################old code#################\n",
    "\n",
    "      # loss, prediction = model(pair_token_ids, \n",
    "      #                       token_type_ids=seg_ids, \n",
    "      #                       attention_mask=mask_ids, \n",
    "      #                       labels=labels).values()\n",
    "\n",
    "      # acc = multi_acc(prediction, labels)\n",
    "      # loss.backward()\n",
    "      # optimizer.step()\n",
    "\n",
    "      ########################################\n",
    "      total_train_loss += loss.item()\n",
    "      total_train_acc  += acc.item()\n",
    "\n",
    "      # log losses for scheduler\n",
    "      losses.append(loss)\n",
    "      mean_loss = sum(losses)/len(losses)\n",
    "      scheduler.step(mean_loss)\n",
    "\n",
    "      \n",
    "\n",
    "\n",
    "    train_acc  = total_train_acc/len(train_loader)\n",
    "    train_loss = total_train_loss/len(train_loader)\n",
    "\n",
    "    val_acc, val_loss, cr, model, optimizer = evaluate_accuracy(model, optimizer, val_loader, rev_label_dict, label_dict, None)\n",
    "    \n",
    "    if val_acc>=best_acc:\n",
    "        if epoch>4:\n",
    "          torch.save(model.state_dict(), 'best_debug/deu_debug_best_'+str(epoch)+'.pt')\n",
    "          print('Saving at.... deu_debug_best_'+str(epoch)+'.pt')\n",
    "        global best_epoch\n",
    "        best_acc = val_acc\n",
    "        best_epoch = epoch\n",
    "    \n",
    "    EarlyStopper.add_metric(val_acc)\n",
    "    if EarlyStopper.should_stop_early(): break\n",
    "\n",
    "    end = time.time()\n",
    "    hours, rem = divmod(end-start, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "    print(f'Epoch {epoch+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
    "    print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "    print(f'train_size: {train_size}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00004: reducing learning rate of group 0 to 1.2000e-05.\n",
      "Epoch 00007: reducing learning rate of group 0 to 7.2000e-06.\n",
      "Epoch 00010: reducing learning rate of group 0 to 4.3200e-06.\n",
      "Epoch 00013: reducing learning rate of group 0 to 2.5920e-06.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.5552e-06.\n",
      "Epoch 00019: reducing learning rate of group 0 to 9.3312e-07.\n",
      "Epoch 00022: reducing learning rate of group 0 to 5.5987e-07.\n",
      "Epoch 00025: reducing learning rate of group 0 to 5.0000e-07.\n",
      "Epoch 1: train_loss: 3.0193 train_acc: 0.0966 | val_loss: 2.9709 val_acc: 0.1148\n",
      "00:00:30.66\n",
      "train_size: 541\n",
      "Epoch 2: train_loss: 2.9587 train_acc: 0.0966 | val_loss: 2.9706 val_acc: 0.1148\n",
      "00:00:31.56\n",
      "train_size: 541\n",
      "Epoch 3: train_loss: 2.9549 train_acc: 0.1054 | val_loss: 2.9724 val_acc: 0.1148\n",
      "00:00:31.62\n",
      "train_size: 541\n",
      "Epoch 4: train_loss: 2.9514 train_acc: 0.0980 | val_loss: 2.9697 val_acc: 0.0533\n",
      "00:00:30.74\n",
      "train_size: 541\n",
      "Epoch 5: train_loss: 2.9477 train_acc: 0.1077 | val_loss: 3.0101 val_acc: 0.0738\n",
      "00:00:30.94\n",
      "train_size: 541\n",
      "Epoch 6: train_loss: 2.8262 train_acc: 0.1192 | val_loss: 3.0789 val_acc: 0.0492\n",
      "00:00:30.86\n",
      "train_size: 541\n",
      "Epoch 7: train_loss: 2.9506 train_acc: 0.1007 | val_loss: 2.9796 val_acc: 0.0738\n",
      "00:00:30.90\n",
      "train_size: 541\n",
      "Epoch 8: train_loss: 2.9424 train_acc: 0.1017 | val_loss: 2.9799 val_acc: 0.0779\n",
      "00:00:30.98\n",
      "train_size: 541\n",
      "Epoch 9: train_loss: 2.9371 train_acc: 0.0952 | val_loss: 2.9924 val_acc: 0.0574\n",
      "00:00:30.75\n",
      "train_size: 541\n",
      "Epoch 10: train_loss: 2.9323 train_acc: 0.1137 | val_loss: 2.9576 val_acc: 0.0779\n",
      "00:00:30.70\n",
      "train_size: 541\n",
      "Saving at.... deu_debug_best_10.pt\n",
      "Epoch 11: train_loss: 2.9289 train_acc: 0.1040 | val_loss: 2.9465 val_acc: 0.1311\n",
      "00:00:31.28\n",
      "train_size: 541\n",
      "Epoch 12: train_loss: 2.9227 train_acc: 0.1100 | val_loss: 2.9546 val_acc: 0.1066\n",
      "00:00:30.38\n",
      "train_size: 541\n",
      "Epoch 13: train_loss: 2.9189 train_acc: 0.1100 | val_loss: 2.9307 val_acc: 0.1107\n",
      "00:00:31.01\n",
      "train_size: 541\n",
      "Epoch 14: train_loss: 2.9118 train_acc: 0.1017 | val_loss: 2.9439 val_acc: 0.0943\n",
      "00:00:30.96\n",
      "train_size: 541\n",
      "Epoch 15: train_loss: 2.9009 train_acc: 0.1141 | val_loss: 2.9360 val_acc: 0.0861\n",
      "00:00:30.92\n",
      "train_size: 541\n",
      "Epoch 16: train_loss: 2.8923 train_acc: 0.1100 | val_loss: 2.9282 val_acc: 0.0697\n",
      "00:00:30.60\n",
      "train_size: 541\n",
      "Epoch 17: train_loss: 2.8831 train_acc: 0.1197 | val_loss: 2.9116 val_acc: 0.0779\n",
      "00:00:30.50\n",
      "train_size: 541\n",
      "Epoch 18: train_loss: 2.8710 train_acc: 0.1123 | val_loss: 2.9108 val_acc: 0.0779\n",
      "00:00:30.73\n",
      "train_size: 541\n",
      "Epoch 19: train_loss: 2.8616 train_acc: 0.1266 | val_loss: 2.8854 val_acc: 0.1270\n",
      "00:00:30.39\n",
      "train_size: 541\n",
      "Epoch 20: train_loss: 2.8582 train_acc: 0.1197 | val_loss: 2.8841 val_acc: 0.1107\n",
      "00:00:30.53\n",
      "train_size: 541\n",
      "Saving at.... deu_debug_best_20.pt\n",
      "Epoch 21: train_loss: 2.8430 train_acc: 0.1285 | val_loss: 2.8803 val_acc: 0.1393\n",
      "00:00:31.27\n",
      "train_size: 541\n",
      "Saving at.... deu_debug_best_21.pt\n",
      "Epoch 22: train_loss: 2.8360 train_acc: 0.1377 | val_loss: 2.8470 val_acc: 0.1516\n",
      "00:00:31.29\n",
      "train_size: 541\n",
      "Saving at.... deu_debug_best_22.pt\n",
      "Epoch 23: train_loss: 2.7494 train_acc: 0.1534 | val_loss: 2.5588 val_acc: 0.1885\n",
      "00:00:31.26\n",
      "train_size: 541\n",
      "Saving at.... deu_debug_best_23.pt\n",
      "Epoch 24: train_loss: 2.5318 train_acc: 0.2006 | val_loss: 2.4918 val_acc: 0.1967\n",
      "00:00:31.43\n",
      "train_size: 541\n",
      "Saving at.... deu_debug_best_24.pt\n",
      "Epoch 25: train_loss: 2.4956 train_acc: 0.1945 | val_loss: 2.4657 val_acc: 0.2090\n",
      "00:00:31.10\n",
      "train_size: 541\n",
      "Epoch 26: train_loss: 2.4769 train_acc: 0.2019 | val_loss: 2.4646 val_acc: 0.1967\n",
      "00:00:30.03\n",
      "train_size: 541\n",
      "Epoch 27: train_loss: 2.4656 train_acc: 0.2084 | val_loss: 2.4520 val_acc: 0.1885\n",
      "00:00:30.65\n",
      "train_size: 541\n",
      "Epoch 28: train_loss: 2.4553 train_acc: 0.2019 | val_loss: 2.4256 val_acc: 0.1967\n",
      "00:00:30.67\n",
      "train_size: 541\n",
      "Epoch 29: train_loss: 2.4465 train_acc: 0.2149 | val_loss: 2.4311 val_acc: 0.1967\n",
      "00:00:31.16\n",
      "train_size: 541\n",
      "Epoch 30: train_loss: 2.4375 train_acc: 0.2056 | val_loss: 2.4190 val_acc: 0.2008\n",
      "00:00:30.30\n",
      "train_size: 541\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/VD/kaveri/bert_categorical_tutorial/allennlp_repro/disrpt_alln/analytics_vidhya_snli_discodisco_bertwithlayer_allennlp_german_wuthfeatures.ipynb Cell 26\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/allennlp_repro/disrpt_alln/analytics_vidhya_snli_discodisco_bertwithlayer_allennlp_german_wuthfeatures.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mwith\u001b[39;00m warnings\u001b[39m.\u001b[39mcatch_warnings():\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/allennlp_repro/disrpt_alln/analytics_vidhya_snli_discodisco_bertwithlayer_allennlp_german_wuthfeatures.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     warnings\u001b[39m.\u001b[39msimplefilter(\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/allennlp_repro/disrpt_alln/analytics_vidhya_snli_discodisco_bertwithlayer_allennlp_german_wuthfeatures.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     train(model, train_loader, val_loader, optimizer, scheduler, rev_label_dict)\n",
      "\u001b[1;32m/home/VD/kaveri/bert_categorical_tutorial/allennlp_repro/disrpt_alln/analytics_vidhya_snli_discodisco_bertwithlayer_allennlp_german_wuthfeatures.ipynb Cell 26\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, optimizer, scheduler, rev_label_dict)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/allennlp_repro/disrpt_alln/analytics_vidhya_snli_discodisco_bertwithlayer_allennlp_german_wuthfeatures.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/allennlp_repro/disrpt_alln/analytics_vidhya_snli_discodisco_bertwithlayer_allennlp_german_wuthfeatures.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/allennlp_repro/disrpt_alln/analytics_vidhya_snli_discodisco_bertwithlayer_allennlp_german_wuthfeatures.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/allennlp_repro/disrpt_alln/analytics_vidhya_snli_discodisco_bertwithlayer_allennlp_german_wuthfeatures.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m acc \u001b[39m=\u001b[39m multi_acc(outputs, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/allennlp_repro/disrpt_alln/analytics_vidhya_snli_discodisco_bertwithlayer_allennlp_german_wuthfeatures.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    train(model, train_loader, val_loader, optimizer, scheduler, rev_label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'debug_last.pt'\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_loss: 5.5290 test_acc: 0.2462\n",
      "00:00:01.23\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    antithesis       0.25      0.39      0.30        18\n",
      "    background       0.29      0.29      0.29        17\n",
      "         cause       0.00      0.00      0.00         2\n",
      "  circumstance       0.14      0.13      0.14        15\n",
      "    concession       0.27      0.23      0.25        13\n",
      "     condition       0.58      0.78      0.67         9\n",
      "   conjunction       0.50      0.43      0.46         7\n",
      "      contrast       0.00      0.00      0.00         8\n",
      "   disjunction       0.00      0.00      0.00         0\n",
      " e-elaboration       0.73      0.73      0.73        11\n",
      "   elaboration       0.06      0.10      0.08        10\n",
      "  evaluation-n       0.00      0.00      0.00         3\n",
      "  evaluation-s       0.18      0.12      0.14        17\n",
      "      evidence       0.00      0.00      0.00        10\n",
      "interpretation       0.00      0.00      0.00        12\n",
      "         joint       0.24      0.48      0.32        29\n",
      "          list       0.50      0.23      0.32        26\n",
      "         means       0.00      0.00      0.00         2\n",
      "   preparation       0.00      0.00      0.00         4\n",
      "       purpose       1.00      0.67      0.80         3\n",
      "        reason       0.33      0.09      0.14        34\n",
      "   restatement       0.00      0.00      0.00         1\n",
      "        result       0.00      0.00      0.00         0\n",
      "      sequence       1.00      0.14      0.25         7\n",
      "  solutionhood       0.00      0.00      0.00         1\n",
      "       summary       0.00      0.00      0.00         1\n",
      "\n",
      "      accuracy                           0.25       260\n",
      "     macro avg       0.23      0.18      0.19       260\n",
      "  weighted avg       0.30      0.25      0.24       260\n",
      "\n",
      "Test Loss: 5.529 |  Test Acc: 24.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "def validate(model, test_loader, optimizer, rev_label_dict, label_dict, save_path):\n",
    "  start = time.time()\n",
    "  test_acc, test_loss, cr, model, optimizer = evaluate_accuracy(model, optimizer, test_loader, rev_label_dict, label_dict, save_path.replace('.pt', '.json'), is_training=False)\n",
    "  end = time.time()\n",
    "  hours, rem = divmod(end-start, 3600)\n",
    "  minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "  print(f'Test_loss: {test_loss:.4f} test_acc: {test_acc:.4f}')\n",
    "  print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "  print(cr)\n",
    "\n",
    "  return test_loss, test_acc\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load('debug_last.pt'))\n",
    "# model.load_state_dict(torch.load('best_debug/deu_debug_best_'+str(best_epoch)+'.pt'))\n",
    "\n",
    "test_loss, test_acc = validate(model, test_loader, optimizer, rev_label_dict, label_dict, save_path)\n",
    "print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e3409ea685db85227fbd9509d1b1ace14d085473eb2d57f3ba9dd0302d25f838"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

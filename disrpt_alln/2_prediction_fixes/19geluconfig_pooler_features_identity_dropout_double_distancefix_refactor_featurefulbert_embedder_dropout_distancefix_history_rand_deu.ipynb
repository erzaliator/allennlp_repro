{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seeding for comparing experiment in part 2\n",
    "import torch\n",
    "import json\n",
    "SEED = 2011\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('cuda:3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNLI Bert\n",
    "## Second Tutorial\n",
    "https://towardsdatascience.com/fine-tuning-pre-trained-transformer-models-for-sentence-entailment-d87caf9ec9db\n",
    "Check his Github code for complete notebook. I never referred to it. Medium was enough.\n",
    "BERT in keras-tf: https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define macros\n",
    "BERT_MODEL = 'bert-base-german-cased'\n",
    "batch_size = 4\n",
    "batches_per_epoch = 110\n",
    "\n",
    "save_path_suffix = '19geluconfig_pooler_features_identity_dropout_double_refactor_featurefulbertemb_dropout_distancefix_history_rand_deu_'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# custom reader needed to handle quotechars\n",
    "def read_df_custom(file):\n",
    "    header = 'doc     unit1_toks      unit2_toks      unit1_txt       unit2_txt       s1_toks s2_toks unit1_sent      unit2_sent      dir     nuc_children    sat_children    genre   u1_discontinuous        u2_discontinuous       u1_issent        u2_issent       u1_length       u2_length       length_ratio    u1_speaker      u2_speaker      same_speaker    u1_func u1_pos  u1_depdir       u2_func u2_pos  u2_depdir       doclen  u1_position      u2_position     percent_distance        distance        lex_overlap_words       lex_overlap_length      unit1_case      unit2_case      label'\n",
    "    extracted_columns = ['unit1_txt', 'unit1_sent', 'unit2_txt', 'unit2_sent', 'dir', 'label', 'distance', 'u1_depdir', 'u2_depdir', 'u2_func', 'u1_position', 'u2_position', 'sat_children', 'nuc_children', 'genre', 'unit1_case', 'unit2_case',\n",
    "                            'u1_discontinuous', 'u2_discontinuous', 'same_speaker', 'lex_overlap_length', 'u1_func']\n",
    "    header = header.split()\n",
    "    df = pd.DataFrame(columns=extracted_columns)\n",
    "    file = open(file, 'r')\n",
    "\n",
    "    rows = []\n",
    "    count = 0 \n",
    "    for line in file:\n",
    "        line = line[:-1].split('\\t')\n",
    "        count+=1\n",
    "        if count ==1: continue\n",
    "        row = {}\n",
    "        for column in extracted_columns:\n",
    "            index = header.index(column)\n",
    "            row[column] = line[index]\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame.from_records(rows)])\n",
    "    return df\n",
    "\n",
    "train_df = read_df_custom('../../processed/deu.rst.pcc_train_enriched.rels')[:1000]\n",
    "test_df = read_df_custom('../../processed/deu.rst.pcc_test_enriched.rels')\n",
    "val_df = read_df_custom('../../processed/deu.rst.pcc_dev_enriched.rels')\n",
    "lang = 'deu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping any empty values\n",
    "train_df.dropna(inplace=True)\n",
    "val_df.dropna(inplace=True)\n",
    "test_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare a dataset handler class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'15': 1,\n",
       "         '1': 142,\n",
       "         '2': 41,\n",
       "         '3': 23,\n",
       "         '5': 9,\n",
       "         '6': 4,\n",
       "         '4': 10,\n",
       "         '7': 3,\n",
       "         '9': 3,\n",
       "         '12': 1,\n",
       "         '14': 1,\n",
       "         '8': 1,\n",
       "         '16': 1,\n",
       "         '10': 1})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "Counter(val_df['distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unit1_txt</th>\n",
       "      <th>unit1_sent</th>\n",
       "      <th>unit2_txt</th>\n",
       "      <th>unit2_sent</th>\n",
       "      <th>dir</th>\n",
       "      <th>label</th>\n",
       "      <th>distance</th>\n",
       "      <th>u1_depdir</th>\n",
       "      <th>u2_depdir</th>\n",
       "      <th>u2_func</th>\n",
       "      <th>...</th>\n",
       "      <th>sat_children</th>\n",
       "      <th>nuc_children</th>\n",
       "      <th>genre</th>\n",
       "      <th>unit1_case</th>\n",
       "      <th>unit2_case</th>\n",
       "      <th>u1_discontinuous</th>\n",
       "      <th>u2_discontinuous</th>\n",
       "      <th>same_speaker</th>\n",
       "      <th>lex_overlap_length</th>\n",
       "      <th>u1_func</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dagmar Ziegler sitzt in der Schuldenfalle .</td>\n",
       "      <td>Dagmar Ziegler sitzt in der Schuldenfalle .</td>\n",
       "      <td>hat sie jetzt eine seit mehr als einem Jahr er...</td>\n",
       "      <td>Auf Grund der dramatischen Kassenlage in Brand...</td>\n",
       "      <td>1&gt;2</td>\n",
       "      <td>interpretation</td>\n",
       "      <td>2</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>root</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>news</td>\n",
       "      <td>cap_initial</td>\n",
       "      <td>other</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>root</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Auf Grund der dramatischen Kassenlage in Brand...</td>\n",
       "      <td>Auf Grund der dramatischen Kassenlage in Brand...</td>\n",
       "      <td>hat sie jetzt eine seit mehr als einem Jahr er...</td>\n",
       "      <td>Auf Grund der dramatischen Kassenlage in Brand...</td>\n",
       "      <td>1&gt;2</td>\n",
       "      <td>cause</td>\n",
       "      <td>1</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>root</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>news</td>\n",
       "      <td>cap_initial</td>\n",
       "      <td>other</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>obl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hat sie jetzt eine seit mehr als einem Jahr er...</td>\n",
       "      <td>Auf Grund der dramatischen Kassenlage in Brand...</td>\n",
       "      <td>Der Rückzieher der Finanzministerin ist aber v...</td>\n",
       "      <td>Der Rückzieher der Finanzministerin ist aber v...</td>\n",
       "      <td>1&gt;2</td>\n",
       "      <td>evaluation-n</td>\n",
       "      <td>4</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>root</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>news</td>\n",
       "      <td>other</td>\n",
       "      <td>cap_initial</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>root</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hat sie jetzt eine seit mehr als einem Jahr er...</td>\n",
       "      <td>Auf Grund der dramatischen Kassenlage in Brand...</td>\n",
       "      <td>und vorgeschlagen , erst 2003 darüber zu entsc...</td>\n",
       "      <td>Auf Grund der dramatischen Kassenlage in Brand...</td>\n",
       "      <td>1&lt;2</td>\n",
       "      <td>conjunction</td>\n",
       "      <td>1</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>LEFT</td>\n",
       "      <td>conj</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>news</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>root</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hat sie jetzt eine seit mehr als einem Jahr er...</td>\n",
       "      <td>Auf Grund der dramatischen Kassenlage in Brand...</td>\n",
       "      <td>Überraschend ,</td>\n",
       "      <td>Überraschend , weil das Finanz- und das Bildun...</td>\n",
       "      <td>1&lt;2</td>\n",
       "      <td>interpretation</td>\n",
       "      <td>2</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>root</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>news</td>\n",
       "      <td>other</td>\n",
       "      <td>title</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>root</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           unit1_txt  \\\n",
       "0        Dagmar Ziegler sitzt in der Schuldenfalle .   \n",
       "1  Auf Grund der dramatischen Kassenlage in Brand...   \n",
       "2  hat sie jetzt eine seit mehr als einem Jahr er...   \n",
       "3  hat sie jetzt eine seit mehr als einem Jahr er...   \n",
       "4  hat sie jetzt eine seit mehr als einem Jahr er...   \n",
       "\n",
       "                                          unit1_sent  \\\n",
       "0        Dagmar Ziegler sitzt in der Schuldenfalle .   \n",
       "1  Auf Grund der dramatischen Kassenlage in Brand...   \n",
       "2  Auf Grund der dramatischen Kassenlage in Brand...   \n",
       "3  Auf Grund der dramatischen Kassenlage in Brand...   \n",
       "4  Auf Grund der dramatischen Kassenlage in Brand...   \n",
       "\n",
       "                                           unit2_txt  \\\n",
       "0  hat sie jetzt eine seit mehr als einem Jahr er...   \n",
       "1  hat sie jetzt eine seit mehr als einem Jahr er...   \n",
       "2  Der Rückzieher der Finanzministerin ist aber v...   \n",
       "3  und vorgeschlagen , erst 2003 darüber zu entsc...   \n",
       "4                                     Überraschend ,   \n",
       "\n",
       "                                          unit2_sent  dir           label  \\\n",
       "0  Auf Grund der dramatischen Kassenlage in Brand...  1>2  interpretation   \n",
       "1  Auf Grund der dramatischen Kassenlage in Brand...  1>2           cause   \n",
       "2  Der Rückzieher der Finanzministerin ist aber v...  1>2    evaluation-n   \n",
       "3  Auf Grund der dramatischen Kassenlage in Brand...  1<2     conjunction   \n",
       "4  Überraschend , weil das Finanz- und das Bildun...  1<2  interpretation   \n",
       "\n",
       "  distance u1_depdir u2_depdir u2_func  ... sat_children nuc_children genre  \\\n",
       "0        2      ROOT      ROOT    root  ...            0            4  news   \n",
       "1        1     RIGHT      ROOT    root  ...            0            4  news   \n",
       "2        4      ROOT      ROOT    root  ...            4            3  news   \n",
       "3        1      ROOT      LEFT    conj  ...            0            4  news   \n",
       "4        2      ROOT      ROOT    root  ...            1            4  news   \n",
       "\n",
       "    unit1_case   unit2_case u1_discontinuous u2_discontinuous same_speaker  \\\n",
       "0  cap_initial        other            False            False         True   \n",
       "1  cap_initial        other            False            False         True   \n",
       "2        other  cap_initial            False            False         True   \n",
       "3        other        other            False            False         True   \n",
       "4        other        title            False            False         True   \n",
       "\n",
       "  lex_overlap_length u1_func  \n",
       "0                  0    root  \n",
       "1                  0     obl  \n",
       "2                  0    root  \n",
       "3                  0    root  \n",
       "4                  0    root  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['unit1_txt', 'unit1_sent', 'unit2_txt', 'unit2_sent', 'dir', 'label',\n",
       "       'distance', 'u1_depdir', 'u2_depdir', 'u2_func', 'u1_position',\n",
       "       'u2_position', 'sat_children', 'nuc_children', 'genre', 'unit1_case',\n",
       "       'unit2_case', 'u1_discontinuous', 'u2_discontinuous', 'same_speaker',\n",
       "       'lex_overlap_length', 'u1_func'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-09 08:54:37.093378: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-09 08:54:37.341533: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda-10.1/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-10.2/targets/x86_64-linux/lib/:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/home/VD/kaveri/anaconda3/envs/py310/lib/\n",
      "2022-12-09 08:54:37.341585: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-09 08:54:37.400639: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-12-09 08:54:38.357327: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda-10.1/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-10.2/targets/x86_64-linux/lib/:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/home/VD/kaveri/anaconda3/envs/py310/lib/\n",
      "2022-12-09 08:54:38.357483: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda-10.1/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-10.2/targets/x86_64-linux/lib/:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/home/VD/kaveri/anaconda3/envs/py310/lib/\n",
      "2022-12-09 08:54:38.357496: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing.sharedctypes import Value\n",
    "import torch\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, ConcatDataset\n",
    "from sys import path\n",
    "path.append('/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/allennlp/data/data_loaders/')\n",
    "from allennlp.data import allennlp_collate, Vocabulary\n",
    "from features_custom2 import get_vocab_feature_name\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "class MNLIDataBert(Dataset):\n",
    "\n",
    "  def __init__(self, train_df, val_df, test_df):\n",
    "    self.lang = lang\n",
    "    self.num_labels = set()\n",
    "    self.train_df = train_df\n",
    "    self.val_df = val_df\n",
    "    self.test_df = test_df\n",
    "\n",
    "    self.tokenizer = BertTokenizer.from_pretrained(BERT_MODEL, do_lower_case=True) # Using a pre-trained BERT tokenizer to encode sentences\n",
    "    self.train_data = None\n",
    "    self.val_data = None\n",
    "    self.test_data = None\n",
    "    self.train_idx = None\n",
    "    self.val_idx = None\n",
    "    self.test_idx = None\n",
    "    self.vocab = Vocabulary(counter=None, max_vocab_size=100000)\n",
    "    self.init_data()\n",
    "\n",
    "  def init_data(self):\n",
    "    self.get_label_mapping()\n",
    "    self.init_feature_list()\n",
    "    self.init_feature_mappings_and_bins()\n",
    "    self.apply_bins()\n",
    "    self.calculate_unique_values()\n",
    "    self.train_data, self.train_idx = self.load_data(self.train_df)\n",
    "    self.val_data, self.val_idx = self.load_data(self.val_df)\n",
    "    self.test_data, self.test_idx = self.load_data(self.test_df)\n",
    "    \n",
    "\n",
    "  def combine_unique_column_values_to_dict(self, column_name):\n",
    "    ini_set = set([*self.train_df[column_name].unique(), *self.val_df[column_name].unique()])\n",
    "    res = dict.fromkeys(ini_set, 0)\n",
    "    return res\n",
    "\n",
    "  def get_label_mapping(self):\n",
    "    labels = {}\n",
    "    labels_list = list(set(list(self.train_df['label'].unique()) + list(self.test_df['label'].unique()) + list(self.val_df['label'].unique())))\n",
    "    for i in range(len(labels_list)):\n",
    "        labels[labels_list[i]] = i\n",
    "    self.label_dict = labels\n",
    "    # needed later for classification report object to generate precision and recall on test dataset\n",
    "    self.rev_label_dict = {self.label_dict[k]:k for k in self.label_dict.keys()} \n",
    "\n",
    "  def init_feature_mappings_and_bins(self):\n",
    "    self.feature_maps = { 'genre': self.combine_unique_column_values_to_dict('genre'),\n",
    "                          'unit1_case': self.combine_unique_column_values_to_dict('unit1_case'),\n",
    "                          'unit2_case': self.combine_unique_column_values_to_dict('unit2_case'),\n",
    "                          'u1_func': self.combine_unique_column_values_to_dict('u1_func'),\n",
    "                          'u2_func': self.combine_unique_column_values_to_dict('u2_func') }\n",
    "\n",
    "    self.bins = {\n",
    "      'distance': [[-1e9, -8], [-8, -2], [-2, 0], [0, 2], [2, 8], [8, 1e9]],\n",
    "      'u1_position': [[0.0, 0.1], [0.1, 0.2], [0.2, 0.3], [0.3, 0.4], [0.4, 0.5], [0.5, 0.6], [0.6, 0.7], [0.7, 0.8], [0.8, 0.9], [0.9, 1.0], [1.0, 1e9]],\n",
    "      'u2_position': [[0.0, 0.1], [0.1, 0.2], [0.2, 0.3], [0.3, 0.4], [0.4, 0.5], [0.5, 0.6], [0.6, 0.7], [0.7, 0.8], [0.8, 0.9], [0.9, 1.0], [1.0, 1e9]],\n",
    "      'lex_overlap_length': [[0, 2], [2, 7], [7, 1e9]]\n",
    "    }   \n",
    "\n",
    "  def add_directionality(self, premise, hypothesis, dir):\n",
    "    if dir == \"1<2\":\n",
    "        hypothesis = '< ' + hypothesis + ' {'\n",
    "    else:\n",
    "        premise = '} ' + premise + ' >'\n",
    "    return premise, hypothesis\n",
    "\n",
    "  def init_feature_list(self):\n",
    "    if self.lang=='nld':\n",
    "      self.feature_list = ['distance', 'u1_depdir', 'sat_children', 'genre', 'u1_position']\n",
    "    elif self.lang=='deu':\n",
    "      self.feature_list = ['distance', 'u1_depdir', 'u2_depdir', 'u2_func', 'u1_position', 'u2_position', 'sat_children', 'nuc_children']\n",
    "    elif self.lang=='eng.rst.gum':\n",
    "      self.feature_list = ['distance', 'same_speaker', 'u2_func', 'u2_depdir', 'unit1_case', 'unit2_case', 'nuc_children',\n",
    "                      'sat_children', 'genre', 'lex_overlap_length', 'u2_discontinuous', 'u1_discontinuous', 'u1_position', 'u2_position']\n",
    "    elif self.lang=='fas':\n",
    "      self.feature_list = ['distance', 'nuc_children', 'sat_children', 'u2_discontinuous', 'genre']\n",
    "    elif self.lang=='spa.rst.sctb':\n",
    "      self.feature_list = ['distance', 'u1_position', 'sat_children']\n",
    "    elif self.lang=='zho.rst.sctb':\n",
    "      self.feature_list = ['sat_children', 'nuc_children', 'genre', 'u2_discontinuous', 'u1_discontinuous', 'u1_depdir', 'u1_func']\n",
    "    else: \n",
    "      raise ValueError()\n",
    "\n",
    "  def get_mapping_from_dictionary(self, column_name, dict_val):\n",
    "    return self.feature_maps[column_name][dict_val]\n",
    "\n",
    "  def get_allen_features_list(self, features, feature_name):\n",
    "    if feature_name in ['distance', 'u1_depdir', 'u2_depdir', 'u1_func', 'u2_func', \n",
    "    'u1_position', 'u2_position', 'genre', 'same_speaker', 'unit1_case', 'unit2_case',\n",
    "    'lex_overlap_length', 'u2_discontinuous', 'u1_discontinuous', 'dir']: feature_value = self.apply_vocab(features[feature_name], feature_name) #for categorical values\n",
    "    elif feature_name in ['sat_children', 'nuc_children']: feature_value = float(features[feature_name]) #for identiy values\n",
    "    else: \n",
    "      print(feature_name)\n",
    "      raise ValueError()\n",
    "    return feature_value\n",
    "\n",
    "  def transform_feature(self, features):\n",
    "    assert len(features)==17\n",
    "    #after applying the vocab. we need to pass them as int\n",
    "    return {feature_name: torch.tensor(int(self.get_allen_features_list(features, feature_name))).to(device) for feature_name in self.feature_list+['dir']}\n",
    "\n",
    "  def calculate_unique_values(self):\n",
    "    for feature_name in self.feature_list+['dir']:\n",
    "      vocab_feature_name = get_vocab_feature_name(feature_name)\n",
    "      self.vocab.add_tokens_to_namespace(train_df[feature_name].apply(lambda x: str(x)), namespace=vocab_feature_name)\n",
    "      self.vocab.add_tokens_to_namespace(val_df[feature_name].apply(lambda x: str(x)), namespace=vocab_feature_name)\n",
    "\n",
    "  def apply_bins(self):\n",
    "    for df in [self.train_df, self.test_df, self.val_df]:\n",
    "      for feature_name in self.bins.keys():\n",
    "        df[feature_name] = df[feature_name].apply(lambda x: self.get_mapping_from_bin(feature_name, float(x)))\n",
    "\n",
    "  def get_mapping_from_bin(self, column_name, dict_val):\n",
    "    bins = self.bins[column_name]\n",
    "    for b,i in zip(bins, range(len(bins))):\n",
    "      left = b[0]\n",
    "      right = b[1]\n",
    "      if left<=dict_val and right>=dict_val: return i\n",
    "\n",
    "  def apply_vocab(self, feature_value, feature_name):\n",
    "    return self.vocab.get_token_index(str(feature_value), namespace=get_vocab_feature_name(feature_name))\n",
    "\n",
    "  def set_labels(self):\n",
    "    self.num_labels = len(self.num_labels)\n",
    "    \n",
    "  def load_data(self, df):\n",
    "    MAX_LEN = 512 \n",
    "    token_ids = []\n",
    "    mask_ids = []\n",
    "    seg_ids = []\n",
    "    y = []\n",
    "    feats = []\n",
    "    idx = []\n",
    "    idx_map = {}\n",
    "\n",
    "    self.num_labels.update(df['label'].unique())\n",
    "\n",
    "    count=0\n",
    "    for row in df.iterrows():\n",
    "      row = row[1]\n",
    "      premise = row['unit1_txt']\n",
    "      hypothesis = row['unit2_txt']\n",
    "      label = row['label']\n",
    "      dir = row['dir']\n",
    "\n",
    "      features = {'distance': row['distance'],\n",
    "                'u1_depdir': row['u1_depdir'],\n",
    "                'u2_depdir': row['u2_depdir'],\n",
    "                'u1_func': row['u1_func'],\n",
    "                'u2_func': row['u2_func'],\n",
    "                'u1_position': row['u1_position'],\n",
    "                'u2_position': row['u2_position'],\n",
    "                'sat_children': row['sat_children'],\n",
    "                'nuc_children': row['nuc_children'],\n",
    "                'genre': row['genre'],\n",
    "                'unit1_case': row['unit1_case'],\n",
    "                'unit2_case': row['unit2_case'],\n",
    "                'u1_discontinuous': row['u1_discontinuous'],\n",
    "                'u2_discontinuous': row['u2_discontinuous'],\n",
    "                'same_speaker': row['same_speaker'],\n",
    "                'lex_overlap_length': row['lex_overlap_length'],\n",
    "                'dir': row['dir']}\n",
    "\n",
    "      premise, hypothesis = self.add_directionality(premise, hypothesis, dir)\n",
    "      encoded = self.tokenizer.encode_plus(premise, hypothesis, add_special_tokens = True, max_length=MAX_LEN, truncation=True, padding=False) #padding='max_length'\n",
    "      pair_token_ids = torch.tensor(encoded['input_ids'])\n",
    "\n",
    "      segment_ids = torch.tensor(encoded['token_type_ids'])\n",
    "      attention_mask_ids = torch.tensor(encoded['attention_mask'])\n",
    "      assert len(pair_token_ids)==len(attention_mask_ids)\n",
    "\n",
    "      features = self.transform_feature(features)\n",
    "\n",
    "      token_ids.append(pair_token_ids)\n",
    "      seg_ids.append(segment_ids)\n",
    "      mask_ids.append(attention_mask_ids)\n",
    "      y.append(self.label_dict[label])\n",
    "      feats.append(features)\n",
    "      \n",
    "      idx_map[count] = [premise, hypothesis]\n",
    "      idx.append(count)\n",
    "      count+=1\n",
    "      \n",
    "    token_ids = pad_sequence(token_ids, batch_first=True)\n",
    "    mask_ids = pad_sequence(mask_ids, batch_first=True)\n",
    "    seg_ids = pad_sequence(seg_ids, batch_first=True)\n",
    "    y = torch.tensor(y)\n",
    "    idx = torch.tensor(idx)\n",
    "\n",
    "    class featureDataset(Dataset):\n",
    "      def __init__(self, token_ids, mask_ids, seg_ids, feats, y, idx):\n",
    "          self.token_ids = token_ids\n",
    "          self.mask_ids = mask_ids\n",
    "          self.seg_ids = seg_ids\n",
    "          self.feats = feats\n",
    "          self.y = y\n",
    "          self.idx = idx\n",
    "\n",
    "      def __len__(self):\n",
    "          return len(self.feats)\n",
    "\n",
    "      def __getitem__(self, idx):\n",
    "          return self.token_ids[idx], self.mask_ids[idx], self.seg_ids[idx], self.feats[idx], self.y[idx], self.idx[idx]\n",
    "\n",
    "    dataset = featureDataset(token_ids, mask_ids, seg_ids, feats, y, idx)\n",
    "    return dataset, idx_map\n",
    "\n",
    "  def get_data_loaders(self, batch_size=4, batches_per_epoch=402, shuffle=True): #1609 samples / 64:25=1600 / 402:4=1608\n",
    "    self.set_labels()\n",
    "    train_loader_torch = DataLoader(\n",
    "      self.train_data,\n",
    "      shuffle=shuffle,\n",
    "      batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    val_loader_torch = DataLoader(\n",
    "      self.val_data,\n",
    "      shuffle=shuffle,\n",
    "      batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    test_loader_torch = DataLoader(\n",
    "      self.test_data,\n",
    "      shuffle=False,\n",
    "      batch_size=batch_size,\n",
    "    )\n",
    "    \n",
    "    train_loader = LoaderWrapper(train_loader_torch, n_step=batches_per_epoch)\n",
    "    val_loader = LoaderWrapper(val_loader_torch, n_step=batches_per_epoch)\n",
    "    test_loader = LoaderWrapper(test_loader_torch, n_step=batches_per_epoch)\n",
    "\n",
    "    return train_loader, val_loader_torch, test_loader_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoaderWrapper:\n",
    "    def __init__(self, loader, n_step):\n",
    "        self.step = n_step\n",
    "        self.idx = 0\n",
    "        self.iter_loader = iter(loader)\n",
    "        self.loader = loader\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.step\n",
    "\n",
    "    def __next__(self):\n",
    "        # if reached number of steps desired, stop\n",
    "        if self.idx == self.step:\n",
    "            self.idx = 0\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            self.idx += 1\n",
    "        # while True\n",
    "        try:\n",
    "            return next(self.iter_loader)\n",
    "        except StopIteration:\n",
    "            # reinstate iter_loader, then continue\n",
    "            self.iter_loader = iter(self.loader)\n",
    "            return next(self.iter_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_dataset = MNLIDataBert(train_df, val_df, test_df)\n",
    "\n",
    "train_loader, val_loader, test_loader = mnli_dataset.get_data_loaders(batch_size=batch_size, batches_per_epoch=batches_per_epoch) #64X250\n",
    "label_dict = mnli_dataset.label_dict # required by custom func to calculate accuracy, bert model\n",
    "rev_label_dict = mnli_dataset.rev_label_dict # required by custom func to calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '3': 2, '4': 3, '5': 4}\n",
      "u1_depdir :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, 'ROOT': 2, 'RIGHT': 3, 'LEFT': 4}\n",
      "u2_depdir :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, 'ROOT': 2, 'RIGHT': 3, 'LEFT': 4}\n",
      "u2_func :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, 'root': 2, 'conj': 3, 'advcl': 4, 'acl': 5, 'xcomp': 6, 'obl': 7, 'ccomp': 8, 'parataxis': 9, 'advmod': 10, 'dep': 11, 'csubj': 12, 'nmod': 13, 'punct': 14, 'cc': 15, 'appos': 16, 'aux': 17, 'obj': 18, 'iobj': 19, 'nsubj': 20}\n",
      "u1_position :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '0': 2, '2': 3, '3': 4, '4': 5, '5': 6, '6': 7, '7': 8, '8': 9, '9': 10, '1': 11}\n",
      "u2_position :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '0': 2, '2': 3, '1': 4, '3': 5, '4': 6, '5': 7, '6': 8, '7': 9, '8': 10, '9': 11}\n",
      "sat_children :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '0': 2, '4': 3, '1': 4, '2': 5, '3': 6, '5': 7, '6': 8}\n",
      "nuc_children :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '4': 2, '3': 3, '1': 4, '2': 5, '5': 6, '6': 7, '7': 8, '8': 9}\n",
      "dir :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '1>2': 2, '1<2': 3}\n"
     ]
    }
   ],
   "source": [
    "for feature in mnli_dataset.feature_list:\n",
    "    vocab_feature_name = get_vocab_feature_name(feature)\n",
    "    print(feature, ': ', mnli_dataset.vocab.get_token_to_index_vocabulary(vocab_feature_name))\n",
    "print('dir', ': ', mnli_dataset.vocab.get_token_to_index_vocabulary('dir'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (pair_token_ids, mask_ids, seg_ids, feat, y, idx) in enumerate(train_loader):\n",
    "    # assert pair_token_ids.shape[-1]==512 #torch.Size([4, 512])\n",
    "    # assert mask_ids.shape[-1]==512\n",
    "    # assert seg_ids.shape[-1]==512\n",
    "    assert len(feat)==len(mnli_dataset.feature_list)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from torch import optim\n",
    "import os\n",
    "path.append(os.path.join(os.getcwd(), '../utils/'))\n",
    "from CategoricalAccuracy import CategoricalAccuracy as CA\n",
    "import numpy as np\n",
    "\n",
    "ca = CA()\n",
    "\n",
    "x = torch.tensor(np.array([[[1,0,0], [1,0,0], [1,0,0]]]))\n",
    "y1 = torch.tensor(np.array([[0], [1], [1]]))\n",
    "y2 = torch.tensor(np.array([[0], [0], [0]]))\n",
    "\n",
    "ca(x,y1)\n",
    "print(ca.get_metric(reset=True))\n",
    "ca(x,y2)\n",
    "print(ca.get_metric(reset=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '@@PADDING@@', 1: '@@UNKNOWN@@'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnli_dataset.vocab.get_index_to_token_vocabulary('u1_depdir')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define evaulation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to evaluate model for train and test. And also use classification report for testing\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# helper function to calculate the batch accuracy\n",
    "def multi_acc(y_pred, y_test, allennlp=False):\n",
    "  if allennlp==False:\n",
    "    acc = (torch.log_softmax(y_pred, dim=1).argmax(dim=1) == y_test).sum().float() / float(y_test.size(0))\n",
    "    return acc\n",
    "\n",
    "# freeze model weights and measure validation / test \n",
    "def evaluate_accuracy(model, optimizer, data_loader, rev_label_dict, label_dict, is_training=True):\n",
    "  model.eval()\n",
    "  total_val_acc  = 0\n",
    "  total_val_loss = 0\n",
    "  \n",
    "  #for classification report\n",
    "  y_true = []\n",
    "  y_pred = []\n",
    "  idx_list = []\n",
    "  premise_list = []\n",
    "  hypo_list = []\n",
    "  idx_map = mnli_dataset.val_idx if is_training else mnli_dataset.test_idx\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for batch_idx, (pair_token_ids, mask_ids, seg_ids, feat, y, idx) in enumerate(data_loader):      \n",
    "      optimizer.zero_grad()\n",
    "      pair_token_ids = pair_token_ids.to(device)\n",
    "      mask_ids = mask_ids.to(device)\n",
    "      seg_ids = seg_ids.to(device)\n",
    "      labels = y.to(device)\n",
    "      # feat = feat.to(device)\n",
    "      \n",
    "      outputs = model(pair_token_ids, \n",
    "                            token_type_ids=seg_ids, \n",
    "                            attention_mask=mask_ids, \n",
    "                            feat=feat)\n",
    "      criterion = nn.CrossEntropyLoss()\n",
    "      loss = criterion(outputs, labels)\n",
    "      acc = multi_acc(outputs, labels)\n",
    "\n",
    "      total_val_loss += loss.item()\n",
    "      total_val_acc  += acc.item()\n",
    "\n",
    "      # log predictions for classification report\n",
    "      argmax_predictions = torch.argmax(outputs,dim=1).tolist()\n",
    "      labels_list = labels.tolist()\n",
    "      assert(len(labels_list)==len(argmax_predictions))\n",
    "      for p in argmax_predictions: y_pred.append(rev_label_dict[int(p)])\n",
    "      for l in labels_list: y_true.append(rev_label_dict[l])\n",
    "      for i in idx.tolist():\n",
    "        idx_list.append(i)\n",
    "        if i not in idx_map.keys():\n",
    "          print(idx_map)\n",
    "        premise_list.append(idx_map[i][0])\n",
    "        hypo_list.append(idx_map[i][1])\n",
    "\n",
    "  val_acc  = total_val_acc/len(data_loader)\n",
    "  val_loss = total_val_loss/len(data_loader)\n",
    "  cr = classification_report(y_true, y_pred)\n",
    "\n",
    "  idx_json = {'idx': idx_list, 'gold_label': y_true, 'pred_label': y_pred, 'premise': premise_list, 'hypothesis': hypo_list}\n",
    "  \n",
    "  return val_acc, val_loss, cr, model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define custom bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASSIGN: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing FeaturefulBert: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing FeaturefulBert from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FeaturefulBert from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768, 768])\n",
      "torch.Size([768, 768])\n",
      "Parameter containing:\n",
      "tensor([[0.0291, 0.6613, 0.4690,  ..., 0.6157, 0.9927, 0.5098],\n",
      "        [0.0258, 0.0247, 0.4681,  ..., 0.6155, 0.4070, 0.0065],\n",
      "        [0.1497, 0.2382, 0.9596,  ..., 0.0467, 0.0159, 0.1928],\n",
      "        ...,\n",
      "        [0.1729, 0.8596, 0.7944,  ..., 0.0689, 0.4413, 0.7067],\n",
      "        [0.8094, 0.8434, 0.3942,  ..., 0.0211, 0.7028, 0.7862],\n",
      "        [0.2945, 0.2699, 0.1790,  ..., 0.6471, 0.8178, 0.1238]],\n",
      "       requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from typing import Any, Dict, Optional\n",
    "from transformers import BertModel, AutoTokenizer, BertConfig\n",
    "from transformers.models.bert.modeling_bert import BertPooler\n",
    "import torch.nn as nn\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from featurefulbertembedder_custom2 import FeaturefulBertEmbedder\n",
    "from featureful_bert_custom2 import get_combined_feature_tensor_2 as get_combined_feature_tensor_forward\n",
    "from featureful_bert_custom2 import get_feature_modules\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "class CustomPooler2(nn.Module):\n",
    "    def __init__(self, *,\n",
    "                        requires_grad: bool = True,\n",
    "                        dropout: float = 0.0,\n",
    "                        transformer_kwargs: Optional[Dict[str, Any]] = None, ) -> None:\n",
    "        super().__init__()\n",
    "        bert = BertModel.from_pretrained(BERT_MODEL) #only used to pass config. BertAttentionClass used in FeatureFulBert\n",
    "        self._dropout = torch.nn.Dropout(p=dropout)\n",
    "        self.pooler = copy.deepcopy(bert.pooler)\n",
    "        print(self.pooler.dense.weight.shape)\n",
    "        self.pooler.dense.weight = nn.Parameter(torch.rand(self.pooler.dense.weight.shape))\n",
    "        self.pooler.dense.bias = nn.Parameter(torch.rand(self.pooler.dense.bias.shape))\n",
    "        print(self.pooler.dense.weight.shape)\n",
    "        for param in self.pooler.parameters():\n",
    "            param.requires_grad = requires_grad\n",
    "        self._embedding_dim = bert.config.hidden_size\n",
    "\n",
    "    def get_input_dim(self) -> int:\n",
    "        return self._embedding_dim\n",
    "\n",
    "    def get_output_dim(self) -> int:\n",
    "        return self._embedding_dim\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, mask: torch.BoolTensor = None, num_wrapping_dims: int = 0):\n",
    "        pooler = self.pooler\n",
    "        \n",
    "        for _ in range(num_wrapping_dims):\n",
    "            pooler = TimeDistributed(pooler)\n",
    "        pooled = pooler(tokens)\n",
    "        pooled = self._dropout(pooled)\n",
    "        return pooled\n",
    "\n",
    "class MyModule(nn.Module):    \n",
    "    def __init__(self, feature_list, vocab):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.feature_list = feature_list\n",
    "        self.feature_modules, self._feature_module_size = get_feature_modules(feature_list, vocab)\n",
    "\n",
    "    def forward(self, features):\n",
    "        return get_combined_feature_tensor_forward(features, self.feature_list, self.feature_modules)\n",
    "\n",
    "class CustomBERTModel(nn.Module):\n",
    "    def __init__(self, num_labels, vocab):\n",
    "          super(CustomBERTModel, self).__init__()\n",
    "          self.num_classes = num_labels\n",
    "          self.feature_list = mnli_dataset.feature_list\n",
    "          print('ASSIGN:', self.num_classes)\n",
    "\n",
    "          self.embedder = self.create_featureful_bert()\n",
    "          self.encoder = CustomPooler2()\n",
    "          self.module1 = MyModule(self.feature_list, vocab)\n",
    "          self.dropout1 = nn.Dropout(p=0.0)\n",
    "        #   self.dropout_decoder = nn.Dropout(p=0.5)\n",
    "          self._decoder_input_size = self.encoder._embedding_dim + self.module1._feature_module_size\n",
    "          self.relation_decoder = nn.Linear(self._decoder_input_size, self.num_classes)\n",
    "\n",
    "          self.history_w = {\n",
    "            'cos': [],\n",
    "            # 'l1_linear': [],\n",
    "            # 'l2': []\n",
    "          }\n",
    "          self.pooler_weight = copy.deepcopy(self.encoder.pooler.dense.weight)\n",
    "          print(self.pooler_weight)\n",
    "\n",
    "    def forward(self, pair_token_ids, token_type_ids, attention_mask, feat):\n",
    "        direction_tensor = feat['dir'].to(device)\n",
    "        embedded_sentence = self.embedder(token_ids=pair_token_ids, #featurefulmebedder\n",
    "                        mask=attention_mask, \n",
    "                        type_ids=token_type_ids,\n",
    "                        segment_concat_mask = None,\n",
    "                        direction_tensor = direction_tensor,\n",
    "                        feature_list = self.feature_list,\n",
    "                        features = feat)\n",
    "        mask = token_type_ids\n",
    "        bertpooler_output = self.encoder(tokens=embedded_sentence, mask=mask)\n",
    "        feat = self.convert_to_feature_list(feat)\n",
    "        feat = self.dropout1(feat)\n",
    "        feat = self.module1(feat)\n",
    "        # print(bertpooler_output.shape, self.module1._feature_module_size, feat.shape)\n",
    "        try:\n",
    "            feat_concat = torch.concat((bertpooler_output, feat),-1)\n",
    "        except:\n",
    "            print(bertpooler_output.shape, feat.shape)\n",
    "            raise ValueError()\n",
    "        assert feat_concat.shape[-1] == self._decoder_input_size\n",
    "        feat_concat = self.dropout1(feat_concat)\n",
    "        # feat_concat = self.dropout_decoder(feat_concat)\n",
    "        linear1_output = self.relation_decoder(feat_concat)\n",
    "        return linear1_output\n",
    "\n",
    "    def compute_pooler_similarity(self):\n",
    "        cur = self.encoder.pooler.dense.weight\n",
    "        pre = self.pooler_weight\n",
    "        print(cur)\n",
    "        print(pre)\n",
    "        assert not torch.all(cur.eq(pre))\n",
    "        for metric in self.history_w.keys():\n",
    "            self.history_w[metric].append(self.similarity(cur, pre, metric))\n",
    "        self.pooler_weight = copy.deepcopy(self.encoder.pooler.dense.weight)\n",
    "\n",
    "    def similarity(self, cur, pre, metric):\n",
    "        metric = 0\n",
    "        n = 0\n",
    "        for A, B in zip(cur.cpu().detach().numpy(), pre.cpu().detach().numpy()):\n",
    "            cosine = np.dot(A,B)/(norm(A)*norm(B))\n",
    "            metric+= cosine\n",
    "            n+=1\n",
    "        return float(metric)/float(n), metric\n",
    "\n",
    "    def create_bert_without_activations(self):\n",
    "        config = BertConfig.from_pretrained(BERT_MODEL, hidden_act='gelu')\n",
    "        bert = BertModel.from_pretrained(BERT_MODEL, config=config)\n",
    "        return bert\n",
    "\n",
    "    def create_featureful_bert(self):\n",
    "        featureful_bert = FeaturefulBertEmbedder(model_name = BERT_MODEL,\n",
    "                                hidden_activation_allen = 'gelu',\n",
    "                                feature_list = self.feature_list, \n",
    "                                vocab=mnli_dataset.vocab)\n",
    "        return featureful_bert\n",
    "\n",
    "    def convert_to_feature_list(self, feat):\n",
    "        feature_linear = [feat[feature_name] for feature_name in self.feature_list]\n",
    "        feature_linear = torch.stack(feature_linear, dim=-1)\n",
    "        return feature_linear\n",
    "        \n",
    "\n",
    "model = CustomBERTModel(mnli_dataset.num_labels, mnli_dataset.vocab)\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-6, correct_bias=False) # original 2e-5\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.8, mode='max', patience=35, min_lr=5e-7, verbose=True) #original factor=0.6, min_lr=5e-7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define training regime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prinintg model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomBERTModel(\n",
      "  (embedder): FeaturefulBertEmbedder(\n",
      "    (transformer_model): FeaturefulBert(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30000, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "      (feature_modules): ModuleDict(\n",
      "        (distance): Embedding(5, 3, padding_idx=0)\n",
      "        (u1_depdir): Embedding(5, 3, padding_idx=0)\n",
      "        (u2_depdir): Embedding(5, 3, padding_idx=0)\n",
      "        (u2_func): Embedding(21, 5, padding_idx=0)\n",
      "        (u1_position): Embedding(12, 4, padding_idx=0)\n",
      "        (u2_position): Embedding(12, 4, padding_idx=0)\n",
      "        (sat_children): Identity()\n",
      "        (nuc_children): Identity()\n",
      "      )\n",
      "      (feature_projector): Linear(in_features=25, out_features=768, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (encoder): CustomPooler2(\n",
      "    (_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (module1): MyModule(\n",
      "    (feature_modules): ModuleDict(\n",
      "      (distance): Embedding(5, 3, padding_idx=0)\n",
      "      (u1_depdir): Embedding(5, 3, padding_idx=0)\n",
      "      (u2_depdir): Embedding(5, 3, padding_idx=0)\n",
      "      (u2_func): Embedding(21, 5, padding_idx=0)\n",
      "      (u1_position): Embedding(12, 4, padding_idx=0)\n",
      "      (u2_position): Embedding(12, 4, padding_idx=0)\n",
      "      (sat_children): Identity()\n",
      "      (nuc_children): Identity()\n",
      "    )\n",
      "  )\n",
      "  (dropout1): Dropout(p=0.0, inplace=False)\n",
      "  (relation_decoder): Linear(in_features=792, out_features=26, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['events.out.tfevents.1670575858.57e5cab0c4d9.30757.0']\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def writer_init(save_path_suffix):\n",
    "    writer_path = 'run1/'+save_path_suffix[:-1]+'/'\n",
    "    if os.path.isdir(writer_path):\n",
    "        filelist = [ f for f in os.listdir(writer_path) if 'events.out' in f ]\n",
    "        print(filelist)\n",
    "        for f in filelist:\n",
    "            os.remove(os.path.join(writer_path, f))\n",
    "    else:\n",
    "        os.mkdir(writer_path)\n",
    "    writer = SummaryWriter(log_dir=writer_path)\n",
    "    return writer\n",
    "\n",
    "writer = writer_init(save_path_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import traceback\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import Optional, Iterable, Dict, Any\n",
    "from EarlyStopperUtil import MetricTracker\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer, scheduler, rev_label_dict):  \n",
    "  EarlyStopper = MetricTracker(patience=12, metric_name='+accuracy')\n",
    "  best_val_acc = 0\n",
    "\n",
    "\n",
    "  for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_train_acc  = 0\n",
    "    \n",
    "    # logging for scheduler\n",
    "    losses = []\n",
    "    accuracies= []\n",
    "\n",
    "    train_size = 0\n",
    "\n",
    "    for batch_idx, (pair_token_ids, mask_ids, seg_ids, feat, y, idx) in enumerate(train_loader):\n",
    "      train_size+=1\n",
    "      optimizer.zero_grad()\n",
    "      pair_token_ids = pair_token_ids.to(device)\n",
    "      mask_ids = mask_ids.to(device)\n",
    "      seg_ids = seg_ids.to(device)\n",
    "      labels = y.to(device)\n",
    "      feat = feat.to(device)\n",
    "      outputs = model(pair_token_ids, \n",
    "                            token_type_ids=seg_ids, \n",
    "                            attention_mask=mask_ids,\n",
    "                            feat=feat)\n",
    "      criterion = nn.CrossEntropyLoss()\n",
    "      loss = criterion(outputs, labels)\n",
    "      loss.backward()\n",
    "      acc = multi_acc(outputs, labels)\n",
    "      optimizer.step()\n",
    "      total_train_loss += loss.item()\n",
    "      total_train_acc  += acc.item()\n",
    "\n",
    "      losses.append(loss)\n",
    "      accuracies.append(acc)\n",
    "      \n",
    "    mean_loss = sum(losses)/len(losses)\n",
    "    scheduler.step(mean_loss)\n",
    "\n",
    "    train_acc  = total_train_acc/len(train_loader)\n",
    "    train_loss = total_train_loss/len(train_loader)\n",
    "\n",
    "    val_acc, val_loss, cr, model, optimizer = evaluate_accuracy(model, optimizer, val_loader, rev_label_dict, label_dict, None)\n",
    "    if val_acc>best_val_acc:\n",
    "      torch.save(model.state_dict(), save_path_suffix+'_best.pt')\n",
    "      best_val_acc = val_acc\n",
    "      print(f'Epoch {epoch+1}: Best val_acc: {best_val_acc:.4f}')\n",
    "    if val_acc>=best_val_acc:\n",
    "      torch.save(model.state_dict(), save_path_suffix+'_best_latest.pt')\n",
    "      best_val_acc = val_acc\n",
    "      print(f'Epoch {epoch+1}: Best val_acc: {best_val_acc:.4f}')\n",
    "    EarlyStopper.add_metric(val_acc)\n",
    "    if EarlyStopper.should_stop_early(): break\n",
    "\n",
    "    end = time.time()\n",
    "    hours, rem = divmod(end-start, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "    print(f'Epoch {epoch+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
    "    print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "    print(f'train_size: {train_size}')\n",
    "\n",
    "    writer.add_scalar('train_loss', train_loss, epoch)\n",
    "    writer.add_scalar('train_acc', train_acc, epoch)\n",
    "    writer.add_scalar('val_loss', val_loss, epoch)\n",
    "    writer.add_scalar('val_acc', val_acc, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODIFIED\n",
    "import time\n",
    "import traceback\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import Optional, Iterable, Dict, Any\n",
    "from EarlyStopperUtil import MetricTracker\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer, scheduler, rev_label_dict):  \n",
    "  EarlyStopper = MetricTracker(patience=12, metric_name='+accuracy')\n",
    "  best_val_acc = 0\n",
    "\n",
    "\n",
    "  for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_train_acc  = 0\n",
    "    \n",
    "    # logging for scheduler\n",
    "    losses = []\n",
    "    accuracies= []\n",
    "\n",
    "    train_size = 0\n",
    "\n",
    "    for batch_idx, (pair_token_ids, mask_ids, seg_ids, feat, y, idx) in enumerate(train_loader):\n",
    "      train_size+=1\n",
    "      optimizer.zero_grad()\n",
    "      pair_token_ids = pair_token_ids.to(device)\n",
    "      mask_ids = mask_ids.to(device)\n",
    "      seg_ids = seg_ids.to(device)\n",
    "      labels = y.to(device)\n",
    "      # feat = feat.to(device)\n",
    "      outputs = model(pair_token_ids, \n",
    "                            token_type_ids=seg_ids, \n",
    "                            attention_mask=mask_ids,\n",
    "                            feat=feat)\n",
    "      criterion = nn.CrossEntropyLoss()\n",
    "      loss = criterion(outputs, labels)\n",
    "      loss.backward()\n",
    "      acc = multi_acc(outputs, labels)\n",
    "      optimizer.step()\n",
    "      total_train_loss += loss.item()\n",
    "      total_train_acc  += acc.item()\n",
    "\n",
    "      losses.append(loss)\n",
    "      accuracies.append(acc)\n",
    "      \n",
    "    mean_loss = sum(losses)/len(losses)\n",
    "    scheduler.step(mean_loss)\n",
    "\n",
    "    train_acc  = total_train_acc/len(train_loader)\n",
    "    train_loss = total_train_loss/len(train_loader)\n",
    "\n",
    "    val_acc, val_loss, cr, model, optimizer = evaluate_accuracy(model, optimizer, val_loader, rev_label_dict, label_dict, None)\n",
    "    if val_acc>best_val_acc:\n",
    "      torch.save(model.state_dict(), save_path_suffix+'_best.pt')\n",
    "      best_val_acc = val_acc\n",
    "      print(f'Epoch {epoch+1}: Best val_acc: {best_val_acc:.4f}')\n",
    "    if val_acc>=best_val_acc:\n",
    "      torch.save(model.state_dict(), save_path_suffix+'_best_latest.pt')\n",
    "      best_val_acc = val_acc\n",
    "      print(f'Epoch {epoch+1}: Best val_acc: {best_val_acc:.4f}')\n",
    "    EarlyStopper.add_metric(val_acc)\n",
    "    if EarlyStopper.should_stop_early(): break\n",
    "\n",
    "    end = time.time()\n",
    "    hours, rem = divmod(end-start, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "    print(f'Epoch {epoch+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
    "    print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "    print(f'train_size: {train_size}')\n",
    "\n",
    "    writer.add_scalar('train_loss', train_loss, epoch)\n",
    "    writer.add_scalar('train_acc', train_acc, epoch)\n",
    "    writer.add_scalar('val_loss', val_loss, epoch)\n",
    "    writer.add_scalar('val_acc', val_acc, epoch)\n",
    "\n",
    "    model.compute_pooler_similarity()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Best val_acc: 0.1393\n",
      "Epoch 1: Best val_acc: 0.1393\n",
      "Epoch 1: train_loss: 2.9675 train_acc: 0.0932 | val_loss: 2.8713 val_acc: 0.1393\n",
      "00:00:11.63\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.0294, 0.6609, 0.4687,  ..., 0.6159, 0.9928, 0.5101],\n",
      "        [0.0255, 0.0251, 0.4682,  ..., 0.6155, 0.4068, 0.0060],\n",
      "        [0.1498, 0.2386, 0.9597,  ..., 0.0466, 0.0158, 0.1927],\n",
      "        ...,\n",
      "        [0.1732, 0.8600, 0.7948,  ..., 0.0694, 0.4411, 0.7061],\n",
      "        [0.8094, 0.8432, 0.3942,  ..., 0.0207, 0.7028, 0.7863],\n",
      "        [0.2950, 0.2699, 0.1792,  ..., 0.6473, 0.8178, 0.1239]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.0291, 0.6613, 0.4690,  ..., 0.6157, 0.9927, 0.5098],\n",
      "        [0.0258, 0.0247, 0.4681,  ..., 0.6155, 0.4070, 0.0065],\n",
      "        [0.1497, 0.2382, 0.9596,  ..., 0.0467, 0.0159, 0.1928],\n",
      "        ...,\n",
      "        [0.1729, 0.8596, 0.7944,  ..., 0.0689, 0.4413, 0.7067],\n",
      "        [0.8094, 0.8434, 0.3942,  ..., 0.0211, 0.7028, 0.7862],\n",
      "        [0.2945, 0.2699, 0.1790,  ..., 0.6471, 0.8178, 0.1238]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Epoch 2: Best val_acc: 0.1639\n",
      "Epoch 2: Best val_acc: 0.1639\n",
      "Epoch 2: train_loss: 2.7498 train_acc: 0.1682 | val_loss: 2.7049 val_acc: 0.1639\n",
      "00:00:10.30\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.0294, 0.6609, 0.4687,  ..., 0.6159, 0.9928, 0.5101],\n",
      "        [0.0257, 0.0250, 0.4681,  ..., 0.6154, 0.4069, 0.0061],\n",
      "        [0.1498, 0.2386, 0.9596,  ..., 0.0466, 0.0158, 0.1927],\n",
      "        ...,\n",
      "        [0.1732, 0.8601, 0.7949,  ..., 0.0695, 0.4411, 0.7061],\n",
      "        [0.8094, 0.8433, 0.3943,  ..., 0.0205, 0.7028, 0.7862],\n",
      "        [0.2951, 0.2699, 0.1791,  ..., 0.6473, 0.8178, 0.1239]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.0294, 0.6609, 0.4687,  ..., 0.6159, 0.9928, 0.5101],\n",
      "        [0.0255, 0.0251, 0.4682,  ..., 0.6155, 0.4068, 0.0060],\n",
      "        [0.1498, 0.2386, 0.9597,  ..., 0.0466, 0.0158, 0.1927],\n",
      "        ...,\n",
      "        [0.1732, 0.8600, 0.7948,  ..., 0.0694, 0.4411, 0.7061],\n",
      "        [0.8094, 0.8432, 0.3942,  ..., 0.0207, 0.7028, 0.7863],\n",
      "        [0.2950, 0.2699, 0.1792,  ..., 0.6473, 0.8178, 0.1239]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Epoch 3: Best val_acc: 0.2008\n",
      "Epoch 3: Best val_acc: 0.2008\n",
      "Epoch 3: train_loss: 2.5916 train_acc: 0.2386 | val_loss: 2.6070 val_acc: 0.2008\n",
      "00:00:09.97\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.0294, 0.6609, 0.4687,  ..., 0.6159, 0.9928, 0.5101],\n",
      "        [0.0256, 0.0250, 0.4681,  ..., 0.6154, 0.4069, 0.0062],\n",
      "        [0.1500, 0.2386, 0.9595,  ..., 0.0466, 0.0158, 0.1925],\n",
      "        ...,\n",
      "        [0.1732, 0.8600, 0.7949,  ..., 0.0694, 0.4411, 0.7061],\n",
      "        [0.8093, 0.8432, 0.3940,  ..., 0.0205, 0.7027, 0.7864],\n",
      "        [0.2950, 0.2700, 0.1791,  ..., 0.6474, 0.8177, 0.1239]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.0294, 0.6609, 0.4687,  ..., 0.6159, 0.9928, 0.5101],\n",
      "        [0.0257, 0.0250, 0.4681,  ..., 0.6154, 0.4069, 0.0061],\n",
      "        [0.1498, 0.2386, 0.9596,  ..., 0.0466, 0.0158, 0.1927],\n",
      "        ...,\n",
      "        [0.1732, 0.8601, 0.7949,  ..., 0.0695, 0.4411, 0.7061],\n",
      "        [0.8094, 0.8433, 0.3943,  ..., 0.0205, 0.7028, 0.7862],\n",
      "        [0.2951, 0.2699, 0.1791,  ..., 0.6473, 0.8178, 0.1239]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Epoch 4: Best val_acc: 0.2172\n",
      "Epoch 4: Best val_acc: 0.2172\n",
      "Epoch 4: train_loss: 2.4171 train_acc: 0.3159 | val_loss: 2.5329 val_acc: 0.2172\n",
      "00:00:10.77\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.0294, 0.6609, 0.4687,  ..., 0.6159, 0.9928, 0.5101],\n",
      "        [0.0258, 0.0249, 0.4682,  ..., 0.6152, 0.4068, 0.0061],\n",
      "        [0.1499, 0.2386, 0.9596,  ..., 0.0467, 0.0158, 0.1925],\n",
      "        ...,\n",
      "        [0.1734, 0.8600, 0.7950,  ..., 0.0695, 0.4411, 0.7061],\n",
      "        [0.8091, 0.8431, 0.3938,  ..., 0.0204, 0.7027, 0.7864],\n",
      "        [0.2949, 0.2701, 0.1792,  ..., 0.6474, 0.8177, 0.1238]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.0294, 0.6609, 0.4687,  ..., 0.6159, 0.9928, 0.5101],\n",
      "        [0.0256, 0.0250, 0.4681,  ..., 0.6154, 0.4069, 0.0062],\n",
      "        [0.1500, 0.2386, 0.9595,  ..., 0.0466, 0.0158, 0.1925],\n",
      "        ...,\n",
      "        [0.1732, 0.8600, 0.7949,  ..., 0.0694, 0.4411, 0.7061],\n",
      "        [0.8093, 0.8432, 0.3940,  ..., 0.0205, 0.7027, 0.7864],\n",
      "        [0.2950, 0.2700, 0.1791,  ..., 0.6474, 0.8177, 0.1239]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Epoch 5: Best val_acc: 0.2459\n",
      "Epoch 5: Best val_acc: 0.2459\n",
      "Epoch 5: train_loss: 2.2627 train_acc: 0.3659 | val_loss: 2.4276 val_acc: 0.2459\n",
      "00:00:10.73\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.0294, 0.6609, 0.4687,  ..., 0.6159, 0.9928, 0.5101],\n",
      "        [0.0258, 0.0249, 0.4680,  ..., 0.6151, 0.4070, 0.0062],\n",
      "        [0.1499, 0.2387, 0.9596,  ..., 0.0466, 0.0158, 0.1925],\n",
      "        ...,\n",
      "        [0.1733, 0.8600, 0.7951,  ..., 0.0695, 0.4409, 0.7060],\n",
      "        [0.8092, 0.8432, 0.3940,  ..., 0.0203, 0.7027, 0.7863],\n",
      "        [0.2947, 0.2701, 0.1792,  ..., 0.6475, 0.8177, 0.1237]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.0294, 0.6609, 0.4687,  ..., 0.6159, 0.9928, 0.5101],\n",
      "        [0.0258, 0.0249, 0.4682,  ..., 0.6152, 0.4068, 0.0061],\n",
      "        [0.1499, 0.2386, 0.9596,  ..., 0.0467, 0.0158, 0.1925],\n",
      "        ...,\n",
      "        [0.1734, 0.8600, 0.7950,  ..., 0.0695, 0.4411, 0.7061],\n",
      "        [0.8091, 0.8431, 0.3938,  ..., 0.0204, 0.7027, 0.7864],\n",
      "        [0.2949, 0.2701, 0.1792,  ..., 0.6474, 0.8177, 0.1238]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Epoch 6: train_loss: 2.1751 train_acc: 0.3955 | val_loss: 2.4219 val_acc: 0.2213\n",
      "00:00:08.00\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.0295, 0.6607, 0.4687,  ..., 0.6159, 0.9928, 0.5101],\n",
      "        [0.0258, 0.0248, 0.4679,  ..., 0.6150, 0.4071, 0.0062],\n",
      "        [0.1500, 0.2386, 0.9595,  ..., 0.0465, 0.0160, 0.1926],\n",
      "        ...,\n",
      "        [0.1733, 0.8600, 0.7952,  ..., 0.0695, 0.4409, 0.7060],\n",
      "        [0.8092, 0.8431, 0.3940,  ..., 0.0203, 0.7027, 0.7864],\n",
      "        [0.2946, 0.2701, 0.1792,  ..., 0.6475, 0.8177, 0.1237]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.0294, 0.6609, 0.4687,  ..., 0.6159, 0.9928, 0.5101],\n",
      "        [0.0258, 0.0249, 0.4680,  ..., 0.6151, 0.4070, 0.0062],\n",
      "        [0.1499, 0.2387, 0.9596,  ..., 0.0466, 0.0158, 0.1925],\n",
      "        ...,\n",
      "        [0.1733, 0.8600, 0.7951,  ..., 0.0695, 0.4409, 0.7060],\n",
      "        [0.8092, 0.8432, 0.3940,  ..., 0.0203, 0.7027, 0.7863],\n",
      "        [0.2947, 0.2701, 0.1792,  ..., 0.6475, 0.8177, 0.1237]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Epoch 7: Best val_acc: 0.2664\n",
      "Epoch 7: Best val_acc: 0.2664\n",
      "Epoch 7: train_loss: 2.0271 train_acc: 0.4864 | val_loss: 2.4062 val_acc: 0.2664\n",
      "00:00:12.12\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.0296, 0.6607, 0.4687,  ..., 0.6158, 0.9930, 0.5102],\n",
      "        [0.0259, 0.0248, 0.4681,  ..., 0.6152, 0.4071, 0.0062],\n",
      "        [0.1501, 0.2386, 0.9595,  ..., 0.0465, 0.0161, 0.1927],\n",
      "        ...,\n",
      "        [0.1735, 0.8600, 0.7952,  ..., 0.0695, 0.4410, 0.7061],\n",
      "        [0.8091, 0.8429, 0.3938,  ..., 0.0201, 0.7028, 0.7865],\n",
      "        [0.2945, 0.2702, 0.1793,  ..., 0.6475, 0.8176, 0.1237]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.0295, 0.6607, 0.4687,  ..., 0.6159, 0.9928, 0.5101],\n",
      "        [0.0258, 0.0248, 0.4679,  ..., 0.6150, 0.4071, 0.0062],\n",
      "        [0.1500, 0.2386, 0.9595,  ..., 0.0465, 0.0160, 0.1926],\n",
      "        ...,\n",
      "        [0.1733, 0.8600, 0.7952,  ..., 0.0695, 0.4409, 0.7060],\n",
      "        [0.8092, 0.8431, 0.3940,  ..., 0.0203, 0.7027, 0.7864],\n",
      "        [0.2946, 0.2701, 0.1792,  ..., 0.6475, 0.8177, 0.1237]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Epoch 8: Best val_acc: 0.2910\n",
      "Epoch 8: Best val_acc: 0.2910\n",
      "Epoch 8: train_loss: 2.0068 train_acc: 0.4659 | val_loss: 2.3370 val_acc: 0.2910\n",
      "00:00:11.98\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.0296, 0.6607, 0.4687,  ..., 0.6159, 0.9929, 0.5102],\n",
      "        [0.0258, 0.0247, 0.4680,  ..., 0.6151, 0.4070, 0.0061],\n",
      "        [0.1503, 0.2385, 0.9593,  ..., 0.0464, 0.0161, 0.1928],\n",
      "        ...,\n",
      "        [0.1737, 0.8601, 0.7952,  ..., 0.0694, 0.4409, 0.7061],\n",
      "        [0.8090, 0.8429, 0.3938,  ..., 0.0202, 0.7028, 0.7865],\n",
      "        [0.2944, 0.2703, 0.1793,  ..., 0.6476, 0.8175, 0.1236]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.0296, 0.6607, 0.4687,  ..., 0.6158, 0.9930, 0.5102],\n",
      "        [0.0259, 0.0248, 0.4681,  ..., 0.6152, 0.4071, 0.0062],\n",
      "        [0.1501, 0.2386, 0.9595,  ..., 0.0465, 0.0161, 0.1927],\n",
      "        ...,\n",
      "        [0.1735, 0.8600, 0.7952,  ..., 0.0695, 0.4410, 0.7061],\n",
      "        [0.8091, 0.8429, 0.3938,  ..., 0.0201, 0.7028, 0.7865],\n",
      "        [0.2945, 0.2702, 0.1793,  ..., 0.6475, 0.8176, 0.1237]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Epoch 9: train_loss: 1.8917 train_acc: 0.4886 | val_loss: 2.3188 val_acc: 0.2705\n",
      "00:00:07.97\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.0296, 0.6605, 0.4686,  ..., 0.6158, 0.9930, 0.5103],\n",
      "        [0.0258, 0.0247, 0.4679,  ..., 0.6151, 0.4071, 0.0061],\n",
      "        [0.1502, 0.2385, 0.9593,  ..., 0.0464, 0.0161, 0.1927],\n",
      "        ...,\n",
      "        [0.1737, 0.8600, 0.7952,  ..., 0.0694, 0.4409, 0.7060],\n",
      "        [0.8091, 0.8428, 0.3938,  ..., 0.0202, 0.7028, 0.7865],\n",
      "        [0.2943, 0.2703, 0.1793,  ..., 0.6476, 0.8175, 0.1235]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.0296, 0.6607, 0.4687,  ..., 0.6159, 0.9929, 0.5102],\n",
      "        [0.0258, 0.0247, 0.4680,  ..., 0.6151, 0.4070, 0.0061],\n",
      "        [0.1503, 0.2385, 0.9593,  ..., 0.0464, 0.0161, 0.1928],\n",
      "        ...,\n",
      "        [0.1737, 0.8601, 0.7952,  ..., 0.0694, 0.4409, 0.7061],\n",
      "        [0.8090, 0.8429, 0.3938,  ..., 0.0202, 0.7028, 0.7865],\n",
      "        [0.2944, 0.2703, 0.1793,  ..., 0.6476, 0.8175, 0.1236]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Epoch 10: train_loss: 1.7771 train_acc: 0.5227 | val_loss: 2.3076 val_acc: 0.2664\n",
      "00:00:08.24\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.0295, 0.6605, 0.4687,  ..., 0.6158, 0.9929, 0.5103],\n",
      "        [0.0258, 0.0247, 0.4679,  ..., 0.6152, 0.4071, 0.0061],\n",
      "        [0.1503, 0.2384, 0.9593,  ..., 0.0463, 0.0161, 0.1928],\n",
      "        ...,\n",
      "        [0.1738, 0.8600, 0.7952,  ..., 0.0694, 0.4409, 0.7060],\n",
      "        [0.8090, 0.8427, 0.3937,  ..., 0.0201, 0.7028, 0.7865],\n",
      "        [0.2943, 0.2702, 0.1793,  ..., 0.6476, 0.8175, 0.1236]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.0296, 0.6605, 0.4686,  ..., 0.6158, 0.9930, 0.5103],\n",
      "        [0.0258, 0.0247, 0.4679,  ..., 0.6151, 0.4071, 0.0061],\n",
      "        [0.1502, 0.2385, 0.9593,  ..., 0.0464, 0.0161, 0.1927],\n",
      "        ...,\n",
      "        [0.1737, 0.8600, 0.7952,  ..., 0.0694, 0.4409, 0.7060],\n",
      "        [0.8091, 0.8428, 0.3938,  ..., 0.0202, 0.7028, 0.7865],\n",
      "        [0.2943, 0.2703, 0.1793,  ..., 0.6476, 0.8175, 0.1235]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Epoch 11: train_loss: 1.6802 train_acc: 0.5909 | val_loss: 2.2644 val_acc: 0.2869\n",
      "00:00:10.94\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.0296, 0.6605, 0.4687,  ..., 0.6157, 0.9929, 0.5103],\n",
      "        [0.0259, 0.0247, 0.4679,  ..., 0.6152, 0.4071, 0.0061],\n",
      "        [0.1504, 0.2383, 0.9592,  ..., 0.0462, 0.0162, 0.1929],\n",
      "        ...,\n",
      "        [0.1738, 0.8599, 0.7952,  ..., 0.0694, 0.4410, 0.7060],\n",
      "        [0.8090, 0.8427, 0.3937,  ..., 0.0203, 0.7029, 0.7866],\n",
      "        [0.2944, 0.2702, 0.1793,  ..., 0.6476, 0.8175, 0.1236]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.0295, 0.6605, 0.4687,  ..., 0.6158, 0.9929, 0.5103],\n",
      "        [0.0258, 0.0247, 0.4679,  ..., 0.6152, 0.4071, 0.0061],\n",
      "        [0.1503, 0.2384, 0.9593,  ..., 0.0463, 0.0161, 0.1928],\n",
      "        ...,\n",
      "        [0.1738, 0.8600, 0.7952,  ..., 0.0694, 0.4409, 0.7060],\n",
      "        [0.8090, 0.8427, 0.3937,  ..., 0.0201, 0.7028, 0.7865],\n",
      "        [0.2943, 0.2702, 0.1793,  ..., 0.6476, 0.8175, 0.1236]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Epoch 12: Best val_acc: 0.2910\n",
      "Epoch 12: train_loss: 1.6618 train_acc: 0.5659 | val_loss: 2.2633 val_acc: 0.2910\n",
      "00:00:09.60\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.0296, 0.6605, 0.4687,  ..., 0.6157, 0.9929, 0.5103],\n",
      "        [0.0258, 0.0246, 0.4679,  ..., 0.6152, 0.4071, 0.0062],\n",
      "        [0.1504, 0.2384, 0.9592,  ..., 0.0463, 0.0162, 0.1929],\n",
      "        ...,\n",
      "        [0.1739, 0.8599, 0.7952,  ..., 0.0693, 0.4409, 0.7061],\n",
      "        [0.8091, 0.8425, 0.3936,  ..., 0.0203, 0.7030, 0.7867],\n",
      "        [0.2942, 0.2703, 0.1793,  ..., 0.6476, 0.8174, 0.1235]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.0296, 0.6605, 0.4687,  ..., 0.6157, 0.9929, 0.5103],\n",
      "        [0.0259, 0.0247, 0.4679,  ..., 0.6152, 0.4071, 0.0061],\n",
      "        [0.1504, 0.2383, 0.9592,  ..., 0.0462, 0.0162, 0.1929],\n",
      "        ...,\n",
      "        [0.1738, 0.8599, 0.7952,  ..., 0.0694, 0.4410, 0.7060],\n",
      "        [0.8090, 0.8427, 0.3937,  ..., 0.0203, 0.7029, 0.7866],\n",
      "        [0.2944, 0.2702, 0.1793,  ..., 0.6476, 0.8175, 0.1236]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Epoch 13: train_loss: 1.5775 train_acc: 0.6386 | val_loss: 2.2619 val_acc: 0.2828\n",
      "00:00:07.64\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.0295, 0.6605, 0.4687,  ..., 0.6158, 0.9928, 0.5102],\n",
      "        [0.0258, 0.0246, 0.4678,  ..., 0.6152, 0.4070, 0.0061],\n",
      "        [0.1505, 0.2383, 0.9591,  ..., 0.0461, 0.0162, 0.1930],\n",
      "        ...,\n",
      "        [0.1739, 0.8599, 0.7953,  ..., 0.0694, 0.4410, 0.7061],\n",
      "        [0.8091, 0.8424, 0.3936,  ..., 0.0202, 0.7031, 0.7868],\n",
      "        [0.2942, 0.2703, 0.1793,  ..., 0.6477, 0.8174, 0.1234]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.0296, 0.6605, 0.4687,  ..., 0.6157, 0.9929, 0.5103],\n",
      "        [0.0258, 0.0246, 0.4679,  ..., 0.6152, 0.4071, 0.0062],\n",
      "        [0.1504, 0.2384, 0.9592,  ..., 0.0463, 0.0162, 0.1929],\n",
      "        ...,\n",
      "        [0.1739, 0.8599, 0.7952,  ..., 0.0693, 0.4409, 0.7061],\n",
      "        [0.8091, 0.8425, 0.3936,  ..., 0.0203, 0.7030, 0.7867],\n",
      "        [0.2942, 0.2703, 0.1793,  ..., 0.6476, 0.8174, 0.1235]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Epoch 14: Best val_acc: 0.3033\n",
      "Epoch 14: Best val_acc: 0.3033\n",
      "Epoch 14: train_loss: 1.4313 train_acc: 0.6864 | val_loss: 2.2627 val_acc: 0.3033\n",
      "00:00:10.04\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.0296, 0.6605, 0.4686,  ..., 0.6159, 0.9927, 0.5103],\n",
      "        [0.0258, 0.0247, 0.4679,  ..., 0.6152, 0.4070, 0.0061],\n",
      "        [0.1506, 0.2383, 0.9590,  ..., 0.0461, 0.0162, 0.1930],\n",
      "        ...,\n",
      "        [0.1738, 0.8600, 0.7954,  ..., 0.0694, 0.4410, 0.7061],\n",
      "        [0.8092, 0.8424, 0.3936,  ..., 0.0202, 0.7030, 0.7868],\n",
      "        [0.2942, 0.2702, 0.1792,  ..., 0.6476, 0.8175, 0.1234]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.0295, 0.6605, 0.4687,  ..., 0.6158, 0.9928, 0.5102],\n",
      "        [0.0258, 0.0246, 0.4678,  ..., 0.6152, 0.4070, 0.0061],\n",
      "        [0.1505, 0.2383, 0.9591,  ..., 0.0461, 0.0162, 0.1930],\n",
      "        ...,\n",
      "        [0.1739, 0.8599, 0.7953,  ..., 0.0694, 0.4410, 0.7061],\n",
      "        [0.8091, 0.8424, 0.3936,  ..., 0.0202, 0.7031, 0.7868],\n",
      "        [0.2942, 0.2703, 0.1793,  ..., 0.6477, 0.8174, 0.1234]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Epoch 15: train_loss: 1.4141 train_acc: 0.6705 | val_loss: 2.2839 val_acc: 0.2910\n",
      "00:00:10.65\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.0296, 0.6605, 0.4686,  ..., 0.6160, 0.9927, 0.5102],\n",
      "        [0.0258, 0.0247, 0.4679,  ..., 0.6152, 0.4070, 0.0060],\n",
      "        [0.1506, 0.2383, 0.9591,  ..., 0.0461, 0.0163, 0.1930],\n",
      "        ...,\n",
      "        [0.1737, 0.8600, 0.7954,  ..., 0.0695, 0.4409, 0.7060],\n",
      "        [0.8091, 0.8424, 0.3936,  ..., 0.0202, 0.7031, 0.7868],\n",
      "        [0.2942, 0.2702, 0.1792,  ..., 0.6477, 0.8175, 0.1234]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.0296, 0.6605, 0.4686,  ..., 0.6159, 0.9927, 0.5103],\n",
      "        [0.0258, 0.0247, 0.4679,  ..., 0.6152, 0.4070, 0.0061],\n",
      "        [0.1506, 0.2383, 0.9590,  ..., 0.0461, 0.0162, 0.1930],\n",
      "        ...,\n",
      "        [0.1738, 0.8600, 0.7954,  ..., 0.0694, 0.4410, 0.7061],\n",
      "        [0.8092, 0.8424, 0.3936,  ..., 0.0202, 0.7030, 0.7868],\n",
      "        [0.2942, 0.2702, 0.1792,  ..., 0.6476, 0.8175, 0.1234]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Epoch 16: train_loss: 1.3303 train_acc: 0.6795 | val_loss: 2.2400 val_acc: 0.2869\n",
      "00:00:09.92\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.0296, 0.6604, 0.4686,  ..., 0.6160, 0.9926, 0.5102],\n",
      "        [0.0258, 0.0248, 0.4679,  ..., 0.6152, 0.4069, 0.0060],\n",
      "        [0.1506, 0.2383, 0.9590,  ..., 0.0460, 0.0163, 0.1930],\n",
      "        ...,\n",
      "        [0.1737, 0.8600, 0.7955,  ..., 0.0694, 0.4409, 0.7060],\n",
      "        [0.8092, 0.8424, 0.3936,  ..., 0.0201, 0.7030, 0.7868],\n",
      "        [0.2942, 0.2702, 0.1791,  ..., 0.6478, 0.8175, 0.1234]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.0296, 0.6605, 0.4686,  ..., 0.6160, 0.9927, 0.5102],\n",
      "        [0.0258, 0.0247, 0.4679,  ..., 0.6152, 0.4070, 0.0060],\n",
      "        [0.1506, 0.2383, 0.9591,  ..., 0.0461, 0.0163, 0.1930],\n",
      "        ...,\n",
      "        [0.1737, 0.8600, 0.7954,  ..., 0.0695, 0.4409, 0.7060],\n",
      "        [0.8091, 0.8424, 0.3936,  ..., 0.0202, 0.7031, 0.7868],\n",
      "        [0.2942, 0.2702, 0.1792,  ..., 0.6477, 0.8175, 0.1234]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Epoch 17: train_loss: 1.2526 train_acc: 0.7500 | val_loss: 2.2300 val_acc: 0.2951\n",
      "00:00:08.11\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.0296, 0.6605, 0.4687,  ..., 0.6159, 0.9926, 0.5103],\n",
      "        [0.0258, 0.0248, 0.4679,  ..., 0.6152, 0.4068, 0.0059],\n",
      "        [0.1507, 0.2383, 0.9590,  ..., 0.0460, 0.0163, 0.1930],\n",
      "        ...,\n",
      "        [0.1738, 0.8599, 0.7954,  ..., 0.0693, 0.4410, 0.7061],\n",
      "        [0.8092, 0.8424, 0.3936,  ..., 0.0202, 0.7030, 0.7869],\n",
      "        [0.2943, 0.2701, 0.1791,  ..., 0.6478, 0.8175, 0.1234]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.0296, 0.6604, 0.4686,  ..., 0.6160, 0.9926, 0.5102],\n",
      "        [0.0258, 0.0248, 0.4679,  ..., 0.6152, 0.4069, 0.0060],\n",
      "        [0.1506, 0.2383, 0.9590,  ..., 0.0460, 0.0163, 0.1930],\n",
      "        ...,\n",
      "        [0.1737, 0.8600, 0.7955,  ..., 0.0694, 0.4409, 0.7060],\n",
      "        [0.8092, 0.8424, 0.3936,  ..., 0.0201, 0.7030, 0.7868],\n",
      "        [0.2942, 0.2702, 0.1791,  ..., 0.6478, 0.8175, 0.1234]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Epoch 18: train_loss: 1.1787 train_acc: 0.7636 | val_loss: 2.2498 val_acc: 0.2910\n",
      "00:00:07.60\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.0297, 0.6602, 0.4686,  ..., 0.6159, 0.9927, 0.5104],\n",
      "        [0.0257, 0.0250, 0.4680,  ..., 0.6152, 0.4067, 0.0058],\n",
      "        [0.1507, 0.2383, 0.9590,  ..., 0.0460, 0.0162, 0.1930],\n",
      "        ...,\n",
      "        [0.1737, 0.8598, 0.7954,  ..., 0.0692, 0.4411, 0.7062],\n",
      "        [0.8090, 0.8424, 0.3936,  ..., 0.0203, 0.7030, 0.7869],\n",
      "        [0.2942, 0.2701, 0.1791,  ..., 0.6478, 0.8174, 0.1233]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.0296, 0.6605, 0.4687,  ..., 0.6159, 0.9926, 0.5103],\n",
      "        [0.0258, 0.0248, 0.4679,  ..., 0.6152, 0.4068, 0.0059],\n",
      "        [0.1507, 0.2383, 0.9590,  ..., 0.0460, 0.0163, 0.1930],\n",
      "        ...,\n",
      "        [0.1738, 0.8599, 0.7954,  ..., 0.0693, 0.4410, 0.7061],\n",
      "        [0.8092, 0.8424, 0.3936,  ..., 0.0202, 0.7030, 0.7869],\n",
      "        [0.2943, 0.2701, 0.1791,  ..., 0.6478, 0.8175, 0.1234]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Epoch 19: train_loss: 1.1369 train_acc: 0.7727 | val_loss: 2.2140 val_acc: 0.2828\n",
      "00:00:07.70\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.0297, 0.6602, 0.4686,  ..., 0.6158, 0.9927, 0.5105],\n",
      "        [0.0258, 0.0250, 0.4679,  ..., 0.6152, 0.4067, 0.0058],\n",
      "        [0.1508, 0.2383, 0.9590,  ..., 0.0460, 0.0163, 0.1931],\n",
      "        ...,\n",
      "        [0.1738, 0.8597, 0.7954,  ..., 0.0691, 0.4412, 0.7063],\n",
      "        [0.8091, 0.8425, 0.3936,  ..., 0.0203, 0.7030, 0.7869],\n",
      "        [0.2941, 0.2702, 0.1792,  ..., 0.6478, 0.8173, 0.1232]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.0297, 0.6602, 0.4686,  ..., 0.6159, 0.9927, 0.5104],\n",
      "        [0.0257, 0.0250, 0.4680,  ..., 0.6152, 0.4067, 0.0058],\n",
      "        [0.1507, 0.2383, 0.9590,  ..., 0.0460, 0.0162, 0.1930],\n",
      "        ...,\n",
      "        [0.1737, 0.8598, 0.7954,  ..., 0.0692, 0.4411, 0.7062],\n",
      "        [0.8090, 0.8424, 0.3936,  ..., 0.0203, 0.7030, 0.7869],\n",
      "        [0.2942, 0.2701, 0.1791,  ..., 0.6478, 0.8174, 0.1233]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Epoch 20: Best val_acc: 0.3197\n",
      "Epoch 20: Best val_acc: 0.3197\n",
      "Epoch 20: train_loss: 1.0310 train_acc: 0.8136 | val_loss: 2.1902 val_acc: 0.3197\n",
      "00:00:10.52\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.0297, 0.6602, 0.4686,  ..., 0.6158, 0.9926, 0.5103],\n",
      "        [0.0258, 0.0249, 0.4679,  ..., 0.6152, 0.4067, 0.0058],\n",
      "        [0.1508, 0.2382, 0.9589,  ..., 0.0460, 0.0163, 0.1931],\n",
      "        ...,\n",
      "        [0.1739, 0.8597, 0.7955,  ..., 0.0692, 0.4411, 0.7062],\n",
      "        [0.8091, 0.8425, 0.3936,  ..., 0.0203, 0.7030, 0.7869],\n",
      "        [0.2940, 0.2703, 0.1792,  ..., 0.6478, 0.8173, 0.1232]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.0297, 0.6602, 0.4686,  ..., 0.6158, 0.9927, 0.5105],\n",
      "        [0.0258, 0.0250, 0.4679,  ..., 0.6152, 0.4067, 0.0058],\n",
      "        [0.1508, 0.2383, 0.9590,  ..., 0.0460, 0.0163, 0.1931],\n",
      "        ...,\n",
      "        [0.1738, 0.8597, 0.7954,  ..., 0.0691, 0.4412, 0.7063],\n",
      "        [0.8091, 0.8425, 0.3936,  ..., 0.0203, 0.7030, 0.7869],\n",
      "        [0.2941, 0.2702, 0.1792,  ..., 0.6478, 0.8173, 0.1232]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Epoch 21: train_loss: 1.0050 train_acc: 0.8091 | val_loss: 2.2010 val_acc: 0.2910\n",
      "00:00:07.75\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.0297, 0.6602, 0.4686,  ..., 0.6159, 0.9926, 0.5103],\n",
      "        [0.0258, 0.0250, 0.4679,  ..., 0.6153, 0.4066, 0.0057],\n",
      "        [0.1510, 0.2382, 0.9588,  ..., 0.0459, 0.0164, 0.1932],\n",
      "        ...,\n",
      "        [0.1739, 0.8597, 0.7955,  ..., 0.0692, 0.4411, 0.7062],\n",
      "        [0.8091, 0.8425, 0.3935,  ..., 0.0203, 0.7030, 0.7869],\n",
      "        [0.2940, 0.2703, 0.1792,  ..., 0.6479, 0.8173, 0.1232]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.0297, 0.6602, 0.4686,  ..., 0.6158, 0.9926, 0.5103],\n",
      "        [0.0258, 0.0249, 0.4679,  ..., 0.6152, 0.4067, 0.0058],\n",
      "        [0.1508, 0.2382, 0.9589,  ..., 0.0460, 0.0163, 0.1931],\n",
      "        ...,\n",
      "        [0.1739, 0.8597, 0.7955,  ..., 0.0692, 0.4411, 0.7062],\n",
      "        [0.8091, 0.8425, 0.3936,  ..., 0.0203, 0.7030, 0.7869],\n",
      "        [0.2940, 0.2703, 0.1792,  ..., 0.6478, 0.8173, 0.1232]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Epoch 22: train_loss: 0.9996 train_acc: 0.8114 | val_loss: 2.2330 val_acc: 0.3156\n",
      "00:00:08.46\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.0296, 0.6602, 0.4686,  ..., 0.6158, 0.9925, 0.5103],\n",
      "        [0.0258, 0.0251, 0.4679,  ..., 0.6153, 0.4066, 0.0057],\n",
      "        [0.1510, 0.2382, 0.9587,  ..., 0.0459, 0.0164, 0.1932],\n",
      "        ...,\n",
      "        [0.1739, 0.8598, 0.7955,  ..., 0.0691, 0.4410, 0.7061],\n",
      "        [0.8091, 0.8425, 0.3934,  ..., 0.0203, 0.7029, 0.7870],\n",
      "        [0.2940, 0.2703, 0.1792,  ..., 0.6479, 0.8173, 0.1231]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.0297, 0.6602, 0.4686,  ..., 0.6159, 0.9926, 0.5103],\n",
      "        [0.0258, 0.0250, 0.4679,  ..., 0.6153, 0.4066, 0.0057],\n",
      "        [0.1510, 0.2382, 0.9588,  ..., 0.0459, 0.0164, 0.1932],\n",
      "        ...,\n",
      "        [0.1739, 0.8597, 0.7955,  ..., 0.0692, 0.4411, 0.7062],\n",
      "        [0.8091, 0.8425, 0.3935,  ..., 0.0203, 0.7030, 0.7869],\n",
      "        [0.2940, 0.2703, 0.1792,  ..., 0.6479, 0.8173, 0.1232]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Epoch 23: train_loss: 0.8753 train_acc: 0.8545 | val_loss: 2.1669 val_acc: 0.3033\n",
      "00:00:08.22\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.0296, 0.6602, 0.4686,  ..., 0.6159, 0.9925, 0.5103],\n",
      "        [0.0258, 0.0251, 0.4678,  ..., 0.6153, 0.4065, 0.0057],\n",
      "        [0.1510, 0.2382, 0.9587,  ..., 0.0459, 0.0165, 0.1933],\n",
      "        ...,\n",
      "        [0.1739, 0.8597, 0.7956,  ..., 0.0691, 0.4410, 0.7062],\n",
      "        [0.8092, 0.8423, 0.3933,  ..., 0.0203, 0.7028, 0.7870],\n",
      "        [0.2940, 0.2703, 0.1792,  ..., 0.6479, 0.8173, 0.1231]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.0296, 0.6602, 0.4686,  ..., 0.6158, 0.9925, 0.5103],\n",
      "        [0.0258, 0.0251, 0.4679,  ..., 0.6153, 0.4066, 0.0057],\n",
      "        [0.1510, 0.2382, 0.9587,  ..., 0.0459, 0.0164, 0.1932],\n",
      "        ...,\n",
      "        [0.1739, 0.8598, 0.7955,  ..., 0.0691, 0.4410, 0.7061],\n",
      "        [0.8091, 0.8425, 0.3934,  ..., 0.0203, 0.7029, 0.7870],\n",
      "        [0.2940, 0.2703, 0.1792,  ..., 0.6479, 0.8173, 0.1231]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Epoch 24: train_loss: 0.8792 train_acc: 0.8318 | val_loss: 2.2130 val_acc: 0.2951\n",
      "00:00:07.98\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.0296, 0.6602, 0.4685,  ..., 0.6160, 0.9925, 0.5102],\n",
      "        [0.0257, 0.0251, 0.4678,  ..., 0.6152, 0.4064, 0.0058],\n",
      "        [0.1510, 0.2382, 0.9587,  ..., 0.0458, 0.0165, 0.1933],\n",
      "        ...,\n",
      "        [0.1739, 0.8598, 0.7955,  ..., 0.0691, 0.4409, 0.7062],\n",
      "        [0.8092, 0.8423, 0.3934,  ..., 0.0203, 0.7028, 0.7871],\n",
      "        [0.2940, 0.2703, 0.1791,  ..., 0.6479, 0.8173, 0.1231]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.0296, 0.6602, 0.4686,  ..., 0.6159, 0.9925, 0.5103],\n",
      "        [0.0258, 0.0251, 0.4678,  ..., 0.6153, 0.4065, 0.0057],\n",
      "        [0.1510, 0.2382, 0.9587,  ..., 0.0459, 0.0165, 0.1933],\n",
      "        ...,\n",
      "        [0.1739, 0.8597, 0.7956,  ..., 0.0691, 0.4410, 0.7062],\n",
      "        [0.8092, 0.8423, 0.3933,  ..., 0.0203, 0.7028, 0.7870],\n",
      "        [0.2940, 0.2703, 0.1792,  ..., 0.6479, 0.8173, 0.1231]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Epoch 25: train_loss: 0.7941 train_acc: 0.8818 | val_loss: 2.1969 val_acc: 0.3156\n",
      "00:00:07.91\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.0296, 0.6601, 0.4684,  ..., 0.6160, 0.9925, 0.5102],\n",
      "        [0.0258, 0.0252, 0.4677,  ..., 0.6152, 0.4064, 0.0058],\n",
      "        [0.1510, 0.2382, 0.9587,  ..., 0.0459, 0.0165, 0.1933],\n",
      "        ...,\n",
      "        [0.1739, 0.8597, 0.7955,  ..., 0.0691, 0.4410, 0.7062],\n",
      "        [0.8092, 0.8424, 0.3934,  ..., 0.0203, 0.7027, 0.7870],\n",
      "        [0.2940, 0.2703, 0.1792,  ..., 0.6479, 0.8174, 0.1231]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.0296, 0.6602, 0.4685,  ..., 0.6160, 0.9925, 0.5102],\n",
      "        [0.0257, 0.0251, 0.4678,  ..., 0.6152, 0.4064, 0.0058],\n",
      "        [0.1510, 0.2382, 0.9587,  ..., 0.0458, 0.0165, 0.1933],\n",
      "        ...,\n",
      "        [0.1739, 0.8598, 0.7955,  ..., 0.0691, 0.4409, 0.7062],\n",
      "        [0.8092, 0.8423, 0.3934,  ..., 0.0203, 0.7028, 0.7871],\n",
      "        [0.2940, 0.2703, 0.1791,  ..., 0.6479, 0.8173, 0.1231]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Epoch 26: train_loss: 0.7596 train_acc: 0.8659 | val_loss: 2.2184 val_acc: 0.2951\n",
      "00:00:08.02\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.0296, 0.6601, 0.4684,  ..., 0.6160, 0.9925, 0.5102],\n",
      "        [0.0257, 0.0251, 0.4677,  ..., 0.6153, 0.4064, 0.0058],\n",
      "        [0.1511, 0.2381, 0.9586,  ..., 0.0459, 0.0166, 0.1933],\n",
      "        ...,\n",
      "        [0.1739, 0.8597, 0.7955,  ..., 0.0690, 0.4411, 0.7062],\n",
      "        [0.8091, 0.8422, 0.3935,  ..., 0.0203, 0.7028, 0.7871],\n",
      "        [0.2940, 0.2703, 0.1792,  ..., 0.6480, 0.8174, 0.1231]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.0296, 0.6601, 0.4684,  ..., 0.6160, 0.9925, 0.5102],\n",
      "        [0.0258, 0.0252, 0.4677,  ..., 0.6152, 0.4064, 0.0058],\n",
      "        [0.1510, 0.2382, 0.9587,  ..., 0.0459, 0.0165, 0.1933],\n",
      "        ...,\n",
      "        [0.1739, 0.8597, 0.7955,  ..., 0.0691, 0.4410, 0.7062],\n",
      "        [0.8092, 0.8424, 0.3934,  ..., 0.0203, 0.7027, 0.7870],\n",
      "        [0.2940, 0.2703, 0.1792,  ..., 0.6479, 0.8174, 0.1231]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Epoch 27: train_loss: 0.6934 train_acc: 0.9000 | val_loss: 2.1977 val_acc: 0.3115\n",
      "00:00:08.43\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.0295, 0.6601, 0.4685,  ..., 0.6160, 0.9925, 0.5102],\n",
      "        [0.0258, 0.0252, 0.4677,  ..., 0.6153, 0.4064, 0.0058],\n",
      "        [0.1510, 0.2381, 0.9586,  ..., 0.0459, 0.0167, 0.1934],\n",
      "        ...,\n",
      "        [0.1740, 0.8597, 0.7956,  ..., 0.0691, 0.4410, 0.7062],\n",
      "        [0.8092, 0.8423, 0.3934,  ..., 0.0203, 0.7028, 0.7872],\n",
      "        [0.2940, 0.2703, 0.1792,  ..., 0.6480, 0.8174, 0.1230]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.0296, 0.6601, 0.4684,  ..., 0.6160, 0.9925, 0.5102],\n",
      "        [0.0257, 0.0251, 0.4677,  ..., 0.6153, 0.4064, 0.0058],\n",
      "        [0.1511, 0.2381, 0.9586,  ..., 0.0459, 0.0166, 0.1933],\n",
      "        ...,\n",
      "        [0.1739, 0.8597, 0.7955,  ..., 0.0690, 0.4411, 0.7062],\n",
      "        [0.8091, 0.8422, 0.3935,  ..., 0.0203, 0.7028, 0.7871],\n",
      "        [0.2940, 0.2703, 0.1792,  ..., 0.6480, 0.8174, 0.1231]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Epoch 28: train_loss: 0.6654 train_acc: 0.9045 | val_loss: 2.2036 val_acc: 0.3033\n",
      "00:00:08.17\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.0296, 0.6601, 0.4684,  ..., 0.6160, 0.9925, 0.5102],\n",
      "        [0.0258, 0.0252, 0.4677,  ..., 0.6153, 0.4063, 0.0058],\n",
      "        [0.1511, 0.2381, 0.9586,  ..., 0.0459, 0.0167, 0.1934],\n",
      "        ...,\n",
      "        [0.1739, 0.8597, 0.7957,  ..., 0.0690, 0.4410, 0.7062],\n",
      "        [0.8093, 0.8423, 0.3934,  ..., 0.0203, 0.7026, 0.7872],\n",
      "        [0.2940, 0.2703, 0.1792,  ..., 0.6480, 0.8175, 0.1230]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.0295, 0.6601, 0.4685,  ..., 0.6160, 0.9925, 0.5102],\n",
      "        [0.0258, 0.0252, 0.4677,  ..., 0.6153, 0.4064, 0.0058],\n",
      "        [0.1510, 0.2381, 0.9586,  ..., 0.0459, 0.0167, 0.1934],\n",
      "        ...,\n",
      "        [0.1740, 0.8597, 0.7956,  ..., 0.0691, 0.4410, 0.7062],\n",
      "        [0.8092, 0.8423, 0.3934,  ..., 0.0203, 0.7028, 0.7872],\n",
      "        [0.2940, 0.2703, 0.1792,  ..., 0.6480, 0.8174, 0.1230]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Epoch 29: train_loss: 0.6717 train_acc: 0.9114 | val_loss: 2.1897 val_acc: 0.3115\n",
      "00:00:08.44\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.0296, 0.6601, 0.4683,  ..., 0.6160, 0.9925, 0.5102],\n",
      "        [0.0258, 0.0251, 0.4676,  ..., 0.6153, 0.4063, 0.0058],\n",
      "        [0.1510, 0.2381, 0.9586,  ..., 0.0459, 0.0166, 0.1934],\n",
      "        ...,\n",
      "        [0.1739, 0.8597, 0.7957,  ..., 0.0689, 0.4410, 0.7061],\n",
      "        [0.8094, 0.8423, 0.3934,  ..., 0.0203, 0.7026, 0.7872],\n",
      "        [0.2940, 0.2703, 0.1793,  ..., 0.6480, 0.8174, 0.1230]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.0296, 0.6601, 0.4684,  ..., 0.6160, 0.9925, 0.5102],\n",
      "        [0.0258, 0.0252, 0.4677,  ..., 0.6153, 0.4063, 0.0058],\n",
      "        [0.1511, 0.2381, 0.9586,  ..., 0.0459, 0.0167, 0.1934],\n",
      "        ...,\n",
      "        [0.1739, 0.8597, 0.7957,  ..., 0.0690, 0.4410, 0.7062],\n",
      "        [0.8093, 0.8423, 0.3934,  ..., 0.0203, 0.7026, 0.7872],\n",
      "        [0.2940, 0.2703, 0.1792,  ..., 0.6480, 0.8175, 0.1230]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Epoch 30: train_loss: 0.5627 train_acc: 0.9432 | val_loss: 2.2003 val_acc: 0.3156\n",
      "00:00:08.30\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.0295, 0.6602, 0.4682,  ..., 0.6160, 0.9924, 0.5102],\n",
      "        [0.0258, 0.0251, 0.4676,  ..., 0.6152, 0.4063, 0.0058],\n",
      "        [0.1510, 0.2381, 0.9586,  ..., 0.0458, 0.0166, 0.1934],\n",
      "        ...,\n",
      "        [0.1739, 0.8597, 0.7957,  ..., 0.0690, 0.4410, 0.7061],\n",
      "        [0.8093, 0.8423, 0.3934,  ..., 0.0204, 0.7026, 0.7873],\n",
      "        [0.2940, 0.2703, 0.1792,  ..., 0.6481, 0.8175, 0.1230]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.0296, 0.6601, 0.4683,  ..., 0.6160, 0.9925, 0.5102],\n",
      "        [0.0258, 0.0251, 0.4676,  ..., 0.6153, 0.4063, 0.0058],\n",
      "        [0.1510, 0.2381, 0.9586,  ..., 0.0459, 0.0166, 0.1934],\n",
      "        ...,\n",
      "        [0.1739, 0.8597, 0.7957,  ..., 0.0689, 0.4410, 0.7061],\n",
      "        [0.8094, 0.8423, 0.3934,  ..., 0.0203, 0.7026, 0.7872],\n",
      "        [0.2940, 0.2703, 0.1793,  ..., 0.6480, 0.8174, 0.1230]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Epoch 31: train_loss: 0.5643 train_acc: 0.9364 | val_loss: 2.2181 val_acc: 0.2951\n",
      "00:00:08.57\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.0296, 0.6602, 0.4682,  ..., 0.6160, 0.9924, 0.5102],\n",
      "        [0.0258, 0.0251, 0.4676,  ..., 0.6152, 0.4063, 0.0059],\n",
      "        [0.1511, 0.2381, 0.9585,  ..., 0.0458, 0.0166, 0.1935],\n",
      "        ...,\n",
      "        [0.1739, 0.8597, 0.7958,  ..., 0.0689, 0.4410, 0.7061],\n",
      "        [0.8093, 0.8422, 0.3934,  ..., 0.0204, 0.7025, 0.7873],\n",
      "        [0.2940, 0.2703, 0.1793,  ..., 0.6481, 0.8175, 0.1229]],\n",
      "       device='cuda:3', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.0295, 0.6602, 0.4682,  ..., 0.6160, 0.9924, 0.5102],\n",
      "        [0.0258, 0.0251, 0.4676,  ..., 0.6152, 0.4063, 0.0058],\n",
      "        [0.1510, 0.2381, 0.9586,  ..., 0.0458, 0.0166, 0.1934],\n",
      "        ...,\n",
      "        [0.1739, 0.8597, 0.7957,  ..., 0.0690, 0.4410, 0.7061],\n",
      "        [0.8093, 0.8423, 0.3934,  ..., 0.0204, 0.7026, 0.7873],\n",
      "        [0.2940, 0.2703, 0.1792,  ..., 0.6481, 0.8175, 0.1230]],\n",
      "       device='cuda:3', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    train(model, train_loader, val_loader, optimizer, scheduler, rev_label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9999998981753985, 0.9999999803646157, 0.9999999864182124, 0.9999999892897904, 0.9999999924718092, 0.9999999944896748, 0.9999999996895591, 0.9999999961194893, 0.9999999962747097, 1.0000000073729705, 1.000000001785035, 1.0000000041133414, 1.0000000067520887, 0.9999999972836425, 1.0000000027163576, 1.000000005820766, 1.0000000027163576, 1.0000000069849193, 1.0000000080714624, 1.0000000003104408, 1.0000000062088172, 1.0000000035700698, 1.0000000029491882, 1.000000002561137, 1.000000002561137, 1.0000000035700698, 1.0000000042685617, 1.000000005743156, 1.0000000041909516, 1.0000000028715779, 1.0000000015522044]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = model.history_w['cos']\n",
    "y = [x[0] for x in y]\n",
    "print(y)\n",
    "\n",
    "import pylab\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(2, 1, 1)\n",
    "line, = ax.plot(y, color='blue')\n",
    "ax.set_yscale('log')\n",
    "plt.title('Row wise average cos similarity')\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModule(\n",
      "  (feature_modules): ModuleDict(\n",
      "    (distance): Embedding(5, 3, padding_idx=0)\n",
      "    (u1_depdir): Embedding(5, 3, padding_idx=0)\n",
      "    (u2_depdir): Embedding(5, 3, padding_idx=0)\n",
      "    (u2_func): Embedding(21, 5, padding_idx=0)\n",
      "    (u1_position): Embedding(12, 4, padding_idx=0)\n",
      "    (u2_position): Embedding(12, 4, padding_idx=0)\n",
      "    (sat_children): Identity()\n",
      "    (nuc_children): Identity()\n",
      "  )\n",
      ")\n",
      "ModuleDict(\n",
      "  (distance): Embedding(5, 3, padding_idx=0)\n",
      "  (u1_depdir): Embedding(5, 3, padding_idx=0)\n",
      "  (u2_depdir): Embedding(5, 3, padding_idx=0)\n",
      "  (u2_func): Embedding(21, 5, padding_idx=0)\n",
      "  (u1_position): Embedding(12, 4, padding_idx=0)\n",
      "  (u2_position): Embedding(12, 4, padding_idx=0)\n",
      "  (sat_children): Identity()\n",
      "  (nuc_children): Identity()\n",
      ")\n",
      "Embedding(5, 3, padding_idx=0)\n",
      "Embedding(5, 3, padding_idx=0)\n",
      "Embedding(5, 3, padding_idx=0)\n",
      "Embedding(21, 5, padding_idx=0)\n",
      "Embedding(12, 4, padding_idx=0)\n",
      "Embedding(12, 4, padding_idx=0)\n",
      "Identity()\n",
      "Identity()\n"
     ]
    }
   ],
   "source": [
    "for i in model.module1.modules():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/modeling_utils.py:763: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_loss: 2.4734 test_acc: 0.3038\n",
      "00:00:01.34\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    antithesis       1.00      0.17      0.29        18\n",
      "    background       0.26      0.47      0.33        17\n",
      "         cause       0.00      0.00      0.00         2\n",
      "  circumstance       0.50      0.13      0.21        15\n",
      "    concession       0.29      0.54      0.38        13\n",
      "     condition       0.67      0.89      0.76         9\n",
      "   conjunction       0.36      0.57      0.44         7\n",
      "      contrast       0.00      0.00      0.00         8\n",
      " e-elaboration       0.54      0.64      0.58        11\n",
      "   elaboration       0.14      0.30      0.19        10\n",
      "  evaluation-n       0.00      0.00      0.00         3\n",
      "  evaluation-s       0.00      0.00      0.00        17\n",
      "      evidence       0.00      0.00      0.00        10\n",
      "interpretation       0.00      0.00      0.00        12\n",
      "         joint       0.19      0.28      0.22        29\n",
      "          list       0.48      0.50      0.49        26\n",
      "         means       0.00      0.00      0.00         2\n",
      "   preparation       0.44      1.00      0.62         4\n",
      "       purpose       1.00      0.67      0.80         3\n",
      "        reason       0.33      0.29      0.31        34\n",
      "   restatement       0.00      0.00      0.00         1\n",
      "        result       0.00      0.00      0.00         0\n",
      "      sequence       0.00      0.00      0.00         7\n",
      "  solutionhood       0.00      0.00      0.00         1\n",
      "       summary       0.00      0.00      0.00         1\n",
      "\n",
      "      accuracy                           0.30       260\n",
      "     macro avg       0.25      0.26      0.23       260\n",
      "  weighted avg       0.32      0.30      0.28       260\n",
      "\n",
      "Test Loss: 2.473 |  Test Acc: 30.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#latest\n",
    "def validate(model, test_loader, optimizer, rev_label_dict, label_dict):\n",
    "  start = time.time()\n",
    "  test_acc, test_loss, cr, model, optimizer = evaluate_accuracy(model, optimizer, test_loader, rev_label_dict, label_dict, is_training=False)\n",
    "  end = time.time()\n",
    "  hours, rem = divmod(end-start, 3600)\n",
    "  minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "  print(f'Test_loss: {test_loss:.4f} test_acc: {test_acc:.4f}')\n",
    "  print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "  print(cr)\n",
    "\n",
    "  return test_loss, test_acc\n",
    "\n",
    "\n",
    "test_loss, test_acc = validate(model, test_loader, optimizer, rev_label_dict, label_dict)\n",
    "writer.add_scalar('test_loss_latest', test_loss, 1)\n",
    "writer.add_scalar('test_acc_latest', test_acc, 1)\n",
    "print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/modeling_utils.py:763: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_loss: 2.3957 test_acc: 0.3038\n",
      "00:00:01.56\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    antithesis       1.00      0.06      0.11        18\n",
      "    background       0.28      0.41      0.33        17\n",
      "         cause       0.00      0.00      0.00         2\n",
      "  circumstance       0.67      0.27      0.38        15\n",
      "    concession       0.32      0.62      0.42        13\n",
      "     condition       0.56      1.00      0.72         9\n",
      "   conjunction       0.36      0.57      0.44         7\n",
      "      contrast       0.00      0.00      0.00         8\n",
      " e-elaboration       0.47      0.64      0.54        11\n",
      "   elaboration       0.20      0.40      0.27        10\n",
      "  evaluation-n       0.00      0.00      0.00         3\n",
      "  evaluation-s       0.00      0.00      0.00        17\n",
      "      evidence       0.00      0.00      0.00        10\n",
      "interpretation       0.00      0.00      0.00        12\n",
      "         joint       0.18      0.28      0.22        29\n",
      "          list       0.46      0.50      0.48        26\n",
      "         means       0.00      0.00      0.00         2\n",
      "   preparation       0.33      0.75      0.46         4\n",
      "       purpose       1.00      0.33      0.50         3\n",
      "        reason       0.32      0.29      0.31        34\n",
      "   restatement       0.00      0.00      0.00         1\n",
      "      sequence       0.00      0.00      0.00         7\n",
      "  solutionhood       0.00      0.00      0.00         1\n",
      "       summary       0.00      0.00      0.00         1\n",
      "\n",
      "      accuracy                           0.30       260\n",
      "     macro avg       0.26      0.25      0.22       260\n",
      "  weighted avg       0.32      0.30      0.27       260\n",
      "\n",
      "Latest Test Loss: 2.396 |  Latest Test Acc: 30.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#best earliest\n",
    "model.load_state_dict(torch.load(save_path_suffix+'_best.pt'))\n",
    "test_loss, test_acc = validate(model, test_loader, optimizer, rev_label_dict, label_dict)\n",
    "writer.add_scalar('test_loss_best_earliest', test_loss, 1)\n",
    "writer.add_scalar('test_acc_best_earliest', test_acc, 1)\n",
    "print(f'Latest Test Loss: {test_loss:.3f} |  Latest Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/modeling_utils.py:763: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_loss: 2.3957 test_acc: 0.3038\n",
      "00:00:01.65\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    antithesis       1.00      0.06      0.11        18\n",
      "    background       0.28      0.41      0.33        17\n",
      "         cause       0.00      0.00      0.00         2\n",
      "  circumstance       0.67      0.27      0.38        15\n",
      "    concession       0.32      0.62      0.42        13\n",
      "     condition       0.56      1.00      0.72         9\n",
      "   conjunction       0.36      0.57      0.44         7\n",
      "      contrast       0.00      0.00      0.00         8\n",
      " e-elaboration       0.47      0.64      0.54        11\n",
      "   elaboration       0.20      0.40      0.27        10\n",
      "  evaluation-n       0.00      0.00      0.00         3\n",
      "  evaluation-s       0.00      0.00      0.00        17\n",
      "      evidence       0.00      0.00      0.00        10\n",
      "interpretation       0.00      0.00      0.00        12\n",
      "         joint       0.18      0.28      0.22        29\n",
      "          list       0.46      0.50      0.48        26\n",
      "         means       0.00      0.00      0.00         2\n",
      "   preparation       0.33      0.75      0.46         4\n",
      "       purpose       1.00      0.33      0.50         3\n",
      "        reason       0.32      0.29      0.31        34\n",
      "   restatement       0.00      0.00      0.00         1\n",
      "      sequence       0.00      0.00      0.00         7\n",
      "  solutionhood       0.00      0.00      0.00         1\n",
      "       summary       0.00      0.00      0.00         1\n",
      "\n",
      "      accuracy                           0.30       260\n",
      "     macro avg       0.26      0.25      0.22       260\n",
      "  weighted avg       0.32      0.30      0.27       260\n",
      "\n",
      "Best Test Loss: 2.396 |  Best Test Acc: 30.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#best lastest\n",
    "model.load_state_dict(torch.load(save_path_suffix+'_best_latest.pt'))\n",
    "test_loss, test_acc = validate(model, test_loader, optimizer, rev_label_dict, label_dict)\n",
    "writer.add_scalar('test_loss_best_latest', test_loss, 1)\n",
    "writer.add_scalar('test_acc_best_latest', test_acc, 1)\n",
    "print(f'Best Test Loss: {test_loss:.3f} |  Best Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/modeling_utils.py:763: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_loss: 2.2431 test_acc: 0.3074\n",
      "00:00:01.22\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    antithesis       0.00      0.00      0.00        11\n",
      "    background       0.24      0.24      0.24        17\n",
      "         cause       0.00      0.00      0.00         7\n",
      "  circumstance       0.10      0.08      0.09        13\n",
      "    concession       0.29      0.91      0.43        11\n",
      "     condition       0.46      0.75      0.57         8\n",
      "   conjunction       0.75      0.75      0.75         8\n",
      "      contrast       0.00      0.00      0.00         3\n",
      " e-elaboration       0.69      0.69      0.69        13\n",
      "   elaboration       0.16      0.11      0.13        28\n",
      "  evaluation-n       0.00      0.00      0.00         8\n",
      "  evaluation-s       0.00      0.00      0.00         5\n",
      "      evidence       1.00      0.12      0.22         8\n",
      "interpretation       0.07      0.08      0.07        13\n",
      "         joint       0.20      0.50      0.29        18\n",
      "          list       0.26      0.39      0.31        18\n",
      "         means       0.00      0.00      0.00         1\n",
      "   preparation       0.62      0.73      0.67        11\n",
      "       purpose       0.00      0.00      0.00         5\n",
      "        reason       0.43      0.36      0.39        28\n",
      "   restatement       0.00      0.00      0.00         1\n",
      "        result       0.00      0.00      0.00         3\n",
      "  solutionhood       0.00      0.00      0.00         1\n",
      "       summary       0.00      0.00      0.00         2\n",
      "\n",
      "      accuracy                           0.31       241\n",
      "     macro avg       0.22      0.24      0.20       241\n",
      "  weighted avg       0.28      0.31      0.27       241\n",
      "\n",
      "Val Loss: 2.243 |  Val Acc: 30.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#best val acc\n",
    "model.load_state_dict(torch.load(save_path_suffix+'_best_latest.pt'))\n",
    "test_loss, test_acc = validate(model, val_loader, optimizer, rev_label_dict, label_dict)\n",
    "writer.add_scalar('val_loss_best_latest', test_loss, 1)\n",
    "writer.add_scalar('val_acc_best_latest', test_acc, 1)\n",
    "print(f'Val Loss: {test_loss:.3f} |  Val Acc: {test_acc*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3409ea685db85227fbd9509d1b1ace14d085473eb2d57f3ba9dd0302d25f838"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

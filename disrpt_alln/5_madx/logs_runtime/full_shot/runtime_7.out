-------------------------------------------------------------------
Lang1:  deu.rst.pcc    Lang2:  nld.rst.nldt
Saving run to:  runs/full_shot/FullShot=v4_deu.rst.pcc_nld.rst.nldt_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2164 examples
read 241 examples
read 260 examples
read 1608 examples
read 331 examples
read 326 examples
Total prediction labels:  39
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=39, bias=True)
    )
  )
)
{'train@deu.rst.pcc_loss': 3.2586898803710938, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.09796672828096119, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.01349280252236547, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.009817436009058541, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.039557904061473165, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.2586898803710938, 'train@deu.rst.pcc_runtime': 26.4206, 'train@deu.rst.pcc_samples_per_second': 81.906, 'train@deu.rst.pcc_steps_per_second': 2.574, 'epoch': 1.0}
{'loss': 3.492, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.2961227893829346, 'eval_accuracy@deu.rst.pcc': 0.06639004149377593, 'eval_f1@deu.rst.pcc': 0.012348423933789788, 'eval_precision@deu.rst.pcc': 0.011240932036238424, 'eval_recall@deu.rst.pcc': 0.045177045177045176, 'eval_loss@deu.rst.pcc': 3.2961230278015137, 'eval_runtime': 3.2472, 'eval_samples_per_second': 74.218, 'eval_steps_per_second': 2.464, 'epoch': 1.0}
{'train@deu.rst.pcc_loss': 2.9960451126098633, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.1099815157116451, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.020464027712397657, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.049174331173809795, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.04412820350181598, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.9960453510284424, 'train@deu.rst.pcc_runtime': 26.4244, 'train@deu.rst.pcc_samples_per_second': 81.894, 'train@deu.rst.pcc_steps_per_second': 2.573, 'epoch': 2.0}
{'loss': 3.1193, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.0406484603881836, 'eval_accuracy@deu.rst.pcc': 0.12863070539419086, 'eval_f1@deu.rst.pcc': 0.017627357659086903, 'eval_precision@deu.rst.pcc': 0.015016619183285848, 'eval_recall@deu.rst.pcc': 0.04861111111111111, 'eval_loss@deu.rst.pcc': 3.0406486988067627, 'eval_runtime': 3.2563, 'eval_samples_per_second': 74.01, 'eval_steps_per_second': 2.457, 'epoch': 2.0}
{'train@deu.rst.pcc_loss': 2.9140512943267822, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.12939001848428835, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.03441161790708164, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.12252343456320441, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.055444412251071995, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.9140512943267822, 'train@deu.rst.pcc_runtime': 26.4818, 'train@deu.rst.pcc_samples_per_second': 81.717, 'train@deu.rst.pcc_steps_per_second': 2.568, 'epoch': 3.0}
{'loss': 2.979, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.967074394226074, 'eval_accuracy@deu.rst.pcc': 0.14522821576763487, 'eval_f1@deu.rst.pcc': 0.03333408478363241, 'eval_precision@deu.rst.pcc': 0.04009597378277153, 'eval_recall@deu.rst.pcc': 0.06254451566951567, 'eval_loss@deu.rst.pcc': 2.967074155807495, 'eval_runtime': 3.284, 'eval_samples_per_second': 73.385, 'eval_steps_per_second': 2.436, 'epoch': 3.0}
{'train@deu.rst.pcc_loss': 2.8586783409118652, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.1866913123844732, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.07892297136190682, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.0812634422396822, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.10479445021391552, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.8586783409118652, 'train@deu.rst.pcc_runtime': 26.7539, 'train@deu.rst.pcc_samples_per_second': 80.885, 'train@deu.rst.pcc_steps_per_second': 2.542, 'epoch': 4.0}
{'loss': 2.923, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.9197890758514404, 'eval_accuracy@deu.rst.pcc': 0.1825726141078838, 'eval_f1@deu.rst.pcc': 0.07630582961621098, 'eval_precision@deu.rst.pcc': 0.10262772033862932, 'eval_recall@deu.rst.pcc': 0.10134310134310133, 'eval_loss@deu.rst.pcc': 2.9197890758514404, 'eval_runtime': 3.2732, 'eval_samples_per_second': 73.628, 'eval_steps_per_second': 2.444, 'epoch': 4.0}
{'train@deu.rst.pcc_loss': 2.816530704498291, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.20748613678373382, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08392940501195002, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.08609355625569294, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.12388438586804955, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.81653094291687, 'train@deu.rst.pcc_runtime': 26.5508, 'train@deu.rst.pcc_samples_per_second': 81.504, 'train@deu.rst.pcc_steps_per_second': 2.561, 'epoch': 5.0}
{'loss': 2.8677, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.8842592239379883, 'eval_accuracy@deu.rst.pcc': 0.18672199170124482, 'eval_f1@deu.rst.pcc': 0.07568734295401051, 'eval_precision@deu.rst.pcc': 0.08885618972300828, 'eval_recall@deu.rst.pcc': 0.10922237484737485, 'eval_loss@deu.rst.pcc': 2.8842594623565674, 'eval_runtime': 3.2755, 'eval_samples_per_second': 73.577, 'eval_steps_per_second': 2.442, 'epoch': 5.0}
{'train@deu.rst.pcc_loss': 2.781006336212158, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.21072088724584104, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08298184131059824, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.07902202000864368, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.12743368029266355, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.781006336212158, 'train@deu.rst.pcc_runtime': 26.5537, 'train@deu.rst.pcc_samples_per_second': 81.495, 'train@deu.rst.pcc_steps_per_second': 2.561, 'epoch': 6.0}
{'loss': 2.8343, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.85254168510437, 'eval_accuracy@deu.rst.pcc': 0.2074688796680498, 'eval_f1@deu.rst.pcc': 0.08713825522493557, 'eval_precision@deu.rst.pcc': 0.09054165546777042, 'eval_recall@deu.rst.pcc': 0.13869810744810745, 'eval_loss@deu.rst.pcc': 2.852541923522949, 'eval_runtime': 3.2952, 'eval_samples_per_second': 73.137, 'eval_steps_per_second': 2.428, 'epoch': 6.0}
{'train@deu.rst.pcc_loss': 2.754326820373535, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.21534195933456562, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.0843288305980472, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.07787976488185679, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.1305952958586899, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.754326581954956, 'train@deu.rst.pcc_runtime': 26.5327, 'train@deu.rst.pcc_samples_per_second': 81.56, 'train@deu.rst.pcc_steps_per_second': 2.563, 'epoch': 7.0}
{'loss': 2.8046, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.830644369125366, 'eval_accuracy@deu.rst.pcc': 0.2033195020746888, 'eval_f1@deu.rst.pcc': 0.08210745062885853, 'eval_precision@deu.rst.pcc': 0.08473216659071921, 'eval_recall@deu.rst.pcc': 0.13377594627594627, 'eval_loss@deu.rst.pcc': 2.8306446075439453, 'eval_runtime': 3.2781, 'eval_samples_per_second': 73.519, 'eval_steps_per_second': 2.44, 'epoch': 7.0}
{'train@deu.rst.pcc_loss': 2.7331886291503906, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.21534195933456562, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.0842191218968886, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.0803786589913318, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.13143338144082736, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7331883907318115, 'train@deu.rst.pcc_runtime': 26.5059, 'train@deu.rst.pcc_samples_per_second': 81.642, 'train@deu.rst.pcc_steps_per_second': 2.565, 'epoch': 8.0}
{'loss': 2.7751, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.814570665359497, 'eval_accuracy@deu.rst.pcc': 0.2074688796680498, 'eval_f1@deu.rst.pcc': 0.08056322314858691, 'eval_precision@deu.rst.pcc': 0.07952457765318058, 'eval_recall@deu.rst.pcc': 0.13876170126170126, 'eval_loss@deu.rst.pcc': 2.814570665359497, 'eval_runtime': 3.2555, 'eval_samples_per_second': 74.028, 'eval_steps_per_second': 2.457, 'epoch': 8.0}
{'train@deu.rst.pcc_loss': 2.716202974319458, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2199630314232902, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08643599826891925, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.08037584454543398, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.13350649679826676, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.716202735900879, 'train@deu.rst.pcc_runtime': 26.5205, 'train@deu.rst.pcc_samples_per_second': 81.597, 'train@deu.rst.pcc_steps_per_second': 2.564, 'epoch': 9.0}
{'loss': 2.7617, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.8038222789764404, 'eval_accuracy@deu.rst.pcc': 0.21991701244813278, 'eval_f1@deu.rst.pcc': 0.09062089570205117, 'eval_precision@deu.rst.pcc': 0.09314103312005677, 'eval_recall@deu.rst.pcc': 0.14570614570614573, 'eval_loss@deu.rst.pcc': 2.8038220405578613, 'eval_runtime': 3.2928, 'eval_samples_per_second': 73.191, 'eval_steps_per_second': 2.43, 'epoch': 9.0}
{'train@deu.rst.pcc_loss': 2.704796552658081, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.22134935304990758, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08762124542141893, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.087619613933481, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.13544066868386656, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.704796552658081, 'train@deu.rst.pcc_runtime': 26.5218, 'train@deu.rst.pcc_samples_per_second': 81.593, 'train@deu.rst.pcc_steps_per_second': 2.564, 'epoch': 10.0}
{'loss': 2.7441, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.7919936180114746, 'eval_accuracy@deu.rst.pcc': 0.21991701244813278, 'eval_f1@deu.rst.pcc': 0.08965700765381655, 'eval_precision@deu.rst.pcc': 0.09006902426713748, 'eval_recall@deu.rst.pcc': 0.1465964590964591, 'eval_loss@deu.rst.pcc': 2.7919933795928955, 'eval_runtime': 3.2625, 'eval_samples_per_second': 73.871, 'eval_steps_per_second': 2.452, 'epoch': 10.0}
{'train@deu.rst.pcc_loss': 2.6979470252990723, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2222735674676525, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08792523313854009, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.08613735982564148, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.1357982836406234, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.697946786880493, 'train@deu.rst.pcc_runtime': 26.5029, 'train@deu.rst.pcc_samples_per_second': 81.651, 'train@deu.rst.pcc_steps_per_second': 2.566, 'epoch': 11.0}
{'loss': 2.7343, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.7864012718200684, 'eval_accuracy@deu.rst.pcc': 0.2074688796680498, 'eval_f1@deu.rst.pcc': 0.08534852445949315, 'eval_precision@deu.rst.pcc': 0.08794112489764662, 'eval_recall@deu.rst.pcc': 0.13869810744810745, 'eval_loss@deu.rst.pcc': 2.7864012718200684, 'eval_runtime': 3.2733, 'eval_samples_per_second': 73.627, 'eval_steps_per_second': 2.444, 'epoch': 11.0}
{'train@deu.rst.pcc_loss': 2.695674419403076, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.22181146025878004, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08779007828043925, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.08555933908882338, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.13581855167044518, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.695674180984497, 'train@deu.rst.pcc_runtime': 26.4564, 'train@deu.rst.pcc_samples_per_second': 81.795, 'train@deu.rst.pcc_steps_per_second': 2.57, 'epoch': 12.0}
{'loss': 2.7218, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.784092664718628, 'eval_accuracy@deu.rst.pcc': 0.2074688796680498, 'eval_f1@deu.rst.pcc': 0.08397779098987546, 'eval_precision@deu.rst.pcc': 0.08476594359746532, 'eval_recall@deu.rst.pcc': 0.13958842083842085, 'eval_loss@deu.rst.pcc': 2.784092664718628, 'eval_runtime': 3.2645, 'eval_samples_per_second': 73.824, 'eval_steps_per_second': 2.451, 'epoch': 12.0}
{'train_runtime': 1019.028, 'train_samples_per_second': 25.483, 'train_steps_per_second': 0.801, 'train_loss': 2.896397403642243, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.8964
  train_runtime            = 0:16:59.02
  train_samples_per_second =     25.483
  train_steps_per_second   =      0.801
{'train@nld.rst.nldt_loss': 3.1831908226013184, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2493781094527363, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.018786642202737708, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.014405528817667467, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03380459367655108, 'train@nld.rst.nldt_loss@nld.rst.nldt': 3.1831908226013184, 'train@nld.rst.nldt_runtime': 19.7581, 'train@nld.rst.nldt_samples_per_second': 81.384, 'train@nld.rst.nldt_steps_per_second': 2.581, 'epoch': 1.0}
{'loss': 3.4507, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.131580114364624, 'eval_accuracy@nld.rst.nldt': 0.27794561933534745, 'eval_f1@nld.rst.nldt': 0.022016079158936306, 'eval_precision@nld.rst.nldt': 0.018271604938271607, 'eval_recall@nld.rst.nldt': 0.03965953531170923, 'eval_loss@nld.rst.nldt': 3.131579637527466, 'eval_runtime': 4.3428, 'eval_samples_per_second': 76.218, 'eval_steps_per_second': 2.533, 'epoch': 1.0}
{'train@nld.rst.nldt_loss': 2.9015986919403076, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2885572139303483, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.025130305045375063, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.01703493130034027, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.0480765639589169, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.9015986919403076, 'train@nld.rst.nldt_runtime': 19.7239, 'train@nld.rst.nldt_samples_per_second': 81.525, 'train@nld.rst.nldt_steps_per_second': 2.586, 'epoch': 2.0}
{'loss': 3.0475, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.8425843715667725, 'eval_accuracy@nld.rst.nldt': 0.3081570996978852, 'eval_f1@nld.rst.nldt': 0.03090587461680235, 'eval_precision@nld.rst.nldt': 0.022995123975516132, 'eval_recall@nld.rst.nldt': 0.051552795031055906, 'eval_loss@nld.rst.nldt': 2.8425848484039307, 'eval_runtime': 4.3653, 'eval_samples_per_second': 75.825, 'eval_steps_per_second': 2.52, 'epoch': 2.0}
{'train@nld.rst.nldt_loss': 2.77778959274292, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.29539800995024873, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.02652247165896387, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.019962301886386147, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.049292717086834734, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.7777891159057617, 'train@nld.rst.nldt_runtime': 19.7575, 'train@nld.rst.nldt_samples_per_second': 81.387, 'train@nld.rst.nldt_steps_per_second': 2.581, 'epoch': 3.0}
{'loss': 2.8625, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.7250924110412598, 'eval_accuracy@nld.rst.nldt': 0.3081570996978852, 'eval_f1@nld.rst.nldt': 0.030093426432294037, 'eval_precision@nld.rst.nldt': 0.022708139107834, 'eval_recall@nld.rst.nldt': 0.05024154589371981, 'eval_loss@nld.rst.nldt': 2.7250924110412598, 'eval_runtime': 4.3867, 'eval_samples_per_second': 75.455, 'eval_steps_per_second': 2.508, 'epoch': 3.0}
{'train@nld.rst.nldt_loss': 2.718433380126953, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3003731343283582, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.02973172166331045, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.024600094678916017, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.04988211951447245, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.7184336185455322, 'train@nld.rst.nldt_runtime': 19.7707, 'train@nld.rst.nldt_samples_per_second': 81.333, 'train@nld.rst.nldt_steps_per_second': 2.58, 'epoch': 4.0}
{'loss': 2.7521, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.6723685264587402, 'eval_accuracy@nld.rst.nldt': 0.3293051359516616, 'eval_f1@nld.rst.nldt': 0.04467032077522607, 'eval_precision@nld.rst.nldt': 0.04371886506389431, 'eval_recall@nld.rst.nldt': 0.06067914014773918, 'eval_loss@nld.rst.nldt': 2.672368288040161, 'eval_runtime': 4.36, 'eval_samples_per_second': 75.918, 'eval_steps_per_second': 2.523, 'epoch': 4.0}
{'train@nld.rst.nldt_loss': 2.674779176712036, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31094527363184077, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.035631119757459744, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.0283782665425696, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05602124183006536, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.674778938293457, 'train@nld.rst.nldt_runtime': 19.7692, 'train@nld.rst.nldt_samples_per_second': 81.339, 'train@nld.rst.nldt_steps_per_second': 2.58, 'epoch': 5.0}
{'loss': 2.7152, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.6324589252471924, 'eval_accuracy@nld.rst.nldt': 0.33534743202416917, 'eval_f1@nld.rst.nldt': 0.04482379954078067, 'eval_precision@nld.rst.nldt': 0.03904143895710415, 'eval_recall@nld.rst.nldt': 0.06313932980599647, 'eval_loss@nld.rst.nldt': 2.6324589252471924, 'eval_runtime': 4.3585, 'eval_samples_per_second': 75.943, 'eval_steps_per_second': 2.524, 'epoch': 5.0}
{'train@nld.rst.nldt_loss': 2.641462802886963, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31654228855721395, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03799051250077732, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.02923190458130498, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05953548085901027, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.641463041305542, 'train@nld.rst.nldt_runtime': 19.8109, 'train@nld.rst.nldt_samples_per_second': 81.167, 'train@nld.rst.nldt_steps_per_second': 2.574, 'epoch': 6.0}
{'loss': 2.6803, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.6054773330688477, 'eval_accuracy@nld.rst.nldt': 0.3413897280966767, 'eval_f1@nld.rst.nldt': 0.04752895968365581, 'eval_precision@nld.rst.nldt': 0.03905997980072055, 'eval_recall@nld.rst.nldt': 0.0689095928226363, 'eval_loss@nld.rst.nldt': 2.6054773330688477, 'eval_runtime': 4.6681, 'eval_samples_per_second': 70.906, 'eval_steps_per_second': 2.356, 'epoch': 6.0}
{'train@nld.rst.nldt_loss': 2.613171339035034, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3208955223880597, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03891655627903505, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.02896154406000657, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.06226190476190476, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.613171100616455, 'train@nld.rst.nldt_runtime': 19.7928, 'train@nld.rst.nldt_samples_per_second': 81.242, 'train@nld.rst.nldt_steps_per_second': 2.577, 'epoch': 7.0}
{'loss': 2.6498, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.5821447372436523, 'eval_accuracy@nld.rst.nldt': 0.3323262839879154, 'eval_f1@nld.rst.nldt': 0.04474411733670993, 'eval_precision@nld.rst.nldt': 0.035037716287716285, 'eval_recall@nld.rst.nldt': 0.06770186335403727, 'eval_loss@nld.rst.nldt': 2.5821447372436523, 'eval_runtime': 4.368, 'eval_samples_per_second': 75.779, 'eval_steps_per_second': 2.518, 'epoch': 7.0}
{'train@nld.rst.nldt_loss': 2.5902812480926514, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3246268656716418, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03975357154826329, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.033211997780387956, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.06564663002882312, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.5902812480926514, 'train@nld.rst.nldt_runtime': 19.823, 'train@nld.rst.nldt_samples_per_second': 81.118, 'train@nld.rst.nldt_steps_per_second': 2.573, 'epoch': 8.0}
{'loss': 2.6358, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.565093994140625, 'eval_accuracy@nld.rst.nldt': 0.3413897280966767, 'eval_f1@nld.rst.nldt': 0.049434871385519524, 'eval_precision@nld.rst.nldt': 0.04784457595142845, 'eval_recall@nld.rst.nldt': 0.07423638269048898, 'eval_loss@nld.rst.nldt': 2.565093994140625, 'eval_runtime': 4.3704, 'eval_samples_per_second': 75.737, 'eval_steps_per_second': 2.517, 'epoch': 8.0}
{'train@nld.rst.nldt_loss': 2.576859951019287, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3246268656716418, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03986805636599854, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.03263615565777729, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.06558827335281939, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.576860189437866, 'train@nld.rst.nldt_runtime': 19.7974, 'train@nld.rst.nldt_samples_per_second': 81.223, 'train@nld.rst.nldt_steps_per_second': 2.576, 'epoch': 9.0}
{'loss': 2.6132, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.553697347640991, 'eval_accuracy@nld.rst.nldt': 0.34441087613293053, 'eval_f1@nld.rst.nldt': 0.04976139264600447, 'eval_precision@nld.rst.nldt': 0.04514591744556901, 'eval_recall@nld.rst.nldt': 0.07463895918002199, 'eval_loss@nld.rst.nldt': 2.553697347640991, 'eval_runtime': 4.3587, 'eval_samples_per_second': 75.94, 'eval_steps_per_second': 2.524, 'epoch': 9.0}
{'train@nld.rst.nldt_loss': 2.566561698913574, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3252487562189055, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.04001849014864754, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.03192628791432351, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.06525447316607802, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.566561698913574, 'train@nld.rst.nldt_runtime': 19.8402, 'train@nld.rst.nldt_samples_per_second': 81.048, 'train@nld.rst.nldt_steps_per_second': 2.571, 'epoch': 10.0}
{'loss': 2.5976, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.5448546409606934, 'eval_accuracy@nld.rst.nldt': 0.34441087613293053, 'eval_f1@nld.rst.nldt': 0.04988610179396794, 'eval_precision@nld.rst.nldt': 0.04540800963565191, 'eval_recall@nld.rst.nldt': 0.07463895918002199, 'eval_loss@nld.rst.nldt': 2.5448548793792725, 'eval_runtime': 4.3765, 'eval_samples_per_second': 75.631, 'eval_steps_per_second': 2.513, 'epoch': 10.0}
{'train@nld.rst.nldt_loss': 2.5595102310180664, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3252487562189055, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03982136098718986, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.03163674573552531, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.06542895962732918, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.5595104694366455, 'train@nld.rst.nldt_runtime': 19.816, 'train@nld.rst.nldt_samples_per_second': 81.147, 'train@nld.rst.nldt_steps_per_second': 2.574, 'epoch': 11.0}
{'loss': 2.5839, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.5395772457122803, 'eval_accuracy@nld.rst.nldt': 0.34441087613293053, 'eval_f1@nld.rst.nldt': 0.04988610179396794, 'eval_precision@nld.rst.nldt': 0.04540800963565191, 'eval_recall@nld.rst.nldt': 0.07463895918002199, 'eval_loss@nld.rst.nldt': 2.5395774841308594, 'eval_runtime': 4.3557, 'eval_samples_per_second': 75.993, 'eval_steps_per_second': 2.525, 'epoch': 11.0}
{'train@nld.rst.nldt_loss': 2.557033061981201, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.32587064676616917, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.039967047606888566, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.03171748807044574, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.06573533217634879, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.5570333003997803, 'train@nld.rst.nldt_runtime': 19.8423, 'train@nld.rst.nldt_samples_per_second': 81.039, 'train@nld.rst.nldt_steps_per_second': 2.57, 'epoch': 12.0}
{'loss': 2.593, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.537999391555786, 'eval_accuracy@nld.rst.nldt': 0.3504531722054381, 'eval_f1@nld.rst.nldt': 0.055032353254800354, 'eval_precision@nld.rst.nldt': 0.052185334414582625, 'eval_recall@nld.rst.nldt': 0.07816629604069218, 'eval_loss@nld.rst.nldt': 2.5379996299743652, 'eval_runtime': 4.3702, 'eval_samples_per_second': 75.74, 'eval_steps_per_second': 2.517, 'epoch': 12.0}
{'train_runtime': 781.5662, 'train_samples_per_second': 24.689, 'train_steps_per_second': 0.783, 'train_loss': 2.7651303110559002, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.8964
  train_runtime            = 0:16:59.02
  train_samples_per_second =     25.483
  train_steps_per_second   =      0.801
-------------------------------------------------------------------
Lang1:  eng.pdtb.pdtb    Lang2:  nld.rst.nldt
Saving run to:  runs/full_shot/FullShot=v4_eng.pdtb.pdtb_nld.rst.nldt_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 43920 examples
read 1674 examples
read 2257 examples
read 1608 examples
read 331 examples
read 326 examples
Total prediction labels:  56
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=56, bias=True)
    )
  )
)
{'train@eng.pdtb.pdtb_loss': 1.256255030632019, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.5986794171220401, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.27488348182005634, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.35064009119643125, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.2665045869763702, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.2562552690505981, 'train@eng.pdtb.pdtb_runtime': 522.686, 'train@eng.pdtb.pdtb_samples_per_second': 84.028, 'train@eng.pdtb.pdtb_steps_per_second': 2.627, 'epoch': 1.0}
{'loss': 1.8674, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.1832520961761475, 'eval_accuracy@eng.pdtb.pdtb': 0.6278375149342891, 'eval_f1@eng.pdtb.pdtb': 0.32664305768124446, 'eval_precision@eng.pdtb.pdtb': 0.41067234154363497, 'eval_recall@eng.pdtb.pdtb': 0.31833322108357515, 'eval_loss@eng.pdtb.pdtb': 1.1832520961761475, 'eval_runtime': 20.4224, 'eval_samples_per_second': 81.969, 'eval_steps_per_second': 2.595, 'epoch': 1.0}
{'train@eng.pdtb.pdtb_loss': 1.095585584640503, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.639344262295082, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.36567500609256454, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.47314896611959883, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.35142458477271554, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.0955857038497925, 'train@eng.pdtb.pdtb_runtime': 522.9313, 'train@eng.pdtb.pdtb_samples_per_second': 83.988, 'train@eng.pdtb.pdtb_steps_per_second': 2.626, 'epoch': 2.0}
{'loss': 1.2149, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.028640866279602, 'eval_accuracy@eng.pdtb.pdtb': 0.6654719235364397, 'eval_f1@eng.pdtb.pdtb': 0.4135647843759128, 'eval_precision@eng.pdtb.pdtb': 0.49084755808437225, 'eval_recall@eng.pdtb.pdtb': 0.39945929268334296, 'eval_loss@eng.pdtb.pdtb': 1.028640866279602, 'eval_runtime': 20.4175, 'eval_samples_per_second': 81.988, 'eval_steps_per_second': 2.596, 'epoch': 2.0}
{'train@eng.pdtb.pdtb_loss': 1.0468475818634033, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6545537340619307, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.43647812083794585, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.4709771427749942, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4237777895004127, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.0468477010726929, 'train@eng.pdtb.pdtb_runtime': 522.471, 'train@eng.pdtb.pdtb_samples_per_second': 84.062, 'train@eng.pdtb.pdtb_steps_per_second': 2.628, 'epoch': 3.0}
{'loss': 1.12, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 0.9894733428955078, 'eval_accuracy@eng.pdtb.pdtb': 0.6857825567502986, 'eval_f1@eng.pdtb.pdtb': 0.49260486775175094, 'eval_precision@eng.pdtb.pdtb': 0.5589185961639769, 'eval_recall@eng.pdtb.pdtb': 0.46951767747521733, 'eval_loss@eng.pdtb.pdtb': 0.9894733428955078, 'eval_runtime': 20.429, 'eval_samples_per_second': 81.942, 'eval_steps_per_second': 2.594, 'epoch': 3.0}
{'train@eng.pdtb.pdtb_loss': 1.0010064840316772, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.667304189435337, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.44900575015865873, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.47615624067579965, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4426754104673903, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.0010064840316772, 'train@eng.pdtb.pdtb_runtime': 523.5932, 'train@eng.pdtb.pdtb_samples_per_second': 83.882, 'train@eng.pdtb.pdtb_steps_per_second': 2.622, 'epoch': 4.0}
{'loss': 1.073, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 0.9498346447944641, 'eval_accuracy@eng.pdtb.pdtb': 0.6839904420549582, 'eval_f1@eng.pdtb.pdtb': 0.5080589220720316, 'eval_precision@eng.pdtb.pdtb': 0.5490623208864338, 'eval_recall@eng.pdtb.pdtb': 0.5000617099464371, 'eval_loss@eng.pdtb.pdtb': 0.9498347640037537, 'eval_runtime': 20.419, 'eval_samples_per_second': 81.982, 'eval_steps_per_second': 2.596, 'epoch': 4.0}
{'train@eng.pdtb.pdtb_loss': 0.9788182973861694, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6739526411657559, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4588588636235702, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5216705690689546, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.45512774950272933, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9788183569908142, 'train@eng.pdtb.pdtb_runtime': 522.6372, 'train@eng.pdtb.pdtb_samples_per_second': 84.035, 'train@eng.pdtb.pdtb_steps_per_second': 2.627, 'epoch': 5.0}
{'loss': 1.0433, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 0.9353471994400024, 'eval_accuracy@eng.pdtb.pdtb': 0.6875746714456392, 'eval_f1@eng.pdtb.pdtb': 0.5206239345730583, 'eval_precision@eng.pdtb.pdtb': 0.5471619465803566, 'eval_recall@eng.pdtb.pdtb': 0.5165005696166426, 'eval_loss@eng.pdtb.pdtb': 0.9353471994400024, 'eval_runtime': 20.4637, 'eval_samples_per_second': 81.803, 'eval_steps_per_second': 2.59, 'epoch': 5.0}
{'train@eng.pdtb.pdtb_loss': 0.9601431488990784, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6784380692167578, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.46522267031813197, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5274989049735155, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.46114426099521316, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9601430892944336, 'train@eng.pdtb.pdtb_runtime': 522.8981, 'train@eng.pdtb.pdtb_samples_per_second': 83.993, 'train@eng.pdtb.pdtb_steps_per_second': 2.626, 'epoch': 6.0}
{'loss': 1.0224, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 0.9280226230621338, 'eval_accuracy@eng.pdtb.pdtb': 0.6851851851851852, 'eval_f1@eng.pdtb.pdtb': 0.5273393937432823, 'eval_precision@eng.pdtb.pdtb': 0.5591097923896517, 'eval_recall@eng.pdtb.pdtb': 0.516912660197559, 'eval_loss@eng.pdtb.pdtb': 0.9280226826667786, 'eval_runtime': 20.436, 'eval_samples_per_second': 81.914, 'eval_steps_per_second': 2.593, 'epoch': 6.0}
{'train@eng.pdtb.pdtb_loss': 0.9484171271324158, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.680327868852459, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4683517732032685, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5339637323622944, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4608096921384131, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9484171271324158, 'train@eng.pdtb.pdtb_runtime': 522.9185, 'train@eng.pdtb.pdtb_samples_per_second': 83.99, 'train@eng.pdtb.pdtb_steps_per_second': 2.626, 'epoch': 7.0}
{'loss': 1.0077, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 0.9147976636886597, 'eval_accuracy@eng.pdtb.pdtb': 0.6923536439665472, 'eval_f1@eng.pdtb.pdtb': 0.547928522927714, 'eval_precision@eng.pdtb.pdtb': 0.6344097515914211, 'eval_recall@eng.pdtb.pdtb': 0.526049665333612, 'eval_loss@eng.pdtb.pdtb': 0.9147976636886597, 'eval_runtime': 20.4509, 'eval_samples_per_second': 81.855, 'eval_steps_per_second': 2.592, 'epoch': 7.0}
{'train@eng.pdtb.pdtb_loss': 0.9370628595352173, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6836748633879781, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.473759329440791, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5526936623731444, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4708542950358655, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9370628595352173, 'train@eng.pdtb.pdtb_runtime': 522.7954, 'train@eng.pdtb.pdtb_samples_per_second': 84.01, 'train@eng.pdtb.pdtb_steps_per_second': 2.626, 'epoch': 8.0}
{'loss': 0.9957, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 0.9115538597106934, 'eval_accuracy@eng.pdtb.pdtb': 0.6881720430107527, 'eval_f1@eng.pdtb.pdtb': 0.5437724702225671, 'eval_precision@eng.pdtb.pdtb': 0.5942911328380059, 'eval_recall@eng.pdtb.pdtb': 0.5279812222160416, 'eval_loss@eng.pdtb.pdtb': 0.9115538597106934, 'eval_runtime': 20.4465, 'eval_samples_per_second': 81.872, 'eval_steps_per_second': 2.592, 'epoch': 8.0}
{'train@eng.pdtb.pdtb_loss': 0.9291703104972839, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6861566484517304, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4738097555580735, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5155123120460388, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.47116350386552697, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9291701316833496, 'train@eng.pdtb.pdtb_runtime': 523.0049, 'train@eng.pdtb.pdtb_samples_per_second': 83.976, 'train@eng.pdtb.pdtb_steps_per_second': 2.625, 'epoch': 9.0}
{'loss': 0.9832, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 0.9093427658081055, 'eval_accuracy@eng.pdtb.pdtb': 0.6887694145758662, 'eval_f1@eng.pdtb.pdtb': 0.5476113765877237, 'eval_precision@eng.pdtb.pdtb': 0.588922798853403, 'eval_recall@eng.pdtb.pdtb': 0.5354242469216504, 'eval_loss@eng.pdtb.pdtb': 0.9093427658081055, 'eval_runtime': 20.4298, 'eval_samples_per_second': 81.939, 'eval_steps_per_second': 2.594, 'epoch': 9.0}
{'train@eng.pdtb.pdtb_loss': 0.9253674149513245, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6863387978142077, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4761119247300513, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5707787542262679, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4713732994227204, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9253672957420349, 'train@eng.pdtb.pdtb_runtime': 522.9703, 'train@eng.pdtb.pdtb_samples_per_second': 83.982, 'train@eng.pdtb.pdtb_steps_per_second': 2.625, 'epoch': 10.0}
{'loss': 0.9787, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 0.903638482093811, 'eval_accuracy@eng.pdtb.pdtb': 0.6875746714456392, 'eval_f1@eng.pdtb.pdtb': 0.5481931956107587, 'eval_precision@eng.pdtb.pdtb': 0.6164780276171514, 'eval_recall@eng.pdtb.pdtb': 0.5337197736794761, 'eval_loss@eng.pdtb.pdtb': 0.903638482093811, 'eval_runtime': 21.4913, 'eval_samples_per_second': 77.892, 'eval_steps_per_second': 2.466, 'epoch': 10.0}
{'train@eng.pdtb.pdtb_loss': 0.9208170771598816, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6882969034608379, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4783788256115011, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5599681093811449, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.47545094934285814, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.920816957950592, 'train@eng.pdtb.pdtb_runtime': 522.6269, 'train@eng.pdtb.pdtb_samples_per_second': 84.037, 'train@eng.pdtb.pdtb_steps_per_second': 2.627, 'epoch': 11.0}
{'loss': 0.9728, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.9040883779525757, 'eval_accuracy@eng.pdtb.pdtb': 0.6863799283154122, 'eval_f1@eng.pdtb.pdtb': 0.5466811071147577, 'eval_precision@eng.pdtb.pdtb': 0.5837404843913916, 'eval_recall@eng.pdtb.pdtb': 0.5345965368684402, 'eval_loss@eng.pdtb.pdtb': 0.9040883779525757, 'eval_runtime': 20.4312, 'eval_samples_per_second': 81.933, 'eval_steps_per_second': 2.594, 'epoch': 11.0}
{'train@eng.pdtb.pdtb_loss': 0.9196643233299255, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6889344262295082, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.47883485591367125, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5667904716500314, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4743302178388376, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9196641445159912, 'train@eng.pdtb.pdtb_runtime': 522.7288, 'train@eng.pdtb.pdtb_samples_per_second': 84.021, 'train@eng.pdtb.pdtb_steps_per_second': 2.627, 'epoch': 12.0}
{'loss': 0.9679, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.902333676815033, 'eval_accuracy@eng.pdtb.pdtb': 0.6893667861409797, 'eval_f1@eng.pdtb.pdtb': 0.5480428341598993, 'eval_precision@eng.pdtb.pdtb': 0.5910848923204985, 'eval_recall@eng.pdtb.pdtb': 0.5348044991092318, 'eval_loss@eng.pdtb.pdtb': 0.9023337364196777, 'eval_runtime': 20.435, 'eval_samples_per_second': 81.918, 'eval_steps_per_second': 2.594, 'epoch': 12.0}
{'train_runtime': 19777.177, 'train_samples_per_second': 26.649, 'train_steps_per_second': 0.833, 'train_loss': 1.1039102790019384, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.1039
  train_runtime            = 5:29:37.17
  train_samples_per_second =     26.649
  train_steps_per_second   =      0.833
{'train@nld.rst.nldt_loss': 3.583571434020996, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.14427860696517414, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03014974936843186, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.027817499642380474, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.039429951423306284, 'train@nld.rst.nldt_loss@nld.rst.nldt': 3.583571672439575, 'train@nld.rst.nldt_runtime': 19.9366, 'train@nld.rst.nldt_samples_per_second': 80.656, 'train@nld.rst.nldt_steps_per_second': 2.558, 'epoch': 1.0}
{'loss': 4.6634, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.6355607509613037, 'eval_accuracy@nld.rst.nldt': 0.1268882175226586, 'eval_f1@nld.rst.nldt': 0.02223930707034587, 'eval_precision@nld.rst.nldt': 0.022960914831882572, 'eval_recall@nld.rst.nldt': 0.0293719806763285, 'eval_loss@nld.rst.nldt': 3.6355607509613037, 'eval_runtime': 4.5198, 'eval_samples_per_second': 73.233, 'eval_steps_per_second': 2.434, 'epoch': 1.0}
{'train@nld.rst.nldt_loss': 2.944187641143799, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3003731343283582, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.030917760990405534, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.024131908542011332, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05261543247582311, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.944188117980957, 'train@nld.rst.nldt_runtime': 19.9096, 'train@nld.rst.nldt_samples_per_second': 80.765, 'train@nld.rst.nldt_steps_per_second': 2.562, 'epoch': 2.0}
{'loss': 3.2621, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.937267541885376, 'eval_accuracy@nld.rst.nldt': 0.3323262839879154, 'eval_f1@nld.rst.nldt': 0.042650605282421414, 'eval_precision@nld.rst.nldt': 0.03278795278795279, 'eval_recall@nld.rst.nldt': 0.06407484088643509, 'eval_loss@nld.rst.nldt': 2.937267541885376, 'eval_runtime': 4.5551, 'eval_samples_per_second': 72.666, 'eval_steps_per_second': 2.415, 'epoch': 2.0}
{'train@nld.rst.nldt_loss': 2.7523133754730225, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30907960199004975, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03295542841697453, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.027986367535724463, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.0536870051437199, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.7523133754730225, 'train@nld.rst.nldt_runtime': 19.9166, 'train@nld.rst.nldt_samples_per_second': 80.736, 'train@nld.rst.nldt_steps_per_second': 2.561, 'epoch': 3.0}
{'loss': 2.9088, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.746742010116577, 'eval_accuracy@nld.rst.nldt': 0.3323262839879154, 'eval_f1@nld.rst.nldt': 0.04097236797685173, 'eval_precision@nld.rst.nldt': 0.033128672344358616, 'eval_recall@nld.rst.nldt': 0.060079748485545585, 'eval_loss@nld.rst.nldt': 2.746742010116577, 'eval_runtime': 4.5402, 'eval_samples_per_second': 72.905, 'eval_steps_per_second': 2.423, 'epoch': 3.0}
{'train@nld.rst.nldt_loss': 2.6481497287750244, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31902985074626866, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03833539022656363, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.039695584324301966, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05900721786665427, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6481497287750244, 'train@nld.rst.nldt_runtime': 19.9463, 'train@nld.rst.nldt_samples_per_second': 80.617, 'train@nld.rst.nldt_steps_per_second': 2.557, 'epoch': 4.0}
{'loss': 2.7461, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.640883207321167, 'eval_accuracy@nld.rst.nldt': 0.33534743202416917, 'eval_f1@nld.rst.nldt': 0.04148720123906659, 'eval_precision@nld.rst.nldt': 0.03257809766581696, 'eval_recall@nld.rst.nldt': 0.06179357411241469, 'eval_loss@nld.rst.nldt': 2.640883207321167, 'eval_runtime': 4.5473, 'eval_samples_per_second': 72.79, 'eval_steps_per_second': 2.419, 'epoch': 4.0}
{'train@nld.rst.nldt_loss': 2.5869948863983154, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3246268656716418, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.04246803991992919, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.06501902287939208, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.061686956428745776, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.5869946479797363, 'train@nld.rst.nldt_runtime': 19.9392, 'train@nld.rst.nldt_samples_per_second': 80.645, 'train@nld.rst.nldt_steps_per_second': 2.558, 'epoch': 5.0}
{'loss': 2.6734, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.5916748046875, 'eval_accuracy@nld.rst.nldt': 0.3413897280966767, 'eval_f1@nld.rst.nldt': 0.04700754313157413, 'eval_precision@nld.rst.nldt': 0.04067355337196607, 'eval_recall@nld.rst.nldt': 0.06590880044986325, 'eval_loss@nld.rst.nldt': 2.5916748046875, 'eval_runtime': 4.5435, 'eval_samples_per_second': 72.851, 'eval_steps_per_second': 2.421, 'epoch': 5.0}
{'train@nld.rst.nldt_loss': 2.5263113975524902, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3333333333333333, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.050286063990386536, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.058515665256687166, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.06797419095872387, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.526311159133911, 'train@nld.rst.nldt_runtime': 19.9569, 'train@nld.rst.nldt_samples_per_second': 80.574, 'train@nld.rst.nldt_steps_per_second': 2.556, 'epoch': 6.0}
{'loss': 2.6123, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.540109872817993, 'eval_accuracy@nld.rst.nldt': 0.3564954682779456, 'eval_f1@nld.rst.nldt': 0.06350677516607973, 'eval_precision@nld.rst.nldt': 0.08726380073444338, 'eval_recall@nld.rst.nldt': 0.07737264524704138, 'eval_loss@nld.rst.nldt': 2.540109872817993, 'eval_runtime': 4.5347, 'eval_samples_per_second': 72.993, 'eval_steps_per_second': 2.426, 'epoch': 6.0}
{'train@nld.rst.nldt_loss': 2.482774019241333, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.34639303482587064, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.058689612983335804, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.06200696141682674, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.07838987701379613, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.482774019241333, 'train@nld.rst.nldt_runtime': 19.9794, 'train@nld.rst.nldt_samples_per_second': 80.483, 'train@nld.rst.nldt_steps_per_second': 2.553, 'epoch': 7.0}
{'loss': 2.5817, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.512550115585327, 'eval_accuracy@nld.rst.nldt': 0.3685800604229607, 'eval_f1@nld.rst.nldt': 0.06912405144830898, 'eval_precision@nld.rst.nldt': 0.08955560963644899, 'eval_recall@nld.rst.nldt': 0.08631751143828438, 'eval_loss@nld.rst.nldt': 2.512550115585327, 'eval_runtime': 4.5201, 'eval_samples_per_second': 73.228, 'eval_steps_per_second': 2.434, 'epoch': 7.0}
{'train@nld.rst.nldt_loss': 2.4514880180358887, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.35074626865671643, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.06293116794012465, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.07748400469242468, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.08108128634357743, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.4514877796173096, 'train@nld.rst.nldt_runtime': 20.012, 'train@nld.rst.nldt_samples_per_second': 80.352, 'train@nld.rst.nldt_steps_per_second': 2.548, 'epoch': 8.0}
{'loss': 2.5441, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.490683078765869, 'eval_accuracy@nld.rst.nldt': 0.37462235649546827, 'eval_f1@nld.rst.nldt': 0.07181989659594121, 'eval_precision@nld.rst.nldt': 0.09245578573309665, 'eval_recall@nld.rst.nldt': 0.08877770109654168, 'eval_loss@nld.rst.nldt': 2.4906833171844482, 'eval_runtime': 4.563, 'eval_samples_per_second': 72.54, 'eval_steps_per_second': 2.411, 'epoch': 8.0}
{'train@nld.rst.nldt_loss': 2.4288036823272705, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.35447761194029853, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.0675861520708918, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.11061040629697866, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.08461654895278567, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.4288034439086914, 'train@nld.rst.nldt_runtime': 19.9741, 'train@nld.rst.nldt_samples_per_second': 80.504, 'train@nld.rst.nldt_steps_per_second': 2.553, 'epoch': 9.0}
{'loss': 2.5327, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.4731268882751465, 'eval_accuracy@nld.rst.nldt': 0.3776435045317221, 'eval_f1@nld.rst.nldt': 0.07319423666832887, 'eval_precision@nld.rst.nldt': 0.0921054795083652, 'eval_recall@nld.rst.nldt': 0.091490938833934, 'eval_loss@nld.rst.nldt': 2.4731264114379883, 'eval_runtime': 4.5458, 'eval_samples_per_second': 72.815, 'eval_steps_per_second': 2.42, 'epoch': 9.0}
{'train@nld.rst.nldt_loss': 2.4113388061523438, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.35883084577114427, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.07240068101782449, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.10729051729271602, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.09023579591714133, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.4113388061523438, 'train@nld.rst.nldt_runtime': 19.9584, 'train@nld.rst.nldt_samples_per_second': 80.568, 'train@nld.rst.nldt_steps_per_second': 2.555, 'epoch': 10.0}
{'loss': 2.4938, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.459134817123413, 'eval_accuracy@nld.rst.nldt': 0.37462235649546827, 'eval_f1@nld.rst.nldt': 0.07525764130879692, 'eval_precision@nld.rst.nldt': 0.09282255028254578, 'eval_recall@nld.rst.nldt': 0.09513329754875649, 'eval_loss@nld.rst.nldt': 2.459135055541992, 'eval_runtime': 4.5219, 'eval_samples_per_second': 73.2, 'eval_steps_per_second': 2.433, 'epoch': 10.0}
{'train@nld.rst.nldt_loss': 2.4029994010925293, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.35883084577114427, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.07443212501986281, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.11199882469874956, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.09041040066895197, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.40299916267395, 'train@nld.rst.nldt_runtime': 19.9667, 'train@nld.rst.nldt_samples_per_second': 80.534, 'train@nld.rst.nldt_steps_per_second': 2.554, 'epoch': 11.0}
{'loss': 2.4716, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.4529807567596436, 'eval_accuracy@nld.rst.nldt': 0.3776435045317221, 'eval_f1@nld.rst.nldt': 0.07261692681380937, 'eval_precision@nld.rst.nldt': 0.09098710319183889, 'eval_recall@nld.rst.nldt': 0.091490938833934, 'eval_loss@nld.rst.nldt': 2.4529807567596436, 'eval_runtime': 4.5143, 'eval_samples_per_second': 73.323, 'eval_steps_per_second': 2.437, 'epoch': 11.0}
{'train@nld.rst.nldt_loss': 2.3998916149139404, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.36007462686567165, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.07465840481000235, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.11201775434698617, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.09110576915124499, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.3998913764953613, 'train@nld.rst.nldt_runtime': 20.7568, 'train@nld.rst.nldt_samples_per_second': 77.469, 'train@nld.rst.nldt_steps_per_second': 2.457, 'epoch': 12.0}
{'loss': 2.4794, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.451183319091797, 'eval_accuracy@nld.rst.nldt': 0.3776435045317221, 'eval_f1@nld.rst.nldt': 0.07246427952500303, 'eval_precision@nld.rst.nldt': 0.09076882731566827, 'eval_recall@nld.rst.nldt': 0.091490938833934, 'eval_loss@nld.rst.nldt': 2.451183319091797, 'eval_runtime': 4.5171, 'eval_samples_per_second': 73.277, 'eval_steps_per_second': 2.435, 'epoch': 12.0}
{'train_runtime': 787.7718, 'train_samples_per_second': 24.494, 'train_steps_per_second': 0.777, 'train_loss': 2.830790700476154, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.1039
  train_runtime            = 5:29:37.17
  train_samples_per_second =     26.649
  train_steps_per_second   =      0.833
-------------------------------------------------------------------
Lang1:  eng.rst.gum    Lang2:  nld.rst.nldt
Saving run to:  runs/full_shot/FullShot=v4_eng.rst.gum_nld.rst.nldt_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 13897 examples
read 2149 examples
read 2091 examples
read 1608 examples
read 331 examples
read 326 examples
Total prediction labels:  38
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=38, bias=True)
    )
  )
)
{'train@eng.rst.gum_loss': 2.552703619003296, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.2478232712096136, 'train@eng.rst.gum_f1@eng.rst.gum': 0.039973298173699566, 'train@eng.rst.gum_precision@eng.rst.gum': 0.0751639615773849, 'train@eng.rst.gum_recall@eng.rst.gum': 0.057355424710082474, 'train@eng.rst.gum_loss@eng.rst.gum': 2.552703619003296, 'train@eng.rst.gum_runtime': 165.6148, 'train@eng.rst.gum_samples_per_second': 83.912, 'train@eng.rst.gum_steps_per_second': 2.627, 'epoch': 1.0}
{'loss': 2.8265, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.636476516723633, 'eval_accuracy@eng.rst.gum': 0.24336900884132154, 'eval_f1@eng.rst.gum': 0.04290792502437762, 'eval_precision@eng.rst.gum': 0.06411262633166698, 'eval_recall@eng.rst.gum': 0.0604589200029479, 'eval_loss@eng.rst.gum': 2.636476516723633, 'eval_runtime': 25.9617, 'eval_samples_per_second': 82.776, 'eval_steps_per_second': 2.619, 'epoch': 1.0}
{'train@eng.rst.gum_loss': 2.1258773803710938, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.37403756206375477, 'train@eng.rst.gum_f1@eng.rst.gum': 0.12480713386963785, 'train@eng.rst.gum_precision@eng.rst.gum': 0.19862647093512237, 'train@eng.rst.gum_recall@eng.rst.gum': 0.14348112467772473, 'train@eng.rst.gum_loss@eng.rst.gum': 2.1258771419525146, 'train@eng.rst.gum_runtime': 165.8788, 'train@eng.rst.gum_samples_per_second': 83.778, 'train@eng.rst.gum_steps_per_second': 2.622, 'epoch': 2.0}
{'loss': 2.4062, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.227644205093384, 'eval_accuracy@eng.rst.gum': 0.34434620753838996, 'eval_f1@eng.rst.gum': 0.11661017677185972, 'eval_precision@eng.rst.gum': 0.21941870492511714, 'eval_recall@eng.rst.gum': 0.14024411030518105, 'eval_loss@eng.rst.gum': 2.2276439666748047, 'eval_runtime': 25.9983, 'eval_samples_per_second': 82.659, 'eval_steps_per_second': 2.616, 'epoch': 2.0}
{'train@eng.rst.gum_loss': 1.8271316289901733, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.47535439303446786, 'train@eng.rst.gum_f1@eng.rst.gum': 0.26668222698792393, 'train@eng.rst.gum_precision@eng.rst.gum': 0.3605061432751542, 'train@eng.rst.gum_recall@eng.rst.gum': 0.27881715385207256, 'train@eng.rst.gum_loss@eng.rst.gum': 1.8271316289901733, 'train@eng.rst.gum_runtime': 165.5315, 'train@eng.rst.gum_samples_per_second': 83.954, 'train@eng.rst.gum_steps_per_second': 2.628, 'epoch': 3.0}
{'loss': 2.0399, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.9566913843154907, 'eval_accuracy@eng.rst.gum': 0.4346207538389949, 'eval_f1@eng.rst.gum': 0.24741225270508038, 'eval_precision@eng.rst.gum': 0.27643078317209235, 'eval_recall@eng.rst.gum': 0.2612293159486862, 'eval_loss@eng.rst.gum': 1.9566911458969116, 'eval_runtime': 25.9168, 'eval_samples_per_second': 82.919, 'eval_steps_per_second': 2.624, 'epoch': 3.0}
{'train@eng.rst.gum_loss': 1.6937774419784546, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5022666762610636, 'train@eng.rst.gum_f1@eng.rst.gum': 0.29911210691884393, 'train@eng.rst.gum_precision@eng.rst.gum': 0.4256654960321718, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3111685339988401, 'train@eng.rst.gum_loss@eng.rst.gum': 1.6937772035598755, 'train@eng.rst.gum_runtime': 165.403, 'train@eng.rst.gum_samples_per_second': 84.019, 'train@eng.rst.gum_steps_per_second': 2.63, 'epoch': 4.0}
{'loss': 1.8359, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.8530166149139404, 'eval_accuracy@eng.rst.gum': 0.4644020474639367, 'eval_f1@eng.rst.gum': 0.28286675099910924, 'eval_precision@eng.rst.gum': 0.3258780269022524, 'eval_recall@eng.rst.gum': 0.29793068910494713, 'eval_loss@eng.rst.gum': 1.85301673412323, 'eval_runtime': 25.9435, 'eval_samples_per_second': 82.834, 'eval_steps_per_second': 2.621, 'epoch': 4.0}
{'train@eng.rst.gum_loss': 1.61423921585083, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5193926746779881, 'train@eng.rst.gum_f1@eng.rst.gum': 0.33257714369242036, 'train@eng.rst.gum_precision@eng.rst.gum': 0.4456255211906809, 'train@eng.rst.gum_recall@eng.rst.gum': 0.34556728771993117, 'train@eng.rst.gum_loss@eng.rst.gum': 1.6142393350601196, 'train@eng.rst.gum_runtime': 165.9936, 'train@eng.rst.gum_samples_per_second': 83.72, 'train@eng.rst.gum_steps_per_second': 2.621, 'epoch': 5.0}
{'loss': 1.7278, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.7911397218704224, 'eval_accuracy@eng.rst.gum': 0.4839460214053048, 'eval_f1@eng.rst.gum': 0.31988775288187643, 'eval_precision@eng.rst.gum': 0.3722048584223456, 'eval_recall@eng.rst.gum': 0.3360253975923239, 'eval_loss@eng.rst.gum': 1.7911397218704224, 'eval_runtime': 25.9667, 'eval_samples_per_second': 82.76, 'eval_steps_per_second': 2.619, 'epoch': 5.0}
{'train@eng.rst.gum_loss': 1.5600645542144775, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5361588832122041, 'train@eng.rst.gum_f1@eng.rst.gum': 0.37065822654128533, 'train@eng.rst.gum_precision@eng.rst.gum': 0.46049771130478157, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3742809550909338, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5600645542144775, 'train@eng.rst.gum_runtime': 165.3401, 'train@eng.rst.gum_samples_per_second': 84.051, 'train@eng.rst.gum_steps_per_second': 2.631, 'epoch': 6.0}
{'loss': 1.6651, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.7513550519943237, 'eval_accuracy@eng.rst.gum': 0.4927873429502094, 'eval_f1@eng.rst.gum': 0.3461526261117444, 'eval_precision@eng.rst.gum': 0.4313726857434263, 'eval_recall@eng.rst.gum': 0.3568162245777685, 'eval_loss@eng.rst.gum': 1.7513550519943237, 'eval_runtime': 25.9201, 'eval_samples_per_second': 82.909, 'eval_steps_per_second': 2.623, 'epoch': 6.0}
{'train@eng.rst.gum_loss': 1.5234169960021973, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5444340505144996, 'train@eng.rst.gum_f1@eng.rst.gum': 0.388446535176014, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5193149308269805, 'train@eng.rst.gum_recall@eng.rst.gum': 0.39314051286575097, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5234169960021973, 'train@eng.rst.gum_runtime': 165.5573, 'train@eng.rst.gum_samples_per_second': 83.941, 'train@eng.rst.gum_steps_per_second': 2.627, 'epoch': 7.0}
{'loss': 1.6193, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.7289795875549316, 'eval_accuracy@eng.rst.gum': 0.4932526756630991, 'eval_f1@eng.rst.gum': 0.351001216185408, 'eval_precision@eng.rst.gum': 0.40622249298834395, 'eval_recall@eng.rst.gum': 0.36546709490501883, 'eval_loss@eng.rst.gum': 1.7289797067642212, 'eval_runtime': 25.9821, 'eval_samples_per_second': 82.711, 'eval_steps_per_second': 2.617, 'epoch': 7.0}
{'train@eng.rst.gum_loss': 1.4995322227478027, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5524213859106282, 'train@eng.rst.gum_f1@eng.rst.gum': 0.399391203974384, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5597460807313521, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4020455061422717, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4995322227478027, 'train@eng.rst.gum_runtime': 165.9093, 'train@eng.rst.gum_samples_per_second': 83.763, 'train@eng.rst.gum_steps_per_second': 2.622, 'epoch': 8.0}
{'loss': 1.5856, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.7140109539031982, 'eval_accuracy@eng.rst.gum': 0.5011633317822243, 'eval_f1@eng.rst.gum': 0.367047288799801, 'eval_precision@eng.rst.gum': 0.43832226639048316, 'eval_recall@eng.rst.gum': 0.37779906053743795, 'eval_loss@eng.rst.gum': 1.7140109539031982, 'eval_runtime': 26.0176, 'eval_samples_per_second': 82.598, 'eval_steps_per_second': 2.614, 'epoch': 8.0}
{'train@eng.rst.gum_loss': 1.4787943363189697, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5583938979635893, 'train@eng.rst.gum_f1@eng.rst.gum': 0.41417375888820407, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5482284239041381, 'train@eng.rst.gum_recall@eng.rst.gum': 0.41762977458504347, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4787943363189697, 'train@eng.rst.gum_runtime': 165.6089, 'train@eng.rst.gum_samples_per_second': 83.915, 'train@eng.rst.gum_steps_per_second': 2.627, 'epoch': 9.0}
{'loss': 1.5598, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.696499228477478, 'eval_accuracy@eng.rst.gum': 0.5058166589111215, 'eval_f1@eng.rst.gum': 0.3734713944616495, 'eval_precision@eng.rst.gum': 0.41619312861799446, 'eval_recall@eng.rst.gum': 0.39216566908838874, 'eval_loss@eng.rst.gum': 1.6964991092681885, 'eval_runtime': 25.922, 'eval_samples_per_second': 82.902, 'eval_steps_per_second': 2.623, 'epoch': 9.0}
{'train@eng.rst.gum_loss': 1.4617266654968262, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5629272504857163, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4203427098294841, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5388692538282769, 'train@eng.rst.gum_recall@eng.rst.gum': 0.41915490534495053, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4617266654968262, 'train@eng.rst.gum_runtime': 165.3988, 'train@eng.rst.gum_samples_per_second': 84.021, 'train@eng.rst.gum_steps_per_second': 2.63, 'epoch': 10.0}
{'loss': 1.5417, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.6858744621276855, 'eval_accuracy@eng.rst.gum': 0.5058166589111215, 'eval_f1@eng.rst.gum': 0.3784318735932782, 'eval_precision@eng.rst.gum': 0.4499064908120389, 'eval_recall@eng.rst.gum': 0.3908221173393712, 'eval_loss@eng.rst.gum': 1.6858744621276855, 'eval_runtime': 25.9027, 'eval_samples_per_second': 82.964, 'eval_steps_per_second': 2.625, 'epoch': 10.0}
{'train@eng.rst.gum_loss': 1.4550093412399292, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5650140318054256, 'train@eng.rst.gum_f1@eng.rst.gum': 0.42505046114709805, 'train@eng.rst.gum_precision@eng.rst.gum': 0.535022962470785, 'train@eng.rst.gum_recall@eng.rst.gum': 0.42597773315920645, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4550095796585083, 'train@eng.rst.gum_runtime': 165.774, 'train@eng.rst.gum_samples_per_second': 83.831, 'train@eng.rst.gum_steps_per_second': 2.624, 'epoch': 11.0}
{'loss': 1.5345, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.6797586679458618, 'eval_accuracy@eng.rst.gum': 0.5090739879013495, 'eval_f1@eng.rst.gum': 0.3854930089263855, 'eval_precision@eng.rst.gum': 0.4586729888687543, 'eval_recall@eng.rst.gum': 0.3988586883283503, 'eval_loss@eng.rst.gum': 1.6797586679458618, 'eval_runtime': 25.9838, 'eval_samples_per_second': 82.705, 'eval_steps_per_second': 2.617, 'epoch': 11.0}
{'train@eng.rst.gum_loss': 1.4515646696090698, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5659494855004678, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4252322497613797, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5447800032959664, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4252037436513771, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4515647888183594, 'train@eng.rst.gum_runtime': 165.573, 'train@eng.rst.gum_samples_per_second': 83.933, 'train@eng.rst.gum_steps_per_second': 2.627, 'epoch': 12.0}
{'loss': 1.523, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.6781138181686401, 'eval_accuracy@eng.rst.gum': 0.5118659841786878, 'eval_f1@eng.rst.gum': 0.3879779609713486, 'eval_precision@eng.rst.gum': 0.46002647479980535, 'eval_recall@eng.rst.gum': 0.4005049488673343, 'eval_loss@eng.rst.gum': 1.6781138181686401, 'eval_runtime': 25.9227, 'eval_samples_per_second': 82.9, 'eval_steps_per_second': 2.623, 'epoch': 12.0}
{'train_runtime': 6466.4912, 'train_samples_per_second': 25.789, 'train_steps_per_second': 0.807, 'train_loss': 1.8221108272157867, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.8221
  train_runtime            = 1:47:46.49
  train_samples_per_second =     25.789
  train_steps_per_second   =      0.807
{'train@nld.rst.nldt_loss': 3.0842926502227783, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2599502487562189, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03920441361377619, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.04390504438554211, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05074218143029326, 'train@nld.rst.nldt_loss@nld.rst.nldt': 3.0842926502227783, 'train@nld.rst.nldt_runtime': 19.782, 'train@nld.rst.nldt_samples_per_second': 81.286, 'train@nld.rst.nldt_steps_per_second': 2.578, 'epoch': 1.0}
{'loss': 3.7216, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.107370615005493, 'eval_accuracy@nld.rst.nldt': 0.2537764350453172, 'eval_f1@nld.rst.nldt': 0.02062925844138686, 'eval_precision@nld.rst.nldt': 0.020281711822660097, 'eval_recall@nld.rst.nldt': 0.03360819590204898, 'eval_loss@nld.rst.nldt': 3.107370615005493, 'eval_runtime': 4.366, 'eval_samples_per_second': 75.813, 'eval_steps_per_second': 2.519, 'epoch': 1.0}
{'train@nld.rst.nldt_loss': 2.741137742996216, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.28233830845771146, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.04463460624330348, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.04830117798867799, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05694811251587091, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.741137742996216, 'train@nld.rst.nldt_runtime': 19.7814, 'train@nld.rst.nldt_samples_per_second': 81.288, 'train@nld.rst.nldt_steps_per_second': 2.578, 'epoch': 2.0}
{'loss': 2.9346, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.7508976459503174, 'eval_accuracy@nld.rst.nldt': 0.27492447129909364, 'eval_f1@nld.rst.nldt': 0.03387830432551977, 'eval_precision@nld.rst.nldt': 0.06445072918086411, 'eval_recall@nld.rst.nldt': 0.042685799957164275, 'eval_loss@nld.rst.nldt': 2.7508981227874756, 'eval_runtime': 4.334, 'eval_samples_per_second': 76.373, 'eval_steps_per_second': 2.538, 'epoch': 2.0}
{'train@nld.rst.nldt_loss': 2.6141600608825684, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.32276119402985076, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.06529770620050211, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.06582960998120023, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.0777761742815567, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6141600608825684, 'train@nld.rst.nldt_runtime': 19.7686, 'train@nld.rst.nldt_samples_per_second': 81.341, 'train@nld.rst.nldt_steps_per_second': 2.58, 'epoch': 3.0}
{'loss': 2.7368, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.6437320709228516, 'eval_accuracy@nld.rst.nldt': 0.29607250755287007, 'eval_f1@nld.rst.nldt': 0.04043222981312487, 'eval_precision@nld.rst.nldt': 0.03702666953766649, 'eval_recall@nld.rst.nldt': 0.053394731205825656, 'eval_loss@nld.rst.nldt': 2.6437320709228516, 'eval_runtime': 4.3726, 'eval_samples_per_second': 75.698, 'eval_steps_per_second': 2.516, 'epoch': 3.0}
{'train@nld.rst.nldt_loss': 2.531996488571167, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.332089552238806, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.0673946015168167, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.06419562268051093, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.08147527229907456, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.531996726989746, 'train@nld.rst.nldt_runtime': 19.767, 'train@nld.rst.nldt_samples_per_second': 81.348, 'train@nld.rst.nldt_steps_per_second': 2.58, 'epoch': 4.0}
{'loss': 2.621, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.5685548782348633, 'eval_accuracy@nld.rst.nldt': 0.3081570996978852, 'eval_f1@nld.rst.nldt': 0.049235433415426785, 'eval_precision@nld.rst.nldt': 0.04340554403579614, 'eval_recall@nld.rst.nldt': 0.06330350980972098, 'eval_loss@nld.rst.nldt': 2.568554401397705, 'eval_runtime': 4.3602, 'eval_samples_per_second': 75.913, 'eval_steps_per_second': 2.523, 'epoch': 4.0}
{'train@nld.rst.nldt_loss': 2.474790096282959, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3451492537313433, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.07225845751582893, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.07143796555851165, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.08681364145318203, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.474790096282959, 'train@nld.rst.nldt_runtime': 19.799, 'train@nld.rst.nldt_samples_per_second': 81.216, 'train@nld.rst.nldt_steps_per_second': 2.576, 'epoch': 5.0}
{'loss': 2.572, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.525789499282837, 'eval_accuracy@nld.rst.nldt': 0.32326283987915405, 'eval_f1@nld.rst.nldt': 0.05269224581724581, 'eval_precision@nld.rst.nldt': 0.04636733880154933, 'eval_recall@nld.rst.nldt': 0.0677733412205462, 'eval_loss@nld.rst.nldt': 2.525789260864258, 'eval_runtime': 4.3755, 'eval_samples_per_second': 75.649, 'eval_steps_per_second': 2.514, 'epoch': 5.0}
{'train@nld.rst.nldt_loss': 2.4222636222839355, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.35199004975124376, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.07742019217977164, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.08870358784493303, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.09342628309114562, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.4222636222839355, 'train@nld.rst.nldt_runtime': 19.8091, 'train@nld.rst.nldt_samples_per_second': 81.175, 'train@nld.rst.nldt_steps_per_second': 2.575, 'epoch': 6.0}
{'loss': 2.5211, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.4840710163116455, 'eval_accuracy@nld.rst.nldt': 0.33534743202416917, 'eval_f1@nld.rst.nldt': 0.056349498061088354, 'eval_precision@nld.rst.nldt': 0.047438063063063064, 'eval_recall@nld.rst.nldt': 0.07384772749679581, 'eval_loss@nld.rst.nldt': 2.4840710163116455, 'eval_runtime': 4.3533, 'eval_samples_per_second': 76.034, 'eval_steps_per_second': 2.527, 'epoch': 6.0}
{'train@nld.rst.nldt_loss': 2.3818178176879883, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3694029850746269, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.09815249747377391, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.11498788530604824, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.11091762682707673, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.3818180561065674, 'train@nld.rst.nldt_runtime': 19.7962, 'train@nld.rst.nldt_samples_per_second': 81.228, 'train@nld.rst.nldt_steps_per_second': 2.576, 'epoch': 7.0}
{'loss': 2.4786, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.4505650997161865, 'eval_accuracy@nld.rst.nldt': 0.33534743202416917, 'eval_f1@nld.rst.nldt': 0.0648641694372273, 'eval_precision@nld.rst.nldt': 0.06878529755684928, 'eval_recall@nld.rst.nldt': 0.0800157744257123, 'eval_loss@nld.rst.nldt': 2.4505650997161865, 'eval_runtime': 4.3537, 'eval_samples_per_second': 76.028, 'eval_steps_per_second': 2.527, 'epoch': 7.0}
{'train@nld.rst.nldt_loss': 2.3503215312957764, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.373134328358209, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.09886162591813107, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.1134320848598303, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.11292170763737941, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.3503212928771973, 'train@nld.rst.nldt_runtime': 19.8088, 'train@nld.rst.nldt_samples_per_second': 81.176, 'train@nld.rst.nldt_steps_per_second': 2.575, 'epoch': 8.0}
{'loss': 2.4491, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.4275612831115723, 'eval_accuracy@nld.rst.nldt': 0.35347432024169184, 'eval_f1@nld.rst.nldt': 0.08312120385088724, 'eval_precision@nld.rst.nldt': 0.11328801406926406, 'eval_recall@nld.rst.nldt': 0.09509513950507739, 'eval_loss@nld.rst.nldt': 2.427561044692993, 'eval_runtime': 4.3561, 'eval_samples_per_second': 75.985, 'eval_steps_per_second': 2.525, 'epoch': 8.0}
{'train@nld.rst.nldt_loss': 2.327894449234009, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3774875621890547, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.10333809097200308, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.11870873487212309, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.11673239858042331, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.327894687652588, 'train@nld.rst.nldt_runtime': 19.8268, 'train@nld.rst.nldt_samples_per_second': 81.103, 'train@nld.rst.nldt_steps_per_second': 2.572, 'epoch': 9.0}
{'loss': 2.4157, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.411323308944702, 'eval_accuracy@nld.rst.nldt': 0.36555891238670696, 'eval_f1@nld.rst.nldt': 0.10163578844086373, 'eval_precision@nld.rst.nldt': 0.1176566249857749, 'eval_recall@nld.rst.nldt': 0.11071305333727692, 'eval_loss@nld.rst.nldt': 2.411323070526123, 'eval_runtime': 4.368, 'eval_samples_per_second': 75.779, 'eval_steps_per_second': 2.518, 'epoch': 9.0}
{'train@nld.rst.nldt_loss': 2.311166763305664, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3812189054726368, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.1085226310115901, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.15257940748038407, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.12062750436000627, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.311166524887085, 'train@nld.rst.nldt_runtime': 19.8269, 'train@nld.rst.nldt_samples_per_second': 81.102, 'train@nld.rst.nldt_steps_per_second': 2.572, 'epoch': 10.0}
{'loss': 2.392, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.3967621326446533, 'eval_accuracy@nld.rst.nldt': 0.37462235649546827, 'eval_f1@nld.rst.nldt': 0.10388825742841425, 'eval_precision@nld.rst.nldt': 0.11932331583659857, 'eval_recall@nld.rst.nldt': 0.11410578724243321, 'eval_loss@nld.rst.nldt': 2.3967623710632324, 'eval_runtime': 4.3689, 'eval_samples_per_second': 75.763, 'eval_steps_per_second': 2.518, 'epoch': 10.0}
{'train@nld.rst.nldt_loss': 2.301295757293701, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.38370646766169153, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.11059823345823626, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.15386741813384736, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.12291887924329292, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.301295757293701, 'train@nld.rst.nldt_runtime': 19.7834, 'train@nld.rst.nldt_samples_per_second': 81.28, 'train@nld.rst.nldt_steps_per_second': 2.578, 'epoch': 11.0}
{'loss': 2.3663, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.389896869659424, 'eval_accuracy@nld.rst.nldt': 0.3716012084592145, 'eval_f1@nld.rst.nldt': 0.10121333929335392, 'eval_precision@nld.rst.nldt': 0.11161346876959562, 'eval_recall@nld.rst.nldt': 0.11371758848466922, 'eval_loss@nld.rst.nldt': 2.389897108078003, 'eval_runtime': 4.3414, 'eval_samples_per_second': 76.243, 'eval_steps_per_second': 2.534, 'epoch': 11.0}
{'train@nld.rst.nldt_loss': 2.2979679107666016, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3849502487562189, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.11213306538960906, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.15509081359927143, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.1243077681321818, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.2979681491851807, 'train@nld.rst.nldt_runtime': 19.7883, 'train@nld.rst.nldt_samples_per_second': 81.26, 'train@nld.rst.nldt_steps_per_second': 2.577, 'epoch': 12.0}
{'loss': 2.3709, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.387655735015869, 'eval_accuracy@nld.rst.nldt': 0.3716012084592145, 'eval_f1@nld.rst.nldt': 0.10121333929335392, 'eval_precision@nld.rst.nldt': 0.11161346876959562, 'eval_recall@nld.rst.nldt': 0.11371758848466922, 'eval_loss@nld.rst.nldt': 2.387655735015869, 'eval_runtime': 4.3538, 'eval_samples_per_second': 76.026, 'eval_steps_per_second': 2.527, 'epoch': 12.0}
{'train_runtime': 782.1553, 'train_samples_per_second': 24.67, 'train_steps_per_second': 0.782, 'train_loss': 2.6316471598506754, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.8221
  train_runtime            = 1:47:46.49
  train_samples_per_second =     25.789
  train_steps_per_second   =      0.807
-------------------------------------------------------------------
Lang1:  eng.rst.rstdt    Lang2:  nld.rst.nldt
Saving run to:  runs/full_shot/FullShot=v4_eng.rst.rstdt_nld.rst.nldt_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 16002 examples
read 1621 examples
read 2155 examples
read 1608 examples
read 331 examples
read 326 examples
Total prediction labels:  42
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=42, bias=True)
    )
  )
)
{'train@eng.rst.rstdt_loss': 1.7433948516845703, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.5138732658417697, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.08644195886667035, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.09376663993204565, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.11182531389082459, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.7433949708938599, 'train@eng.rst.rstdt_runtime': 190.1921, 'train@eng.rst.rstdt_samples_per_second': 84.136, 'train@eng.rst.rstdt_steps_per_second': 2.634, 'epoch': 1.0}
{'loss': 2.2157, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.717879056930542, 'eval_accuracy@eng.rst.rstdt': 0.527452190006169, 'eval_f1@eng.rst.rstdt': 0.0892980404952976, 'eval_precision@eng.rst.rstdt': 0.09975928071824951, 'eval_recall@eng.rst.rstdt': 0.11184962667548924, 'eval_loss@eng.rst.rstdt': 1.717878818511963, 'eval_runtime': 19.634, 'eval_samples_per_second': 82.561, 'eval_steps_per_second': 2.598, 'epoch': 1.0}
{'train@eng.rst.rstdt_loss': 1.4223164319992065, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6066116735408074, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.21230528200709073, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.33708745266262174, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.21531672872880722, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.422316312789917, 'train@eng.rst.rstdt_runtime': 190.743, 'train@eng.rst.rstdt_samples_per_second': 83.893, 'train@eng.rst.rstdt_steps_per_second': 2.627, 'epoch': 2.0}
{'loss': 1.6119, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.4207326173782349, 'eval_accuracy@eng.rst.rstdt': 0.6181369524984578, 'eval_f1@eng.rst.rstdt': 0.23247898579676807, 'eval_precision@eng.rst.rstdt': 0.32091835593382895, 'eval_recall@eng.rst.rstdt': 0.2278771469829164, 'eval_loss@eng.rst.rstdt': 1.4207326173782349, 'eval_runtime': 19.7158, 'eval_samples_per_second': 82.218, 'eval_steps_per_second': 2.587, 'epoch': 2.0}
{'train@eng.rst.rstdt_loss': 1.3093243837356567, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.636357955255593, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.29634441461923655, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.441555170939507, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.2798651136200719, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.3093242645263672, 'train@eng.rst.rstdt_runtime': 191.4871, 'train@eng.rst.rstdt_samples_per_second': 83.567, 'train@eng.rst.rstdt_steps_per_second': 2.616, 'epoch': 3.0}
{'loss': 1.4216, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.3330764770507812, 'eval_accuracy@eng.rst.rstdt': 0.6391116594694632, 'eval_f1@eng.rst.rstdt': 0.2935508704934033, 'eval_precision@eng.rst.rstdt': 0.37050146956721347, 'eval_recall@eng.rst.rstdt': 0.28303443766202296, 'eval_loss@eng.rst.rstdt': 1.3330764770507812, 'eval_runtime': 19.6951, 'eval_samples_per_second': 82.305, 'eval_steps_per_second': 2.589, 'epoch': 3.0}
{'train@eng.rst.rstdt_loss': 1.2436788082122803, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6485439320084989, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.34051784543983316, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.44003864636536094, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3225886583678542, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.2436786890029907, 'train@eng.rst.rstdt_runtime': 190.1843, 'train@eng.rst.rstdt_samples_per_second': 84.139, 'train@eng.rst.rstdt_steps_per_second': 2.634, 'epoch': 4.0}
{'loss': 1.3253, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.29526948928833, 'eval_accuracy@eng.rst.rstdt': 0.6378778531770513, 'eval_f1@eng.rst.rstdt': 0.3278044949411952, 'eval_precision@eng.rst.rstdt': 0.40957741068680004, 'eval_recall@eng.rst.rstdt': 0.3250288729896737, 'eval_loss@eng.rst.rstdt': 1.29526948928833, 'eval_runtime': 19.6702, 'eval_samples_per_second': 82.409, 'eval_steps_per_second': 2.593, 'epoch': 4.0}
{'train@eng.rst.rstdt_loss': 1.1939488649368286, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6577302837145357, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.35484130325639945, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.4620576697082548, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3335547678618075, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1939488649368286, 'train@eng.rst.rstdt_runtime': 190.8222, 'train@eng.rst.rstdt_samples_per_second': 83.858, 'train@eng.rst.rstdt_steps_per_second': 2.625, 'epoch': 5.0}
{'loss': 1.2692, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.2537133693695068, 'eval_accuracy@eng.rst.rstdt': 0.6415792720542874, 'eval_f1@eng.rst.rstdt': 0.3238024435859745, 'eval_precision@eng.rst.rstdt': 0.40021860376751206, 'eval_recall@eng.rst.rstdt': 0.31902846804501217, 'eval_loss@eng.rst.rstdt': 1.2537133693695068, 'eval_runtime': 19.7025, 'eval_samples_per_second': 82.274, 'eval_steps_per_second': 2.589, 'epoch': 5.0}
{'train@eng.rst.rstdt_loss': 1.1614378690719604, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6622297212848394, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.36368488052165704, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5100362851414071, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.34035375242772825, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1614378690719604, 'train@eng.rst.rstdt_runtime': 190.4686, 'train@eng.rst.rstdt_samples_per_second': 84.014, 'train@eng.rst.rstdt_steps_per_second': 2.63, 'epoch': 6.0}
{'loss': 1.2271, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.2293425798416138, 'eval_accuracy@eng.rst.rstdt': 0.6415792720542874, 'eval_f1@eng.rst.rstdt': 0.3275944455791319, 'eval_precision@eng.rst.rstdt': 0.44632253687342177, 'eval_recall@eng.rst.rstdt': 0.3245406394047351, 'eval_loss@eng.rst.rstdt': 1.2293425798416138, 'eval_runtime': 19.6739, 'eval_samples_per_second': 82.394, 'eval_steps_per_second': 2.592, 'epoch': 6.0}
{'train@eng.rst.rstdt_loss': 1.1409010887145996, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6642294713160855, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3724096876871713, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5761619100547661, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3456268742355604, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.14090096950531, 'train@eng.rst.rstdt_runtime': 190.3279, 'train@eng.rst.rstdt_samples_per_second': 84.076, 'train@eng.rst.rstdt_steps_per_second': 2.632, 'epoch': 7.0}
{'loss': 1.2047, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.2138978242874146, 'eval_accuracy@eng.rst.rstdt': 0.6421961752004935, 'eval_f1@eng.rst.rstdt': 0.33668589684057665, 'eval_precision@eng.rst.rstdt': 0.4501193048380585, 'eval_recall@eng.rst.rstdt': 0.32984845788744527, 'eval_loss@eng.rst.rstdt': 1.2138978242874146, 'eval_runtime': 19.6404, 'eval_samples_per_second': 82.534, 'eval_steps_per_second': 2.597, 'epoch': 7.0}
{'train@eng.rst.rstdt_loss': 1.1242761611938477, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6680414948131483, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.38874896190152386, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5786346023529454, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3604001814485877, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1242761611938477, 'train@eng.rst.rstdt_runtime': 190.8076, 'train@eng.rst.rstdt_samples_per_second': 83.865, 'train@eng.rst.rstdt_steps_per_second': 2.626, 'epoch': 8.0}
{'loss': 1.1833, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.2081267833709717, 'eval_accuracy@eng.rst.rstdt': 0.6428130783466995, 'eval_f1@eng.rst.rstdt': 0.3415435356528928, 'eval_precision@eng.rst.rstdt': 0.39525099951841264, 'eval_recall@eng.rst.rstdt': 0.3376255726914629, 'eval_loss@eng.rst.rstdt': 1.2081267833709717, 'eval_runtime': 19.6865, 'eval_samples_per_second': 82.341, 'eval_steps_per_second': 2.591, 'epoch': 8.0}
{'train@eng.rst.rstdt_loss': 1.1124582290649414, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6702287214098238, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3974924405458847, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5714722574858945, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.36626556957992634, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1124582290649414, 'train@eng.rst.rstdt_runtime': 190.1805, 'train@eng.rst.rstdt_samples_per_second': 84.141, 'train@eng.rst.rstdt_steps_per_second': 2.634, 'epoch': 9.0}
{'loss': 1.1676, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.1992655992507935, 'eval_accuracy@eng.rst.rstdt': 0.6434299814929056, 'eval_f1@eng.rst.rstdt': 0.3514904100529086, 'eval_precision@eng.rst.rstdt': 0.4874210983315892, 'eval_recall@eng.rst.rstdt': 0.34218917123829995, 'eval_loss@eng.rst.rstdt': 1.1992655992507935, 'eval_runtime': 19.6215, 'eval_samples_per_second': 82.613, 'eval_steps_per_second': 2.599, 'epoch': 9.0}
{'train@eng.rst.rstdt_loss': 1.1071596145629883, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6711036120484939, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.41269218808311714, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6234086724353349, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.37968894922569746, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1071596145629883, 'train@eng.rst.rstdt_runtime': 190.78, 'train@eng.rst.rstdt_samples_per_second': 83.877, 'train@eng.rst.rstdt_steps_per_second': 2.626, 'epoch': 10.0}
{'loss': 1.1598, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.2032742500305176, 'eval_accuracy@eng.rst.rstdt': 0.6354102405922271, 'eval_f1@eng.rst.rstdt': 0.3594471885201739, 'eval_precision@eng.rst.rstdt': 0.4876057023191168, 'eval_recall@eng.rst.rstdt': 0.34909456248472226, 'eval_loss@eng.rst.rstdt': 1.2032743692398071, 'eval_runtime': 19.7107, 'eval_samples_per_second': 82.24, 'eval_steps_per_second': 2.587, 'epoch': 10.0}
{'train@eng.rst.rstdt_loss': 1.1003299951553345, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6724159480064992, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4106323974023094, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6290510740007128, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3765284821965151, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.100330114364624, 'train@eng.rst.rstdt_runtime': 190.7775, 'train@eng.rst.rstdt_samples_per_second': 83.878, 'train@eng.rst.rstdt_steps_per_second': 2.626, 'epoch': 11.0}
{'loss': 1.1531, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.195230484008789, 'eval_accuracy@eng.rst.rstdt': 0.6440468846391116, 'eval_f1@eng.rst.rstdt': 0.3620925564456153, 'eval_precision@eng.rst.rstdt': 0.48503912269003197, 'eval_recall@eng.rst.rstdt': 0.34996768632874586, 'eval_loss@eng.rst.rstdt': 1.195230484008789, 'eval_runtime': 19.6858, 'eval_samples_per_second': 82.344, 'eval_steps_per_second': 2.591, 'epoch': 11.0}
{'train@eng.rst.rstdt_loss': 1.098878026008606, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6726034245719285, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.40969671920797573, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6267597954929791, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3751975458951458, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.0988781452178955, 'train@eng.rst.rstdt_runtime': 190.1834, 'train@eng.rst.rstdt_samples_per_second': 84.14, 'train@eng.rst.rstdt_steps_per_second': 2.634, 'epoch': 12.0}
{'loss': 1.1487, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.1931625604629517, 'eval_accuracy@eng.rst.rstdt': 0.6458975940777298, 'eval_f1@eng.rst.rstdt': 0.3620617943281426, 'eval_precision@eng.rst.rstdt': 0.48612671071517455, 'eval_recall@eng.rst.rstdt': 0.34969661015634146, 'eval_loss@eng.rst.rstdt': 1.1931625604629517, 'eval_runtime': 19.629, 'eval_samples_per_second': 82.582, 'eval_steps_per_second': 2.598, 'epoch': 12.0}
{'train_runtime': 7322.6302, 'train_samples_per_second': 26.223, 'train_steps_per_second': 0.821, 'train_loss': 1.3406591967432322, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.3407
  train_runtime            = 2:02:02.63
  train_samples_per_second =     26.223
  train_steps_per_second   =      0.821
{'train@nld.rst.nldt_loss': 3.225903034210205, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.23694029850746268, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03730305247789258, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.044138308839479765, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.047881864117555406, 'train@nld.rst.nldt_loss@nld.rst.nldt': 3.225902557373047, 'train@nld.rst.nldt_runtime': 19.8145, 'train@nld.rst.nldt_samples_per_second': 81.153, 'train@nld.rst.nldt_steps_per_second': 2.574, 'epoch': 1.0}
{'loss': 3.8058, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.1696951389312744, 'eval_accuracy@nld.rst.nldt': 0.2628398791540785, 'eval_f1@nld.rst.nldt': 0.029048167852139865, 'eval_precision@nld.rst.nldt': 0.025995855995855997, 'eval_recall@nld.rst.nldt': 0.03982746721877157, 'eval_loss@nld.rst.nldt': 3.1696951389312744, 'eval_runtime': 4.3854, 'eval_samples_per_second': 75.478, 'eval_steps_per_second': 2.508, 'epoch': 1.0}
{'train@nld.rst.nldt_loss': 2.8110530376434326, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2916666666666667, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03161805967879931, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.06204962457886206, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.04967499137336094, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.8110530376434326, 'train@nld.rst.nldt_runtime': 19.739, 'train@nld.rst.nldt_samples_per_second': 81.463, 'train@nld.rst.nldt_steps_per_second': 2.584, 'epoch': 2.0}
{'loss': 3.0382, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.749742031097412, 'eval_accuracy@nld.rst.nldt': 0.311178247734139, 'eval_f1@nld.rst.nldt': 0.04000283575891998, 'eval_precision@nld.rst.nldt': 0.06392871365989646, 'eval_recall@nld.rst.nldt': 0.05533317997086113, 'eval_loss@nld.rst.nldt': 2.749742031097412, 'eval_runtime': 4.3695, 'eval_samples_per_second': 75.752, 'eval_steps_per_second': 2.517, 'epoch': 2.0}
{'train@nld.rst.nldt_loss': 2.661501169204712, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3041044776119403, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03713741431879595, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.0719422431299615, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05533617251248325, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.661500930786133, 'train@nld.rst.nldt_runtime': 19.8063, 'train@nld.rst.nldt_samples_per_second': 81.186, 'train@nld.rst.nldt_steps_per_second': 2.575, 'epoch': 3.0}
{'loss': 2.7996, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.620683193206787, 'eval_accuracy@nld.rst.nldt': 0.3293051359516616, 'eval_f1@nld.rst.nldt': 0.054682100123913314, 'eval_precision@nld.rst.nldt': 0.06631393298059965, 'eval_recall@nld.rst.nldt': 0.06825780231577333, 'eval_loss@nld.rst.nldt': 2.6206836700439453, 'eval_runtime': 4.4047, 'eval_samples_per_second': 75.148, 'eval_steps_per_second': 2.497, 'epoch': 3.0}
{'train@nld.rst.nldt_loss': 2.576993227005005, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31156716417910446, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.041961560981528985, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.0819243100445837, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05852448950594731, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.576993227005005, 'train@nld.rst.nldt_runtime': 19.8307, 'train@nld.rst.nldt_samples_per_second': 81.086, 'train@nld.rst.nldt_steps_per_second': 2.572, 'epoch': 4.0}
{'loss': 2.6777, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.542367696762085, 'eval_accuracy@nld.rst.nldt': 0.3323262839879154, 'eval_f1@nld.rst.nldt': 0.06037417927707031, 'eval_precision@nld.rst.nldt': 0.0724148639231022, 'eval_recall@nld.rst.nldt': 0.07189377092758736, 'eval_loss@nld.rst.nldt': 2.542367696762085, 'eval_runtime': 4.3913, 'eval_samples_per_second': 75.377, 'eval_steps_per_second': 2.505, 'epoch': 4.0}
{'train@nld.rst.nldt_loss': 2.5197513103485107, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.32027363184079605, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.0485144344911647, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.04980455186299295, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.06653511042097998, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.5197510719299316, 'train@nld.rst.nldt_runtime': 19.8587, 'train@nld.rst.nldt_samples_per_second': 80.972, 'train@nld.rst.nldt_steps_per_second': 2.568, 'epoch': 5.0}
{'loss': 2.6065, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.4958226680755615, 'eval_accuracy@nld.rst.nldt': 0.35347432024169184, 'eval_f1@nld.rst.nldt': 0.0788025806463413, 'eval_precision@nld.rst.nldt': 0.09320189614307262, 'eval_recall@nld.rst.nldt': 0.08893361960994811, 'eval_loss@nld.rst.nldt': 2.4958226680755615, 'eval_runtime': 4.3872, 'eval_samples_per_second': 75.447, 'eval_steps_per_second': 2.507, 'epoch': 5.0}
{'train@nld.rst.nldt_loss': 2.4677388668060303, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3389303482587065, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.062285338199871, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.07338831542142461, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.08167619662450284, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.4677388668060303, 'train@nld.rst.nldt_runtime': 19.8152, 'train@nld.rst.nldt_samples_per_second': 81.15, 'train@nld.rst.nldt_steps_per_second': 2.574, 'epoch': 6.0}
{'loss': 2.5648, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.4556784629821777, 'eval_accuracy@nld.rst.nldt': 0.3716012084592145, 'eval_f1@nld.rst.nldt': 0.09293368985928936, 'eval_precision@nld.rst.nldt': 0.09506130224835634, 'eval_recall@nld.rst.nldt': 0.1123405669299389, 'eval_loss@nld.rst.nldt': 2.4556784629821777, 'eval_runtime': 4.3964, 'eval_samples_per_second': 75.288, 'eval_steps_per_second': 2.502, 'epoch': 6.0}
{'train@nld.rst.nldt_loss': 2.4258430004119873, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3482587064676617, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.07456359376872677, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.10352830647403702, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.09455381371716315, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.4258430004119873, 'train@nld.rst.nldt_runtime': 19.849, 'train@nld.rst.nldt_samples_per_second': 81.012, 'train@nld.rst.nldt_steps_per_second': 2.569, 'epoch': 7.0}
{'loss': 2.505, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.427530527114868, 'eval_accuracy@nld.rst.nldt': 0.3806646525679758, 'eval_f1@nld.rst.nldt': 0.10143887217632515, 'eval_precision@nld.rst.nldt': 0.09248035980062123, 'eval_recall@nld.rst.nldt': 0.12611507297497637, 'eval_loss@nld.rst.nldt': 2.4275307655334473, 'eval_runtime': 4.38, 'eval_samples_per_second': 75.57, 'eval_steps_per_second': 2.511, 'epoch': 7.0}
{'train@nld.rst.nldt_loss': 2.395962953567505, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.36256218905472637, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.08541249341689738, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.11026456386990464, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.10228812645884007, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.395963191986084, 'train@nld.rst.nldt_runtime': 19.8245, 'train@nld.rst.nldt_samples_per_second': 81.112, 'train@nld.rst.nldt_steps_per_second': 2.573, 'epoch': 8.0}
{'loss': 2.4957, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.4044363498687744, 'eval_accuracy@nld.rst.nldt': 0.38368580060422963, 'eval_f1@nld.rst.nldt': 0.10599134358200553, 'eval_precision@nld.rst.nldt': 0.10411547976907454, 'eval_recall@nld.rst.nldt': 0.1294292999003144, 'eval_loss@nld.rst.nldt': 2.4044363498687744, 'eval_runtime': 4.4206, 'eval_samples_per_second': 74.877, 'eval_steps_per_second': 2.488, 'epoch': 8.0}
{'train@nld.rst.nldt_loss': 2.3736093044281006, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.37002487562189057, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.09035654842124898, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.11780894432331653, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.10753631723693309, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.3736093044281006, 'train@nld.rst.nldt_runtime': 19.7886, 'train@nld.rst.nldt_samples_per_second': 81.259, 'train@nld.rst.nldt_steps_per_second': 2.577, 'epoch': 9.0}
{'loss': 2.4579, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.3864798545837402, 'eval_accuracy@nld.rst.nldt': 0.40181268882175225, 'eval_f1@nld.rst.nldt': 0.11607608053202587, 'eval_precision@nld.rst.nldt': 0.12934011442414803, 'eval_recall@nld.rst.nldt': 0.1363729912254757, 'eval_loss@nld.rst.nldt': 2.3864798545837402, 'eval_runtime': 4.3717, 'eval_samples_per_second': 75.714, 'eval_steps_per_second': 2.516, 'epoch': 9.0}
{'train@nld.rst.nldt_loss': 2.3577914237976074, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3793532338308458, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.09566385433626198, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.12512255057168642, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.11235952478013085, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.3577916622161865, 'train@nld.rst.nldt_runtime': 19.7857, 'train@nld.rst.nldt_samples_per_second': 81.271, 'train@nld.rst.nldt_steps_per_second': 2.578, 'epoch': 10.0}
{'loss': 2.4363, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.373081922531128, 'eval_accuracy@nld.rst.nldt': 0.40483383685800606, 'eval_f1@nld.rst.nldt': 0.11757752889278782, 'eval_precision@nld.rst.nldt': 0.12894610991482316, 'eval_recall@nld.rst.nldt': 0.13807367149758454, 'eval_loss@nld.rst.nldt': 2.373082160949707, 'eval_runtime': 5.2766, 'eval_samples_per_second': 62.729, 'eval_steps_per_second': 2.085, 'epoch': 10.0}
{'train@nld.rst.nldt_loss': 2.3493223190307617, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3812189054726368, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.09844628385822235, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.13235033548581437, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.1138642126077501, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.3493220806121826, 'train@nld.rst.nldt_runtime': 19.8104, 'train@nld.rst.nldt_samples_per_second': 81.17, 'train@nld.rst.nldt_steps_per_second': 2.574, 'epoch': 11.0}
{'loss': 2.4124, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.3664042949676514, 'eval_accuracy@nld.rst.nldt': 0.40181268882175225, 'eval_f1@nld.rst.nldt': 0.12069722418515391, 'eval_precision@nld.rst.nldt': 0.13286925107833358, 'eval_recall@nld.rst.nldt': 0.14112989801395598, 'eval_loss@nld.rst.nldt': 2.3664045333862305, 'eval_runtime': 4.3955, 'eval_samples_per_second': 75.304, 'eval_steps_per_second': 2.503, 'epoch': 11.0}
{'train@nld.rst.nldt_loss': 2.345852851867676, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3824626865671642, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.09906817793833433, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.13108722897104386, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.11457122618241074, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.345852851867676, 'train@nld.rst.nldt_runtime': 19.7928, 'train@nld.rst.nldt_samples_per_second': 81.242, 'train@nld.rst.nldt_steps_per_second': 2.577, 'epoch': 12.0}
{'loss': 2.4133, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.3644895553588867, 'eval_accuracy@nld.rst.nldt': 0.40483383685800606, 'eval_f1@nld.rst.nldt': 0.11757752889278782, 'eval_precision@nld.rst.nldt': 0.12894610991482316, 'eval_recall@nld.rst.nldt': 0.13807367149758454, 'eval_loss@nld.rst.nldt': 2.3644895553588867, 'eval_runtime': 4.3716, 'eval_samples_per_second': 75.715, 'eval_steps_per_second': 2.516, 'epoch': 12.0}
{'train_runtime': 783.3962, 'train_samples_per_second': 24.631, 'train_steps_per_second': 0.781, 'train_loss': 2.6844247057546977, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.3407
  train_runtime            = 2:02:02.63
  train_samples_per_second =     26.223
  train_steps_per_second   =      0.821
-------------------------------------------------------------------
Lang1:  eng.sdrt.stac    Lang2:  nld.rst.nldt
Saving run to:  runs/full_shot/FullShot=v4_eng.sdrt.stac_nld.rst.nldt_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 9580 examples
read 1145 examples
read 1510 examples
read 1608 examples
read 331 examples
read 326 examples
Total prediction labels:  49
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=49, bias=True)
    )
  )
)
{'train@eng.sdrt.stac_loss': 2.1428840160369873, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.3544885177453027, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.06654537311279236, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.08414454191276614, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.10927227987934625, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 2.1428840160369873, 'train@eng.sdrt.stac_runtime': 113.9733, 'train@eng.sdrt.stac_samples_per_second': 84.055, 'train@eng.sdrt.stac_steps_per_second': 2.632, 'epoch': 1.0}
{'loss': 2.6354, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.1078875064849854, 'eval_accuracy@eng.sdrt.stac': 0.36331877729257644, 'eval_f1@eng.sdrt.stac': 0.06960770553161857, 'eval_precision@eng.sdrt.stac': 0.09355428604055117, 'eval_recall@eng.sdrt.stac': 0.11061326522057076, 'eval_loss@eng.sdrt.stac': 2.1078875064849854, 'eval_runtime': 14.074, 'eval_samples_per_second': 81.356, 'eval_steps_per_second': 2.558, 'epoch': 1.0}
{'train@eng.sdrt.stac_loss': 1.9022976160049438, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.41304801670146135, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.11924638792446049, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.1376887045335553, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.15740899455932184, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.9022976160049438, 'train@eng.sdrt.stac_runtime': 114.2416, 'train@eng.sdrt.stac_samples_per_second': 83.857, 'train@eng.sdrt.stac_steps_per_second': 2.626, 'epoch': 2.0}
{'loss': 2.0534, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.8604397773742676, 'eval_accuracy@eng.sdrt.stac': 0.4131004366812227, 'eval_f1@eng.sdrt.stac': 0.11257800475407934, 'eval_precision@eng.sdrt.stac': 0.11565668532886617, 'eval_recall@eng.sdrt.stac': 0.1528285821815311, 'eval_loss@eng.sdrt.stac': 1.860439658164978, 'eval_runtime': 14.0951, 'eval_samples_per_second': 81.234, 'eval_steps_per_second': 2.554, 'epoch': 2.0}
{'train@eng.sdrt.stac_loss': 1.7799237966537476, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4503131524008351, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.14623059656645782, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.13646606433753672, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.19038324496236125, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.7799240350723267, 'train@eng.sdrt.stac_runtime': 114.3581, 'train@eng.sdrt.stac_samples_per_second': 83.772, 'train@eng.sdrt.stac_steps_per_second': 2.623, 'epoch': 3.0}
{'loss': 1.8638, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.7360817193984985, 'eval_accuracy@eng.sdrt.stac': 0.4558951965065502, 'eval_f1@eng.sdrt.stac': 0.13957879558929237, 'eval_precision@eng.sdrt.stac': 0.1297235467110658, 'eval_recall@eng.sdrt.stac': 0.1860894610391712, 'eval_loss@eng.sdrt.stac': 1.7360814809799194, 'eval_runtime': 14.0699, 'eval_samples_per_second': 81.38, 'eval_steps_per_second': 2.559, 'epoch': 3.0}
{'train@eng.sdrt.stac_loss': 1.697012186050415, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4802713987473904, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.19552729537788982, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.22961912312318394, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.22474589982986165, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.6970123052597046, 'train@eng.sdrt.stac_runtime': 114.4303, 'train@eng.sdrt.stac_samples_per_second': 83.719, 'train@eng.sdrt.stac_steps_per_second': 2.622, 'epoch': 4.0}
{'loss': 1.7778, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.6478983163833618, 'eval_accuracy@eng.sdrt.stac': 0.4829694323144105, 'eval_f1@eng.sdrt.stac': 0.1836097489371326, 'eval_precision@eng.sdrt.stac': 0.2159628339287306, 'eval_recall@eng.sdrt.stac': 0.21355485913412034, 'eval_loss@eng.sdrt.stac': 1.6478981971740723, 'eval_runtime': 14.1398, 'eval_samples_per_second': 80.977, 'eval_steps_per_second': 2.546, 'epoch': 4.0}
{'train@eng.sdrt.stac_loss': 1.6400846242904663, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4968684759916493, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2187182884056867, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.23323123095281698, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2436469782790313, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.6400846242904663, 'train@eng.sdrt.stac_runtime': 114.3048, 'train@eng.sdrt.stac_samples_per_second': 83.811, 'train@eng.sdrt.stac_steps_per_second': 2.625, 'epoch': 5.0}
{'loss': 1.7084, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.5930057764053345, 'eval_accuracy@eng.sdrt.stac': 0.4978165938864629, 'eval_f1@eng.sdrt.stac': 0.20479483827969483, 'eval_precision@eng.sdrt.stac': 0.21656454386629664, 'eval_recall@eng.sdrt.stac': 0.22871217890933707, 'eval_loss@eng.sdrt.stac': 1.5930057764053345, 'eval_runtime': 14.0794, 'eval_samples_per_second': 81.324, 'eval_steps_per_second': 2.557, 'epoch': 5.0}
{'train@eng.sdrt.stac_loss': 1.587146520614624, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5037578288100208, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.23569405516802258, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.30219783192521843, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2518779224909008, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5871466398239136, 'train@eng.sdrt.stac_runtime': 114.0074, 'train@eng.sdrt.stac_samples_per_second': 84.03, 'train@eng.sdrt.stac_steps_per_second': 2.631, 'epoch': 6.0}
{'loss': 1.6494, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.5580859184265137, 'eval_accuracy@eng.sdrt.stac': 0.4978165938864629, 'eval_f1@eng.sdrt.stac': 0.21927751921854755, 'eval_precision@eng.sdrt.stac': 0.26011074845924803, 'eval_recall@eng.sdrt.stac': 0.23775741463968697, 'eval_loss@eng.sdrt.stac': 1.5580857992172241, 'eval_runtime': 14.0405, 'eval_samples_per_second': 81.55, 'eval_steps_per_second': 2.564, 'epoch': 6.0}
{'train@eng.sdrt.stac_loss': 1.5650817155838013, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.517223382045929, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2723360808714177, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.28141977107540983, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.29239309607696456, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5650817155838013, 'train@eng.sdrt.stac_runtime': 114.2968, 'train@eng.sdrt.stac_samples_per_second': 83.817, 'train@eng.sdrt.stac_steps_per_second': 2.625, 'epoch': 7.0}
{'loss': 1.6115, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.5410175323486328, 'eval_accuracy@eng.sdrt.stac': 0.5065502183406113, 'eval_f1@eng.sdrt.stac': 0.24321698675474065, 'eval_precision@eng.sdrt.stac': 0.2684827930088035, 'eval_recall@eng.sdrt.stac': 0.2608918627843363, 'eval_loss@eng.sdrt.stac': 1.5410175323486328, 'eval_runtime': 14.1154, 'eval_samples_per_second': 81.117, 'eval_steps_per_second': 2.55, 'epoch': 7.0}
{'train@eng.sdrt.stac_loss': 1.5286519527435303, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5250521920668059, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.28112039598911137, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.46327872468660364, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.30615396037206877, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5286520719528198, 'train@eng.sdrt.stac_runtime': 114.3271, 'train@eng.sdrt.stac_samples_per_second': 83.795, 'train@eng.sdrt.stac_steps_per_second': 2.624, 'epoch': 8.0}
{'loss': 1.5809, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.504507303237915, 'eval_accuracy@eng.sdrt.stac': 0.5205240174672489, 'eval_f1@eng.sdrt.stac': 0.25778907825942515, 'eval_precision@eng.sdrt.stac': 0.2620261087395296, 'eval_recall@eng.sdrt.stac': 0.27969146856551996, 'eval_loss@eng.sdrt.stac': 1.504507303237915, 'eval_runtime': 14.1971, 'eval_samples_per_second': 80.65, 'eval_steps_per_second': 2.536, 'epoch': 8.0}
{'train@eng.sdrt.stac_loss': 1.511871099472046, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5293319415448852, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.29442729395299544, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.41384248584993344, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3161608201648106, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.511871099472046, 'train@eng.sdrt.stac_runtime': 114.3066, 'train@eng.sdrt.stac_samples_per_second': 83.81, 'train@eng.sdrt.stac_steps_per_second': 2.625, 'epoch': 9.0}
{'loss': 1.5552, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.494781732559204, 'eval_accuracy@eng.sdrt.stac': 0.5213973799126638, 'eval_f1@eng.sdrt.stac': 0.26723411130221236, 'eval_precision@eng.sdrt.stac': 0.3226228064303771, 'eval_recall@eng.sdrt.stac': 0.29045326183336834, 'eval_loss@eng.sdrt.stac': 1.494781732559204, 'eval_runtime': 14.1032, 'eval_samples_per_second': 81.187, 'eval_steps_per_second': 2.553, 'epoch': 9.0}
{'train@eng.sdrt.stac_loss': 1.4939390420913696, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5319415448851774, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.30494651529818306, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.41203599423782256, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.324015589267956, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4939390420913696, 'train@eng.sdrt.stac_runtime': 114.083, 'train@eng.sdrt.stac_samples_per_second': 83.974, 'train@eng.sdrt.stac_steps_per_second': 2.63, 'epoch': 10.0}
{'loss': 1.5425, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.4796478748321533, 'eval_accuracy@eng.sdrt.stac': 0.5275109170305677, 'eval_f1@eng.sdrt.stac': 0.27560130541309075, 'eval_precision@eng.sdrt.stac': 0.3196213392052141, 'eval_recall@eng.sdrt.stac': 0.297014631557323, 'eval_loss@eng.sdrt.stac': 1.4796475172042847, 'eval_runtime': 14.9037, 'eval_samples_per_second': 76.826, 'eval_steps_per_second': 2.416, 'epoch': 10.0}
{'train@eng.sdrt.stac_loss': 1.4856311082839966, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5341336116910229, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.30896213320969546, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.41022072628979395, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3276299311150382, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4856312274932861, 'train@eng.sdrt.stac_runtime': 114.1266, 'train@eng.sdrt.stac_samples_per_second': 83.942, 'train@eng.sdrt.stac_steps_per_second': 2.629, 'epoch': 11.0}
{'loss': 1.5295, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.474700689315796, 'eval_accuracy@eng.sdrt.stac': 0.5292576419213973, 'eval_f1@eng.sdrt.stac': 0.278332195721656, 'eval_precision@eng.sdrt.stac': 0.322610718938536, 'eval_recall@eng.sdrt.stac': 0.29880209306049194, 'eval_loss@eng.sdrt.stac': 1.474700689315796, 'eval_runtime': 14.0813, 'eval_samples_per_second': 81.314, 'eval_steps_per_second': 2.557, 'epoch': 11.0}
{'train@eng.sdrt.stac_loss': 1.4842522144317627, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5352818371607516, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3099444452989849, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.41229014437052514, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3285952235480507, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4842522144317627, 'train@eng.sdrt.stac_runtime': 114.1985, 'train@eng.sdrt.stac_samples_per_second': 83.889, 'train@eng.sdrt.stac_steps_per_second': 2.627, 'epoch': 12.0}
{'loss': 1.5272, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.4746524095535278, 'eval_accuracy@eng.sdrt.stac': 0.5283842794759825, 'eval_f1@eng.sdrt.stac': 0.2783715798961127, 'eval_precision@eng.sdrt.stac': 0.3240644517709945, 'eval_recall@eng.sdrt.stac': 0.29928696403967125, 'eval_loss@eng.sdrt.stac': 1.4746524095535278, 'eval_runtime': 14.0988, 'eval_samples_per_second': 81.213, 'eval_steps_per_second': 2.553, 'epoch': 12.0}
{'train_runtime': 4414.5161, 'train_samples_per_second': 26.041, 'train_steps_per_second': 0.815, 'train_loss': 1.7529100036621095, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.7529
  train_runtime            = 1:13:34.51
  train_samples_per_second =     26.041
  train_steps_per_second   =      0.815
{'train@nld.rst.nldt_loss': 3.3822507858276367, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.25870646766169153, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.025446972866082967, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.019471520403579343, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.04044715820131421, 'train@nld.rst.nldt_loss@nld.rst.nldt': 3.3822505474090576, 'train@nld.rst.nldt_runtime': 19.9274, 'train@nld.rst.nldt_samples_per_second': 80.693, 'train@nld.rst.nldt_steps_per_second': 2.559, 'epoch': 1.0}
{'loss': 3.7387, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.3723995685577393, 'eval_accuracy@nld.rst.nldt': 0.2688821752265861, 'eval_f1@nld.rst.nldt': 0.025646434284308035, 'eval_precision@nld.rst.nldt': 0.021604938271604937, 'eval_recall@nld.rst.nldt': 0.041223832528180356, 'eval_loss@nld.rst.nldt': 3.37239933013916, 'eval_runtime': 4.4885, 'eval_samples_per_second': 73.743, 'eval_steps_per_second': 2.451, 'epoch': 1.0}
{'train@nld.rst.nldt_loss': 2.9301679134368896, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26616915422885573, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.019489668494642082, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.017691836243314826, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03506080511914911, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.9301679134368896, 'train@nld.rst.nldt_runtime': 19.915, 'train@nld.rst.nldt_samples_per_second': 80.743, 'train@nld.rst.nldt_steps_per_second': 2.561, 'epoch': 2.0}
{'loss': 3.1555, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.8866424560546875, 'eval_accuracy@nld.rst.nldt': 0.29003021148036257, 'eval_f1@nld.rst.nldt': 0.0282465487840257, 'eval_precision@nld.rst.nldt': 0.022565864833906074, 'eval_recall@nld.rst.nldt': 0.0468138946399816, 'eval_loss@nld.rst.nldt': 2.8866424560546875, 'eval_runtime': 4.4611, 'eval_samples_per_second': 74.196, 'eval_steps_per_second': 2.466, 'epoch': 2.0}
{'train@nld.rst.nldt_loss': 2.814650535583496, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2810945273631841, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.02472103296778826, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.028582443626463512, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.04022996336215646, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.814650774002075, 'train@nld.rst.nldt_runtime': 19.9095, 'train@nld.rst.nldt_samples_per_second': 80.766, 'train@nld.rst.nldt_steps_per_second': 2.562, 'epoch': 3.0}
{'loss': 2.9133, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.780597448348999, 'eval_accuracy@nld.rst.nldt': 0.2990936555891239, 'eval_f1@nld.rst.nldt': 0.032552980788274906, 'eval_precision@nld.rst.nldt': 0.03480228728455679, 'eval_recall@nld.rst.nldt': 0.047166628326048614, 'eval_loss@nld.rst.nldt': 2.780597686767578, 'eval_runtime': 4.4893, 'eval_samples_per_second': 73.73, 'eval_steps_per_second': 2.45, 'epoch': 3.0}
{'train@nld.rst.nldt_loss': 2.753304958343506, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2873134328358209, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.027909679227490478, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.03624878106322743, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.04271895171111923, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.753304958343506, 'train@nld.rst.nldt_runtime': 19.9506, 'train@nld.rst.nldt_samples_per_second': 80.599, 'train@nld.rst.nldt_steps_per_second': 2.556, 'epoch': 4.0}
{'loss': 2.8069, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.732621431350708, 'eval_accuracy@nld.rst.nldt': 0.31419939577039274, 'eval_f1@nld.rst.nldt': 0.03839846977101879, 'eval_precision@nld.rst.nldt': 0.03986078276858419, 'eval_recall@nld.rst.nldt': 0.05326278659611993, 'eval_loss@nld.rst.nldt': 2.732621669769287, 'eval_runtime': 4.4589, 'eval_samples_per_second': 74.234, 'eval_steps_per_second': 2.467, 'epoch': 4.0}
{'train@nld.rst.nldt_loss': 2.715873956680298, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2966417910447761, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03209387220253895, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.04219434517948656, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.047229377258149643, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.715873956680298, 'train@nld.rst.nldt_runtime': 19.9715, 'train@nld.rst.nldt_samples_per_second': 80.515, 'train@nld.rst.nldt_steps_per_second': 2.554, 'epoch': 5.0}
{'loss': 2.764, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.7008731365203857, 'eval_accuracy@nld.rst.nldt': 0.31722054380664655, 'eval_f1@nld.rst.nldt': 0.040999380588299265, 'eval_precision@nld.rst.nldt': 0.04276557348293628, 'eval_recall@nld.rst.nldt': 0.05497661222298903, 'eval_loss@nld.rst.nldt': 2.700873374938965, 'eval_runtime': 4.4381, 'eval_samples_per_second': 74.582, 'eval_steps_per_second': 2.479, 'epoch': 5.0}
{'train@nld.rst.nldt_loss': 2.6717662811279297, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30659203980099503, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.036281064106200904, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.04067284059662109, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.052071230666179524, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6717660427093506, 'train@nld.rst.nldt_runtime': 19.9185, 'train@nld.rst.nldt_samples_per_second': 80.729, 'train@nld.rst.nldt_steps_per_second': 2.56, 'epoch': 6.0}
{'loss': 2.7383, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.6664323806762695, 'eval_accuracy@nld.rst.nldt': 0.3293051359516616, 'eval_f1@nld.rst.nldt': 0.04558102797392244, 'eval_precision@nld.rst.nldt': 0.04379538074556946, 'eval_recall@nld.rst.nldt': 0.06062035120006134, 'eval_loss@nld.rst.nldt': 2.6664321422576904, 'eval_runtime': 4.4706, 'eval_samples_per_second': 74.04, 'eval_steps_per_second': 2.461, 'epoch': 6.0}
{'train@nld.rst.nldt_loss': 2.6368186473846436, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3215174129353234, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.044339573462555334, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.04254686446145274, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.0611277554500061, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6368188858032227, 'train@nld.rst.nldt_runtime': 19.9046, 'train@nld.rst.nldt_samples_per_second': 80.785, 'train@nld.rst.nldt_steps_per_second': 2.562, 'epoch': 7.0}
{'loss': 2.7012, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.640472173690796, 'eval_accuracy@nld.rst.nldt': 0.3293051359516616, 'eval_f1@nld.rst.nldt': 0.05062512613095827, 'eval_precision@nld.rst.nldt': 0.05067711443476973, 'eval_recall@nld.rst.nldt': 0.06458604912711192, 'eval_loss@nld.rst.nldt': 2.640472412109375, 'eval_runtime': 4.4708, 'eval_samples_per_second': 74.036, 'eval_steps_per_second': 2.46, 'epoch': 7.0}
{'train@nld.rst.nldt_loss': 2.6111907958984375, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3252487562189055, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.04542147084475679, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.04132248134549565, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.06345614876385337, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6111907958984375, 'train@nld.rst.nldt_runtime': 19.9825, 'train@nld.rst.nldt_samples_per_second': 80.47, 'train@nld.rst.nldt_steps_per_second': 2.552, 'epoch': 8.0}
{'loss': 2.6804, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.619105339050293, 'eval_accuracy@nld.rst.nldt': 0.34441087613293053, 'eval_f1@nld.rst.nldt': 0.057942410876171885, 'eval_precision@nld.rst.nldt': 0.05717612824002082, 'eval_recall@nld.rst.nldt': 0.07192572144262967, 'eval_loss@nld.rst.nldt': 2.619105100631714, 'eval_runtime': 4.4949, 'eval_samples_per_second': 73.64, 'eval_steps_per_second': 2.447, 'epoch': 8.0}
{'train@nld.rst.nldt_loss': 2.5932915210723877, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3277363184079602, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.04646633491680157, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.04117167144042362, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.0652967563837129, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.5932915210723877, 'train@nld.rst.nldt_runtime': 19.9441, 'train@nld.rst.nldt_samples_per_second': 80.625, 'train@nld.rst.nldt_steps_per_second': 2.557, 'epoch': 9.0}
{'loss': 2.6575, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.603672981262207, 'eval_accuracy@nld.rst.nldt': 0.3504531722054381, 'eval_f1@nld.rst.nldt': 0.059673729844716715, 'eval_precision@nld.rst.nldt': 0.056595585484474374, 'eval_recall@nld.rst.nldt': 0.07469774812769982, 'eval_loss@nld.rst.nldt': 2.603672981262207, 'eval_runtime': 4.4509, 'eval_samples_per_second': 74.367, 'eval_steps_per_second': 2.471, 'epoch': 9.0}
{'train@nld.rst.nldt_loss': 2.582094430923462, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3283582089552239, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.046962995104878905, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.04172014374665113, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.06560312893273251, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.582094430923462, 'train@nld.rst.nldt_runtime': 19.9232, 'train@nld.rst.nldt_samples_per_second': 80.71, 'train@nld.rst.nldt_steps_per_second': 2.56, 'epoch': 10.0}
{'loss': 2.6428, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.592683792114258, 'eval_accuracy@nld.rst.nldt': 0.35347432024169184, 'eval_f1@nld.rst.nldt': 0.06048992104411477, 'eval_precision@nld.rst.nldt': 0.058043958324070374, 'eval_recall@nld.rst.nldt': 0.07510032461723283, 'eval_loss@nld.rst.nldt': 2.592684030532837, 'eval_runtime': 4.4611, 'eval_samples_per_second': 74.197, 'eval_steps_per_second': 2.466, 'epoch': 10.0}
{'train@nld.rst.nldt_loss': 2.5736682415008545, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3302238805970149, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.047446672890479644, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.041658120631518726, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.06649719126781148, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.5736684799194336, 'train@nld.rst.nldt_runtime': 19.8839, 'train@nld.rst.nldt_samples_per_second': 80.87, 'train@nld.rst.nldt_steps_per_second': 2.565, 'epoch': 11.0}
{'loss': 2.6137, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.585960626602173, 'eval_accuracy@nld.rst.nldt': 0.3564954682779456, 'eval_f1@nld.rst.nldt': 0.0625098175069458, 'eval_precision@nld.rst.nldt': 0.05968106792120348, 'eval_recall@nld.rst.nldt': 0.07715793778595711, 'eval_loss@nld.rst.nldt': 2.585960626602173, 'eval_runtime': 4.4503, 'eval_samples_per_second': 74.377, 'eval_steps_per_second': 2.472, 'epoch': 11.0}
{'train@nld.rst.nldt_loss': 2.5712101459503174, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.32960199004975127, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.047187341661838134, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.04108013807640586, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.06642366185604677, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.5712103843688965, 'train@nld.rst.nldt_runtime': 19.8883, 'train@nld.rst.nldt_samples_per_second': 80.852, 'train@nld.rst.nldt_steps_per_second': 2.564, 'epoch': 12.0}
{'loss': 2.6294, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.5843822956085205, 'eval_accuracy@nld.rst.nldt': 0.3564954682779456, 'eval_f1@nld.rst.nldt': 0.0625098175069458, 'eval_precision@nld.rst.nldt': 0.05968106792120348, 'eval_recall@nld.rst.nldt': 0.07715793778595711, 'eval_loss@nld.rst.nldt': 2.5843822956085205, 'eval_runtime': 4.4787, 'eval_samples_per_second': 73.905, 'eval_steps_per_second': 2.456, 'epoch': 12.0}
{'train_runtime': 785.7734, 'train_samples_per_second': 24.557, 'train_steps_per_second': 0.779, 'train_loss': 2.8367859834159903, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.7529
  train_runtime            = 1:13:34.51
  train_samples_per_second =     26.041
  train_steps_per_second   =      0.815
-------------------------------------------------------------------
Lang1:  fas.rst.prstc    Lang2:  nld.rst.nldt
Saving run to:  runs/full_shot/FullShot=v4_fas.rst.prstc_nld.rst.nldt_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 4100 examples
read 499 examples
read 592 examples
read 1608 examples
read 331 examples
read 326 examples
Total prediction labels:  42
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (ar): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=42, bias=True)
    )
  )
)
{'train@fas.rst.prstc_loss': 2.44179105758667, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.23780487804878048, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.02260214430599826, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.013988522238163558, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.058823529411764705, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.441791296005249, 'train@fas.rst.prstc_runtime': 48.8363, 'train@fas.rst.prstc_samples_per_second': 83.954, 'train@fas.rst.prstc_steps_per_second': 2.641, 'epoch': 1.0}
{'loss': 2.9109, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.3587806224823, 'eval_accuracy@fas.rst.prstc': 0.24448897795591182, 'eval_f1@fas.rst.prstc': 0.026194310252281264, 'eval_precision@fas.rst.prstc': 0.016299265197060788, 'eval_recall@fas.rst.prstc': 0.06666666666666667, 'eval_loss@fas.rst.prstc': 2.358781099319458, 'eval_runtime': 7.5922, 'eval_samples_per_second': 65.726, 'eval_steps_per_second': 2.107, 'epoch': 1.0}
{'train@fas.rst.prstc_loss': 2.3667125701904297, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.27390243902439027, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.042795071060053445, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.032108939593295034, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07067809120374827, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3667125701904297, 'train@fas.rst.prstc_runtime': 48.9817, 'train@fas.rst.prstc_samples_per_second': 83.705, 'train@fas.rst.prstc_steps_per_second': 2.634, 'epoch': 2.0}
{'loss': 2.4234, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.2770540714263916, 'eval_accuracy@fas.rst.prstc': 0.3026052104208417, 'eval_f1@fas.rst.prstc': 0.052062016348443985, 'eval_precision@fas.rst.prstc': 0.0444619560458804, 'eval_recall@fas.rst.prstc': 0.08371109527203612, 'eval_loss@fas.rst.prstc': 2.2770543098449707, 'eval_runtime': 6.3034, 'eval_samples_per_second': 79.164, 'eval_steps_per_second': 2.538, 'epoch': 2.0}
{'train@fas.rst.prstc_loss': 2.343611478805542, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.24829268292682927, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.03000680116301, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.030999897553097244, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.06198816897189863, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.343611478805542, 'train@fas.rst.prstc_runtime': 49.0809, 'train@fas.rst.prstc_samples_per_second': 83.536, 'train@fas.rst.prstc_steps_per_second': 2.628, 'epoch': 3.0}
{'loss': 2.3687, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.25653338432312, 'eval_accuracy@fas.rst.prstc': 0.2625250501002004, 'eval_f1@fas.rst.prstc': 0.03593464052287582, 'eval_precision@fas.rst.prstc': 0.04558677027296274, 'eval_recall@fas.rst.prstc': 0.07188405797101449, 'eval_loss@fas.rst.prstc': 2.25653338432312, 'eval_runtime': 6.32, 'eval_samples_per_second': 78.955, 'eval_steps_per_second': 2.532, 'epoch': 3.0}
{'train@fas.rst.prstc_loss': 2.3275651931762695, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2375609756097561, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.022587864240906294, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.013980995033445296, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.058763197586726997, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3275651931762695, 'train@fas.rst.prstc_runtime': 49.1178, 'train@fas.rst.prstc_samples_per_second': 83.473, 'train@fas.rst.prstc_steps_per_second': 2.626, 'epoch': 4.0}
{'loss': 2.354, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.2464587688446045, 'eval_accuracy@fas.rst.prstc': 0.24448897795591182, 'eval_f1@fas.rst.prstc': 0.026236559139784947, 'eval_precision@fas.rst.prstc': 0.016331994645247656, 'eval_recall@fas.rst.prstc': 0.06666666666666667, 'eval_loss@fas.rst.prstc': 2.2464585304260254, 'eval_runtime': 6.3275, 'eval_samples_per_second': 78.862, 'eval_steps_per_second': 2.529, 'epoch': 4.0}
{'train@fas.rst.prstc_loss': 2.3112759590148926, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2673170731707317, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.038290669858062974, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03135665936158564, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.06775199768941947, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3112759590148926, 'train@fas.rst.prstc_runtime': 49.1096, 'train@fas.rst.prstc_samples_per_second': 83.487, 'train@fas.rst.prstc_steps_per_second': 2.627, 'epoch': 5.0}
{'loss': 2.3365, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.227588653564453, 'eval_accuracy@fas.rst.prstc': 0.28857715430861725, 'eval_f1@fas.rst.prstc': 0.047556621369571, 'eval_precision@fas.rst.prstc': 0.044792626728110595, 'eval_recall@fas.rst.prstc': 0.07955333808505584, 'eval_loss@fas.rst.prstc': 2.2275888919830322, 'eval_runtime': 6.3116, 'eval_samples_per_second': 79.061, 'eval_steps_per_second': 2.535, 'epoch': 5.0}
{'train@fas.rst.prstc_loss': 2.2861557006835938, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.25951219512195123, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.03495589304121876, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03137845249592522, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.06529700587272552, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.286156177520752, 'train@fas.rst.prstc_runtime': 49.2025, 'train@fas.rst.prstc_samples_per_second': 83.329, 'train@fas.rst.prstc_steps_per_second': 2.622, 'epoch': 6.0}
{'loss': 2.3178, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.206646203994751, 'eval_accuracy@fas.rst.prstc': 0.280561122244489, 'eval_f1@fas.rst.prstc': 0.04423219057365399, 'eval_precision@fas.rst.prstc': 0.04498866213151927, 'eval_recall@fas.rst.prstc': 0.07716797339035401, 'eval_loss@fas.rst.prstc': 2.206645965576172, 'eval_runtime': 6.3541, 'eval_samples_per_second': 78.531, 'eval_steps_per_second': 2.518, 'epoch': 6.0}
{'train@fas.rst.prstc_loss': 2.241973400115967, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2995121951219512, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04885910677858691, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.035537617184222374, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07882524095290053, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.241973400115967, 'train@fas.rst.prstc_runtime': 49.1369, 'train@fas.rst.prstc_samples_per_second': 83.44, 'train@fas.rst.prstc_steps_per_second': 2.625, 'epoch': 7.0}
{'loss': 2.2841, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.14982008934021, 'eval_accuracy@fas.rst.prstc': 0.3166332665330661, 'eval_f1@fas.rst.prstc': 0.057466032140147595, 'eval_precision@fas.rst.prstc': 0.042602736783312924, 'eval_recall@fas.rst.prstc': 0.08866714183891661, 'eval_loss@fas.rst.prstc': 2.14982008934021, 'eval_runtime': 6.3142, 'eval_samples_per_second': 79.028, 'eval_steps_per_second': 2.534, 'epoch': 7.0}
{'train@fas.rst.prstc_loss': 2.2011094093322754, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3104878048780488, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.051825681138867645, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03892973066003652, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08248772504091653, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.2011091709136963, 'train@fas.rst.prstc_runtime': 49.0661, 'train@fas.rst.prstc_samples_per_second': 83.561, 'train@fas.rst.prstc_steps_per_second': 2.629, 'epoch': 8.0}
{'loss': 2.246, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.1053502559661865, 'eval_accuracy@fas.rst.prstc': 0.33266533066132264, 'eval_f1@fas.rst.prstc': 0.06173835193241284, 'eval_precision@fas.rst.prstc': 0.047264038231780166, 'eval_recall@fas.rst.prstc': 0.09343787122832027, 'eval_loss@fas.rst.prstc': 2.1053502559661865, 'eval_runtime': 6.3237, 'eval_samples_per_second': 78.91, 'eval_steps_per_second': 2.53, 'epoch': 8.0}
{'train@fas.rst.prstc_loss': 2.1821417808532715, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3195121951219512, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.05414138938630651, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.04194855508387232, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08523517644919398, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1821417808532715, 'train@fas.rst.prstc_runtime': 49.0971, 'train@fas.rst.prstc_samples_per_second': 83.508, 'train@fas.rst.prstc_steps_per_second': 2.627, 'epoch': 9.0}
{'loss': 2.214, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.0810532569885254, 'eval_accuracy@fas.rst.prstc': 0.3466933867735471, 'eval_f1@fas.rst.prstc': 0.06492657040602245, 'eval_precision@fas.rst.prstc': 0.05078610167471244, 'eval_recall@fas.rst.prstc': 0.09752910430030885, 'eval_loss@fas.rst.prstc': 2.0810530185699463, 'eval_runtime': 6.2953, 'eval_samples_per_second': 79.266, 'eval_steps_per_second': 2.542, 'epoch': 9.0}
{'train@fas.rst.prstc_loss': 2.174753189086914, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.32390243902439025, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.055084433239811714, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.04305887931914924, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08649594043836847, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.174753189086914, 'train@fas.rst.prstc_runtime': 48.9549, 'train@fas.rst.prstc_samples_per_second': 83.751, 'train@fas.rst.prstc_steps_per_second': 2.635, 'epoch': 10.0}
{'loss': 2.1984, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.074287176132202, 'eval_accuracy@fas.rst.prstc': 0.3486973947895792, 'eval_f1@fas.rst.prstc': 0.06568733570434512, 'eval_precision@fas.rst.prstc': 0.0516556370785978, 'eval_recall@fas.rst.prstc': 0.09804229033024471, 'eval_loss@fas.rst.prstc': 2.0742876529693604, 'eval_runtime': 6.3162, 'eval_samples_per_second': 79.003, 'eval_steps_per_second': 2.533, 'epoch': 10.0}
{'train@fas.rst.prstc_loss': 2.1678528785705566, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3278048780487805, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.055867000426902624, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.04405882393947185, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08767283891188772, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1678531169891357, 'train@fas.rst.prstc_runtime': 48.9758, 'train@fas.rst.prstc_samples_per_second': 83.715, 'train@fas.rst.prstc_steps_per_second': 2.634, 'epoch': 11.0}
{'loss': 2.1947, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.065969228744507, 'eval_accuracy@fas.rst.prstc': 0.3527054108216433, 'eval_f1@fas.rst.prstc': 0.0665329991645781, 'eval_precision@fas.rst.prstc': 0.052976477721271516, 'eval_recall@fas.rst.prstc': 0.09930149679258732, 'eval_loss@fas.rst.prstc': 2.065969228744507, 'eval_runtime': 6.3129, 'eval_samples_per_second': 79.044, 'eval_steps_per_second': 2.534, 'epoch': 11.0}
{'train@fas.rst.prstc_loss': 2.1681110858917236, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3280487804878049, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.05582857716448088, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.04386480121354938, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08770557213611031, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1681108474731445, 'train@fas.rst.prstc_runtime': 48.9525, 'train@fas.rst.prstc_samples_per_second': 83.755, 'train@fas.rst.prstc_steps_per_second': 2.635, 'epoch': 12.0}
{'loss': 2.1874, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.067058563232422, 'eval_accuracy@fas.rst.prstc': 0.3527054108216433, 'eval_f1@fas.rst.prstc': 0.06655627414836338, 'eval_precision@fas.rst.prstc': 0.05287241887905605, 'eval_recall@fas.rst.prstc': 0.09926823473509147, 'eval_loss@fas.rst.prstc': 2.0670583248138428, 'eval_runtime': 6.3173, 'eval_samples_per_second': 78.99, 'eval_steps_per_second': 2.533, 'epoch': 12.0}
{'train_runtime': 1896.9161, 'train_samples_per_second': 25.937, 'train_steps_per_second': 0.816, 'train_loss': 2.336329063395813, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.3363
  train_runtime            = 0:31:36.91
  train_samples_per_second =     25.937
  train_steps_per_second   =      0.816
{'train@nld.rst.nldt_loss': 3.1970112323760986, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.263681592039801, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.017234162732744536, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.011945731035823023, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03414274042950514, 'train@nld.rst.nldt_loss@nld.rst.nldt': 3.1970107555389404, 'train@nld.rst.nldt_runtime': 19.8143, 'train@nld.rst.nldt_samples_per_second': 81.153, 'train@nld.rst.nldt_steps_per_second': 2.574, 'epoch': 1.0}
{'loss': 3.4731, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.159374475479126, 'eval_accuracy@nld.rst.nldt': 0.2719033232628399, 'eval_f1@nld.rst.nldt': 0.019849507005708353, 'eval_precision@nld.rst.nldt': 0.013709919089002097, 'eval_recall@nld.rst.nldt': 0.038198757763975154, 'eval_loss@nld.rst.nldt': 3.159374713897705, 'eval_runtime': 4.3952, 'eval_samples_per_second': 75.309, 'eval_steps_per_second': 2.503, 'epoch': 1.0}
{'train@nld.rst.nldt_loss': 2.901352643966675, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26803482587064675, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01880370037011964, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.018391757899195442, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03442577030812325, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.901352643966675, 'train@nld.rst.nldt_runtime': 19.8131, 'train@nld.rst.nldt_samples_per_second': 81.158, 'train@nld.rst.nldt_steps_per_second': 2.574, 'epoch': 2.0}
{'loss': 3.0545, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.8436386585235596, 'eval_accuracy@nld.rst.nldt': 0.29003021148036257, 'eval_f1@nld.rst.nldt': 0.02700132215736167, 'eval_precision@nld.rst.nldt': 0.023763381321520855, 'eval_recall@nld.rst.nldt': 0.044923702170078975, 'eval_loss@nld.rst.nldt': 2.8436381816864014, 'eval_runtime': 4.4033, 'eval_samples_per_second': 75.17, 'eval_steps_per_second': 2.498, 'epoch': 2.0}
{'train@nld.rst.nldt_loss': 2.8169662952423096, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2736318407960199, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.022874064011852623, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.024522824529826626, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.037823295985060686, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.8169665336608887, 'train@nld.rst.nldt_runtime': 19.8508, 'train@nld.rst.nldt_samples_per_second': 81.004, 'train@nld.rst.nldt_steps_per_second': 2.569, 'epoch': 3.0}
{'loss': 2.8941, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.7658841609954834, 'eval_accuracy@nld.rst.nldt': 0.30513595166163143, 'eval_f1@nld.rst.nldt': 0.03570064910271095, 'eval_precision@nld.rst.nldt': 0.0323122267566712, 'eval_recall@nld.rst.nldt': 0.056866804692891654, 'eval_loss@nld.rst.nldt': 2.7658841609954834, 'eval_runtime': 4.396, 'eval_samples_per_second': 75.296, 'eval_steps_per_second': 2.502, 'epoch': 3.0}
{'train@nld.rst.nldt_loss': 2.777496099472046, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.27922885572139305, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.02461132333374986, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.027588228259473584, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03999883286647993, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.777496099472046, 'train@nld.rst.nldt_runtime': 19.8769, 'train@nld.rst.nldt_samples_per_second': 80.898, 'train@nld.rst.nldt_steps_per_second': 2.566, 'epoch': 4.0}
{'loss': 2.7951, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.7319788932800293, 'eval_accuracy@nld.rst.nldt': 0.3081570996978852, 'eval_f1@nld.rst.nldt': 0.037126202793155094, 'eval_precision@nld.rst.nldt': 0.035522776263517, 'eval_recall@nld.rst.nldt': 0.058924417861615924, 'eval_loss@nld.rst.nldt': 2.73197865486145, 'eval_runtime': 4.4025, 'eval_samples_per_second': 75.184, 'eval_steps_per_second': 2.499, 'epoch': 4.0}
{'train@nld.rst.nldt_loss': 2.752103805541992, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2860696517412935, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.0279160803992844, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.029973450323976223, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.043368347338935576, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.752103328704834, 'train@nld.rst.nldt_runtime': 19.8168, 'train@nld.rst.nldt_samples_per_second': 81.143, 'train@nld.rst.nldt_steps_per_second': 2.574, 'epoch': 5.0}
{'loss': 2.7766, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.7081549167633057, 'eval_accuracy@nld.rst.nldt': 0.31419939577039274, 'eval_f1@nld.rst.nldt': 0.039246632996632995, 'eval_precision@nld.rst.nldt': 0.03446079260386719, 'eval_recall@nld.rst.nldt': 0.06303964419906449, 'eval_loss@nld.rst.nldt': 2.7081546783447266, 'eval_runtime': 4.4031, 'eval_samples_per_second': 75.175, 'eval_steps_per_second': 2.498, 'epoch': 5.0}
{'train@nld.rst.nldt_loss': 2.7199583053588867, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2922885572139303, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03073767575848568, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.02979358731672543, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.0477124183006536, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.7199580669403076, 'train@nld.rst.nldt_runtime': 19.8052, 'train@nld.rst.nldt_samples_per_second': 81.191, 'train@nld.rst.nldt_steps_per_second': 2.575, 'epoch': 6.0}
{'loss': 2.7466, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.6822028160095215, 'eval_accuracy@nld.rst.nldt': 0.31419939577039274, 'eval_f1@nld.rst.nldt': 0.037001787603231064, 'eval_precision@nld.rst.nldt': 0.032651072124756333, 'eval_recall@nld.rst.nldt': 0.06303964419906449, 'eval_loss@nld.rst.nldt': 2.6822028160095215, 'eval_runtime': 4.3853, 'eval_samples_per_second': 75.48, 'eval_steps_per_second': 2.508, 'epoch': 6.0}
{'train@nld.rst.nldt_loss': 2.6914162635803223, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30223880597014924, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03439915156385079, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.029585087143445578, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05400968720821662, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.691416025161743, 'train@nld.rst.nldt_runtime': 19.858, 'train@nld.rst.nldt_samples_per_second': 80.975, 'train@nld.rst.nldt_steps_per_second': 2.568, 'epoch': 7.0}
{'loss': 2.7203, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.6593682765960693, 'eval_accuracy@nld.rst.nldt': 0.3202416918429003, 'eval_f1@nld.rst.nldt': 0.04031335453987387, 'eval_precision@nld.rst.nldt': 0.03634720903239422, 'eval_recall@nld.rst.nldt': 0.06746670756332593, 'eval_loss@nld.rst.nldt': 2.6593685150146484, 'eval_runtime': 4.3682, 'eval_samples_per_second': 75.775, 'eval_steps_per_second': 2.518, 'epoch': 7.0}
{'train@nld.rst.nldt_loss': 2.6726906299591064, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30472636815920395, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03504759021868728, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.029317528692494918, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.056224323062558354, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6726913452148438, 'train@nld.rst.nldt_runtime': 19.8332, 'train@nld.rst.nldt_samples_per_second': 81.076, 'train@nld.rst.nldt_steps_per_second': 2.571, 'epoch': 8.0}
{'loss': 2.7131, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.6465251445770264, 'eval_accuracy@nld.rst.nldt': 0.31722054380664655, 'eval_f1@nld.rst.nldt': 0.04030794323671146, 'eval_precision@nld.rst.nldt': 0.03683099928310656, 'eval_recall@nld.rst.nldt': 0.06771975564246097, 'eval_loss@nld.rst.nldt': 2.6465251445770264, 'eval_runtime': 4.3936, 'eval_samples_per_second': 75.337, 'eval_steps_per_second': 2.504, 'epoch': 8.0}
{'train@nld.rst.nldt_loss': 2.659580707550049, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3072139303482587, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.035948848734107745, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.029162522407363986, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05715802987861811, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.659580707550049, 'train@nld.rst.nldt_runtime': 19.8295, 'train@nld.rst.nldt_samples_per_second': 81.091, 'train@nld.rst.nldt_steps_per_second': 2.572, 'epoch': 9.0}
{'loss': 2.6969, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.6355278491973877, 'eval_accuracy@nld.rst.nldt': 0.31722054380664655, 'eval_f1@nld.rst.nldt': 0.04018095089523661, 'eval_precision@nld.rst.nldt': 0.03672161172161172, 'eval_recall@nld.rst.nldt': 0.06771975564246097, 'eval_loss@nld.rst.nldt': 2.6355273723602295, 'eval_runtime': 4.3833, 'eval_samples_per_second': 75.513, 'eval_steps_per_second': 2.51, 'epoch': 9.0}
{'train@nld.rst.nldt_loss': 2.6529884338378906, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30534825870646765, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03529012077472951, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.028959019621244496, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05600665266106443, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6529877185821533, 'train@nld.rst.nldt_runtime': 19.8442, 'train@nld.rst.nldt_samples_per_second': 81.031, 'train@nld.rst.nldt_steps_per_second': 2.57, 'epoch': 10.0}
{'loss': 2.6778, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.6305019855499268, 'eval_accuracy@nld.rst.nldt': 0.3202416918429003, 'eval_f1@nld.rst.nldt': 0.043056068763202685, 'eval_precision@nld.rst.nldt': 0.046480316794782206, 'eval_recall@nld.rst.nldt': 0.06882779950412801, 'eval_loss@nld.rst.nldt': 2.6305019855499268, 'eval_runtime': 4.3931, 'eval_samples_per_second': 75.345, 'eval_steps_per_second': 2.504, 'epoch': 10.0}
{'train@nld.rst.nldt_loss': 2.646263360977173, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30597014925373134, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03559910031401699, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.029018740259015893, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.056661998132586364, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.646263599395752, 'train@nld.rst.nldt_runtime': 19.8331, 'train@nld.rst.nldt_samples_per_second': 81.077, 'train@nld.rst.nldt_steps_per_second': 2.571, 'epoch': 11.0}
{'loss': 2.6651, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.625622034072876, 'eval_accuracy@nld.rst.nldt': 0.3202416918429003, 'eval_f1@nld.rst.nldt': 0.042690482575084086, 'eval_precision@nld.rst.nldt': 0.044397323532183584, 'eval_recall@nld.rst.nldt': 0.06882779950412801, 'eval_loss@nld.rst.nldt': 2.625621795654297, 'eval_runtime': 4.3773, 'eval_samples_per_second': 75.617, 'eval_steps_per_second': 2.513, 'epoch': 11.0}
{'train@nld.rst.nldt_loss': 2.6437981128692627, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30783582089552236, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.036084857221592506, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.02936395761068067, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.057522759103641456, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6437981128692627, 'train@nld.rst.nldt_runtime': 19.8214, 'train@nld.rst.nldt_samples_per_second': 81.125, 'train@nld.rst.nldt_steps_per_second': 2.573, 'epoch': 12.0}
{'loss': 2.6776, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.624297618865967, 'eval_accuracy@nld.rst.nldt': 0.31722054380664655, 'eval_f1@nld.rst.nldt': 0.042264044192733774, 'eval_precision@nld.rst.nldt': 0.045811456411711846, 'eval_recall@nld.rst.nldt': 0.068425223014595, 'eval_loss@nld.rst.nldt': 2.624298095703125, 'eval_runtime': 4.3992, 'eval_samples_per_second': 75.241, 'eval_steps_per_second': 2.5, 'epoch': 12.0}
{'train_runtime': 783.3901, 'train_samples_per_second': 24.631, 'train_steps_per_second': 0.781, 'train_loss': 2.8242417129815793, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.3363
  train_runtime            = 0:31:36.91
  train_samples_per_second =     25.937
  train_steps_per_second   =      0.816
-------------------------------------------------------------------
Lang1:  fra.sdrt.annodis    Lang2:  nld.rst.nldt
Saving run to:  runs/full_shot/FullShot=v4_fra.sdrt.annodis_nld.rst.nldt_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2185 examples
read 528 examples
read 625 examples
read 1608 examples
read 331 examples
read 326 examples
Total prediction labels:  48
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (fr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=48, bias=True)
    )
  )
)
{'train@fra.sdrt.annodis_loss': 2.7737910747528076, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.22334096109839818, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.04366430768227136, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.042974217681223066, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.06379860113957253, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.7737913131713867, 'train@fra.sdrt.annodis_runtime': 26.618, 'train@fra.sdrt.annodis_samples_per_second': 82.087, 'train@fra.sdrt.annodis_steps_per_second': 2.592, 'epoch': 1.0}
{'loss': 3.3501, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.8014540672302246, 'eval_accuracy@fra.sdrt.annodis': 0.2215909090909091, 'eval_f1@fra.sdrt.annodis': 0.039681870624505206, 'eval_precision@fra.sdrt.annodis': 0.04160887281338548, 'eval_recall@fra.sdrt.annodis': 0.06042265872657011, 'eval_loss@fra.sdrt.annodis': 2.8014540672302246, 'eval_runtime': 6.7954, 'eval_samples_per_second': 77.699, 'eval_steps_per_second': 2.502, 'epoch': 1.0}
{'train@fra.sdrt.annodis_loss': 2.4163312911987305, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.2791762013729977, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.060518530938888225, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.04723546832953143, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.0844768282362733, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.4163312911987305, 'train@fra.sdrt.annodis_runtime': 26.84, 'train@fra.sdrt.annodis_samples_per_second': 81.408, 'train@fra.sdrt.annodis_steps_per_second': 2.571, 'epoch': 2.0}
{'loss': 2.5777, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.434605121612549, 'eval_accuracy@fra.sdrt.annodis': 0.26325757575757575, 'eval_f1@fra.sdrt.annodis': 0.055541190379562316, 'eval_precision@fra.sdrt.annodis': 0.04350150907869521, 'eval_recall@fra.sdrt.annodis': 0.07701003183869226, 'eval_loss@fra.sdrt.annodis': 2.434605121612549, 'eval_runtime': 6.8325, 'eval_samples_per_second': 77.277, 'eval_steps_per_second': 2.488, 'epoch': 2.0}
{'train@fra.sdrt.annodis_loss': 2.32763934135437, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.28924485125858124, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.06230164106323342, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.05578998151080578, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.08472700751477462, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.32763934135437, 'train@fra.sdrt.annodis_runtime': 26.8501, 'train@fra.sdrt.annodis_samples_per_second': 81.378, 'train@fra.sdrt.annodis_steps_per_second': 2.57, 'epoch': 3.0}
{'loss': 2.3955, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.3482837677001953, 'eval_accuracy@fra.sdrt.annodis': 0.2689393939393939, 'eval_f1@fra.sdrt.annodis': 0.05739982570300232, 'eval_precision@fra.sdrt.annodis': 0.05002120792143973, 'eval_recall@fra.sdrt.annodis': 0.07742321761013349, 'eval_loss@fra.sdrt.annodis': 2.3482840061187744, 'eval_runtime': 6.8033, 'eval_samples_per_second': 77.609, 'eval_steps_per_second': 2.499, 'epoch': 3.0}
{'train@fra.sdrt.annodis_loss': 2.2721221446990967, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.30297482837528605, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.06635174805761895, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.07131794259387483, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.0891744472137387, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.2721219062805176, 'train@fra.sdrt.annodis_runtime': 27.6594, 'train@fra.sdrt.annodis_samples_per_second': 78.997, 'train@fra.sdrt.annodis_steps_per_second': 2.495, 'epoch': 4.0}
{'loss': 2.3329, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.295255422592163, 'eval_accuracy@fra.sdrt.annodis': 0.2784090909090909, 'eval_f1@fra.sdrt.annodis': 0.05907476119222082, 'eval_precision@fra.sdrt.annodis': 0.04908454453173924, 'eval_recall@fra.sdrt.annodis': 0.07994269797661974, 'eval_loss@fra.sdrt.annodis': 2.295255422592163, 'eval_runtime': 6.7848, 'eval_samples_per_second': 77.821, 'eval_steps_per_second': 2.506, 'epoch': 4.0}
{'train@fra.sdrt.annodis_loss': 2.225679397583008, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.3318077803203661, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.07211850838374595, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.07512991773633944, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.10142476651216634, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.225679397583008, 'train@fra.sdrt.annodis_runtime': 26.8563, 'train@fra.sdrt.annodis_samples_per_second': 81.359, 'train@fra.sdrt.annodis_steps_per_second': 2.569, 'epoch': 5.0}
{'loss': 2.2836, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.2571568489074707, 'eval_accuracy@fra.sdrt.annodis': 0.2935606060606061, 'eval_f1@fra.sdrt.annodis': 0.0625012025012025, 'eval_precision@fra.sdrt.annodis': 0.04930970934020189, 'eval_recall@fra.sdrt.annodis': 0.0891353451062693, 'eval_loss@fra.sdrt.annodis': 2.2571568489074707, 'eval_runtime': 6.8097, 'eval_samples_per_second': 77.536, 'eval_steps_per_second': 2.496, 'epoch': 5.0}
{'train@fra.sdrt.annodis_loss': 2.1814608573913574, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.34782608695652173, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.0876867266025719, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.13165915544922577, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.11114879414460502, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.1814608573913574, 'train@fra.sdrt.annodis_runtime': 26.7875, 'train@fra.sdrt.annodis_samples_per_second': 81.568, 'train@fra.sdrt.annodis_steps_per_second': 2.576, 'epoch': 6.0}
{'loss': 2.2354, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.2194838523864746, 'eval_accuracy@fra.sdrt.annodis': 0.29924242424242425, 'eval_f1@fra.sdrt.annodis': 0.06638034295392854, 'eval_precision@fra.sdrt.annodis': 0.10594680958132045, 'eval_recall@fra.sdrt.annodis': 0.09234515500168178, 'eval_loss@fra.sdrt.annodis': 2.2194840908050537, 'eval_runtime': 6.8009, 'eval_samples_per_second': 77.637, 'eval_steps_per_second': 2.5, 'epoch': 6.0}
{'train@fra.sdrt.annodis_loss': 2.139392137527466, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.3716247139588101, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.11094783736041411, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12604262323219886, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.12920647540112573, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.139392137527466, 'train@fra.sdrt.annodis_runtime': 26.7724, 'train@fra.sdrt.annodis_samples_per_second': 81.614, 'train@fra.sdrt.annodis_steps_per_second': 2.577, 'epoch': 7.0}
{'loss': 2.196, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.1843206882476807, 'eval_accuracy@fra.sdrt.annodis': 0.3068181818181818, 'eval_f1@fra.sdrt.annodis': 0.0777902311339634, 'eval_precision@fra.sdrt.annodis': 0.0967202001075344, 'eval_recall@fra.sdrt.annodis': 0.09895421147533623, 'eval_loss@fra.sdrt.annodis': 2.1843209266662598, 'eval_runtime': 6.8151, 'eval_samples_per_second': 77.475, 'eval_steps_per_second': 2.494, 'epoch': 7.0}
{'train@fra.sdrt.annodis_loss': 2.1023106575012207, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.3922196796338673, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.12456031544738133, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.1279588858787108, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.14552537644617006, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.1023108959198, 'train@fra.sdrt.annodis_runtime': 26.76, 'train@fra.sdrt.annodis_samples_per_second': 81.652, 'train@fra.sdrt.annodis_steps_per_second': 2.578, 'epoch': 8.0}
{'loss': 2.1706, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.155651092529297, 'eval_accuracy@fra.sdrt.annodis': 0.32386363636363635, 'eval_f1@fra.sdrt.annodis': 0.08616130814787086, 'eval_precision@fra.sdrt.annodis': 0.09107900965276038, 'eval_recall@fra.sdrt.annodis': 0.1070649885525784, 'eval_loss@fra.sdrt.annodis': 2.155651092529297, 'eval_runtime': 6.8049, 'eval_samples_per_second': 77.591, 'eval_steps_per_second': 2.498, 'epoch': 8.0}
{'train@fra.sdrt.annodis_loss': 2.0744922161102295, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.39862700228832953, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.12625719050033965, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.1272465669863519, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.15200692058674992, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.0744924545288086, 'train@fra.sdrt.annodis_runtime': 26.7908, 'train@fra.sdrt.annodis_samples_per_second': 81.558, 'train@fra.sdrt.annodis_steps_per_second': 2.576, 'epoch': 9.0}
{'loss': 2.1299, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.133269786834717, 'eval_accuracy@fra.sdrt.annodis': 0.3409090909090909, 'eval_f1@fra.sdrt.annodis': 0.09624078290887275, 'eval_precision@fra.sdrt.annodis': 0.09001642743029241, 'eval_recall@fra.sdrt.annodis': 0.1185932027766165, 'eval_loss@fra.sdrt.annodis': 2.133269786834717, 'eval_runtime': 6.806, 'eval_samples_per_second': 77.578, 'eval_steps_per_second': 2.498, 'epoch': 9.0}
{'train@fra.sdrt.annodis_loss': 2.054058790206909, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4059496567505721, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.13013972091255527, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.1276234398422648, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.15665055791257645, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.054058790206909, 'train@fra.sdrt.annodis_runtime': 26.8109, 'train@fra.sdrt.annodis_samples_per_second': 81.497, 'train@fra.sdrt.annodis_steps_per_second': 2.574, 'epoch': 10.0}
{'loss': 2.104, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.118180274963379, 'eval_accuracy@fra.sdrt.annodis': 0.3503787878787879, 'eval_f1@fra.sdrt.annodis': 0.10010837841570956, 'eval_precision@fra.sdrt.annodis': 0.09777927386623039, 'eval_recall@fra.sdrt.annodis': 0.12299255157207403, 'eval_loss@fra.sdrt.annodis': 2.118180751800537, 'eval_runtime': 6.8392, 'eval_samples_per_second': 77.202, 'eval_steps_per_second': 2.486, 'epoch': 10.0}
{'train@fra.sdrt.annodis_loss': 2.0416083335876465, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4137299771167048, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.13287731797052277, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12834528341766221, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.16162079293296885, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.0416083335876465, 'train@fra.sdrt.annodis_runtime': 26.8289, 'train@fra.sdrt.annodis_samples_per_second': 81.442, 'train@fra.sdrt.annodis_steps_per_second': 2.572, 'epoch': 11.0}
{'loss': 2.0817, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.1079702377319336, 'eval_accuracy@fra.sdrt.annodis': 0.35795454545454547, 'eval_f1@fra.sdrt.annodis': 0.10183323516825918, 'eval_precision@fra.sdrt.annodis': 0.09525985654862558, 'eval_recall@fra.sdrt.annodis': 0.1273363676256207, 'eval_loss@fra.sdrt.annodis': 2.1079699993133545, 'eval_runtime': 6.8093, 'eval_samples_per_second': 77.541, 'eval_steps_per_second': 2.497, 'epoch': 11.0}
{'train@fra.sdrt.annodis_loss': 2.037405014038086, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.41327231121281466, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.1328673300800639, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12766794971970616, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.16163626413746443, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.037404775619507, 'train@fra.sdrt.annodis_runtime': 26.7953, 'train@fra.sdrt.annodis_samples_per_second': 81.544, 'train@fra.sdrt.annodis_steps_per_second': 2.575, 'epoch': 12.0}
{'loss': 2.0809, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.1049327850341797, 'eval_accuracy@fra.sdrt.annodis': 0.3560606060606061, 'eval_f1@fra.sdrt.annodis': 0.10161394853151069, 'eval_precision@fra.sdrt.annodis': 0.09461612026253347, 'eval_recall@fra.sdrt.annodis': 0.12677973622786784, 'eval_loss@fra.sdrt.annodis': 2.1049323081970215, 'eval_runtime': 6.8164, 'eval_samples_per_second': 77.46, 'eval_steps_per_second': 2.494, 'epoch': 12.0}
{'train_runtime': 1071.3752, 'train_samples_per_second': 24.473, 'train_steps_per_second': 0.773, 'train_loss': 2.328203763362866, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.3282
  train_runtime            = 0:17:51.37
  train_samples_per_second =     24.473
  train_steps_per_second   =      0.773
{'train@nld.rst.nldt_loss': 3.424372673034668, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.18345771144278608, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01831749692910491, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.01313706193092158, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.033126255219068756, 'train@nld.rst.nldt_loss@nld.rst.nldt': 3.4243721961975098, 'train@nld.rst.nldt_runtime': 19.8792, 'train@nld.rst.nldt_samples_per_second': 80.889, 'train@nld.rst.nldt_steps_per_second': 2.565, 'epoch': 1.0}
{'loss': 3.7864, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.4265048503875732, 'eval_accuracy@nld.rst.nldt': 0.21450151057401812, 'eval_f1@nld.rst.nldt': 0.023061871807546548, 'eval_precision@nld.rst.nldt': 0.017257820738603914, 'eval_recall@nld.rst.nldt': 0.06692546583850932, 'eval_loss@nld.rst.nldt': 3.426504373550415, 'eval_runtime': 4.4277, 'eval_samples_per_second': 74.757, 'eval_steps_per_second': 2.484, 'epoch': 1.0}
{'train@nld.rst.nldt_loss': 3.0445027351379395, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.263681592039801, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.013060620995564317, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.008260598503740649, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.031176470588235295, 'train@nld.rst.nldt_loss@nld.rst.nldt': 3.0445027351379395, 'train@nld.rst.nldt_runtime': 19.9093, 'train@nld.rst.nldt_samples_per_second': 80.766, 'train@nld.rst.nldt_steps_per_second': 2.562, 'epoch': 2.0}
{'loss': 3.2454, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.0124332904815674, 'eval_accuracy@nld.rst.nldt': 0.27794561933534745, 'eval_f1@nld.rst.nldt': 0.016110673321075214, 'eval_precision@nld.rst.nldt': 0.010294282197605462, 'eval_recall@nld.rst.nldt': 0.037037037037037035, 'eval_loss@nld.rst.nldt': 3.0124337673187256, 'eval_runtime': 4.4337, 'eval_samples_per_second': 74.655, 'eval_steps_per_second': 2.481, 'epoch': 2.0}
{'train@nld.rst.nldt_loss': 2.8784804344177246, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2667910447761194, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01708000356838439, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.017058982804244564, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03340569561157796, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.8784801959991455, 'train@nld.rst.nldt_runtime': 19.8924, 'train@nld.rst.nldt_samples_per_second': 80.835, 'train@nld.rst.nldt_steps_per_second': 2.564, 'epoch': 3.0}
{'loss': 2.9836, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.825256824493408, 'eval_accuracy@nld.rst.nldt': 0.28700906344410876, 'eval_f1@nld.rst.nldt': 0.025046763935652822, 'eval_precision@nld.rst.nldt': 0.0365079365079365, 'eval_recall@nld.rst.nldt': 0.042210464432686653, 'eval_loss@nld.rst.nldt': 2.825256109237671, 'eval_runtime': 4.4578, 'eval_samples_per_second': 74.252, 'eval_steps_per_second': 2.468, 'epoch': 3.0}
{'train@nld.rst.nldt_loss': 2.7859766483306885, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2829601990049751, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.027837672567373944, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.023562849650786748, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.04329014939309057, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.7859771251678467, 'train@nld.rst.nldt_runtime': 19.8947, 'train@nld.rst.nldt_samples_per_second': 80.825, 'train@nld.rst.nldt_steps_per_second': 2.563, 'epoch': 4.0}
{'loss': 2.8388, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.7276415824890137, 'eval_accuracy@nld.rst.nldt': 0.3081570996978852, 'eval_f1@nld.rst.nldt': 0.036446591519055285, 'eval_precision@nld.rst.nldt': 0.03824599462183355, 'eval_recall@nld.rst.nldt': 0.05427114485085499, 'eval_loss@nld.rst.nldt': 2.7276413440704346, 'eval_runtime': 4.4564, 'eval_samples_per_second': 74.275, 'eval_steps_per_second': 2.468, 'epoch': 4.0}
{'train@nld.rst.nldt_loss': 2.7337381839752197, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2916666666666667, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03151813543276823, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.02525376447186617, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.04862628384687208, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.7337381839752197, 'train@nld.rst.nldt_runtime': 19.883, 'train@nld.rst.nldt_samples_per_second': 80.873, 'train@nld.rst.nldt_steps_per_second': 2.565, 'epoch': 5.0}
{'loss': 2.7914, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.6773383617401123, 'eval_accuracy@nld.rst.nldt': 0.32326283987915405, 'eval_f1@nld.rst.nldt': 0.041262296164256945, 'eval_precision@nld.rst.nldt': 0.037063901425603556, 'eval_recall@nld.rst.nldt': 0.06355979858395318, 'eval_loss@nld.rst.nldt': 2.6773383617401123, 'eval_runtime': 4.4336, 'eval_samples_per_second': 74.657, 'eval_steps_per_second': 2.481, 'epoch': 5.0}
{'train@nld.rst.nldt_loss': 2.6895995140075684, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2947761194029851, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.033089980134297745, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.025054097429421936, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.052543767507002806, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6895995140075684, 'train@nld.rst.nldt_runtime': 19.8858, 'train@nld.rst.nldt_samples_per_second': 80.862, 'train@nld.rst.nldt_steps_per_second': 2.565, 'epoch': 6.0}
{'loss': 2.7395, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.6410956382751465, 'eval_accuracy@nld.rst.nldt': 0.3323262839879154, 'eval_f1@nld.rst.nldt': 0.045684789067142004, 'eval_precision@nld.rst.nldt': 0.07230769230769231, 'eval_recall@nld.rst.nldt': 0.07174935459959614, 'eval_loss@nld.rst.nldt': 2.6410958766937256, 'eval_runtime': 4.4486, 'eval_samples_per_second': 74.406, 'eval_steps_per_second': 2.473, 'epoch': 6.0}
{'train@nld.rst.nldt_loss': 2.6580989360809326, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30223880597014924, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03523482524632831, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.025966069346134026, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05691701680672269, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6580986976623535, 'train@nld.rst.nldt_runtime': 19.8961, 'train@nld.rst.nldt_samples_per_second': 80.82, 'train@nld.rst.nldt_steps_per_second': 2.563, 'epoch': 7.0}
{'loss': 2.6989, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.6150271892547607, 'eval_accuracy@nld.rst.nldt': 0.3323262839879154, 'eval_f1@nld.rst.nldt': 0.047977460943810134, 'eval_precision@nld.rst.nldt': 0.07040016737038306, 'eval_recall@nld.rst.nldt': 0.07311044654039824, 'eval_loss@nld.rst.nldt': 2.6150271892547607, 'eval_runtime': 4.4304, 'eval_samples_per_second': 74.711, 'eval_steps_per_second': 2.483, 'epoch': 7.0}
{'train@nld.rst.nldt_loss': 2.6333088874816895, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30597014925373134, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.036166145101205166, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.026201579858394615, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.0596860410830999, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6333088874816895, 'train@nld.rst.nldt_runtime': 19.8992, 'train@nld.rst.nldt_samples_per_second': 80.807, 'train@nld.rst.nldt_steps_per_second': 2.563, 'epoch': 8.0}
{'loss': 2.6838, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.597236394882202, 'eval_accuracy@nld.rst.nldt': 0.3293051359516616, 'eval_f1@nld.rst.nldt': 0.049653740889573826, 'eval_precision@nld.rst.nldt': 0.06987773847720324, 'eval_recall@nld.rst.nldt': 0.0740689619916673, 'eval_loss@nld.rst.nldt': 2.5972366333007812, 'eval_runtime': 4.4302, 'eval_samples_per_second': 74.714, 'eval_steps_per_second': 2.483, 'epoch': 8.0}
{'train@nld.rst.nldt_loss': 2.617778778076172, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31032338308457713, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.037335381561894025, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.0270415481351806, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.061422152194211016, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.617778778076172, 'train@nld.rst.nldt_runtime': 19.863, 'train@nld.rst.nldt_samples_per_second': 80.954, 'train@nld.rst.nldt_steps_per_second': 2.568, 'epoch': 9.0}
{'loss': 2.6591, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.5841901302337646, 'eval_accuracy@nld.rst.nldt': 0.33534743202416917, 'eval_f1@nld.rst.nldt': 0.051074881448109635, 'eval_precision@nld.rst.nldt': 0.06986364700908512, 'eval_recall@nld.rst.nldt': 0.07552973953940138, 'eval_loss@nld.rst.nldt': 2.5841901302337646, 'eval_runtime': 4.4514, 'eval_samples_per_second': 74.359, 'eval_steps_per_second': 2.471, 'epoch': 9.0}
{'train@nld.rst.nldt_loss': 2.6080737113952637, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30907960199004975, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03708004789854638, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.026981053881945, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.060693277310924366, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6080737113952637, 'train@nld.rst.nldt_runtime': 19.8994, 'train@nld.rst.nldt_samples_per_second': 80.806, 'train@nld.rst.nldt_steps_per_second': 2.563, 'epoch': 10.0}
{'loss': 2.6457, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.574944496154785, 'eval_accuracy@nld.rst.nldt': 0.3323262839879154, 'eval_f1@nld.rst.nldt': 0.04748094243237261, 'eval_precision@nld.rst.nldt': 0.06963075196408529, 'eval_recall@nld.rst.nldt': 0.07311044654039824, 'eval_loss@nld.rst.nldt': 2.5749449729919434, 'eval_runtime': 4.462, 'eval_samples_per_second': 74.183, 'eval_steps_per_second': 2.465, 'epoch': 10.0}
{'train@nld.rst.nldt_loss': 2.600637912750244, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31156716417910446, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03761895721017698, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.027308142502108015, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.0617436974789916, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.600637674331665, 'train@nld.rst.nldt_runtime': 19.9046, 'train@nld.rst.nldt_samples_per_second': 80.785, 'train@nld.rst.nldt_steps_per_second': 2.562, 'epoch': 11.0}
{'loss': 2.6321, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.5694403648376465, 'eval_accuracy@nld.rst.nldt': 0.33534743202416917, 'eval_f1@nld.rst.nldt': 0.051158976085229776, 'eval_precision@nld.rst.nldt': 0.07053220658483816, 'eval_recall@nld.rst.nldt': 0.07552973953940138, 'eval_loss@nld.rst.nldt': 2.5694401264190674, 'eval_runtime': 4.4848, 'eval_samples_per_second': 73.804, 'eval_steps_per_second': 2.453, 'epoch': 11.0}
{'train@nld.rst.nldt_loss': 2.598149061203003, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31218905472636815, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03777137627061504, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.027414884686239305, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.0620500700280112, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.598148822784424, 'train@nld.rst.nldt_runtime': 19.8979, 'train@nld.rst.nldt_samples_per_second': 80.812, 'train@nld.rst.nldt_steps_per_second': 2.563, 'epoch': 12.0}
{'loss': 2.6394, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.568129777908325, 'eval_accuracy@nld.rst.nldt': 0.33534743202416917, 'eval_f1@nld.rst.nldt': 0.051158976085229776, 'eval_precision@nld.rst.nldt': 0.07053220658483816, 'eval_recall@nld.rst.nldt': 0.07552973953940138, 'eval_loss@nld.rst.nldt': 2.568129539489746, 'eval_runtime': 4.4528, 'eval_samples_per_second': 74.335, 'eval_steps_per_second': 2.47, 'epoch': 12.0}
{'train_runtime': 784.9872, 'train_samples_per_second': 24.581, 'train_steps_per_second': 0.78, 'train_loss': 2.8620089362649357, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.3282
  train_runtime            = 0:17:51.37
  train_samples_per_second =     24.473
  train_steps_per_second   =      0.773
-------------------------------------------------------------------
Lang1:  por.rst.cstn    Lang2:  nld.rst.nldt
Saving run to:  runs/full_shot/FullShot=v4_por.rst.cstn_nld.rst.nldt_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 4148 examples
read 573 examples
read 272 examples
read 1608 examples
read 331 examples
read 326 examples
Total prediction labels:  42
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (pt): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (pt): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=42, bias=True)
    )
  )
)
{'train@por.rst.cstn_loss': 2.5172691345214844, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.2774831243972999, 'train@por.rst.cstn_f1@por.rst.cstn': 0.013575674655595393, 'train@por.rst.cstn_precision@por.rst.cstn': 0.008671347637415621, 'train@por.rst.cstn_recall@por.rst.cstn': 0.03125, 'train@por.rst.cstn_loss@por.rst.cstn': 2.5172691345214844, 'train@por.rst.cstn_runtime': 50.3699, 'train@por.rst.cstn_samples_per_second': 82.351, 'train@por.rst.cstn_steps_per_second': 2.581, 'epoch': 1.0}
{'loss': 3.0145, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.590374231338501, 'eval_accuracy@por.rst.cstn': 0.28097731239092494, 'eval_f1@por.rst.cstn': 0.019940549913301957, 'eval_precision@por.rst.cstn': 0.012771696017769315, 'eval_recall@por.rst.cstn': 0.045454545454545456, 'eval_loss@por.rst.cstn': 2.590374231338501, 'eval_runtime': 7.3038, 'eval_samples_per_second': 78.453, 'eval_steps_per_second': 2.464, 'epoch': 1.0}
{'train@por.rst.cstn_loss': 2.2498016357421875, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.3633076181292189, 'train@por.rst.cstn_f1@por.rst.cstn': 0.052210734625220995, 'train@por.rst.cstn_precision@por.rst.cstn': 0.08135803281576423, 'train@por.rst.cstn_recall@por.rst.cstn': 0.05870533734765186, 'train@por.rst.cstn_loss@por.rst.cstn': 2.2498016357421875, 'train@por.rst.cstn_runtime': 50.464, 'train@por.rst.cstn_samples_per_second': 82.197, 'train@por.rst.cstn_steps_per_second': 2.576, 'epoch': 2.0}
{'loss': 2.4017, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.356395959854126, 'eval_accuracy@por.rst.cstn': 0.3193717277486911, 'eval_f1@por.rst.cstn': 0.06510211781366984, 'eval_precision@por.rst.cstn': 0.11004344919786097, 'eval_recall@por.rst.cstn': 0.07608846496406305, 'eval_loss@por.rst.cstn': 2.356395721435547, 'eval_runtime': 7.3042, 'eval_samples_per_second': 78.448, 'eval_steps_per_second': 2.464, 'epoch': 2.0}
{'train@por.rst.cstn_loss': 2.016462802886963, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.4679363548698168, 'train@por.rst.cstn_f1@por.rst.cstn': 0.07706750363320078, 'train@por.rst.cstn_precision@por.rst.cstn': 0.07506500062962601, 'train@por.rst.cstn_recall@por.rst.cstn': 0.08881444919440223, 'train@por.rst.cstn_loss@por.rst.cstn': 2.016462802886963, 'train@por.rst.cstn_runtime': 50.5414, 'train@por.rst.cstn_samples_per_second': 82.071, 'train@por.rst.cstn_steps_per_second': 2.572, 'epoch': 3.0}
{'loss': 2.1864, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.133362054824829, 'eval_accuracy@por.rst.cstn': 0.3787085514834206, 'eval_f1@por.rst.cstn': 0.09929682195304115, 'eval_precision@por.rst.cstn': 0.0963460798816726, 'eval_recall@por.rst.cstn': 0.12069488309346849, 'eval_loss@por.rst.cstn': 2.133362054824829, 'eval_runtime': 7.3331, 'eval_samples_per_second': 78.139, 'eval_steps_per_second': 2.455, 'epoch': 3.0}
{'train@por.rst.cstn_loss': 1.8433893918991089, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5214561234329798, 'train@por.rst.cstn_f1@por.rst.cstn': 0.09134651543108432, 'train@por.rst.cstn_precision@por.rst.cstn': 0.11443671766457364, 'train@por.rst.cstn_recall@por.rst.cstn': 0.10523560844167514, 'train@por.rst.cstn_loss@por.rst.cstn': 1.8433893918991089, 'train@por.rst.cstn_runtime': 50.6067, 'train@por.rst.cstn_samples_per_second': 81.965, 'train@por.rst.cstn_steps_per_second': 2.569, 'epoch': 4.0}
{'loss': 1.9907, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.974450707435608, 'eval_accuracy@por.rst.cstn': 0.4397905759162304, 'eval_f1@por.rst.cstn': 0.1190795993319941, 'eval_precision@por.rst.cstn': 0.1629283781751858, 'eval_recall@por.rst.cstn': 0.14857333626316363, 'eval_loss@por.rst.cstn': 1.974450707435608, 'eval_runtime': 7.3239, 'eval_samples_per_second': 78.237, 'eval_steps_per_second': 2.458, 'epoch': 4.0}
{'train@por.rst.cstn_loss': 1.7257537841796875, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5532786885245902, 'train@por.rst.cstn_f1@por.rst.cstn': 0.11877867238502839, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13585893032296092, 'train@por.rst.cstn_recall@por.rst.cstn': 0.12913631518937524, 'train@por.rst.cstn_loss@por.rst.cstn': 1.7257537841796875, 'train@por.rst.cstn_runtime': 50.6448, 'train@por.rst.cstn_samples_per_second': 81.904, 'train@por.rst.cstn_steps_per_second': 2.567, 'epoch': 5.0}
{'loss': 1.8432, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.865031361579895, 'eval_accuracy@por.rst.cstn': 0.4642233856893543, 'eval_f1@por.rst.cstn': 0.15310865791079264, 'eval_precision@por.rst.cstn': 0.18077080472284887, 'eval_recall@por.rst.cstn': 0.17710943651499092, 'eval_loss@por.rst.cstn': 1.865031361579895, 'eval_runtime': 7.3158, 'eval_samples_per_second': 78.323, 'eval_steps_per_second': 2.46, 'epoch': 5.0}
{'train@por.rst.cstn_loss': 1.6576817035675049, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5658148505303761, 'train@por.rst.cstn_f1@por.rst.cstn': 0.12555802085618195, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13543535672072232, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1371061198661656, 'train@por.rst.cstn_loss@por.rst.cstn': 1.6576818227767944, 'train@por.rst.cstn_runtime': 50.5818, 'train@por.rst.cstn_samples_per_second': 82.006, 'train@por.rst.cstn_steps_per_second': 2.57, 'epoch': 6.0}
{'loss': 1.7549, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.7944947481155396, 'eval_accuracy@por.rst.cstn': 0.4694589877835951, 'eval_f1@por.rst.cstn': 0.16307527876729708, 'eval_precision@por.rst.cstn': 0.17165916934429934, 'eval_recall@por.rst.cstn': 0.18133122886867425, 'eval_loss@por.rst.cstn': 1.7944945096969604, 'eval_runtime': 7.3306, 'eval_samples_per_second': 78.166, 'eval_steps_per_second': 2.455, 'epoch': 6.0}
{'train@por.rst.cstn_loss': 1.6086002588272095, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5737704918032787, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1311916404516871, 'train@por.rst.cstn_precision@por.rst.cstn': 0.1316556199453542, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1441030844553518, 'train@por.rst.cstn_loss@por.rst.cstn': 1.6086002588272095, 'train@por.rst.cstn_runtime': 50.5661, 'train@por.rst.cstn_samples_per_second': 82.031, 'train@por.rst.cstn_steps_per_second': 2.571, 'epoch': 7.0}
{'loss': 1.6881, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.7500157356262207, 'eval_accuracy@por.rst.cstn': 0.5008726003490401, 'eval_f1@por.rst.cstn': 0.18334673559954065, 'eval_precision@por.rst.cstn': 0.18401029974797203, 'eval_recall@por.rst.cstn': 0.20036818750673546, 'eval_loss@por.rst.cstn': 1.7500158548355103, 'eval_runtime': 7.3149, 'eval_samples_per_second': 78.334, 'eval_steps_per_second': 2.461, 'epoch': 7.0}
{'train@por.rst.cstn_loss': 1.578768253326416, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5800385728061717, 'train@por.rst.cstn_f1@por.rst.cstn': 0.13463946230858487, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13460211059436455, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14643290695356131, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5787683725357056, 'train@por.rst.cstn_runtime': 50.5155, 'train@por.rst.cstn_samples_per_second': 82.113, 'train@por.rst.cstn_steps_per_second': 2.573, 'epoch': 8.0}
{'loss': 1.6565, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.7258297204971313, 'eval_accuracy@por.rst.cstn': 0.5043630017452007, 'eval_f1@por.rst.cstn': 0.18678204165803972, 'eval_precision@por.rst.cstn': 0.18925589854654773, 'eval_recall@por.rst.cstn': 0.20156184943734348, 'eval_loss@por.rst.cstn': 1.725830078125, 'eval_runtime': 7.3337, 'eval_samples_per_second': 78.133, 'eval_steps_per_second': 2.454, 'epoch': 8.0}
{'train@por.rst.cstn_loss': 1.559973120689392, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5853423336547734, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1377790848606966, 'train@por.rst.cstn_precision@por.rst.cstn': 0.1365688763295571, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14903326500188757, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5599730014801025, 'train@por.rst.cstn_runtime': 50.5153, 'train@por.rst.cstn_samples_per_second': 82.114, 'train@por.rst.cstn_steps_per_second': 2.573, 'epoch': 9.0}
{'loss': 1.6267, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.7083011865615845, 'eval_accuracy@por.rst.cstn': 0.506108202443281, 'eval_f1@por.rst.cstn': 0.18924869257671045, 'eval_precision@por.rst.cstn': 0.190527749471715, 'eval_recall@por.rst.cstn': 0.20321676076044703, 'eval_loss@por.rst.cstn': 1.708301305770874, 'eval_runtime': 7.3205, 'eval_samples_per_second': 78.273, 'eval_steps_per_second': 2.459, 'epoch': 9.0}
{'train@por.rst.cstn_loss': 1.5432376861572266, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5855834136933462, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1383904336387896, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13349228113493805, 'train@por.rst.cstn_recall@por.rst.cstn': 0.15121057337501065, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5432378053665161, 'train@por.rst.cstn_runtime': 50.575, 'train@por.rst.cstn_samples_per_second': 82.017, 'train@por.rst.cstn_steps_per_second': 2.57, 'epoch': 10.0}
{'loss': 1.5996, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.6936053037643433, 'eval_accuracy@por.rst.cstn': 0.5078534031413613, 'eval_f1@por.rst.cstn': 0.18974470570102825, 'eval_precision@por.rst.cstn': 0.18620908974241973, 'eval_recall@por.rst.cstn': 0.20714972393254066, 'eval_loss@por.rst.cstn': 1.6936051845550537, 'eval_runtime': 7.3229, 'eval_samples_per_second': 78.248, 'eval_steps_per_second': 2.458, 'epoch': 10.0}
{'train@por.rst.cstn_loss': 1.5335564613342285, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5899228543876567, 'train@por.rst.cstn_f1@por.rst.cstn': 0.13965373571483736, 'train@por.rst.cstn_precision@por.rst.cstn': 0.16482132067128763, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1521953398712821, 'train@por.rst.cstn_loss@por.rst.cstn': 1.533556580543518, 'train@por.rst.cstn_runtime': 50.5427, 'train@por.rst.cstn_samples_per_second': 82.069, 'train@por.rst.cstn_steps_per_second': 2.572, 'epoch': 11.0}
{'loss': 1.5958, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.687781810760498, 'eval_accuracy@por.rst.cstn': 0.5130890052356021, 'eval_f1@por.rst.cstn': 0.19785774815050114, 'eval_precision@por.rst.cstn': 0.2099877933579764, 'eval_recall@por.rst.cstn': 0.21263539958412023, 'eval_loss@por.rst.cstn': 1.6877819299697876, 'eval_runtime': 7.3104, 'eval_samples_per_second': 78.382, 'eval_steps_per_second': 2.462, 'epoch': 11.0}
{'train@por.rst.cstn_loss': 1.5307843685150146, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5913693346190936, 'train@por.rst.cstn_f1@por.rst.cstn': 0.141036604631203, 'train@por.rst.cstn_precision@por.rst.cstn': 0.16565015273484612, 'train@por.rst.cstn_recall@por.rst.cstn': 0.15339499512661575, 'train@por.rst.cstn_loss@por.rst.cstn': 1.530784249305725, 'train@por.rst.cstn_runtime': 50.5704, 'train@por.rst.cstn_samples_per_second': 82.024, 'train@por.rst.cstn_steps_per_second': 2.571, 'epoch': 12.0}
{'loss': 1.5897, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.6844831705093384, 'eval_accuracy@por.rst.cstn': 0.5165794066317626, 'eval_f1@por.rst.cstn': 0.19857930668431142, 'eval_precision@por.rst.cstn': 0.2105637375020293, 'eval_recall@por.rst.cstn': 0.213200052322686, 'eval_loss@por.rst.cstn': 1.684483289718628, 'eval_runtime': 7.3216, 'eval_samples_per_second': 78.261, 'eval_steps_per_second': 2.458, 'epoch': 12.0}
{'train_runtime': 1967.5331, 'train_samples_per_second': 25.299, 'train_steps_per_second': 0.793, 'train_loss': 1.912315427339994, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.9123
  train_runtime            = 0:32:47.53
  train_samples_per_second =     25.299
  train_steps_per_second   =      0.793
{'train@nld.rst.nldt_loss': 3.057539939880371, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26243781094527363, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.013018262586377095, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.008237039350405996, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.031029411764705882, 'train@nld.rst.nldt_loss@nld.rst.nldt': 3.057539701461792, 'train@nld.rst.nldt_runtime': 19.8561, 'train@nld.rst.nldt_samples_per_second': 80.983, 'train@nld.rst.nldt_steps_per_second': 2.568, 'epoch': 1.0}
{'loss': 3.5258, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.0492172241210938, 'eval_accuracy@nld.rst.nldt': 0.27492447129909364, 'eval_f1@nld.rst.nldt': 0.01543942992874109, 'eval_precision@nld.rst.nldt': 0.009878419452887538, 'eval_recall@nld.rst.nldt': 0.035326086956521736, 'eval_loss@nld.rst.nldt': 3.0492169857025146, 'eval_runtime': 4.3861, 'eval_samples_per_second': 75.465, 'eval_steps_per_second': 2.508, 'epoch': 1.0}
{'train@nld.rst.nldt_loss': 2.8311827182769775, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26865671641791045, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01733708563556357, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.029804045512010113, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.033509160420925124, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.8311824798583984, 'train@nld.rst.nldt_runtime': 19.8472, 'train@nld.rst.nldt_samples_per_second': 81.019, 'train@nld.rst.nldt_steps_per_second': 2.57, 'epoch': 2.0}
{'loss': 2.9716, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.7996861934661865, 'eval_accuracy@nld.rst.nldt': 0.283987915407855, 'eval_f1@nld.rst.nldt': 0.02012438503666574, 'eval_precision@nld.rst.nldt': 0.03507979524239687, 'eval_recall@nld.rst.nldt': 0.039153439153439155, 'eval_loss@nld.rst.nldt': 2.7996861934661865, 'eval_runtime': 4.3989, 'eval_samples_per_second': 75.246, 'eval_steps_per_second': 2.501, 'epoch': 2.0}
{'train@nld.rst.nldt_loss': 2.732722282409668, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.29539800995024873, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.02934734078142683, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.05823237611960067, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.045627681252681254, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.732722282409668, 'train@nld.rst.nldt_runtime': 19.8421, 'train@nld.rst.nldt_samples_per_second': 81.04, 'train@nld.rst.nldt_steps_per_second': 2.57, 'epoch': 3.0}
{'loss': 2.827, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.705599784851074, 'eval_accuracy@nld.rst.nldt': 0.30513595166163143, 'eval_f1@nld.rst.nldt': 0.03156038243757542, 'eval_precision@nld.rst.nldt': 0.06224543140938364, 'eval_recall@nld.rst.nldt': 0.04756025867136978, 'eval_loss@nld.rst.nldt': 2.7056000232696533, 'eval_runtime': 4.3733, 'eval_samples_per_second': 75.686, 'eval_steps_per_second': 2.515, 'epoch': 3.0}
{'train@nld.rst.nldt_loss': 2.6741580963134766, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.29850746268656714, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.030348580463158182, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.04283690484231224, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.046578343048931284, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6741585731506348, 'train@nld.rst.nldt_runtime': 19.8591, 'train@nld.rst.nldt_samples_per_second': 80.971, 'train@nld.rst.nldt_steps_per_second': 2.568, 'epoch': 4.0}
{'loss': 2.7364, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.6514017581939697, 'eval_accuracy@nld.rst.nldt': 0.31419939577039274, 'eval_f1@nld.rst.nldt': 0.036328963258355704, 'eval_precision@nld.rst.nldt': 0.039806763285024145, 'eval_recall@nld.rst.nldt': 0.05173427395649618, 'eval_loss@nld.rst.nldt': 2.651402235031128, 'eval_runtime': 4.3959, 'eval_samples_per_second': 75.298, 'eval_steps_per_second': 2.502, 'epoch': 4.0}
{'train@nld.rst.nldt_loss': 2.6263163089752197, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31902985074626866, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03910161891802229, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.035147460087492075, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.056976919146036795, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6263163089752197, 'train@nld.rst.nldt_runtime': 19.8475, 'train@nld.rst.nldt_samples_per_second': 81.018, 'train@nld.rst.nldt_steps_per_second': 2.57, 'epoch': 5.0}
{'loss': 2.6938, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.6081318855285645, 'eval_accuracy@nld.rst.nldt': 0.33534743202416917, 'eval_f1@nld.rst.nldt': 0.04729897009308774, 'eval_precision@nld.rst.nldt': 0.043720505783357344, 'eval_recall@nld.rst.nldt': 0.06379495437466452, 'eval_loss@nld.rst.nldt': 2.6081323623657227, 'eval_runtime': 4.4144, 'eval_samples_per_second': 74.983, 'eval_steps_per_second': 2.492, 'epoch': 5.0}
{'train@nld.rst.nldt_loss': 2.5875425338745117, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3252487562189055, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.04075435790643739, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.03368164592283493, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.060622460695990116, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.5875425338745117, 'train@nld.rst.nldt_runtime': 19.8113, 'train@nld.rst.nldt_samples_per_second': 81.166, 'train@nld.rst.nldt_steps_per_second': 2.574, 'epoch': 6.0}
{'loss': 2.6515, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.575986623764038, 'eval_accuracy@nld.rst.nldt': 0.35347432024169184, 'eval_f1@nld.rst.nldt': 0.055852902373717005, 'eval_precision@nld.rst.nldt': 0.05274624254384684, 'eval_recall@nld.rst.nldt': 0.07417120363980266, 'eval_loss@nld.rst.nldt': 2.575986623764038, 'eval_runtime': 4.3807, 'eval_samples_per_second': 75.559, 'eval_steps_per_second': 2.511, 'epoch': 6.0}
{'train@nld.rst.nldt_loss': 2.555009365081787, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3327114427860697, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.04341094583803442, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.03438134235635725, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.06627701756378228, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.555009126663208, 'train@nld.rst.nldt_runtime': 19.8636, 'train@nld.rst.nldt_samples_per_second': 80.952, 'train@nld.rst.nldt_steps_per_second': 2.568, 'epoch': 7.0}
{'loss': 2.61, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.549802780151367, 'eval_accuracy@nld.rst.nldt': 0.35347432024169184, 'eval_f1@nld.rst.nldt': 0.05572340920366673, 'eval_precision@nld.rst.nldt': 0.048850691631742924, 'eval_recall@nld.rst.nldt': 0.07748127699818522, 'eval_loss@nld.rst.nldt': 2.5498030185699463, 'eval_runtime': 4.389, 'eval_samples_per_second': 75.417, 'eval_steps_per_second': 2.506, 'epoch': 7.0}
{'train@nld.rst.nldt_loss': 2.5255932807922363, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3370646766169154, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.0436675647425901, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.06519121119325444, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.06894033957167832, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.5255932807922363, 'train@nld.rst.nldt_runtime': 19.8359, 'train@nld.rst.nldt_samples_per_second': 81.065, 'train@nld.rst.nldt_steps_per_second': 2.571, 'epoch': 8.0}
{'loss': 2.5941, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.5272278785705566, 'eval_accuracy@nld.rst.nldt': 0.36555891238670696, 'eval_f1@nld.rst.nldt': 0.06395686721830005, 'eval_precision@nld.rst.nldt': 0.07530540447207112, 'eval_recall@nld.rst.nldt': 0.0884339135546865, 'eval_loss@nld.rst.nldt': 2.5272278785705566, 'eval_runtime': 4.3609, 'eval_samples_per_second': 75.903, 'eval_steps_per_second': 2.522, 'epoch': 8.0}
{'train@nld.rst.nldt_loss': 2.507181167602539, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3383084577114428, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.044952097630167234, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.06815410437665048, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.06995942062752407, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.50718092918396, 'train@nld.rst.nldt_runtime': 19.8152, 'train@nld.rst.nldt_samples_per_second': 81.15, 'train@nld.rst.nldt_steps_per_second': 2.574, 'epoch': 9.0}
{'loss': 2.5681, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.5121800899505615, 'eval_accuracy@nld.rst.nldt': 0.3716012084592145, 'eval_f1@nld.rst.nldt': 0.06674269462378254, 'eval_precision@nld.rst.nldt': 0.0768084130829229, 'eval_recall@nld.rst.nldt': 0.0906001584745546, 'eval_loss@nld.rst.nldt': 2.5121803283691406, 'eval_runtime': 4.3734, 'eval_samples_per_second': 75.684, 'eval_steps_per_second': 2.515, 'epoch': 9.0}
{'train@nld.rst.nldt_loss': 2.49308443069458, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.33955223880597013, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.04595802949305104, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.07600212972690476, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.07037262395233226, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.493084192276001, 'train@nld.rst.nldt_runtime': 20.0143, 'train@nld.rst.nldt_samples_per_second': 80.343, 'train@nld.rst.nldt_steps_per_second': 2.548, 'epoch': 10.0}
{'loss': 2.5532, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.49977445602417, 'eval_accuracy@nld.rst.nldt': 0.3776435045317221, 'eval_f1@nld.rst.nldt': 0.06970739384418953, 'eval_precision@nld.rst.nldt': 0.07924073403949565, 'eval_recall@nld.rst.nldt': 0.09276640339442271, 'eval_loss@nld.rst.nldt': 2.49977445602417, 'eval_runtime': 4.3632, 'eval_samples_per_second': 75.863, 'eval_steps_per_second': 2.521, 'epoch': 10.0}
{'train@nld.rst.nldt_loss': 2.4849021434783936, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3407960199004975, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.04608695406362762, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.07229970829488906, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.07098536905037148, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.4849023818969727, 'train@nld.rst.nldt_runtime': 19.812, 'train@nld.rst.nldt_samples_per_second': 81.163, 'train@nld.rst.nldt_steps_per_second': 2.574, 'epoch': 11.0}
{'loss': 2.5318, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.4936697483062744, 'eval_accuracy@nld.rst.nldt': 0.37462235649546827, 'eval_f1@nld.rst.nldt': 0.06930985690725688, 'eval_precision@nld.rst.nldt': 0.07871804707247747, 'eval_recall@nld.rst.nldt': 0.0923638269048897, 'eval_loss@nld.rst.nldt': 2.4936697483062744, 'eval_runtime': 4.3758, 'eval_samples_per_second': 75.643, 'eval_steps_per_second': 2.514, 'epoch': 11.0}
{'train@nld.rst.nldt_loss': 2.48211932182312, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3407960199004975, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.04598722684316472, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.07218097132091271, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.07098536905037148, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.48211932182312, 'train@nld.rst.nldt_runtime': 19.7926, 'train@nld.rst.nldt_samples_per_second': 81.243, 'train@nld.rst.nldt_steps_per_second': 2.577, 'epoch': 12.0}
{'loss': 2.5343, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.4917490482330322, 'eval_accuracy@nld.rst.nldt': 0.37462235649546827, 'eval_f1@nld.rst.nldt': 0.06930985690725688, 'eval_precision@nld.rst.nldt': 0.07871804707247747, 'eval_recall@nld.rst.nldt': 0.0923638269048897, 'eval_loss@nld.rst.nldt': 2.4917492866516113, 'eval_runtime': 4.3913, 'eval_samples_per_second': 75.376, 'eval_steps_per_second': 2.505, 'epoch': 12.0}
{'train_runtime': 785.1958, 'train_samples_per_second': 24.575, 'train_steps_per_second': 0.779, 'train_loss': 2.733146193759893, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.9123
  train_runtime            = 0:32:47.53
  train_samples_per_second =     25.299
  train_steps_per_second   =      0.793
-------------------------------------------------------------------
Lang1:  rus.rst.rrt    Lang2:  nld.rst.nldt
Saving run to:  runs/full_shot/FullShot=v4_rus.rst.rrt_nld.rst.nldt_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 28822 examples
read 2855 examples
read 2843 examples
read 1608 examples
read 331 examples
read 326 examples
Total prediction labels:  40
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (ru): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=40, bias=True)
    )
  )
)
{'train@rus.rst.rrt_loss': 1.7235018014907837, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.49372007494275205, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.1784558970340505, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.22940929687907652, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.19444938921972563, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.7235018014907837, 'train@rus.rst.rrt_runtime': 350.3165, 'train@rus.rst.rrt_samples_per_second': 82.274, 'train@rus.rst.rrt_steps_per_second': 2.572, 'epoch': 1.0}
{'loss': 2.1771, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.7637933492660522, 'eval_accuracy@rus.rst.rrt': 0.4756567425569177, 'eval_f1@rus.rst.rrt': 0.19870868717963977, 'eval_precision@rus.rst.rrt': 0.20838819676806933, 'eval_recall@rus.rst.rrt': 0.2161992398222251, 'eval_loss@rus.rst.rrt': 1.7637934684753418, 'eval_runtime': 35.0816, 'eval_samples_per_second': 81.382, 'eval_steps_per_second': 2.565, 'epoch': 1.0}
{'train@rus.rst.rrt_loss': 1.5030230283737183, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.542849212407189, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.2257761808695884, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.3178232078516119, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.23192707999897105, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.5030231475830078, 'train@rus.rst.rrt_runtime': 349.4222, 'train@rus.rst.rrt_samples_per_second': 82.485, 'train@rus.rst.rrt_steps_per_second': 2.579, 'epoch': 2.0}
{'loss': 1.6436, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.5672831535339355, 'eval_accuracy@rus.rst.rrt': 0.5187390542907181, 'eval_f1@rus.rst.rrt': 0.2509475179576926, 'eval_precision@rus.rst.rrt': 0.36105136944983457, 'eval_recall@rus.rst.rrt': 0.256310481780114, 'eval_loss@rus.rst.rrt': 1.5672831535339355, 'eval_runtime': 34.9784, 'eval_samples_per_second': 81.622, 'eval_steps_per_second': 2.573, 'epoch': 2.0}
{'train@rus.rst.rrt_loss': 1.422722578048706, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5667198667684408, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.27908012459552833, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4230363847260304, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.27360952084682855, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.422722578048706, 'train@rus.rst.rrt_runtime': 349.1984, 'train@rus.rst.rrt_samples_per_second': 82.538, 'train@rus.rst.rrt_steps_per_second': 2.58, 'epoch': 3.0}
{'loss': 1.51, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.4928885698318481, 'eval_accuracy@rus.rst.rrt': 0.5418563922942207, 'eval_f1@rus.rst.rrt': 0.3071826454620438, 'eval_precision@rus.rst.rrt': 0.4946002195572959, 'eval_recall@rus.rst.rrt': 0.30247754050092923, 'eval_loss@rus.rst.rrt': 1.4928885698318481, 'eval_runtime': 35.0313, 'eval_samples_per_second': 81.499, 'eval_steps_per_second': 2.569, 'epoch': 3.0}
{'train@rus.rst.rrt_loss': 1.3702094554901123, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5818818957740615, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3134886266101605, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.462968827086655, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.2958985109564445, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3702094554901123, 'train@rus.rst.rrt_runtime': 350.1089, 'train@rus.rst.rrt_samples_per_second': 82.323, 'train@rus.rst.rrt_steps_per_second': 2.573, 'epoch': 4.0}
{'loss': 1.4471, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.4450303316116333, 'eval_accuracy@rus.rst.rrt': 0.5558669001751313, 'eval_f1@rus.rst.rrt': 0.3530177444769292, 'eval_precision@rus.rst.rrt': 0.5254519384658166, 'eval_recall@rus.rst.rrt': 0.33396187451962267, 'eval_loss@rus.rst.rrt': 1.4450299739837646, 'eval_runtime': 35.0536, 'eval_samples_per_second': 81.447, 'eval_steps_per_second': 2.567, 'epoch': 4.0}
{'train@rus.rst.rrt_loss': 1.3364958763122559, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.589584345291791, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.32937241437603776, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4457698708765896, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.310790337972336, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3364958763122559, 'train@rus.rst.rrt_runtime': 350.0025, 'train@rus.rst.rrt_samples_per_second': 82.348, 'train@rus.rst.rrt_steps_per_second': 2.574, 'epoch': 5.0}
{'loss': 1.4074, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.4189515113830566, 'eval_accuracy@rus.rst.rrt': 0.5632224168126094, 'eval_f1@rus.rst.rrt': 0.3793334671535205, 'eval_precision@rus.rst.rrt': 0.5164840824569558, 'eval_recall@rus.rst.rrt': 0.3581234609397649, 'eval_loss@rus.rst.rrt': 1.4189516305923462, 'eval_runtime': 35.0891, 'eval_samples_per_second': 81.364, 'eval_steps_per_second': 2.565, 'epoch': 5.0}
{'train@rus.rst.rrt_loss': 1.3134074211120605, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5955867046006523, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3419370338276493, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4524692908125727, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3226222357413224, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3134071826934814, 'train@rus.rst.rrt_runtime': 350.1448, 'train@rus.rst.rrt_samples_per_second': 82.315, 'train@rus.rst.rrt_steps_per_second': 2.573, 'epoch': 6.0}
{'loss': 1.3798, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.399664044380188, 'eval_accuracy@rus.rst.rrt': 0.5723292469352014, 'eval_f1@rus.rst.rrt': 0.38946061146891375, 'eval_precision@rus.rst.rrt': 0.4937862778270126, 'eval_recall@rus.rst.rrt': 0.3713174915620227, 'eval_loss@rus.rst.rrt': 1.3996641635894775, 'eval_runtime': 38.6231, 'eval_samples_per_second': 73.92, 'eval_steps_per_second': 2.33, 'epoch': 6.0}
{'train@rus.rst.rrt_loss': 1.298093557357788, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5998889737006453, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3526863770054146, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.43737339546052245, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3344518731464468, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2980936765670776, 'train@rus.rst.rrt_runtime': 349.8512, 'train@rus.rst.rrt_samples_per_second': 82.384, 'train@rus.rst.rrt_steps_per_second': 2.575, 'epoch': 7.0}
{'loss': 1.358, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.3907679319381714, 'eval_accuracy@rus.rst.rrt': 0.5737302977232924, 'eval_f1@rus.rst.rrt': 0.39931994577691665, 'eval_precision@rus.rst.rrt': 0.4846033417193634, 'eval_recall@rus.rst.rrt': 0.3851688645500277, 'eval_loss@rus.rst.rrt': 1.3907679319381714, 'eval_runtime': 35.0145, 'eval_samples_per_second': 81.538, 'eval_steps_per_second': 2.57, 'epoch': 7.0}
{'train@rus.rst.rrt_loss': 1.2849507331848145, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6015890639095136, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3583398768595889, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4270793170551915, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3420953685274335, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2849507331848145, 'train@rus.rst.rrt_runtime': 349.806, 'train@rus.rst.rrt_samples_per_second': 82.394, 'train@rus.rst.rrt_steps_per_second': 2.576, 'epoch': 8.0}
{'loss': 1.3414, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.377918004989624, 'eval_accuracy@rus.rst.rrt': 0.5810858143607706, 'eval_f1@rus.rst.rrt': 0.4124299387585397, 'eval_precision@rus.rst.rrt': 0.4760521260593326, 'eval_recall@rus.rst.rrt': 0.40149754533857673, 'eval_loss@rus.rst.rrt': 1.377918004989624, 'eval_runtime': 35.0487, 'eval_samples_per_second': 81.458, 'eval_steps_per_second': 2.568, 'epoch': 8.0}
{'train@rus.rst.rrt_loss': 1.2721866369247437, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6059607244466033, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3630474152979914, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4329644623543366, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3445300091606223, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2721868753433228, 'train@rus.rst.rrt_runtime': 349.4179, 'train@rus.rst.rrt_samples_per_second': 82.486, 'train@rus.rst.rrt_steps_per_second': 2.579, 'epoch': 9.0}
{'loss': 1.3319, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.3715389966964722, 'eval_accuracy@rus.rst.rrt': 0.5772329246935202, 'eval_f1@rus.rst.rrt': 0.41351292680872953, 'eval_precision@rus.rst.rrt': 0.4861254676842431, 'eval_recall@rus.rst.rrt': 0.398795252982064, 'eval_loss@rus.rst.rrt': 1.3715391159057617, 'eval_runtime': 34.957, 'eval_samples_per_second': 81.672, 'eval_steps_per_second': 2.575, 'epoch': 9.0}
{'train@rus.rst.rrt_loss': 1.266348958015442, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6084588161820831, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3657048746311648, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4640417534567606, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34475596533323283, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.266348958015442, 'train@rus.rst.rrt_runtime': 349.7439, 'train@rus.rst.rrt_samples_per_second': 82.409, 'train@rus.rst.rrt_steps_per_second': 2.576, 'epoch': 10.0}
{'loss': 1.3211, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.3695861101150513, 'eval_accuracy@rus.rst.rrt': 0.5782837127845885, 'eval_f1@rus.rst.rrt': 0.41258045106318003, 'eval_precision@rus.rst.rrt': 0.49104961896015836, 'eval_recall@rus.rst.rrt': 0.39676339311071435, 'eval_loss@rus.rst.rrt': 1.3695861101150513, 'eval_runtime': 34.9793, 'eval_samples_per_second': 81.62, 'eval_steps_per_second': 2.573, 'epoch': 10.0}
{'train@rus.rst.rrt_loss': 1.2619884014129639, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6096731663312748, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.368223613908715, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.45735276194878605, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34936984455304587, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2619882822036743, 'train@rus.rst.rrt_runtime': 349.6882, 'train@rus.rst.rrt_samples_per_second': 82.422, 'train@rus.rst.rrt_steps_per_second': 2.577, 'epoch': 11.0}
{'loss': 1.3162, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.3683772087097168, 'eval_accuracy@rus.rst.rrt': 0.5772329246935202, 'eval_f1@rus.rst.rrt': 0.41366266488432885, 'eval_precision@rus.rst.rrt': 0.4804257464920197, 'eval_recall@rus.rst.rrt': 0.4017764260820603, 'eval_loss@rus.rst.rrt': 1.3683772087097168, 'eval_runtime': 35.0934, 'eval_samples_per_second': 81.354, 'eval_steps_per_second': 2.565, 'epoch': 11.0}
{'train@rus.rst.rrt_loss': 1.2608259916305542, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.609187426271598, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3684724390573358, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4579755513180315, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3499443267482356, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2608258724212646, 'train@rus.rst.rrt_runtime': 349.0113, 'train@rus.rst.rrt_samples_per_second': 82.582, 'train@rus.rst.rrt_steps_per_second': 2.582, 'epoch': 12.0}
{'loss': 1.3117, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.36654531955719, 'eval_accuracy@rus.rst.rrt': 0.5782837127845885, 'eval_f1@rus.rst.rrt': 0.41443160216435804, 'eval_precision@rus.rst.rrt': 0.4832865670667611, 'eval_recall@rus.rst.rrt': 0.40227231500955984, 'eval_loss@rus.rst.rrt': 1.3665452003479004, 'eval_runtime': 34.9635, 'eval_samples_per_second': 81.657, 'eval_steps_per_second': 2.574, 'epoch': 12.0}
{'train_runtime': 13426.1395, 'train_samples_per_second': 25.76, 'train_steps_per_second': 0.805, 'train_loss': 1.462115243678176, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.4621
  train_runtime            = 3:43:46.13
  train_samples_per_second =      25.76
  train_steps_per_second   =      0.805
{'train@nld.rst.nldt_loss': 3.2152724266052246, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2568407960199005, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.04811498408264382, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.044540571789940496, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.070300121103521, 'train@nld.rst.nldt_loss@nld.rst.nldt': 3.2152721881866455, 'train@nld.rst.nldt_runtime': 19.8189, 'train@nld.rst.nldt_samples_per_second': 81.135, 'train@nld.rst.nldt_steps_per_second': 2.573, 'epoch': 1.0}
{'loss': 4.4656, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.1561837196350098, 'eval_accuracy@nld.rst.nldt': 0.2537764350453172, 'eval_f1@nld.rst.nldt': 0.048514356988316486, 'eval_precision@nld.rst.nldt': 0.04200548343263524, 'eval_recall@nld.rst.nldt': 0.08559406932478461, 'eval_loss@nld.rst.nldt': 3.156183958053589, 'eval_runtime': 4.3736, 'eval_samples_per_second': 75.681, 'eval_steps_per_second': 2.515, 'epoch': 1.0}
{'train@nld.rst.nldt_loss': 2.8012702465057373, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30659203980099503, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.058987280375394774, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.055725491968767804, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.07786687528215662, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.801269769668579, 'train@nld.rst.nldt_runtime': 19.8458, 'train@nld.rst.nldt_samples_per_second': 81.025, 'train@nld.rst.nldt_steps_per_second': 2.57, 'epoch': 2.0}
{'loss': 3.0184, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.7335920333862305, 'eval_accuracy@nld.rst.nldt': 0.311178247734139, 'eval_f1@nld.rst.nldt': 0.06447738947738947, 'eval_precision@nld.rst.nldt': 0.05228760943046657, 'eval_recall@nld.rst.nldt': 0.09854702750665482, 'eval_loss@nld.rst.nldt': 2.7335917949676514, 'eval_runtime': 4.3613, 'eval_samples_per_second': 75.895, 'eval_steps_per_second': 2.522, 'epoch': 2.0}
{'train@nld.rst.nldt_loss': 2.657782793045044, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3177860696517413, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.06139335098917872, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.061818366144915614, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.0795873370700951, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.657782793045044, 'train@nld.rst.nldt_runtime': 19.8118, 'train@nld.rst.nldt_samples_per_second': 81.164, 'train@nld.rst.nldt_steps_per_second': 2.574, 'epoch': 3.0}
{'loss': 2.7851, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.6094627380371094, 'eval_accuracy@nld.rst.nldt': 0.32628398791540786, 'eval_f1@nld.rst.nldt': 0.059409713252389564, 'eval_precision@nld.rst.nldt': 0.04675010067451928, 'eval_recall@nld.rst.nldt': 0.09209553386572021, 'eval_loss@nld.rst.nldt': 2.6094624996185303, 'eval_runtime': 4.381, 'eval_samples_per_second': 75.554, 'eval_steps_per_second': 2.511, 'epoch': 3.0}
{'train@nld.rst.nldt_loss': 2.5723071098327637, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.32276119402985076, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.06490510377854997, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.0628213854288073, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.08400413378696667, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.5723071098327637, 'train@nld.rst.nldt_runtime': 19.8024, 'train@nld.rst.nldt_samples_per_second': 81.202, 'train@nld.rst.nldt_steps_per_second': 2.575, 'epoch': 4.0}
{'loss': 2.6481, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.537020206451416, 'eval_accuracy@nld.rst.nldt': 0.3413897280966767, 'eval_f1@nld.rst.nldt': 0.06288085421896414, 'eval_precision@nld.rst.nldt': 0.049296802054154994, 'eval_recall@nld.rst.nldt': 0.0991533569949719, 'eval_loss@nld.rst.nldt': 2.537020206451416, 'eval_runtime': 4.3959, 'eval_samples_per_second': 75.297, 'eval_steps_per_second': 2.502, 'epoch': 4.0}
{'train@nld.rst.nldt_loss': 2.510174036026001, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3314676616915423, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.07007268366538395, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.07984427994620873, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.08918568083495641, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.51017427444458, 'train@nld.rst.nldt_runtime': 19.7972, 'train@nld.rst.nldt_samples_per_second': 81.224, 'train@nld.rst.nldt_steps_per_second': 2.576, 'epoch': 5.0}
{'loss': 2.5964, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.497147798538208, 'eval_accuracy@nld.rst.nldt': 0.3504531722054381, 'eval_f1@nld.rst.nldt': 0.07073406753035606, 'eval_precision@nld.rst.nldt': 0.07517349430614738, 'eval_recall@nld.rst.nldt': 0.10606945676821453, 'eval_loss@nld.rst.nldt': 2.49714732170105, 'eval_runtime': 4.3811, 'eval_samples_per_second': 75.552, 'eval_steps_per_second': 2.511, 'epoch': 5.0}
{'train@nld.rst.nldt_loss': 2.4612414836883545, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3383084577114428, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.07793860989039814, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.12099928088313175, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.09635907487173141, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.4612417221069336, 'train@nld.rst.nldt_runtime': 19.8258, 'train@nld.rst.nldt_samples_per_second': 81.107, 'train@nld.rst.nldt_steps_per_second': 2.572, 'epoch': 6.0}
{'loss': 2.5573, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.4619784355163574, 'eval_accuracy@nld.rst.nldt': 0.3564954682779456, 'eval_f1@nld.rst.nldt': 0.071148747134048, 'eval_precision@nld.rst.nldt': 0.06638033556540686, 'eval_recall@nld.rst.nldt': 0.10811027309474515, 'eval_loss@nld.rst.nldt': 2.4619786739349365, 'eval_runtime': 4.3721, 'eval_samples_per_second': 75.708, 'eval_steps_per_second': 2.516, 'epoch': 6.0}
{'train@nld.rst.nldt_loss': 2.4247732162475586, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.34701492537313433, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.08433963668892473, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.12065557659783328, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.10218780200237851, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.4247732162475586, 'train@nld.rst.nldt_runtime': 19.808, 'train@nld.rst.nldt_samples_per_second': 81.179, 'train@nld.rst.nldt_steps_per_second': 2.575, 'epoch': 7.0}
{'loss': 2.4953, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.439567804336548, 'eval_accuracy@nld.rst.nldt': 0.36555891238670696, 'eval_f1@nld.rst.nldt': 0.07594688844688845, 'eval_precision@nld.rst.nldt': 0.07029073534175576, 'eval_recall@nld.rst.nldt': 0.11477127082717144, 'eval_loss@nld.rst.nldt': 2.439567804336548, 'eval_runtime': 4.3726, 'eval_samples_per_second': 75.698, 'eval_steps_per_second': 2.516, 'epoch': 7.0}
{'train@nld.rst.nldt_loss': 2.3911447525024414, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.35261194029850745, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.08950960906418343, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.1233513602620889, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.10594097810875214, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.3911449909210205, 'train@nld.rst.nldt_runtime': 19.8096, 'train@nld.rst.nldt_samples_per_second': 81.173, 'train@nld.rst.nldt_steps_per_second': 2.575, 'epoch': 8.0}
{'loss': 2.4709, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.4194438457489014, 'eval_accuracy@nld.rst.nldt': 0.36555891238670696, 'eval_f1@nld.rst.nldt': 0.07571077860163322, 'eval_precision@nld.rst.nldt': 0.06552432028917177, 'eval_recall@nld.rst.nldt': 0.11477127082717144, 'eval_loss@nld.rst.nldt': 2.4194436073303223, 'eval_runtime': 4.3869, 'eval_samples_per_second': 75.452, 'eval_steps_per_second': 2.507, 'epoch': 8.0}
{'train@nld.rst.nldt_loss': 2.369447708129883, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.35696517412935325, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.09180310546631412, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.10869399808837651, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.10860893159297862, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.369447946548462, 'train@nld.rst.nldt_runtime': 19.8034, 'train@nld.rst.nldt_samples_per_second': 81.198, 'train@nld.rst.nldt_steps_per_second': 2.575, 'epoch': 9.0}
{'loss': 2.4395, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.403480291366577, 'eval_accuracy@nld.rst.nldt': 0.3685800604229607, 'eval_f1@nld.rst.nldt': 0.07613300560381422, 'eval_precision@nld.rst.nldt': 0.0659906532399495, 'eval_recall@nld.rst.nldt': 0.11515946958493542, 'eval_loss@nld.rst.nldt': 2.403480291366577, 'eval_runtime': 4.3563, 'eval_samples_per_second': 75.983, 'eval_steps_per_second': 2.525, 'epoch': 9.0}
{'train@nld.rst.nldt_loss': 2.35493803024292, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.36256218905472637, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.09713933788169787, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.11263790296086325, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.114137825356763, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.35493803024292, 'train@nld.rst.nldt_runtime': 19.7904, 'train@nld.rst.nldt_samples_per_second': 81.252, 'train@nld.rst.nldt_steps_per_second': 2.577, 'epoch': 10.0}
{'loss': 2.423, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.391998767852783, 'eval_accuracy@nld.rst.nldt': 0.3685800604229607, 'eval_f1@nld.rst.nldt': 0.07940483011386372, 'eval_precision@nld.rst.nldt': 0.0686493212003416, 'eval_recall@nld.rst.nldt': 0.1190599428177068, 'eval_loss@nld.rst.nldt': 2.3919990062713623, 'eval_runtime': 4.3609, 'eval_samples_per_second': 75.902, 'eval_steps_per_second': 2.522, 'epoch': 10.0}
{'train@nld.rst.nldt_loss': 2.3448164463043213, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3619402985074627, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.09542635193273365, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.11023128199332523, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.11343398691081552, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.344816207885742, 'train@nld.rst.nldt_runtime': 19.8006, 'train@nld.rst.nldt_samples_per_second': 81.21, 'train@nld.rst.nldt_steps_per_second': 2.576, 'epoch': 11.0}
{'loss': 2.4039, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.386765241622925, 'eval_accuracy@nld.rst.nldt': 0.36555891238670696, 'eval_f1@nld.rst.nldt': 0.07774816823281037, 'eval_precision@nld.rst.nldt': 0.06457477471508084, 'eval_recall@nld.rst.nldt': 0.11735926254559795, 'eval_loss@nld.rst.nldt': 2.386765241622925, 'eval_runtime': 4.3716, 'eval_samples_per_second': 75.715, 'eval_steps_per_second': 2.516, 'epoch': 11.0}
{'train@nld.rst.nldt_loss': 2.3413517475128174, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.36318407960199006, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.09603657295262057, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.10797354535168066, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.11404673200885473, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.3413517475128174, 'train@nld.rst.nldt_runtime': 19.8022, 'train@nld.rst.nldt_samples_per_second': 81.203, 'train@nld.rst.nldt_steps_per_second': 2.575, 'epoch': 12.0}
{'loss': 2.4026, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.3850982189178467, 'eval_accuracy@nld.rst.nldt': 0.36555891238670696, 'eval_f1@nld.rst.nldt': 0.07774816823281037, 'eval_precision@nld.rst.nldt': 0.06457477471508084, 'eval_recall@nld.rst.nldt': 0.11735926254559795, 'eval_loss@nld.rst.nldt': 2.385098457336426, 'eval_runtime': 4.3714, 'eval_samples_per_second': 75.719, 'eval_steps_per_second': 2.516, 'epoch': 12.0}
{'train_runtime': 782.2737, 'train_samples_per_second': 24.667, 'train_steps_per_second': 0.782, 'train_loss': 2.7255015653722428, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.4621
  train_runtime            = 3:43:46.13
  train_samples_per_second =      25.76
  train_steps_per_second   =      0.805
-------------------------------------------------------------------
Lang1:  spa.rst.rststb    Lang2:  nld.rst.nldt
Saving run to:  runs/full_shot/FullShot=v4_spa.rst.rststb_nld.rst.nldt_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2240 examples
read 383 examples
read 426 examples
read 1608 examples
read 331 examples
read 326 examples
Total prediction labels:  37
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=37, bias=True)
    )
  )
)
{'train@spa.rst.rststb_loss': 2.8637425899505615, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.2174107142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.021776069331686805, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.03629940115622819, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.04020192742963693, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.8637425899505615, 'train@spa.rst.rststb_runtime': 29.5463, 'train@spa.rst.rststb_samples_per_second': 75.813, 'train@spa.rst.rststb_steps_per_second': 2.369, 'epoch': 1.0}
{'loss': 3.255, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.9051647186279297, 'eval_accuracy@spa.rst.rststb': 0.21671018276762402, 'eval_f1@spa.rst.rststb': 0.025116088229884405, 'eval_precision@spa.rst.rststb': 0.04583688547882921, 'eval_recall@spa.rst.rststb': 0.047726544642077054, 'eval_loss@spa.rst.rststb': 2.9051640033721924, 'eval_runtime': 4.9132, 'eval_samples_per_second': 77.953, 'eval_steps_per_second': 2.442, 'epoch': 1.0}
{'train@spa.rst.rststb_loss': 2.579585075378418, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.27723214285714287, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.04085645344612345, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.041499544322849094, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.0568279390439717, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.579585075378418, 'train@spa.rst.rststb_runtime': 27.2693, 'train@spa.rst.rststb_samples_per_second': 82.144, 'train@spa.rst.rststb_steps_per_second': 2.567, 'epoch': 2.0}
{'loss': 2.7247, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.6871378421783447, 'eval_accuracy@spa.rst.rststb': 0.23237597911227154, 'eval_f1@spa.rst.rststb': 0.03502990898351054, 'eval_precision@spa.rst.rststb': 0.039663886286762436, 'eval_recall@spa.rst.rststb': 0.0549687520769097, 'eval_loss@spa.rst.rststb': 2.6871378421783447, 'eval_runtime': 4.9715, 'eval_samples_per_second': 77.039, 'eval_steps_per_second': 2.414, 'epoch': 2.0}
{'train@spa.rst.rststb_loss': 2.444084405899048, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.3254464285714286, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.05482454645864493, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.055770553335262485, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.07397276911629791, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.444084405899048, 'train@spa.rst.rststb_runtime': 27.3152, 'train@spa.rst.rststb_samples_per_second': 82.006, 'train@spa.rst.rststb_steps_per_second': 2.563, 'epoch': 3.0}
{'loss': 2.5471, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.5853214263916016, 'eval_accuracy@spa.rst.rststb': 0.29765013054830286, 'eval_f1@spa.rst.rststb': 0.05697038972042336, 'eval_precision@spa.rst.rststb': 0.05976739618737518, 'eval_recall@spa.rst.rststb': 0.07906205131416442, 'eval_loss@spa.rst.rststb': 2.5853211879730225, 'eval_runtime': 4.9808, 'eval_samples_per_second': 76.895, 'eval_steps_per_second': 2.409, 'epoch': 3.0}
{'train@spa.rst.rststb_loss': 2.342540979385376, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.3517857142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.07220306598627403, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.08013013592832871, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.09324688636997691, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.342540979385376, 'train@spa.rst.rststb_runtime': 27.3099, 'train@spa.rst.rststb_samples_per_second': 82.022, 'train@spa.rst.rststb_steps_per_second': 2.563, 'epoch': 4.0}
{'loss': 2.4369, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.5157628059387207, 'eval_accuracy@spa.rst.rststb': 0.3289817232375979, 'eval_f1@spa.rst.rststb': 0.08288942308626528, 'eval_precision@spa.rst.rststb': 0.0875933636943515, 'eval_recall@spa.rst.rststb': 0.10272208652041738, 'eval_loss@spa.rst.rststb': 2.5157623291015625, 'eval_runtime': 4.9784, 'eval_samples_per_second': 76.932, 'eval_steps_per_second': 2.41, 'epoch': 4.0}
{'train@spa.rst.rststb_loss': 2.2594540119171143, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.37991071428571427, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.08620297776483454, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.07826659971384177, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.11056446601768702, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.2594540119171143, 'train@spa.rst.rststb_runtime': 27.3262, 'train@spa.rst.rststb_samples_per_second': 81.972, 'train@spa.rst.rststb_steps_per_second': 2.562, 'epoch': 5.0}
{'loss': 2.349, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.4583771228790283, 'eval_accuracy@spa.rst.rststb': 0.34203655352480417, 'eval_f1@spa.rst.rststb': 0.09622608350637953, 'eval_precision@spa.rst.rststb': 0.09162181153467276, 'eval_recall@spa.rst.rststb': 0.12012804324943409, 'eval_loss@spa.rst.rststb': 2.4583771228790283, 'eval_runtime': 4.9537, 'eval_samples_per_second': 77.316, 'eval_steps_per_second': 2.422, 'epoch': 5.0}
{'train@spa.rst.rststb_loss': 2.194711685180664, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.390625, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.09046674799897637, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.0995201995516397, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.1154673425070403, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.194711446762085, 'train@spa.rst.rststb_runtime': 27.3198, 'train@spa.rst.rststb_samples_per_second': 81.992, 'train@spa.rst.rststb_steps_per_second': 2.562, 'epoch': 6.0}
{'loss': 2.2835, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.413482666015625, 'eval_accuracy@spa.rst.rststb': 0.360313315926893, 'eval_f1@spa.rst.rststb': 0.10106466391980812, 'eval_precision@spa.rst.rststb': 0.09075229036182926, 'eval_recall@spa.rst.rststb': 0.13051753895895873, 'eval_loss@spa.rst.rststb': 2.413482427597046, 'eval_runtime': 4.953, 'eval_samples_per_second': 77.326, 'eval_steps_per_second': 2.423, 'epoch': 6.0}
{'train@spa.rst.rststb_loss': 2.1408090591430664, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.40982142857142856, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.09804934879435621, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.09233978190655842, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.12572735307333138, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.1408088207244873, 'train@spa.rst.rststb_runtime': 27.3767, 'train@spa.rst.rststb_samples_per_second': 81.821, 'train@spa.rst.rststb_steps_per_second': 2.557, 'epoch': 7.0}
{'loss': 2.2254, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.3764114379882812, 'eval_accuracy@spa.rst.rststb': 0.370757180156658, 'eval_f1@spa.rst.rststb': 0.10060873491928614, 'eval_precision@spa.rst.rststb': 0.08835146380024156, 'eval_recall@spa.rst.rststb': 0.1340171217901444, 'eval_loss@spa.rst.rststb': 2.3764114379882812, 'eval_runtime': 5.0003, 'eval_samples_per_second': 76.596, 'eval_steps_per_second': 2.4, 'epoch': 7.0}
{'train@spa.rst.rststb_loss': 2.1006569862365723, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.415625, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10195067383312728, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.11789200189449604, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.12844907253528273, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.100656747817993, 'train@spa.rst.rststb_runtime': 27.4026, 'train@spa.rst.rststb_samples_per_second': 81.744, 'train@spa.rst.rststb_steps_per_second': 2.555, 'epoch': 8.0}
{'loss': 2.1764, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.346484422683716, 'eval_accuracy@spa.rst.rststb': 0.37597911227154046, 'eval_f1@spa.rst.rststb': 0.10324274825198967, 'eval_precision@spa.rst.rststb': 0.09206398250898958, 'eval_recall@spa.rst.rststb': 0.1352164087893805, 'eval_loss@spa.rst.rststb': 2.346484422683716, 'eval_runtime': 4.975, 'eval_samples_per_second': 76.985, 'eval_steps_per_second': 2.412, 'epoch': 8.0}
{'train@spa.rst.rststb_loss': 2.070232391357422, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4205357142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10489046028952385, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.11852137427533219, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13360374933538968, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.070232391357422, 'train@spa.rst.rststb_runtime': 27.4238, 'train@spa.rst.rststb_samples_per_second': 81.681, 'train@spa.rst.rststb_steps_per_second': 2.553, 'epoch': 9.0}
{'loss': 2.1427, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.320300340652466, 'eval_accuracy@spa.rst.rststb': 0.3629242819843342, 'eval_f1@spa.rst.rststb': 0.09706551204435802, 'eval_precision@spa.rst.rststb': 0.08181975150101763, 'eval_recall@spa.rst.rststb': 0.13270526161834617, 'eval_loss@spa.rst.rststb': 2.320300340652466, 'eval_runtime': 4.9977, 'eval_samples_per_second': 76.635, 'eval_steps_per_second': 2.401, 'epoch': 9.0}
{'train@spa.rst.rststb_loss': 2.0490777492523193, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.425, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10815056431486474, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.12193278166041557, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13598251694964297, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0490782260894775, 'train@spa.rst.rststb_runtime': 27.3958, 'train@spa.rst.rststb_samples_per_second': 81.764, 'train@spa.rst.rststb_steps_per_second': 2.555, 'epoch': 10.0}
{'loss': 2.1074, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.3055031299591064, 'eval_accuracy@spa.rst.rststb': 0.3733681462140992, 'eval_f1@spa.rst.rststb': 0.10109914931456024, 'eval_precision@spa.rst.rststb': 0.08608932045436424, 'eval_recall@spa.rst.rststb': 0.1353995502193697, 'eval_loss@spa.rst.rststb': 2.3055033683776855, 'eval_runtime': 4.97, 'eval_samples_per_second': 77.063, 'eval_steps_per_second': 2.415, 'epoch': 10.0}
{'train@spa.rst.rststb_loss': 2.037017822265625, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.43080357142857145, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.11222336464961331, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.12589366198220867, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13840046366963504, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.037017822265625, 'train@spa.rst.rststb_runtime': 27.4223, 'train@spa.rst.rststb_samples_per_second': 81.685, 'train@spa.rst.rststb_steps_per_second': 2.553, 'epoch': 11.0}
{'loss': 2.1004, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.2960448265075684, 'eval_accuracy@spa.rst.rststb': 0.381201044386423, 'eval_f1@spa.rst.rststb': 0.10475475132494448, 'eval_precision@spa.rst.rststb': 0.09162060320988064, 'eval_recall@spa.rst.rststb': 0.13724776648531567, 'eval_loss@spa.rst.rststb': 2.2960448265075684, 'eval_runtime': 5.0005, 'eval_samples_per_second': 76.593, 'eval_steps_per_second': 2.4, 'epoch': 11.0}
{'train@spa.rst.rststb_loss': 2.033083915710449, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4330357142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.11299372768003438, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.1266889666337703, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13881482734129233, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.033083915710449, 'train@spa.rst.rststb_runtime': 27.4245, 'train@spa.rst.rststb_samples_per_second': 81.679, 'train@spa.rst.rststb_steps_per_second': 2.552, 'epoch': 12.0}
{'loss': 2.085, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.2933266162872314, 'eval_accuracy@spa.rst.rststb': 0.381201044386423, 'eval_f1@spa.rst.rststb': 0.10503918786527482, 'eval_precision@spa.rst.rststb': 0.09246579507449072, 'eval_recall@spa.rst.rststb': 0.13724776648531567, 'eval_loss@spa.rst.rststb': 2.2933266162872314, 'eval_runtime': 5.0058, 'eval_samples_per_second': 76.511, 'eval_steps_per_second': 2.397, 'epoch': 12.0}
{'train_runtime': 1074.1719, 'train_samples_per_second': 25.024, 'train_steps_per_second': 0.782, 'train_loss': 2.3694634755452473, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.3695
  train_runtime            = 0:17:54.17
  train_samples_per_second =     25.024
  train_steps_per_second   =      0.782
{'train@nld.rst.nldt_loss': 3.4476184844970703, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.01990049751243781, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.004634276310656056, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.013022609629505386, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.027015210814653006, 'train@nld.rst.nldt_loss@nld.rst.nldt': 3.4476184844970703, 'train@nld.rst.nldt_runtime': 19.8395, 'train@nld.rst.nldt_samples_per_second': 81.05, 'train@nld.rst.nldt_steps_per_second': 2.571, 'epoch': 1.0}
{'loss': 3.7919, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.4073541164398193, 'eval_accuracy@nld.rst.nldt': 0.027190332326283987, 'eval_f1@nld.rst.nldt': 0.01028975159929282, 'eval_precision@nld.rst.nldt': 0.021321566626730945, 'eval_recall@nld.rst.nldt': 0.029664136185875316, 'eval_loss@nld.rst.nldt': 3.4073545932769775, 'eval_runtime': 4.3768, 'eval_samples_per_second': 75.625, 'eval_steps_per_second': 2.513, 'epoch': 1.0}
{'train@nld.rst.nldt_loss': 2.9792540073394775, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.25808457711442784, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.015523330328723132, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.010687240249360613, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.032085084033613445, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.9792544841766357, 'train@nld.rst.nldt_runtime': 19.8552, 'train@nld.rst.nldt_samples_per_second': 80.986, 'train@nld.rst.nldt_steps_per_second': 2.569, 'epoch': 2.0}
{'loss': 3.2196, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.9033362865448, 'eval_accuracy@nld.rst.nldt': 0.2628398791540785, 'eval_f1@nld.rst.nldt': 0.016609392898052695, 'eval_precision@nld.rst.nldt': 0.010885885885885885, 'eval_recall@nld.rst.nldt': 0.03502415458937198, 'eval_loss@nld.rst.nldt': 2.9033362865448, 'eval_runtime': 4.3503, 'eval_samples_per_second': 76.087, 'eval_steps_per_second': 2.529, 'epoch': 2.0}
{'train@nld.rst.nldt_loss': 2.8175017833709717, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.27238805970149255, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01857290589451913, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.014379631303090548, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.035346638655462184, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.8175017833709717, 'train@nld.rst.nldt_runtime': 19.8362, 'train@nld.rst.nldt_samples_per_second': 81.064, 'train@nld.rst.nldt_steps_per_second': 2.571, 'epoch': 3.0}
{'loss': 2.9135, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.7545604705810547, 'eval_accuracy@nld.rst.nldt': 0.283987915407855, 'eval_f1@nld.rst.nldt': 0.019593640978774637, 'eval_precision@nld.rst.nldt': 0.014394211805215043, 'eval_recall@nld.rst.nldt': 0.039153439153439155, 'eval_loss@nld.rst.nldt': 2.7545604705810547, 'eval_runtime': 4.3499, 'eval_samples_per_second': 76.094, 'eval_steps_per_second': 2.529, 'epoch': 3.0}
{'train@nld.rst.nldt_loss': 2.7354860305786133, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2835820895522388, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.02362074483756261, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.039970604020296646, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.040160480859010264, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.7354862689971924, 'train@nld.rst.nldt_runtime': 19.8265, 'train@nld.rst.nldt_samples_per_second': 81.103, 'train@nld.rst.nldt_steps_per_second': 2.572, 'epoch': 4.0}
{'loss': 2.7871, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.6843247413635254, 'eval_accuracy@nld.rst.nldt': 0.2990936555891239, 'eval_f1@nld.rst.nldt': 0.03047041363624278, 'eval_precision@nld.rst.nldt': 0.03847217899505481, 'eval_recall@nld.rst.nldt': 0.04644326866549089, 'eval_loss@nld.rst.nldt': 2.6843249797821045, 'eval_runtime': 4.368, 'eval_samples_per_second': 75.779, 'eval_steps_per_second': 2.518, 'epoch': 4.0}
{'train@nld.rst.nldt_loss': 2.683483123779297, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2966417910447761, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03194086356092935, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.030813503779442183, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.04734943977591037, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.683483600616455, 'train@nld.rst.nldt_runtime': 19.8226, 'train@nld.rst.nldt_samples_per_second': 81.12, 'train@nld.rst.nldt_steps_per_second': 2.573, 'epoch': 5.0}
{'loss': 2.7355, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.6423723697662354, 'eval_accuracy@nld.rst.nldt': 0.31419939577039274, 'eval_f1@nld.rst.nldt': 0.03911553124665028, 'eval_precision@nld.rst.nldt': 0.04000633111744222, 'eval_recall@nld.rst.nldt': 0.05473251028806585, 'eval_loss@nld.rst.nldt': 2.6423723697662354, 'eval_runtime': 4.3511, 'eval_samples_per_second': 76.072, 'eval_steps_per_second': 2.528, 'epoch': 5.0}
{'train@nld.rst.nldt_loss': 2.6413652896881104, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31032338308457713, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03727152551676512, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.029930256960775628, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05577614379084967, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6413650512695312, 'train@nld.rst.nldt_runtime': 19.8324, 'train@nld.rst.nldt_samples_per_second': 81.079, 'train@nld.rst.nldt_steps_per_second': 2.572, 'epoch': 6.0}
{'loss': 2.6962, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.605025291442871, 'eval_accuracy@nld.rst.nldt': 0.32326283987915405, 'eval_f1@nld.rst.nldt': 0.04207025705254369, 'eval_precision@nld.rst.nldt': 0.03745942201400636, 'eval_recall@nld.rst.nldt': 0.06256038647342994, 'eval_loss@nld.rst.nldt': 2.605025053024292, 'eval_runtime': 4.3609, 'eval_samples_per_second': 75.902, 'eval_steps_per_second': 2.522, 'epoch': 6.0}
{'train@nld.rst.nldt_loss': 2.6090335845947266, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31405472636815923, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03798694298809052, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.028994978570194757, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05883578431372549, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6090333461761475, 'train@nld.rst.nldt_runtime': 19.83, 'train@nld.rst.nldt_samples_per_second': 81.089, 'train@nld.rst.nldt_steps_per_second': 2.572, 'epoch': 7.0}
{'loss': 2.6545, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.5774571895599365, 'eval_accuracy@nld.rst.nldt': 0.3202416918429003, 'eval_f1@nld.rst.nldt': 0.039783430992065494, 'eval_precision@nld.rst.nldt': 0.03300596344074605, 'eval_recall@nld.rst.nldt': 0.062157809983896944, 'eval_loss@nld.rst.nldt': 2.5774574279785156, 'eval_runtime': 4.3477, 'eval_samples_per_second': 76.132, 'eval_steps_per_second': 2.53, 'epoch': 7.0}
{'train@nld.rst.nldt_loss': 2.582015037536621, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31902985074626866, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03947337728976331, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.029064448436038448, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.06320669934640523, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.582015037536621, 'train@nld.rst.nldt_runtime': 19.8476, 'train@nld.rst.nldt_samples_per_second': 81.017, 'train@nld.rst.nldt_steps_per_second': 2.57, 'epoch': 8.0}
{'loss': 2.6322, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.554320812225342, 'eval_accuracy@nld.rst.nldt': 0.3413897280966767, 'eval_f1@nld.rst.nldt': 0.05555613595773492, 'eval_precision@nld.rst.nldt': 0.07182421790264927, 'eval_recall@nld.rst.nldt': 0.07669657234874626, 'eval_loss@nld.rst.nldt': 2.554320812225342, 'eval_runtime': 4.3587, 'eval_samples_per_second': 75.94, 'eval_steps_per_second': 2.524, 'epoch': 8.0}
{'train@nld.rst.nldt_loss': 2.563832998275757, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3208955223880597, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03971872894300816, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.029038790133613976, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.06418359010270776, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.563833475112915, 'train@nld.rst.nldt_runtime': 19.8171, 'train@nld.rst.nldt_samples_per_second': 81.142, 'train@nld.rst.nldt_steps_per_second': 2.574, 'epoch': 9.0}
{'loss': 2.6078, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.539569854736328, 'eval_accuracy@nld.rst.nldt': 0.3413897280966767, 'eval_f1@nld.rst.nldt': 0.055632165718406845, 'eval_precision@nld.rst.nldt': 0.06419360269360269, 'eval_recall@nld.rst.nldt': 0.07735219691741431, 'eval_loss@nld.rst.nldt': 2.539569854736328, 'eval_runtime': 4.343, 'eval_samples_per_second': 76.214, 'eval_steps_per_second': 2.533, 'epoch': 9.0}
{'train@nld.rst.nldt_loss': 2.551107883453369, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.32338308457711445, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.040190590659340664, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.02945734177807381, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.06471055088702148, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.5511083602905273, 'train@nld.rst.nldt_runtime': 19.7982, 'train@nld.rst.nldt_samples_per_second': 81.219, 'train@nld.rst.nldt_steps_per_second': 2.576, 'epoch': 10.0}
{'loss': 2.5936, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.5286383628845215, 'eval_accuracy@nld.rst.nldt': 0.338368580060423, 'eval_f1@nld.rst.nldt': 0.05418286069735482, 'eval_precision@nld.rst.nldt': 0.05878693747510951, 'eval_recall@nld.rst.nldt': 0.07529458374869004, 'eval_loss@nld.rst.nldt': 2.5286386013031006, 'eval_runtime': 4.331, 'eval_samples_per_second': 76.427, 'eval_steps_per_second': 2.54, 'epoch': 10.0}
{'train@nld.rst.nldt_loss': 2.542933940887451, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.32338308457711445, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.04035455172236649, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.029366803741892857, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.06558356676003735, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.542933702468872, 'train@nld.rst.nldt_runtime': 19.7833, 'train@nld.rst.nldt_samples_per_second': 81.281, 'train@nld.rst.nldt_steps_per_second': 2.578, 'epoch': 11.0}
{'loss': 2.5774, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.5225582122802734, 'eval_accuracy@nld.rst.nldt': 0.3413897280966767, 'eval_f1@nld.rst.nldt': 0.05489092910303288, 'eval_precision@nld.rst.nldt': 0.05342822027000295, 'eval_recall@nld.rst.nldt': 0.07800782148608236, 'eval_loss@nld.rst.nldt': 2.5225582122802734, 'eval_runtime': 4.3307, 'eval_samples_per_second': 76.432, 'eval_steps_per_second': 2.54, 'epoch': 11.0}
{'train@nld.rst.nldt_loss': 2.540071964263916, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3240049751243781, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.04049731457319796, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.029463440696523444, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.06583158263305322, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.540071487426758, 'train@nld.rst.nldt_runtime': 19.7581, 'train@nld.rst.nldt_samples_per_second': 81.384, 'train@nld.rst.nldt_steps_per_second': 2.581, 'epoch': 12.0}
{'loss': 2.5783, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.5207552909851074, 'eval_accuracy@nld.rst.nldt': 0.3413897280966767, 'eval_f1@nld.rst.nldt': 0.05547537148403383, 'eval_precision@nld.rst.nldt': 0.053912528589121225, 'eval_recall@nld.rst.nldt': 0.07866344605475041, 'eval_loss@nld.rst.nldt': 2.5207550525665283, 'eval_runtime': 4.3404, 'eval_samples_per_second': 76.26, 'eval_steps_per_second': 2.534, 'epoch': 12.0}
{'train_runtime': 782.8139, 'train_samples_per_second': 24.65, 'train_steps_per_second': 0.782, 'train_loss': 2.815634534249898, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.3695
  train_runtime            = 0:17:54.17
  train_samples_per_second =     25.024
  train_steps_per_second   =      0.782
-------------------------------------------------------------------
Lang1:  spa.rst.sctb    Lang2:  nld.rst.nldt
Saving run to:  runs/full_shot/FullShot=v4_spa.rst.sctb_nld.rst.nldt_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 439 examples
read 94 examples
read 159 examples
read 1608 examples
read 331 examples
read 326 examples
Total prediction labels:  36
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=36, bias=True)
    )
  )
)
{'train@spa.rst.sctb_loss': 3.355199098587036, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.33940774487471526, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02821665650613019, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.02403419006822148, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.043431899641577065, 'train@spa.rst.sctb_loss@spa.rst.sctb': 3.355199098587036, 'train@spa.rst.sctb_runtime': 5.5339, 'train@spa.rst.sctb_samples_per_second': 79.33, 'train@spa.rst.sctb_steps_per_second': 2.53, 'epoch': 1.0}
{'loss': 3.5018, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.3444221019744873, 'eval_accuracy@spa.rst.sctb': 0.32978723404255317, 'eval_f1@spa.rst.sctb': 0.04258731015341916, 'eval_precision@spa.rst.sctb': 0.03560371517027864, 'eval_recall@spa.rst.sctb': 0.06051224317478187, 'eval_loss@spa.rst.sctb': 3.3444223403930664, 'eval_runtime': 1.4392, 'eval_samples_per_second': 65.312, 'eval_steps_per_second': 2.084, 'epoch': 1.0}
{'train@spa.rst.sctb_loss': 3.120911121368408, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3462414578587699, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02381243593327742, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.039304915514592934, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04273297491039427, 'train@spa.rst.sctb_loss@spa.rst.sctb': 3.120910406112671, 'train@spa.rst.sctb_runtime': 5.5875, 'train@spa.rst.sctb_samples_per_second': 78.568, 'train@spa.rst.sctb_steps_per_second': 2.506, 'epoch': 2.0}
{'loss': 3.2528, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.1082358360290527, 'eval_accuracy@spa.rst.sctb': 0.35106382978723405, 'eval_f1@spa.rst.sctb': 0.03105882352941176, 'eval_precision@spa.rst.sctb': 0.021099744245524295, 'eval_recall@spa.rst.sctb': 0.058823529411764705, 'eval_loss@spa.rst.sctb': 3.108236074447632, 'eval_runtime': 1.4465, 'eval_samples_per_second': 64.984, 'eval_steps_per_second': 2.074, 'epoch': 2.0}
{'train@spa.rst.sctb_loss': 2.88578724861145, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3416856492027335, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.021222410865874366, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.014236902050113895, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.041666666666666664, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.8857874870300293, 'train@spa.rst.sctb_runtime': 5.5833, 'train@spa.rst.sctb_samples_per_second': 78.627, 'train@spa.rst.sctb_steps_per_second': 2.507, 'epoch': 3.0}
{'loss': 3.0388, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.874492883682251, 'eval_accuracy@spa.rst.sctb': 0.35106382978723405, 'eval_f1@spa.rst.sctb': 0.03081232492997199, 'eval_precision@spa.rst.sctb': 0.020872865275142316, 'eval_recall@spa.rst.sctb': 0.058823529411764705, 'eval_loss@spa.rst.sctb': 2.874492883682251, 'eval_runtime': 1.4626, 'eval_samples_per_second': 64.271, 'eval_steps_per_second': 2.051, 'epoch': 3.0}
{'train@spa.rst.sctb_loss': 2.6732900142669678, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3416856492027335, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.021222410865874366, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.014236902050113895, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.041666666666666664, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.673290252685547, 'train@spa.rst.sctb_runtime': 5.6128, 'train@spa.rst.sctb_samples_per_second': 78.214, 'train@spa.rst.sctb_steps_per_second': 2.494, 'epoch': 4.0}
{'loss': 2.8114, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.6672191619873047, 'eval_accuracy@spa.rst.sctb': 0.35106382978723405, 'eval_f1@spa.rst.sctb': 0.03056970819823993, 'eval_precision@spa.rst.sctb': 0.02065081351689612, 'eval_recall@spa.rst.sctb': 0.058823529411764705, 'eval_loss@spa.rst.sctb': 2.6672184467315674, 'eval_runtime': 1.4564, 'eval_samples_per_second': 64.543, 'eval_steps_per_second': 2.06, 'epoch': 4.0}
{'train@spa.rst.sctb_loss': 2.5084750652313232, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3416856492027335, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.021222410865874366, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.014236902050113895, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.041666666666666664, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.508474826812744, 'train@spa.rst.sctb_runtime': 5.622, 'train@spa.rst.sctb_samples_per_second': 78.085, 'train@spa.rst.sctb_steps_per_second': 2.49, 'epoch': 5.0}
{'loss': 2.6189, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.50883150100708, 'eval_accuracy@spa.rst.sctb': 0.35106382978723405, 'eval_f1@spa.rst.sctb': 0.03056970819823993, 'eval_precision@spa.rst.sctb': 0.02065081351689612, 'eval_recall@spa.rst.sctb': 0.058823529411764705, 'eval_loss@spa.rst.sctb': 2.508831024169922, 'eval_runtime': 1.466, 'eval_samples_per_second': 64.119, 'eval_steps_per_second': 2.046, 'epoch': 5.0}
{'train@spa.rst.sctb_loss': 2.3986704349517822, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3416856492027335, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.021222410865874366, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.014236902050113895, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.041666666666666664, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.3986704349517822, 'train@spa.rst.sctb_runtime': 5.6223, 'train@spa.rst.sctb_samples_per_second': 78.082, 'train@spa.rst.sctb_steps_per_second': 2.49, 'epoch': 6.0}
{'loss': 2.4963, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.4082694053649902, 'eval_accuracy@spa.rst.sctb': 0.35106382978723405, 'eval_f1@spa.rst.sctb': 0.03056970819823993, 'eval_precision@spa.rst.sctb': 0.02065081351689612, 'eval_recall@spa.rst.sctb': 0.058823529411764705, 'eval_loss@spa.rst.sctb': 2.408268928527832, 'eval_runtime': 1.465, 'eval_samples_per_second': 64.162, 'eval_steps_per_second': 2.048, 'epoch': 6.0}
{'train@spa.rst.sctb_loss': 2.3305366039276123, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3416856492027335, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.021222410865874366, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.014236902050113895, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.041666666666666664, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.3305368423461914, 'train@spa.rst.sctb_runtime': 5.6142, 'train@spa.rst.sctb_samples_per_second': 78.194, 'train@spa.rst.sctb_steps_per_second': 2.494, 'epoch': 7.0}
{'loss': 2.3939, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.3483870029449463, 'eval_accuracy@spa.rst.sctb': 0.35106382978723405, 'eval_f1@spa.rst.sctb': 0.03056970819823993, 'eval_precision@spa.rst.sctb': 0.02065081351689612, 'eval_recall@spa.rst.sctb': 0.058823529411764705, 'eval_loss@spa.rst.sctb': 2.3483874797821045, 'eval_runtime': 1.4644, 'eval_samples_per_second': 64.19, 'eval_steps_per_second': 2.049, 'epoch': 7.0}
{'train@spa.rst.sctb_loss': 2.2912416458129883, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.35079726651480636, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.024807826694619145, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.04221195791634591, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04345878136200717, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.2912421226501465, 'train@spa.rst.sctb_runtime': 5.6218, 'train@spa.rst.sctb_samples_per_second': 78.089, 'train@spa.rst.sctb_steps_per_second': 2.49, 'epoch': 8.0}
{'loss': 2.341, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.3162426948547363, 'eval_accuracy@spa.rst.sctb': 0.35106382978723405, 'eval_f1@spa.rst.sctb': 0.03056970819823993, 'eval_precision@spa.rst.sctb': 0.02065081351689612, 'eval_recall@spa.rst.sctb': 0.058823529411764705, 'eval_loss@spa.rst.sctb': 2.3162424564361572, 'eval_runtime': 1.4697, 'eval_samples_per_second': 63.957, 'eval_steps_per_second': 2.041, 'epoch': 8.0}
{'train@spa.rst.sctb_loss': 2.26436185836792, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.357630979498861, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.027235271936829026, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.04111795525346927, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.044802867383512544, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.264362096786499, 'train@spa.rst.sctb_runtime': 5.6346, 'train@spa.rst.sctb_samples_per_second': 77.912, 'train@spa.rst.sctb_steps_per_second': 2.485, 'epoch': 9.0}
{'loss': 2.3239, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.293814182281494, 'eval_accuracy@spa.rst.sctb': 0.3617021276595745, 'eval_f1@spa.rst.sctb': 0.03669467787114846, 'eval_precision@spa.rst.sctb': 0.07969639468690703, 'eval_recall@spa.rst.sctb': 0.06191950464396284, 'eval_loss@spa.rst.sctb': 2.293813943862915, 'eval_runtime': 1.4468, 'eval_samples_per_second': 64.972, 'eval_steps_per_second': 2.074, 'epoch': 9.0}
{'train@spa.rst.sctb_loss': 2.246561050415039, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.35990888382687924, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.028525641025641025, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.0367705138927609, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04542114695340502, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.246561050415039, 'train@spa.rst.sctb_runtime': 5.643, 'train@spa.rst.sctb_samples_per_second': 77.796, 'train@spa.rst.sctb_steps_per_second': 2.481, 'epoch': 10.0}
{'loss': 2.2954, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.2795817852020264, 'eval_accuracy@spa.rst.sctb': 0.3829787234042553, 'eval_f1@spa.rst.sctb': 0.04735207866137657, 'eval_precision@spa.rst.sctb': 0.0801551389786684, 'eval_recall@spa.rst.sctb': 0.06811145510835914, 'eval_loss@spa.rst.sctb': 2.2795817852020264, 'eval_runtime': 1.489, 'eval_samples_per_second': 63.128, 'eval_steps_per_second': 2.015, 'epoch': 10.0}
{'train@spa.rst.sctb_loss': 2.2369892597198486, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.35990888382687924, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02848005430242272, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03451858813700919, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04542114695340502, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.2369890213012695, 'train@spa.rst.sctb_runtime': 5.6178, 'train@spa.rst.sctb_samples_per_second': 78.144, 'train@spa.rst.sctb_steps_per_second': 2.492, 'epoch': 11.0}
{'loss': 2.2846, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.2716872692108154, 'eval_accuracy@spa.rst.sctb': 0.3829787234042553, 'eval_f1@spa.rst.sctb': 0.04690911359241469, 'eval_precision@spa.rst.sctb': 0.06568627450980392, 'eval_recall@spa.rst.sctb': 0.06811145510835914, 'eval_loss@spa.rst.sctb': 2.2716867923736572, 'eval_runtime': 1.4554, 'eval_samples_per_second': 64.586, 'eval_steps_per_second': 2.061, 'epoch': 11.0}
{'train@spa.rst.sctb_loss': 2.233851671218872, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.35990888382687924, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.028459077924838375, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.033567024661893395, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04542114695340502, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.233851671218872, 'train@spa.rst.sctb_runtime': 5.6326, 'train@spa.rst.sctb_samples_per_second': 77.939, 'train@spa.rst.sctb_steps_per_second': 2.486, 'epoch': 12.0}
{'loss': 2.2732, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.2694039344787598, 'eval_accuracy@spa.rst.sctb': 0.3829787234042553, 'eval_f1@spa.rst.sctb': 0.04690911359241469, 'eval_precision@spa.rst.sctb': 0.06568627450980392, 'eval_recall@spa.rst.sctb': 0.06811145510835914, 'eval_loss@spa.rst.sctb': 2.2694036960601807, 'eval_runtime': 1.4579, 'eval_samples_per_second': 64.474, 'eval_steps_per_second': 2.058, 'epoch': 12.0}
{'train_runtime': 218.7911, 'train_samples_per_second': 24.078, 'train_steps_per_second': 0.768, 'train_loss': 2.6359901428222656, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =      2.636
  train_runtime            = 0:03:38.79
  train_samples_per_second =     24.078
  train_steps_per_second   =      0.768
{'train@nld.rst.nldt_loss': 3.205798625946045, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2512437810945274, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.014064018555812716, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.009022433695204068, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03196266968325792, 'train@nld.rst.nldt_loss@nld.rst.nldt': 3.205798625946045, 'train@nld.rst.nldt_runtime': 19.7206, 'train@nld.rst.nldt_samples_per_second': 81.539, 'train@nld.rst.nldt_steps_per_second': 2.586, 'epoch': 1.0}
{'loss': 3.4962, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.1952221393585205, 'eval_accuracy@nld.rst.nldt': 0.2628398791540785, 'eval_f1@nld.rst.nldt': 0.018524408632551122, 'eval_precision@nld.rst.nldt': 0.011816578483245148, 'eval_recall@nld.rst.nldt': 0.053140096618357495, 'eval_loss@nld.rst.nldt': 3.1952221393585205, 'eval_runtime': 4.325, 'eval_samples_per_second': 76.531, 'eval_steps_per_second': 2.543, 'epoch': 1.0}
{'train@nld.rst.nldt_loss': 2.874661684036255, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26430348258706465, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01306566650270536, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.00825948383084577, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03125, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.874661684036255, 'train@nld.rst.nldt_runtime': 19.7736, 'train@nld.rst.nldt_samples_per_second': 81.321, 'train@nld.rst.nldt_steps_per_second': 2.579, 'epoch': 2.0}
{'loss': 3.0249, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.8252997398376465, 'eval_accuracy@nld.rst.nldt': 0.27794561933534745, 'eval_f1@nld.rst.nldt': 0.016110673321075214, 'eval_precision@nld.rst.nldt': 0.010294282197605462, 'eval_recall@nld.rst.nldt': 0.037037037037037035, 'eval_loss@nld.rst.nldt': 2.8252999782562256, 'eval_runtime': 4.3254, 'eval_samples_per_second': 76.524, 'eval_steps_per_second': 2.543, 'epoch': 2.0}
{'train@nld.rst.nldt_loss': 2.779961585998535, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26430348258706465, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01306566650270536, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.00825948383084577, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03125, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.779961585998535, 'train@nld.rst.nldt_runtime': 19.7606, 'train@nld.rst.nldt_samples_per_second': 81.374, 'train@nld.rst.nldt_steps_per_second': 2.581, 'epoch': 3.0}
{'loss': 2.8518, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.7356767654418945, 'eval_accuracy@nld.rst.nldt': 0.27794561933534745, 'eval_f1@nld.rst.nldt': 0.01614885027207302, 'eval_precision@nld.rst.nldt': 0.010325476992143659, 'eval_recall@nld.rst.nldt': 0.037037037037037035, 'eval_loss@nld.rst.nldt': 2.7356772422790527, 'eval_runtime': 4.3398, 'eval_samples_per_second': 76.271, 'eval_steps_per_second': 2.535, 'epoch': 3.0}
{'train@nld.rst.nldt_loss': 2.7341995239257812, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26990049751243783, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.018856585636527097, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.024691377829339573, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03464752567693744, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.734199285507202, 'train@nld.rst.nldt_runtime': 19.7844, 'train@nld.rst.nldt_samples_per_second': 81.276, 'train@nld.rst.nldt_steps_per_second': 2.578, 'epoch': 4.0}
{'loss': 2.7641, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.6919357776641846, 'eval_accuracy@nld.rst.nldt': 0.2990936555891239, 'eval_f1@nld.rst.nldt': 0.03347895572834449, 'eval_precision@nld.rst.nldt': 0.04778595630330646, 'eval_recall@nld.rst.nldt': 0.05044091710758378, 'eval_loss@nld.rst.nldt': 2.6919357776641846, 'eval_runtime': 4.35, 'eval_samples_per_second': 76.091, 'eval_steps_per_second': 2.529, 'epoch': 4.0}
{'train@nld.rst.nldt_loss': 2.698129177093506, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.28544776119402987, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.029238287219103654, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.02965911272709818, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.04358485060690943, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.698129177093506, 'train@nld.rst.nldt_runtime': 19.7869, 'train@nld.rst.nldt_samples_per_second': 81.266, 'train@nld.rst.nldt_steps_per_second': 2.577, 'epoch': 5.0}
{'loss': 2.7343, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.6563799381256104, 'eval_accuracy@nld.rst.nldt': 0.311178247734139, 'eval_f1@nld.rst.nldt': 0.038986637385950956, 'eval_precision@nld.rst.nldt': 0.04023764779578734, 'eval_recall@nld.rst.nldt': 0.05832758224062572, 'eval_loss@nld.rst.nldt': 2.6563801765441895, 'eval_runtime': 4.3519, 'eval_samples_per_second': 76.058, 'eval_steps_per_second': 2.528, 'epoch': 5.0}
{'train@nld.rst.nldt_loss': 2.665370464324951, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2997512437810945, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03401082511820767, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.029057315197384512, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.050920284780578896, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.665370464324951, 'train@nld.rst.nldt_runtime': 19.8086, 'train@nld.rst.nldt_samples_per_second': 81.177, 'train@nld.rst.nldt_steps_per_second': 2.575, 'epoch': 6.0}
{'loss': 2.7027, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.6280455589294434, 'eval_accuracy@nld.rst.nldt': 0.32326283987915405, 'eval_f1@nld.rst.nldt': 0.04368993213322455, 'eval_precision@nld.rst.nldt': 0.042705959772596676, 'eval_recall@nld.rst.nldt': 0.06621424737366767, 'eval_loss@nld.rst.nldt': 2.6280455589294434, 'eval_runtime': 4.345, 'eval_samples_per_second': 76.18, 'eval_steps_per_second': 2.532, 'epoch': 6.0}
{'train@nld.rst.nldt_loss': 2.6385457515716553, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30597014925373134, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.035280819472331075, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.027890808616079094, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05485644257703081, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.638545513153076, 'train@nld.rst.nldt_runtime': 19.8164, 'train@nld.rst.nldt_samples_per_second': 81.145, 'train@nld.rst.nldt_steps_per_second': 2.574, 'epoch': 7.0}
{'loss': 2.6752, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.605512857437134, 'eval_accuracy@nld.rst.nldt': 0.32628398791540786, 'eval_f1@nld.rst.nldt': 0.043309728536787134, 'eval_precision@nld.rst.nldt': 0.03839193422705659, 'eval_recall@nld.rst.nldt': 0.06792807300053677, 'eval_loss@nld.rst.nldt': 2.605513095855713, 'eval_runtime': 4.349, 'eval_samples_per_second': 76.11, 'eval_steps_per_second': 2.529, 'epoch': 7.0}
{'train@nld.rst.nldt_loss': 2.616539478302002, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3041044776119403, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03536275795830811, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.026757892254035586, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.056789215686274507, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.616539478302002, 'train@nld.rst.nldt_runtime': 19.8188, 'train@nld.rst.nldt_samples_per_second': 81.135, 'train@nld.rst.nldt_steps_per_second': 2.573, 'epoch': 8.0}
{'loss': 2.6564, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.5881295204162598, 'eval_accuracy@nld.rst.nldt': 0.3323262839879154, 'eval_f1@nld.rst.nldt': 0.04452197769428043, 'eval_precision@nld.rst.nldt': 0.038297184220816835, 'eval_recall@nld.rst.nldt': 0.07269892390665338, 'eval_loss@nld.rst.nldt': 2.588129758834839, 'eval_runtime': 4.3507, 'eval_samples_per_second': 76.08, 'eval_steps_per_second': 2.528, 'epoch': 8.0}
{'train@nld.rst.nldt_loss': 2.6020472049713135, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3072139303482587, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03623378377984688, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.027004905753099395, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05843662464985995, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6020472049713135, 'train@nld.rst.nldt_runtime': 19.7956, 'train@nld.rst.nldt_samples_per_second': 81.23, 'train@nld.rst.nldt_steps_per_second': 2.576, 'epoch': 9.0}
{'loss': 2.635, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.5761046409606934, 'eval_accuracy@nld.rst.nldt': 0.3323262839879154, 'eval_f1@nld.rst.nldt': 0.04442827167464848, 'eval_precision@nld.rst.nldt': 0.037614632416274, 'eval_recall@nld.rst.nldt': 0.07269892390665338, 'eval_loss@nld.rst.nldt': 2.5761044025421143, 'eval_runtime': 4.3308, 'eval_samples_per_second': 76.429, 'eval_steps_per_second': 2.54, 'epoch': 9.0}
{'train@nld.rst.nldt_loss': 2.5918564796447754, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30907960199004975, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03672979982503595, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.027415097329644745, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05906454248366013, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.591856002807617, 'train@nld.rst.nldt_runtime': 19.8103, 'train@nld.rst.nldt_samples_per_second': 81.17, 'train@nld.rst.nldt_steps_per_second': 2.574, 'epoch': 10.0}
{'loss': 2.6266, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.5671842098236084, 'eval_accuracy@nld.rst.nldt': 0.3323262839879154, 'eval_f1@nld.rst.nldt': 0.04442827167464848, 'eval_precision@nld.rst.nldt': 0.037614632416274, 'eval_recall@nld.rst.nldt': 0.07269892390665338, 'eval_loss@nld.rst.nldt': 2.5671842098236084, 'eval_runtime': 4.3315, 'eval_samples_per_second': 76.417, 'eval_steps_per_second': 2.54, 'epoch': 10.0}
{'train@nld.rst.nldt_loss': 2.5852057933807373, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30907960199004975, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.0366921439628483, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.027265354520540755, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.0592390289449113, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.5852057933807373, 'train@nld.rst.nldt_runtime': 19.8189, 'train@nld.rst.nldt_samples_per_second': 81.135, 'train@nld.rst.nldt_steps_per_second': 2.573, 'epoch': 11.0}
{'loss': 2.6106, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.561910390853882, 'eval_accuracy@nld.rst.nldt': 0.33534743202416917, 'eval_f1@nld.rst.nldt': 0.04759667848886345, 'eval_precision@nld.rst.nldt': 0.04950733946842897, 'eval_recall@nld.rst.nldt': 0.07446259233698847, 'eval_loss@nld.rst.nldt': 2.561910390853882, 'eval_runtime': 4.3522, 'eval_samples_per_second': 76.054, 'eval_steps_per_second': 2.527, 'epoch': 11.0}
{'train@nld.rst.nldt_loss': 2.582611322402954, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31032338308457713, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.036880273817935745, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.027321477314548204, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05979341736694678, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.582610845565796, 'train@nld.rst.nldt_runtime': 19.8383, 'train@nld.rst.nldt_samples_per_second': 81.055, 'train@nld.rst.nldt_steps_per_second': 2.571, 'epoch': 12.0}
{'loss': 2.6162, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.560312271118164, 'eval_accuracy@nld.rst.nldt': 0.3323262839879154, 'eval_f1@nld.rst.nldt': 0.04412820045003953, 'eval_precision@nld.rst.nldt': 0.036812338895672224, 'eval_recall@nld.rst.nldt': 0.07269892390665338, 'eval_loss@nld.rst.nldt': 2.5603127479553223, 'eval_runtime': 4.3566, 'eval_samples_per_second': 75.977, 'eval_steps_per_second': 2.525, 'epoch': 12.0}
{'train_runtime': 782.1705, 'train_samples_per_second': 24.67, 'train_steps_per_second': 0.782, 'train_loss': 2.782817616182215, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =      2.636
  train_runtime            = 0:03:38.79
  train_samples_per_second =     24.078
  train_steps_per_second   =      0.768
-------------------------------------------------------------------
Lang1:  tur.pdtb.tdb    Lang2:  nld.rst.nldt
Saving run to:  runs/full_shot/FullShot=v4_tur.pdtb.tdb_nld.rst.nldt_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2451 examples
read 312 examples
read 422 examples
read 1608 examples
read 331 examples
read 326 examples
Total prediction labels:  56
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (tr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (tr): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=56, bias=True)
    )
  )
)
{'train@tur.pdtb.tdb_loss': 3.021345853805542, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01741967515426626, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010891738953044898, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 3.021345615386963, 'train@tur.pdtb.tdb_runtime': 30.048, 'train@tur.pdtb.tdb_samples_per_second': 81.569, 'train@tur.pdtb.tdb_steps_per_second': 2.563, 'epoch': 1.0}
{'loss': 3.524, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.9486403465270996, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.9486403465270996, 'eval_runtime': 4.2754, 'eval_samples_per_second': 72.975, 'eval_steps_per_second': 2.339, 'epoch': 1.0}
{'train@tur.pdtb.tdb_loss': 2.4992685317993164, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01741967515426626, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010891738953044898, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.4992685317993164, 'train@tur.pdtb.tdb_runtime': 31.1414, 'train@tur.pdtb.tdb_samples_per_second': 78.706, 'train@tur.pdtb.tdb_steps_per_second': 2.473, 'epoch': 2.0}
{'loss': 2.701, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.375267267227173, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.3752670288085938, 'eval_runtime': 4.262, 'eval_samples_per_second': 73.205, 'eval_steps_per_second': 2.346, 'epoch': 2.0}
{'train@tur.pdtb.tdb_loss': 2.419874668121338, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01741967515426626, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010891738953044898, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.419874668121338, 'train@tur.pdtb.tdb_runtime': 30.2154, 'train@tur.pdtb.tdb_samples_per_second': 81.118, 'train@tur.pdtb.tdb_steps_per_second': 2.548, 'epoch': 3.0}
{'loss': 2.4752, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.325164794921875, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.325164794921875, 'eval_runtime': 4.3478, 'eval_samples_per_second': 71.761, 'eval_steps_per_second': 2.3, 'epoch': 3.0}
{'train@tur.pdtb.tdb_loss': 2.3560237884521484, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25744594043247654, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.023555612310302617, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.051404468341522194, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.04665163343343286, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.3560237884521484, 'train@tur.pdtb.tdb_runtime': 30.1157, 'train@tur.pdtb.tdb_samples_per_second': 81.386, 'train@tur.pdtb.tdb_steps_per_second': 2.557, 'epoch': 4.0}
{'loss': 2.4093, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.2880895137786865, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.2880890369415283, 'eval_runtime': 4.3397, 'eval_samples_per_second': 71.894, 'eval_steps_per_second': 2.304, 'epoch': 4.0}
{'train@tur.pdtb.tdb_loss': 2.3084888458251953, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.284781721746226, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.0426936101366055, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.0453720687412115, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.061215148458476996, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.3084888458251953, 'train@tur.pdtb.tdb_runtime': 30.1237, 'train@tur.pdtb.tdb_samples_per_second': 81.365, 'train@tur.pdtb.tdb_steps_per_second': 2.556, 'epoch': 5.0}
{'loss': 2.3612, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.255706310272217, 'eval_accuracy@tur.pdtb.tdb': 0.27884615384615385, 'eval_f1@tur.pdtb.tdb': 0.031860042000495775, 'eval_precision@tur.pdtb.tdb': 0.03830027648548865, 'eval_recall@tur.pdtb.tdb': 0.052724266048149816, 'eval_loss@tur.pdtb.tdb': 2.255706548690796, 'eval_runtime': 4.3324, 'eval_samples_per_second': 72.016, 'eval_steps_per_second': 2.308, 'epoch': 5.0}
{'train@tur.pdtb.tdb_loss': 2.262291193008423, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3092615259077927, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.06673704104482316, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.08495749097476298, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.08078032538606719, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.262291193008423, 'train@tur.pdtb.tdb_runtime': 30.1233, 'train@tur.pdtb.tdb_samples_per_second': 81.366, 'train@tur.pdtb.tdb_steps_per_second': 2.556, 'epoch': 6.0}
{'loss': 2.3241, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.2290055751800537, 'eval_accuracy@tur.pdtb.tdb': 0.28846153846153844, 'eval_f1@tur.pdtb.tdb': 0.05777906180448553, 'eval_precision@tur.pdtb.tdb': 0.0681099343460967, 'eval_recall@tur.pdtb.tdb': 0.0714318951384862, 'eval_loss@tur.pdtb.tdb': 2.2290055751800537, 'eval_runtime': 4.305, 'eval_samples_per_second': 72.474, 'eval_steps_per_second': 2.323, 'epoch': 6.0}
{'train@tur.pdtb.tdb_loss': 2.2292978763580322, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.32762137902896776, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.0846679138748771, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.1381454176366027, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.10162623503831435, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.2292978763580322, 'train@tur.pdtb.tdb_runtime': 30.1137, 'train@tur.pdtb.tdb_samples_per_second': 81.391, 'train@tur.pdtb.tdb_steps_per_second': 2.557, 'epoch': 7.0}
{'loss': 2.2835, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.207279682159424, 'eval_accuracy@tur.pdtb.tdb': 0.2948717948717949, 'eval_f1@tur.pdtb.tdb': 0.07388798484567329, 'eval_precision@tur.pdtb.tdb': 0.10800589794332692, 'eval_recall@tur.pdtb.tdb': 0.10043942335255927, 'eval_loss@tur.pdtb.tdb': 2.207279682159424, 'eval_runtime': 4.2857, 'eval_samples_per_second': 72.801, 'eval_steps_per_second': 2.333, 'epoch': 7.0}
{'train@tur.pdtb.tdb_loss': 2.21176815032959, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.33414932680538556, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.08930491821456851, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.1378772538402457, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.10538450038225623, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.21176815032959, 'train@tur.pdtb.tdb_runtime': 30.0887, 'train@tur.pdtb.tdb_samples_per_second': 81.459, 'train@tur.pdtb.tdb_steps_per_second': 2.559, 'epoch': 8.0}
{'loss': 2.2601, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.1927592754364014, 'eval_accuracy@tur.pdtb.tdb': 0.2980769230769231, 'eval_f1@tur.pdtb.tdb': 0.0756394356461261, 'eval_precision@tur.pdtb.tdb': 0.07688046472631452, 'eval_recall@tur.pdtb.tdb': 0.10170204961518553, 'eval_loss@tur.pdtb.tdb': 2.192758798599243, 'eval_runtime': 4.2931, 'eval_samples_per_second': 72.675, 'eval_steps_per_second': 2.329, 'epoch': 8.0}
{'train@tur.pdtb.tdb_loss': 2.1854560375213623, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3378212974296206, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.08884200634709326, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.11346904638004689, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.1109981822340283, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.185455799102783, 'train@tur.pdtb.tdb_runtime': 30.1458, 'train@tur.pdtb.tdb_samples_per_second': 81.305, 'train@tur.pdtb.tdb_steps_per_second': 2.554, 'epoch': 9.0}
{'loss': 2.2384, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.174517869949341, 'eval_accuracy@tur.pdtb.tdb': 0.3076923076923077, 'eval_f1@tur.pdtb.tdb': 0.07897062094690555, 'eval_precision@tur.pdtb.tdb': 0.07915389195232465, 'eval_recall@tur.pdtb.tdb': 0.10578701693544698, 'eval_loss@tur.pdtb.tdb': 2.174517869949341, 'eval_runtime': 4.2999, 'eval_samples_per_second': 72.56, 'eval_steps_per_second': 2.326, 'epoch': 9.0}
{'train@tur.pdtb.tdb_loss': 2.172724485397339, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3398612811097511, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.0911556145822436, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.12124909421868611, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.112406304903966, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.172724485397339, 'train@tur.pdtb.tdb_runtime': 30.1961, 'train@tur.pdtb.tdb_samples_per_second': 81.17, 'train@tur.pdtb.tdb_steps_per_second': 2.55, 'epoch': 10.0}
{'loss': 2.2203, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.1645426750183105, 'eval_accuracy@tur.pdtb.tdb': 0.30448717948717946, 'eval_f1@tur.pdtb.tdb': 0.07656329651656754, 'eval_precision@tur.pdtb.tdb': 0.07486524912995501, 'eval_recall@tur.pdtb.tdb': 0.10516509967639295, 'eval_loss@tur.pdtb.tdb': 2.1645426750183105, 'eval_runtime': 4.2867, 'eval_samples_per_second': 72.783, 'eval_steps_per_second': 2.333, 'epoch': 10.0}
{'train@tur.pdtb.tdb_loss': 2.167711019515991, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.34108527131782945, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09048933132869286, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.11252696151285697, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11233702543793535, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.167710781097412, 'train@tur.pdtb.tdb_runtime': 30.1986, 'train@tur.pdtb.tdb_samples_per_second': 81.163, 'train@tur.pdtb.tdb_steps_per_second': 2.55, 'epoch': 11.0}
{'loss': 2.2169, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.1613590717315674, 'eval_accuracy@tur.pdtb.tdb': 0.3141025641025641, 'eval_f1@tur.pdtb.tdb': 0.08037297690894317, 'eval_precision@tur.pdtb.tdb': 0.080278293135436, 'eval_recall@tur.pdtb.tdb': 0.10823799732760385, 'eval_loss@tur.pdtb.tdb': 2.1613590717315674, 'eval_runtime': 4.2907, 'eval_samples_per_second': 72.715, 'eval_steps_per_second': 2.331, 'epoch': 11.0}
{'train@tur.pdtb.tdb_loss': 2.1642887592315674, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.34353325173398613, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09232333976392844, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.11660906459128005, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11369061163048041, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1642885208129883, 'train@tur.pdtb.tdb_runtime': 30.1401, 'train@tur.pdtb.tdb_samples_per_second': 81.32, 'train@tur.pdtb.tdb_steps_per_second': 2.555, 'epoch': 12.0}
{'loss': 2.2067, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.1586599349975586, 'eval_accuracy@tur.pdtb.tdb': 0.3108974358974359, 'eval_f1@tur.pdtb.tdb': 0.07957491486962266, 'eval_precision@tur.pdtb.tdb': 0.07832896270396271, 'eval_recall@tur.pdtb.tdb': 0.10769035220164547, 'eval_loss@tur.pdtb.tdb': 2.1586594581604004, 'eval_runtime': 4.2986, 'eval_samples_per_second': 72.582, 'eval_steps_per_second': 2.326, 'epoch': 12.0}
{'train_runtime': 1165.042, 'train_samples_per_second': 25.245, 'train_steps_per_second': 0.793, 'train_loss': 2.435072547945625, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.4351
  train_runtime            = 0:19:25.04
  train_samples_per_second =     25.245
  train_steps_per_second   =      0.793
{'train@nld.rst.nldt_loss': 3.3044493198394775, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.263681592039801, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.013041338582677166, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.008245177349097698, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.031176470588235295, 'train@nld.rst.nldt_loss@nld.rst.nldt': 3.3044490814208984, 'train@nld.rst.nldt_runtime': 19.9272, 'train@nld.rst.nldt_samples_per_second': 80.694, 'train@nld.rst.nldt_steps_per_second': 2.559, 'epoch': 1.0}
{'loss': 3.8742, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.2268216609954834, 'eval_accuracy@nld.rst.nldt': 0.27794561933534745, 'eval_f1@nld.rst.nldt': 0.016110673321075214, 'eval_precision@nld.rst.nldt': 0.010294282197605462, 'eval_recall@nld.rst.nldt': 0.037037037037037035, 'eval_loss@nld.rst.nldt': 3.2268216609954834, 'eval_runtime': 4.5277, 'eval_samples_per_second': 73.105, 'eval_steps_per_second': 2.429, 'epoch': 1.0}
{'train@nld.rst.nldt_loss': 2.957223653793335, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26430348258706465, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01306566650270536, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.00825948383084577, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03125, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.957224130630493, 'train@nld.rst.nldt_runtime': 19.913, 'train@nld.rst.nldt_samples_per_second': 80.751, 'train@nld.rst.nldt_steps_per_second': 2.561, 'epoch': 2.0}
{'loss': 3.1267, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.8677754402160645, 'eval_accuracy@nld.rst.nldt': 0.27794561933534745, 'eval_f1@nld.rst.nldt': 0.016110673321075214, 'eval_precision@nld.rst.nldt': 0.010294282197605462, 'eval_recall@nld.rst.nldt': 0.037037037037037035, 'eval_loss@nld.rst.nldt': 2.8677754402160645, 'eval_runtime': 4.4982, 'eval_samples_per_second': 73.585, 'eval_steps_per_second': 2.445, 'epoch': 2.0}
{'train@nld.rst.nldt_loss': 2.8473212718963623, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26430348258706465, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.013078532742491385, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.008269769613947696, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03125, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.847321033477783, 'train@nld.rst.nldt_runtime': 19.8765, 'train@nld.rst.nldt_samples_per_second': 80.9, 'train@nld.rst.nldt_steps_per_second': 2.566, 'epoch': 3.0}
{'loss': 2.9321, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.7748970985412598, 'eval_accuracy@nld.rst.nldt': 0.27794561933534745, 'eval_f1@nld.rst.nldt': 0.016110673321075214, 'eval_precision@nld.rst.nldt': 0.010294282197605462, 'eval_recall@nld.rst.nldt': 0.037037037037037035, 'eval_loss@nld.rst.nldt': 2.7748970985412598, 'eval_runtime': 4.5207, 'eval_samples_per_second': 73.218, 'eval_steps_per_second': 2.433, 'epoch': 3.0}
{'train@nld.rst.nldt_loss': 2.791029691696167, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26492537313432835, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.013589810924369748, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.011436170212765957, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03149801587301587, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.791029930114746, 'train@nld.rst.nldt_runtime': 19.9081, 'train@nld.rst.nldt_samples_per_second': 80.771, 'train@nld.rst.nldt_steps_per_second': 2.562, 'epoch': 4.0}
{'loss': 2.8298, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.7333199977874756, 'eval_accuracy@nld.rst.nldt': 0.2809667673716012, 'eval_f1@nld.rst.nldt': 0.0182064634407973, 'eval_precision@nld.rst.nldt': 0.047362514029180694, 'eval_recall@nld.rst.nldt': 0.03809523809523809, 'eval_loss@nld.rst.nldt': 2.733320474624634, 'eval_runtime': 4.5049, 'eval_samples_per_second': 73.475, 'eval_steps_per_second': 2.442, 'epoch': 4.0}
{'train@nld.rst.nldt_loss': 2.749040365219116, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2835820895522388, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.024850033108244074, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.03338942896485454, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03923027544351074, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.7490408420562744, 'train@nld.rst.nldt_runtime': 19.9557, 'train@nld.rst.nldt_samples_per_second': 80.578, 'train@nld.rst.nldt_steps_per_second': 2.556, 'epoch': 5.0}
{'loss': 2.7948, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.698538064956665, 'eval_accuracy@nld.rst.nldt': 0.29607250755287007, 'eval_f1@nld.rst.nldt': 0.029980507785452096, 'eval_precision@nld.rst.nldt': 0.04408225259960276, 'eval_recall@nld.rst.nldt': 0.04538506760728983, 'eval_loss@nld.rst.nldt': 2.698538064956665, 'eval_runtime': 4.529, 'eval_samples_per_second': 73.085, 'eval_steps_per_second': 2.429, 'epoch': 5.0}
{'train@nld.rst.nldt_loss': 2.712341547012329, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30223880597014924, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03439847015412748, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.03284211248771825, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.0488264472455649, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.7123420238494873, 'train@nld.rst.nldt_runtime': 19.939, 'train@nld.rst.nldt_samples_per_second': 80.646, 'train@nld.rst.nldt_steps_per_second': 2.558, 'epoch': 6.0}
{'loss': 2.7638, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.667386531829834, 'eval_accuracy@nld.rst.nldt': 0.31419939577039274, 'eval_f1@nld.rst.nldt': 0.03941094537870337, 'eval_precision@nld.rst.nldt': 0.041731523933510685, 'eval_recall@nld.rst.nldt': 0.05473251028806585, 'eval_loss@nld.rst.nldt': 2.667386770248413, 'eval_runtime': 4.5188, 'eval_samples_per_second': 73.249, 'eval_steps_per_second': 2.434, 'epoch': 6.0}
{'train@nld.rst.nldt_loss': 2.6838700771331787, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31343283582089554, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03768160074346282, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.031188668583384926, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05532854808590102, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6838700771331787, 'train@nld.rst.nldt_runtime': 19.9724, 'train@nld.rst.nldt_samples_per_second': 80.511, 'train@nld.rst.nldt_steps_per_second': 2.554, 'epoch': 7.0}
{'loss': 2.7197, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.6422271728515625, 'eval_accuracy@nld.rst.nldt': 0.3202416918429003, 'eval_f1@nld.rst.nldt': 0.0417960010617609, 'eval_precision@nld.rst.nldt': 0.04035711794657181, 'eval_recall@nld.rst.nldt': 0.05884773662551441, 'eval_loss@nld.rst.nldt': 2.6422274112701416, 'eval_runtime': 4.5417, 'eval_samples_per_second': 72.881, 'eval_steps_per_second': 2.422, 'epoch': 7.0}
{'train@nld.rst.nldt_loss': 2.6597681045532227, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3271144278606965, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.04137810299889359, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.03168365148519049, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.06340627917833801, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.659768581390381, 'train@nld.rst.nldt_runtime': 19.9721, 'train@nld.rst.nldt_samples_per_second': 80.512, 'train@nld.rst.nldt_steps_per_second': 2.554, 'epoch': 8.0}
{'loss': 2.7076, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.6233327388763428, 'eval_accuracy@nld.rst.nldt': 0.32628398791540786, 'eval_f1@nld.rst.nldt': 0.04265555644865989, 'eval_precision@nld.rst.nldt': 0.037739291248063175, 'eval_recall@nld.rst.nldt': 0.06296296296296296, 'eval_loss@nld.rst.nldt': 2.6233327388763428, 'eval_runtime': 4.5198, 'eval_samples_per_second': 73.234, 'eval_steps_per_second': 2.434, 'epoch': 8.0}
{'train@nld.rst.nldt_loss': 2.645663261413574, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.32338308457711445, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.04049518457050895, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.030609482560173762, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.06290674603174602, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.645663261413574, 'train@nld.rst.nldt_runtime': 19.9321, 'train@nld.rst.nldt_samples_per_second': 80.674, 'train@nld.rst.nldt_steps_per_second': 2.559, 'epoch': 9.0}
{'loss': 2.6862, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.6110610961914062, 'eval_accuracy@nld.rst.nldt': 0.3323262839879154, 'eval_f1@nld.rst.nldt': 0.044721231184629194, 'eval_precision@nld.rst.nldt': 0.03883540671796899, 'eval_recall@nld.rst.nldt': 0.06507936507936508, 'eval_loss@nld.rst.nldt': 2.6110613346099854, 'eval_runtime': 4.504, 'eval_samples_per_second': 73.491, 'eval_steps_per_second': 2.442, 'epoch': 9.0}
{'train@nld.rst.nldt_loss': 2.6359894275665283, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3252487562189055, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.040970042508007495, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.0314097452255347, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.06272000466853409, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6359894275665283, 'train@nld.rst.nldt_runtime': 19.9228, 'train@nld.rst.nldt_samples_per_second': 80.712, 'train@nld.rst.nldt_steps_per_second': 2.56, 'epoch': 10.0}
{'loss': 2.6676, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.6024246215820312, 'eval_accuracy@nld.rst.nldt': 0.3293051359516616, 'eval_f1@nld.rst.nldt': 0.04419068171259872, 'eval_precision@nld.rst.nldt': 0.039312717821489755, 'eval_recall@nld.rst.nldt': 0.0630217519106408, 'eval_loss@nld.rst.nldt': 2.6024246215820312, 'eval_runtime': 4.5129, 'eval_samples_per_second': 73.345, 'eval_steps_per_second': 2.437, 'epoch': 10.0}
{'train@nld.rst.nldt_loss': 2.6288018226623535, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3252487562189055, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.04076016705271154, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.03076251476429054, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.06341853408029878, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6288020610809326, 'train@nld.rst.nldt_runtime': 19.9208, 'train@nld.rst.nldt_samples_per_second': 80.72, 'train@nld.rst.nldt_steps_per_second': 2.56, 'epoch': 11.0}
{'loss': 2.6628, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.596658229827881, 'eval_accuracy@nld.rst.nldt': 0.3323262839879154, 'eval_f1@nld.rst.nldt': 0.04490363453831207, 'eval_precision@nld.rst.nldt': 0.03831717820887496, 'eval_recall@nld.rst.nldt': 0.06573498964803313, 'eval_loss@nld.rst.nldt': 2.5966577529907227, 'eval_runtime': 4.513, 'eval_samples_per_second': 73.344, 'eval_steps_per_second': 2.437, 'epoch': 11.0}
{'train@nld.rst.nldt_loss': 2.6259047985076904, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.32649253731343286, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.041013573041132095, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.030845726202625212, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.06403127917833801, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6259052753448486, 'train@nld.rst.nldt_runtime': 19.9079, 'train@nld.rst.nldt_samples_per_second': 80.772, 'train@nld.rst.nldt_steps_per_second': 2.562, 'epoch': 12.0}
{'loss': 2.6601, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.594512939453125, 'eval_accuracy@nld.rst.nldt': 0.34441087613293053, 'eval_f1@nld.rst.nldt': 0.048983593164433084, 'eval_precision@nld.rst.nldt': 0.040501131241871983, 'eval_recall@nld.rst.nldt': 0.07296603021240701, 'eval_loss@nld.rst.nldt': 2.594512939453125, 'eval_runtime': 4.5176, 'eval_samples_per_second': 73.269, 'eval_steps_per_second': 2.435, 'epoch': 12.0}
{'train_runtime': 786.8932, 'train_samples_per_second': 24.522, 'train_steps_per_second': 0.778, 'train_loss': 2.8687900219088287, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.4351
  train_runtime            = 0:19:25.04
  train_samples_per_second =     25.245
  train_steps_per_second   =      0.793
-------------------------------------------------------------------
Lang1:  zho.rst.sctb    Lang2:  nld.rst.nldt
Saving run to:  runs/full_shot/FullShot=v4_zho.rst.sctb_nld.rst.nldt_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 439 examples
read 94 examples
read 159 examples
read 1608 examples
read 331 examples
read 326 examples
Total prediction labels:  36
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (zh): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=36, bias=True)
    )
  )
)
{'train@zho.rst.sctb_loss': 3.319204807281494, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.319204330444336, 'train@zho.rst.sctb_runtime': 5.4386, 'train@zho.rst.sctb_samples_per_second': 80.719, 'train@zho.rst.sctb_steps_per_second': 2.574, 'epoch': 1.0}
{'loss': 3.4682, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.3265204429626465, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 3.3265202045440674, 'eval_runtime': 1.4499, 'eval_samples_per_second': 64.834, 'eval_steps_per_second': 2.069, 'epoch': 1.0}
{'train@zho.rst.sctb_loss': 3.1038765907287598, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.103877067565918, 'train@zho.rst.sctb_runtime': 5.4942, 'train@zho.rst.sctb_samples_per_second': 79.903, 'train@zho.rst.sctb_steps_per_second': 2.548, 'epoch': 2.0}
{'loss': 3.2285, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.1196022033691406, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 3.1196019649505615, 'eval_runtime': 1.4413, 'eval_samples_per_second': 65.219, 'eval_steps_per_second': 2.081, 'epoch': 2.0}
{'train@zho.rst.sctb_loss': 2.9126956462860107, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.91269588470459, 'train@zho.rst.sctb_runtime': 5.5141, 'train@zho.rst.sctb_samples_per_second': 79.614, 'train@zho.rst.sctb_steps_per_second': 2.539, 'epoch': 3.0}
{'loss': 3.0478, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.936527729034424, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.936528205871582, 'eval_runtime': 1.4647, 'eval_samples_per_second': 64.179, 'eval_steps_per_second': 2.048, 'epoch': 3.0}
{'train@zho.rst.sctb_loss': 2.753777503967285, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.753777265548706, 'train@zho.rst.sctb_runtime': 6.2778, 'train@zho.rst.sctb_samples_per_second': 69.928, 'train@zho.rst.sctb_steps_per_second': 2.23, 'epoch': 4.0}
{'loss': 2.8626, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.787766695022583, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.787766695022583, 'eval_runtime': 1.4345, 'eval_samples_per_second': 65.526, 'eval_steps_per_second': 2.091, 'epoch': 4.0}
{'train@zho.rst.sctb_loss': 2.6285831928253174, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.6285834312438965, 'train@zho.rst.sctb_runtime': 5.5423, 'train@zho.rst.sctb_samples_per_second': 79.209, 'train@zho.rst.sctb_steps_per_second': 2.526, 'epoch': 5.0}
{'loss': 2.7307, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.6737470626831055, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.6737465858459473, 'eval_runtime': 1.4326, 'eval_samples_per_second': 65.617, 'eval_steps_per_second': 2.094, 'epoch': 5.0}
{'train@zho.rst.sctb_loss': 2.531057834625244, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.531057834625244, 'train@zho.rst.sctb_runtime': 5.5223, 'train@zho.rst.sctb_samples_per_second': 79.497, 'train@zho.rst.sctb_steps_per_second': 2.535, 'epoch': 6.0}
{'loss': 2.6197, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.58951735496521, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.5895168781280518, 'eval_runtime': 1.4454, 'eval_samples_per_second': 65.032, 'eval_steps_per_second': 2.075, 'epoch': 6.0}
{'train@zho.rst.sctb_loss': 2.460155963897705, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.460155963897705, 'train@zho.rst.sctb_runtime': 5.5353, 'train@zho.rst.sctb_samples_per_second': 79.309, 'train@zho.rst.sctb_steps_per_second': 2.529, 'epoch': 7.0}
{'loss': 2.5357, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.5307390689849854, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.5307395458221436, 'eval_runtime': 1.456, 'eval_samples_per_second': 64.56, 'eval_steps_per_second': 2.06, 'epoch': 7.0}
{'train@zho.rst.sctb_loss': 2.414207696914673, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3348519362186788, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.02005677652438983, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03208061960922373, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038866396761133605, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.414207696914673, 'train@zho.rst.sctb_runtime': 5.5347, 'train@zho.rst.sctb_samples_per_second': 79.318, 'train@zho.rst.sctb_steps_per_second': 2.53, 'epoch': 8.0}
{'loss': 2.4727, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.494555711746216, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 2.494555950164795, 'eval_runtime': 1.4338, 'eval_samples_per_second': 65.56, 'eval_steps_per_second': 2.092, 'epoch': 8.0}
{'train@zho.rst.sctb_loss': 2.3821072578430176, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.33940774487471526, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.0216710875331565, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03601559730591988, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.03967611336032389, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.3821072578430176, 'train@zho.rst.sctb_runtime': 5.5342, 'train@zho.rst.sctb_samples_per_second': 79.324, 'train@zho.rst.sctb_steps_per_second': 2.53, 'epoch': 9.0}
{'loss': 2.428, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.4700188636779785, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 2.4700186252593994, 'eval_runtime': 1.4594, 'eval_samples_per_second': 64.408, 'eval_steps_per_second': 2.056, 'epoch': 9.0}
{'train@zho.rst.sctb_loss': 2.3612558841705322, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3439635535307517, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.02320113565788306, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.04047110297110297, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04048582995951417, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.361255645751953, 'train@zho.rst.sctb_runtime': 5.56, 'train@zho.rst.sctb_samples_per_second': 78.957, 'train@zho.rst.sctb_steps_per_second': 2.518, 'epoch': 10.0}
{'loss': 2.4082, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.4548327922821045, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 2.4548332691192627, 'eval_runtime': 1.4377, 'eval_samples_per_second': 65.381, 'eval_steps_per_second': 2.087, 'epoch': 10.0}
{'train@zho.rst.sctb_loss': 2.3506529331207275, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.35079726651480636, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.02593586544264237, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.0419068636281751, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.041841827962952695, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.3506531715393066, 'train@zho.rst.sctb_runtime': 5.5455, 'train@zho.rst.sctb_samples_per_second': 79.163, 'train@zho.rst.sctb_steps_per_second': 2.525, 'epoch': 11.0}
{'loss': 2.3841, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.447112798690796, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.03185595567867036, 'eval_precision@zho.rst.sctb': 0.044050343249427915, 'eval_recall@zho.rst.sctb': 0.0540828173374613, 'eval_loss@zho.rst.sctb': 2.4471123218536377, 'eval_runtime': 1.4679, 'eval_samples_per_second': 64.037, 'eval_steps_per_second': 2.044, 'epoch': 11.0}
{'train@zho.rst.sctb_loss': 2.3471107482910156, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.357630979498861, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.02791854988691544, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.04203036915802873, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04305640286173811, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.3471107482910156, 'train@zho.rst.sctb_runtime': 5.5328, 'train@zho.rst.sctb_samples_per_second': 79.345, 'train@zho.rst.sctb_steps_per_second': 2.53, 'epoch': 12.0}
{'loss': 2.3718, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.444671630859375, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.035909445745511324, 'eval_precision@zho.rst.sctb': 0.043859649122807015, 'eval_recall@zho.rst.sctb': 0.05553405572755418, 'eval_loss@zho.rst.sctb': 2.444671869277954, 'eval_runtime': 1.4468, 'eval_samples_per_second': 64.97, 'eval_steps_per_second': 2.073, 'epoch': 12.0}
{'train_runtime': 215.9651, 'train_samples_per_second': 24.393, 'train_steps_per_second': 0.778, 'train_loss': 2.7131614230927967, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.7132
  train_runtime            = 0:03:35.96
  train_samples_per_second =     24.393
  train_steps_per_second   =      0.778
{'train@nld.rst.nldt_loss': 3.0785293579101562, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26430348258706465, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01306566650270536, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.00825948383084577, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03125, 'train@nld.rst.nldt_loss@nld.rst.nldt': 3.0785293579101562, 'train@nld.rst.nldt_runtime': 19.7312, 'train@nld.rst.nldt_samples_per_second': 81.495, 'train@nld.rst.nldt_steps_per_second': 2.585, 'epoch': 1.0}
{'loss': 3.3881, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.0412797927856445, 'eval_accuracy@nld.rst.nldt': 0.27794561933534745, 'eval_f1@nld.rst.nldt': 0.016110673321075214, 'eval_precision@nld.rst.nldt': 0.010294282197605462, 'eval_recall@nld.rst.nldt': 0.037037037037037035, 'eval_loss@nld.rst.nldt': 3.0412795543670654, 'eval_runtime': 4.3465, 'eval_samples_per_second': 76.153, 'eval_steps_per_second': 2.531, 'epoch': 1.0}
{'train@nld.rst.nldt_loss': 2.860896587371826, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26430348258706465, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01306566650270536, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.00825948383084577, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03125, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.860896348953247, 'train@nld.rst.nldt_runtime': 19.7419, 'train@nld.rst.nldt_samples_per_second': 81.451, 'train@nld.rst.nldt_steps_per_second': 2.583, 'epoch': 2.0}
{'loss': 2.9746, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.806896924972534, 'eval_accuracy@nld.rst.nldt': 0.27794561933534745, 'eval_f1@nld.rst.nldt': 0.016110673321075214, 'eval_precision@nld.rst.nldt': 0.010294282197605462, 'eval_recall@nld.rst.nldt': 0.037037037037037035, 'eval_loss@nld.rst.nldt': 2.8068971633911133, 'eval_runtime': 4.3466, 'eval_samples_per_second': 76.152, 'eval_steps_per_second': 2.531, 'epoch': 2.0}
{'train@nld.rst.nldt_loss': 2.7755978107452393, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26741293532338306, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01635806817987737, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.021705278723785623, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03301470588235294, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.7755978107452393, 'train@nld.rst.nldt_runtime': 19.7662, 'train@nld.rst.nldt_samples_per_second': 81.351, 'train@nld.rst.nldt_steps_per_second': 2.58, 'epoch': 3.0}
{'loss': 2.8476, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.7249393463134766, 'eval_accuracy@nld.rst.nldt': 0.28700906344410876, 'eval_f1@nld.rst.nldt': 0.025965220473229626, 'eval_precision@nld.rst.nldt': 0.032674392183594636, 'eval_recall@nld.rst.nldt': 0.04320987654320988, 'eval_loss@nld.rst.nldt': 2.7249393463134766, 'eval_runtime': 4.3424, 'eval_samples_per_second': 76.224, 'eval_steps_per_second': 2.533, 'epoch': 3.0}
{'train@nld.rst.nldt_loss': 2.728771924972534, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.27549751243781095, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.02267489675585704, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.028519632722113074, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03821953781512605, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.728771924972534, 'train@nld.rst.nldt_runtime': 19.7431, 'train@nld.rst.nldt_samples_per_second': 81.446, 'train@nld.rst.nldt_steps_per_second': 2.583, 'epoch': 4.0}
{'loss': 2.7647, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.6830596923828125, 'eval_accuracy@nld.rst.nldt': 0.30513595166163143, 'eval_f1@nld.rst.nldt': 0.036246865140590424, 'eval_precision@nld.rst.nldt': 0.04178537511870845, 'eval_recall@nld.rst.nldt': 0.05455614344503234, 'eval_loss@nld.rst.nldt': 2.6830594539642334, 'eval_runtime': 4.344, 'eval_samples_per_second': 76.197, 'eval_steps_per_second': 2.532, 'epoch': 4.0}
{'train@nld.rst.nldt_loss': 2.6938695907592773, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2866915422885572, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.02967937447625226, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.02955365380119176, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.044430438842203546, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6938695907592773, 'train@nld.rst.nldt_runtime': 19.7957, 'train@nld.rst.nldt_samples_per_second': 81.23, 'train@nld.rst.nldt_steps_per_second': 2.576, 'epoch': 5.0}
{'loss': 2.7344, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.648358106613159, 'eval_accuracy@nld.rst.nldt': 0.31722054380664655, 'eval_f1@nld.rst.nldt': 0.04106311090438074, 'eval_precision@nld.rst.nldt': 0.04204585537918871, 'eval_recall@nld.rst.nldt': 0.059788359788359786, 'eval_loss@nld.rst.nldt': 2.648358106613159, 'eval_runtime': 4.3323, 'eval_samples_per_second': 76.403, 'eval_steps_per_second': 2.539, 'epoch': 5.0}
{'train@nld.rst.nldt_loss': 2.6583826541900635, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3009950248756219, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03498274285136293, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.060142891411470015, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05209037419315552, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6583821773529053, 'train@nld.rst.nldt_runtime': 19.7749, 'train@nld.rst.nldt_samples_per_second': 81.315, 'train@nld.rst.nldt_steps_per_second': 2.579, 'epoch': 6.0}
{'loss': 2.6985, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.6195733547210693, 'eval_accuracy@nld.rst.nldt': 0.3202416918429003, 'eval_f1@nld.rst.nldt': 0.04267873422024445, 'eval_precision@nld.rst.nldt': 0.039319704812662566, 'eval_recall@nld.rst.nldt': 0.06581167088413464, 'eval_loss@nld.rst.nldt': 2.6195733547210693, 'eval_runtime': 4.3376, 'eval_samples_per_second': 76.309, 'eval_steps_per_second': 2.536, 'epoch': 6.0}
{'train@nld.rst.nldt_loss': 2.6310336589813232, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30659203980099503, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03563951719464597, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.027889964396973742, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05545343137254902, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6310336589813232, 'train@nld.rst.nldt_runtime': 19.7614, 'train@nld.rst.nldt_samples_per_second': 81.371, 'train@nld.rst.nldt_steps_per_second': 2.581, 'epoch': 7.0}
{'loss': 2.6719, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.596604108810425, 'eval_accuracy@nld.rst.nldt': 0.32326283987915405, 'eval_f1@nld.rst.nldt': 0.042187853298964406, 'eval_precision@nld.rst.nldt': 0.03728850301867569, 'eval_recall@nld.rst.nldt': 0.06752549651100376, 'eval_loss@nld.rst.nldt': 2.596604347229004, 'eval_runtime': 4.3379, 'eval_samples_per_second': 76.305, 'eval_steps_per_second': 2.536, 'epoch': 7.0}
{'train@nld.rst.nldt_loss': 2.6076724529266357, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30845771144278605, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.036472525701249106, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.027484013247771066, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05852532679738562, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6076724529266357, 'train@nld.rst.nldt_runtime': 19.7684, 'train@nld.rst.nldt_samples_per_second': 81.342, 'train@nld.rst.nldt_steps_per_second': 2.58, 'epoch': 8.0}
{'loss': 2.6495, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.5794520378112793, 'eval_accuracy@nld.rst.nldt': 0.3323262839879154, 'eval_f1@nld.rst.nldt': 0.04390907800869487, 'eval_precision@nld.rst.nldt': 0.037148491083676266, 'eval_recall@nld.rst.nldt': 0.07269892390665338, 'eval_loss@nld.rst.nldt': 2.5794520378112793, 'eval_runtime': 4.3218, 'eval_samples_per_second': 76.588, 'eval_steps_per_second': 2.545, 'epoch': 8.0}
{'train@nld.rst.nldt_loss': 2.5929558277130127, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30783582089552236, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.036500210291418075, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.027134753092429808, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05903361344537815, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.5929555892944336, 'train@nld.rst.nldt_runtime': 19.7757, 'train@nld.rst.nldt_samples_per_second': 81.312, 'train@nld.rst.nldt_steps_per_second': 2.579, 'epoch': 9.0}
{'loss': 2.6297, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.5673043727874756, 'eval_accuracy@nld.rst.nldt': 0.3323262839879154, 'eval_f1@nld.rst.nldt': 0.04390907800869487, 'eval_precision@nld.rst.nldt': 0.037148491083676266, 'eval_recall@nld.rst.nldt': 0.07269892390665338, 'eval_loss@nld.rst.nldt': 2.5673043727874756, 'eval_runtime': 4.3326, 'eval_samples_per_second': 76.398, 'eval_steps_per_second': 2.539, 'epoch': 9.0}
{'train@nld.rst.nldt_loss': 2.5824315547943115, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30845771144278605, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.037334524894946694, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.05859191566509594, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05937328735842163, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.5824315547943115, 'train@nld.rst.nldt_runtime': 19.7528, 'train@nld.rst.nldt_samples_per_second': 81.406, 'train@nld.rst.nldt_steps_per_second': 2.582, 'epoch': 10.0}
{'loss': 2.6212, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.558490753173828, 'eval_accuracy@nld.rst.nldt': 0.338368580060423, 'eval_f1@nld.rst.nldt': 0.050802840056571395, 'eval_precision@nld.rst.nldt': 0.05654813174193019, 'eval_recall@nld.rst.nldt': 0.07622626076732357, 'eval_loss@nld.rst.nldt': 2.558490753173828, 'eval_runtime': 4.3147, 'eval_samples_per_second': 76.714, 'eval_steps_per_second': 2.549, 'epoch': 10.0}
{'train@nld.rst.nldt_loss': 2.575901508331299, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31032338308457713, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.037706663464232064, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.037906458962566864, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.06017569165347299, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.575901508331299, 'train@nld.rst.nldt_runtime': 19.772, 'train@nld.rst.nldt_samples_per_second': 81.327, 'train@nld.rst.nldt_steps_per_second': 2.579, 'epoch': 11.0}
{'loss': 2.6008, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.5534729957580566, 'eval_accuracy@nld.rst.nldt': 0.338368580060423, 'eval_f1@nld.rst.nldt': 0.05292282760436291, 'eval_precision@nld.rst.nldt': 0.059840232389252, 'eval_recall@nld.rst.nldt': 0.07758735270812565, 'eval_loss@nld.rst.nldt': 2.5534729957580566, 'eval_runtime': 4.3457, 'eval_samples_per_second': 76.167, 'eval_steps_per_second': 2.531, 'epoch': 11.0}
{'train@nld.rst.nldt_loss': 2.5732421875, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31156716417910446, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03798228299453348, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.03805406386749236, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.0607884367515122, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.5732421875, 'train@nld.rst.nldt_runtime': 19.7556, 'train@nld.rst.nldt_samples_per_second': 81.395, 'train@nld.rst.nldt_steps_per_second': 2.582, 'epoch': 12.0}
{'loss': 2.6115, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.551983118057251, 'eval_accuracy@nld.rst.nldt': 0.338368580060423, 'eval_f1@nld.rst.nldt': 0.05292282760436291, 'eval_precision@nld.rst.nldt': 0.059840232389252, 'eval_recall@nld.rst.nldt': 0.07758735270812565, 'eval_loss@nld.rst.nldt': 2.551983118057251, 'eval_runtime': 4.3417, 'eval_samples_per_second': 76.237, 'eval_steps_per_second': 2.534, 'epoch': 12.0}
{'train_runtime': 781.124, 'train_samples_per_second': 24.703, 'train_steps_per_second': 0.783, 'train_loss': 2.766037436092601, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.7132
  train_runtime            = 0:03:35.96
  train_samples_per_second =     24.393
  train_steps_per_second   =      0.778

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, models\n",
    "\n",
    "## Step 1: use an existing language model\n",
    "word_embedding_model = models.Transformer('distilroberta-base')\n",
    "\n",
    "## Step 2: use a pool function over the token embeddings\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "\n",
    "## Join steps 1 and 2 using the modules argument\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model = SentenceTransformer(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration embedding-data--QQP_triplets-1f161ec5c28ee86f\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/embedding-data___json/embedding-data--QQP_triplets-1f161ec5c28ee86f/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f037d12b54b74f9990439311024ebf09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_id = \"embedding-data/QQP_triplets\"\n",
    "dataset = load_dataset(dataset_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- The embedding-data/QQP_triplets dataset has 101762 examples.\n",
      "- Each example is a <class 'dict'> with a <class 'dict'> as value.\n",
      "- Examples look like this: {'set': {'query': 'Why in India do we not have one on one political debate as in USA?', 'pos': ['Why cant we have a public debate between politicians in India like the one in US?'], 'neg': ['Can people on Quora stop India Pakistan debate? We are sick and tired seeing this everyday in bulk?', 'Why do politicians, instead of having a decent debate on issues going in and around the world, end up fighting always?', 'Can educated politicians make a difference in India?', 'What are some unusual aspects about politics and government in India?', 'What is debate?', 'Why does civic public communication and discourse seem so hollow in modern India?', 'What is a Parliamentary debate?', \"Why do we always have two candidates at the U.S. presidential debate. yet the ballot has about 7 candidates? Isn't that a misrepresentation of democracy?\", 'Why is civic public communication and discourse so hollow in modern India?', \"Aren't the Presidential debates teaching our whole country terrible communication skills and why is deliberate misrepresentation even allowed?\", 'Why are most Indian politicians uneducated?', 'Does Indian political leaders capable of doing face to face debates while running for office?', 'What is wrong with the Indian political system and the environment it has built in connection with the people of India? Have parties divided people more?', 'What is a debate?', 'Why do we have legislative council in india?', 'Why does the office of president of India, being politically neutral, not ask for Population control in India?', \"Why don't we discuss tax and foreign policies more in Indian elections but are instead obsessed with socialist schemes?\", 'Why do Indian politicians lack nationalist thinking?', 'Do you hate indian politicians?', 'Is India facing more stessful times and politically charged atmosphere when compared to Congress regime?', 'Who is the best politician in India? Why?', \"We all know about the present condition of Indian politicians; they are all just using us to run their train, but still, they win elections and rule over us. Why aren't people giving their vote to NOTA?\", 'Who are clean politicians in India?', 'Why are you not believing in Democracy of India?', 'What does politics in India mean? What are they actually doing?', 'What are the strongest arguments for a debate in favour of brain drain in India and what sources must be used for making a good short speech?', 'Do we really need an election commission in India?', 'Why is there no concept of political correctness in India? Is it a good thing or a bad thing?', 'Why is population control not on agenda of any political party in India?', 'Who are some of the most dangerous or worst politicians in India?']}}\n"
     ]
    }
   ],
   "source": [
    "print(f\"- The {dataset_id} dataset has {dataset['train'].num_rows} examples.\")\n",
    "print(f\"- Each example is a {type(dataset['train'][0])} with a {type(dataset['train'][0]['set'])} as value.\")\n",
    "print(f\"- Examples look like this: {dataset['train'][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import InputExample\n",
    "\n",
    "train_examples = []\n",
    "train_data = dataset['train']['set']\n",
    "# For agility we only 1/2 of our available data\n",
    "n_examples = dataset['train'].num_rows // 2\n",
    "\n",
    "for i in range(n_examples):\n",
    "  example = train_data[i]\n",
    "  train_examples.append(InputExample(texts=[example['query'], example['pos'][0], example['neg'][0]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import losses\n",
    "\n",
    "train_loss = losses.TripletLoss(model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fddfba3ec46414fb76c28edecec130a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cc611d21f944526a50c6cd84a9fca88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/3181 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55516d3f4a814a00a2a85d21c4516da2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/3181 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "487c27cda8ee4dda9bf05229251e47d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/3181 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1908e3c3636c4a00b737f3ab2046db5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/3181 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e441cad3eb6643479e2f4729e42b6d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/3181 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c27e2bc5d9884936b6df986f0c2ac06f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/3181 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a1a477c5f14fdebd9da6d3b6b54555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/3181 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a5e13c50ff42e3af9a75a2fb0a2597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/3181 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b72f47424264674ad3e7fbb0212abd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/3181 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aeaeaf0ecce4f049becbabb96f7495c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/3181 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'texts'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/VD/kaveri/bert_categorical_tutorial/allennlp_repro/disrpt_alln/3_from_scratch/2_sentence_transformer.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/allennlp_repro/disrpt_alln/3_from_scratch/2_sentence_transformer.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msentence_transformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mevaluation\u001b[39;00m \u001b[39mimport\u001b[39;00m EmbeddingSimilarityEvaluator\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/allennlp_repro/disrpt_alln/3_from_scratch/2_sentence_transformer.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m evaluator \u001b[39m=\u001b[39m EmbeddingSimilarityEvaluator\u001b[39m.\u001b[39;49mfrom_input_examples(\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/allennlp_repro/disrpt_alln/3_from_scratch/2_sentence_transformer.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     [\u001b[39m'\u001b[39;49m\u001b[39masdsadas\u001b[39;49m\u001b[39m'\u001b[39;49m], write_csv\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/allennlp_repro/disrpt_alln/3_from_scratch/2_sentence_transformer.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m )\n",
      "File \u001b[0;32m/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sentence_transformers/evaluation/EmbeddingSimilarityEvaluator.py:60\u001b[0m, in \u001b[0;36mEmbeddingSimilarityEvaluator.from_input_examples\u001b[0;34m(cls, examples, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m scores \u001b[39m=\u001b[39m []\n\u001b[1;32m     59\u001b[0m \u001b[39mfor\u001b[39;00m example \u001b[39min\u001b[39;00m examples:\n\u001b[0;32m---> 60\u001b[0m     sentences1\u001b[39m.\u001b[39mappend(example\u001b[39m.\u001b[39;49mtexts[\u001b[39m0\u001b[39m])\n\u001b[1;32m     61\u001b[0m     sentences2\u001b[39m.\u001b[39mappend(example\u001b[39m.\u001b[39mtexts[\u001b[39m1\u001b[39m])\n\u001b[1;32m     62\u001b[0m     scores\u001b[39m.\u001b[39mappend(example\u001b[39m.\u001b[39mlabel)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'texts'"
     ]
    }
   ],
   "source": [
    "# evaluator is given in teh training steps: https://www.kaggle.com/code/ncthuan/sbert-multilingualize"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3409ea685db85227fbd9509d1b1ace14d085473eb2d57f3ba9dd0302d25f838"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
